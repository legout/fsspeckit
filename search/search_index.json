{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"fsspeckit","text":"<p><code>fsspeckit</code> enhances <code>fsspec</code> with advanced utilities for multi-format I/O, cloud storage configuration, and high-performance data processing.</p>"},{"location":"#quick-start","title":"Quick Start","text":"<p>New to fsspeckit? Start with our Getting Started tutorial for a complete walkthrough.</p> <p>Looking for specific tasks? Browse our How-to Guides for practical recipes:</p> <ul> <li>Configure Cloud Storage - AWS, GCP, Azure setup</li> <li>Work with Filesystems - Local and remote operations  </li> <li>Read and Write Datasets - JSON, CSV, Parquet operations</li> <li>Use SQL Filters - Cross-framework filtering</li> <li>Sync and Manage Files - File synchronization</li> <li>Optimize Performance - Caching and parallel processing</li> </ul>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Multi-Cloud Support: Unified interface for AWS S3, Azure Blob Storage, Google Cloud Storage</li> <li>Advanced Dataset Operations: High-performance Parquet processing with DuckDB integration</li> <li>SQL Filter Translation: Write filters once, use across PyArrow and Polars</li> <li>Enhanced Filesystem API: Extended I/O methods with automatic batching and threading</li> <li>Path Safety by Default: Built-in protection against directory traversal attacks</li> <li>Domain Package Architecture: Organized APIs for better discoverability and type safety</li> </ul>"},{"location":"#documentation","title":"Documentation","text":""},{"location":"#learning-paths","title":"Learning Paths","text":"<p>Beginners: Start with Getting Started for hands-on learning</p> <p>Practical Users: Jump to How-to Guides for specific task solutions</p> <p>Developers: Reference API Guide for capability overview</p> <p>Architects: Understand design decisions in Architecture &amp; Concepts</p>"},{"location":"#reference-materials","title":"Reference Materials","text":"<ul> <li>Installation - Setup and dependency management</li> <li>API Reference - Complete API documentation</li> <li>Utils Reference - Backwards compatibility guide</li> </ul>"},{"location":"#architecture-overview","title":"Architecture Overview","text":"<p>fsspeckit is organized into domain-specific packages:</p> <ul> <li><code>fsspeckit.core</code> - Filesystem creation and extended I/O</li> <li><code>fsspeckit.storage_options</code> - Cloud provider configuration</li> <li><code>fsspeckit.datasets</code> - Large-scale dataset operations</li> <li><code>fsspeckit.sql</code> - Cross-framework SQL translation</li> <li><code>fsspeckit.common</code> - Shared utilities and helpers</li> <li><code>fsspeckit.utils</code> - Backwards compatibility fa\u00e7ade</li> </ul>"},{"location":"#getting-help","title":"Getting Help","text":"<p>Contributing: See our Contributing Guide to help improve fsspeckit</p> <p>Issues: Report bugs and request features on GitHub</p> <p>Community: Join discussions and connect with other users</p>"},{"location":"#badges","title":"Badges","text":""},{"location":"contributing/","title":"Contributing to fsspeckit","text":"<p>We welcome contributions to <code>fsspeckit</code>! Your help makes this project better. This guide outlines how you can contribute, from reporting issues to submitting pull requests.</p>"},{"location":"contributing/#how-to-contribute","title":"How to Contribute","text":""},{"location":"contributing/#reporting-issues","title":"Reporting Issues","text":"<p>If you encounter any bugs, unexpected behavior, or have suggestions for new features, please open an issue on our GitHub Issues page.</p> <p>When reporting an issue, please include: - A clear and concise description of the problem. - Steps to reproduce the behavior. - Expected behavior. - Screenshots or error messages if applicable. - Your <code>fsspeckit</code> version and Python environment details.</p>"},{"location":"contributing/#submitting-pull-requests","title":"Submitting Pull Requests","text":"<p>We gladly accept pull requests for bug fixes, new features, and improvements. To submit a pull request:</p> <ol> <li>Fork the Repository: Start by forking the <code>fsspeckit</code> repository on GitHub.</li> <li>Clone Your Fork: Clone your forked repository to your local machine.     <pre><code>git clone https://github.com/your-username/fsspeckit.git\ncd fsspeckit\n</code></pre></li> <li>Create a New Branch: Create a new branch for your changes.     <pre><code>git checkout -b feature/your-feature-name\n# or\ngit checkout -b bugfix/issue-description\n</code></pre></li> <li>Make Your Changes: Implement your bug fix or feature.</li> <li>Write Tests: Ensure your changes are covered by appropriate unit tests.</li> <li>Run Tests: Verify all tests pass before submitting.     <pre><code>uv run pytest\n</code></pre></li> <li>Format Code: Ensure your code adheres to the project's style guidelines. The project uses <code>ruff</code> for linting and formatting.     <pre><code>uv run ruff check . --fix\nuv run ruff format .\n</code></pre></li> <li>Commit Your Changes: Write clear and concise commit messages.     <pre><code>git commit -m \"feat: Add new awesome feature\"\n</code></pre></li> <li>Push to Your Fork: Push your branch to your forked repository.     <pre><code>git push origin feature/your-feature-name\n</code></pre></li> <li>Open a Pull Request: Go to the original <code>fsspeckit</code> repository on GitHub and open a pull request from your new branch. Provide a detailed description of your changes.</li> </ol>"},{"location":"contributing/#development-setup","title":"Development Setup","text":"<p>To set up your development environment, follow these steps:</p> <ol> <li>Clone the repository:     <pre><code>git clone https://github.com/legout/fsspeckit.git\ncd fsspeckit\n</code></pre></li> <li>Install <code>uv</code>:     <code>fsspeckit</code> uses <code>uv</code> for dependency management and running commands. If you don't have <code>uv</code> installed, you can install it via <code>pip</code>:     <pre><code>pip install uv\n</code></pre></li> <li>Install Development Dependencies:     The project uses <code>uv</code> to manage dependencies. Install the <code>dev</code> dependency group which includes tools for testing, linting, and documentation generation.     <pre><code>uv pip install -e \".[dev]\"\n</code></pre>     This command installs the project in editable mode (<code>-e</code>) and includes all development-related dependencies specified in <code>pyproject.toml</code> under the <code>[project.optional-dependencies] dev</code> section.</li> </ol>"},{"location":"contributing/#best-practices-for-contributions","title":"Best Practices for Contributions","text":"<ul> <li>Code Style: Adhere to the existing code style. We use <code>ruff</code> for linting and formatting.</li> <li>Testing: All new features and bug fixes should be accompanied by relevant unit tests.</li> <li>Documentation: If your changes introduce new features or modify existing behavior, please update the documentation accordingly.</li> <li>Commit Messages: Write descriptive commit messages that explain the purpose of your changes.</li> <li>Atomic Commits: Try to keep your commits focused on a single logical change.</li> <li>Branch Naming: Use clear and concise branch names (e.g., <code>feature/new-feature</code>, <code>bugfix/fix-issue-123</code>).</li> </ul>"},{"location":"contributing/#coding-guidelines","title":"Coding Guidelines","text":""},{"location":"contributing/#avoid-mutable-default-arguments","title":"Avoid Mutable Default Arguments","text":"<p>Core helper functions SHALL avoid mutable default arguments (e.g., <code>def func(param=[]):</code> or <code>def func(param={}):</code>). Instead use <code>None</code> and initialize inside the function:</p> <pre><code># Bad\ndef process_items(items=[]):\n    items.append(\"processed\")\n    return items\n\n# Good  \ndef process_items(items=None):\n    if items is None:\n        items = []\n    items.append(\"processed\")\n    return items\n</code></pre>"},{"location":"contributing/#avoid-unreachable-code","title":"Avoid Unreachable Code","text":"<p>Ensure all code branches can be exercised. Avoid patterns like:</p> <pre><code># Bad - unreachable code after return\ndef some_function():\n    if condition:\n        return result\n    else:\n        return other_result\n    unreachable_code()  # This will never execute\n\n# Good - all paths reachable\ndef some_function():\n    if condition:\n        return result\n    return other_result\n</code></pre>"},{"location":"contributing/#type-annotations","title":"Type Annotations","text":"<p>The project uses <code>mypy</code> for static type checking. Type annotations help catch bugs early and improve code documentation.</p> <p>Requirements: - All new public functions and methods SHOULD have type hints for parameters and return values - Type hints are REQUIRED for changes to core modules (<code>core.*</code>, <code>datasets.*</code>) - Use precise types instead of overly broad ones (e.g., prefer <code>list[str]</code> over <code>list[Any]</code>)</p> <p>Running Type Checks: <pre><code># Check types\nuv run mypy src/fsspeckit\n\n# Check specific modules\nuv run mypy src/fsspeckit/datasets/pyarrow_dataset.py\n</code></pre></p> <p>Common Type Patterns: <pre><code># Good - precise types\ndef process_dataset(path: str, filesystem: AbstractFileSystem | None = None) -&gt; dict[str, Any]:\n    ...\n\n# Good - using Literal for specific string values\nfrom typing import Literal\n\ndef merge_strategy(strategy: Literal[\"upsert\", \"insert\", \"update\"]) -&gt; None:\n    ...\n\n# Good - proper optional handling\ndef get_config(key: str, default: str | None = None) -&gt; str | None:\n    ...\n</code></pre></p>"},{"location":"contributing/#testing-expectations","title":"Testing Expectations","text":"<p>All contributions MUST include appropriate tests. The project maintains a minimum of 80% code coverage.</p> <p>Testing Requirements: 1. New Features: Add unit tests for all public APIs 2. Bug Fixes: Add regression tests to ensure the bug doesn't reoccur 3. Refactors: Ensure existing tests continue to pass; add new tests for new behavior</p> <p>Running Tests: <pre><code># Run all tests\nuv run pytest\n\n# Run tests with coverage report\nuv run pytest --cov=fsspeckit --cov-report=term-missing\n\n# Run specific test file\nuv run pytest tests/test_utils/test_pyarrow_dataset_merge.py\n\n# Run tests matching pattern\nuv run pytest -k \"test_merge\"\n</code></pre></p> <p>Testing for Refactors:</p> <p>When refactoring code (especially large module decomposition): - Preserve Behavior: Ensure all existing functionality is maintained - Add Unit Tests: For new submodules, add focused unit tests - Keep Integration Tests: Maintain existing integration tests to verify end-to-end behavior - Coverage: Refactors should not decrease test coverage below 80%</p> <p>Example: If splitting <code>large_module.py</code> into <code>submodule_a.py</code> and <code>submodule_b.py</code>: <pre><code># Before refactor: tests/test_large_module.py\n# After refactor:\n# - tests/test_submodule_a.py (unit tests for submodule_a)\n# - tests/test_submodule_b.py (unit tests for submodule_b)\n# - tests/test_integration.py (integration tests verifying end-to-end behavior)\n</code></pre></p> <p>High-Risk Changes:</p> <p>Changes to the following modules REQUIRE both type checking AND comprehensive tests: - <code>core/filesystem.py</code> and core submodules - <code>core/ext*.py</code> and core submodules - <code>datasets/pyarrow*.py</code> and dataset submodules - <code>datasets/duckdb*.py</code> and dataset submodules</p> <p>For these changes: <pre><code># Run all checks\nuv run pytest\nuv run mypy src/fsspeckit/core/ src/fsspeckit/datasets/\nuv run ruff check . --fix\n</code></pre></p>"},{"location":"installation/","title":"Installation","text":"<p><code>fsspeckit</code> can be installed using <code>pip</code>, the Python package installer.</p>"},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.11 or higher is required.</li> </ul>"},{"location":"installation/#install-with-pip","title":"Install with pip","text":"<p>To install <code>fsspeckit</code> using <code>pip</code>, run the following command:</p> <pre><code>pip install fsspeckit\n</code></pre>"},{"location":"installation/#optional-dependencies","title":"Optional Dependencies","text":"<p><code>fsspeckit</code> uses lazy imports for optional dependencies, allowing you to install only what you need:</p>"},{"location":"installation/#cloud-provider-support","title":"Cloud Provider Support","text":"<pre><code># AWS S3 support\npip install \"fsspeckit[aws]\"\n\n# Google Cloud Storage support\npip install \"fsspeckit[gcp]\"\n\n# Azure Storage support\npip install \"fsspeckit[azure]\"\n\n# All cloud providers\npip install \"fsspeckit[aws,gcp,azure]\"\n</code></pre>"},{"location":"installation/#data-processing-libraries","title":"Data Processing Libraries","text":"<p>The following libraries are optional and loaded only when needed:</p> <pre><code># PyArrow for dataset operations\npip install pyarrow\n\n# Polars for high-performance DataFrames\npip install polars\n\n# DuckDB for SQL analytics\npip install duckdb\n\n# SQLglot for SQL parsing\npip install sqlglot\n\n# Install all data processing libraries\npip install pyarrow polars duckdb sqlglot\n</code></pre> <p>Note: You can use <code>fsspeckit</code> core functionality without any of these dependencies. They are only required when you use specific features:</p> <ul> <li>PyArrow: Required for <code>fsspeckit.datasets.pyarrow</code> operations</li> <li>Polars: Required for Polars-specific utilities in <code>fsspeckit.common.polars</code></li> <li>DuckDB: Required for <code>fsspeckit.datasets.DuckDBParquetHandler</code></li> <li>SQLglot: Required for SQL filter translation in <code>fsspeckit.sql.filters</code></li> </ul> <p>If you attempt to use a feature requiring an optional dependency that isn't installed, you'll receive a clear error message indicating which package to install.</p>"},{"location":"installation/#upgrading","title":"Upgrading","text":"<p>To upgrade <code>fsspeckit</code> to the latest version, use:</p> <pre><code>pip install --upgrade fsspeckit\n</code></pre>"},{"location":"installation/#environment-management-with-uv-and-pixi","title":"Environment Management with <code>uv</code> and <code>pixi</code>","text":"<p>For robust dependency management and faster installations, we recommend using <code>uv</code> or <code>pixi</code>.</p>"},{"location":"installation/#using-uv","title":"Using <code>uv</code>","text":"<p><code>uv</code> is a fast Python package installer and resolver. To install <code>fsspeckit</code> with <code>uv</code>:</p> <pre><code>uv pip install fsspeckit\n</code></pre>"},{"location":"installation/#using-pixi","title":"Using <code>pixi</code>","text":"<p><code>pixi</code> is a modern package manager for Python and other languages. To add <code>fsspeckit</code> to your <code>pixi</code> project:</p> <pre><code>pixi add fsspeckit\n</code></pre>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter any issues during installation, consider the following:</p> <ul> <li>Python Version: Ensure you are using Python 3.11 or higher. You can check your Python version with <code>python --version</code>.</li> <li>Virtual Environments: It is highly recommended to use a virtual environment (e.g., <code>venv</code>, <code>conda</code>, <code>uv</code>, <code>pixi</code>) to avoid conflicts with system-wide packages.</li> <li>Permissions: If you encounter permission errors, you might need to run the installation command with <code>sudo</code> (e.g., <code>sudo pip install fsspeckit</code>), but this is generally not recommended in a virtual environment.</li> <li>Network Issues: Check your internet connection if the installation fails to download packages.</li> </ul> <p>For further assistance, please refer to the official fsspeckit GitHub repository or open an issue.</p>"},{"location":"api/","title":"<code>fsspeckit</code> API Reference","text":"<p>Welcome to the <code>fsspeckit</code> API reference documentation. This section provides detailed information on the various modules, classes, and functions available in the library.</p> <p>Package Structure: fsspeckit is organized into domain-specific packages for better discoverability. See Architecture for details.</p>"},{"location":"api/#domain-packages-primary-api","title":"Domain Packages (Primary API)","text":""},{"location":"api/#dataset-operations","title":"Dataset Operations","text":"<ul> <li><code>fsspeckit.datasets</code> - Dataset-level operations (DuckDB &amp; PyArrow helpers)</li> </ul>"},{"location":"api/#sql-utilities","title":"SQL Utilities","text":"<ul> <li><code>fsspeckit.sql.filters</code> - SQL-to-filter translation helpers</li> </ul>"},{"location":"api/#common-utilities","title":"Common Utilities","text":"<ul> <li><code>fsspeckit.common</code> - Cross-cutting utilities (logging, parallelism, type conversion)</li> </ul>"},{"location":"api/#core-infrastructure","title":"Core Infrastructure","text":""},{"location":"api/#core-modules","title":"Core Modules","text":"<ul> <li><code>fsspeckit.core.base</code> - Base classes and interfaces</li> <li><code>fsspeckit.core.ext</code> - Extended filesystem methods</li> <li><code>fsspeckit.core.filesystem</code> - Filesystem factory functions</li> <li><code>fsspeckit.core.maintenance</code> - Dataset maintenance utilities</li> <li><code>fsspeckit.core.merge</code> - Dataset merging operations</li> </ul>"},{"location":"api/#storage-options","title":"Storage Options","text":"<ul> <li><code>fsspeckit.storage_options.base</code> - Base storage options</li> <li><code>fsspeckit.storage_options.cloud</code> - Cloud storage configurations</li> <li><code>fsspeckit.storage_options.core</code> - Core storage utilities</li> <li><code>fsspeckit.storage_options.git</code> - Git-based storage options</li> </ul>"},{"location":"api/#backwards-compatibility-fsspeckitutils","title":"Backwards Compatibility (<code>fsspeckit.utils</code>)","text":"<p>The <code>fsspeckit.utils</code> module provides a backwards-compatible fa\u00e7ade that re-exports selected helpers from the domain packages. New code should import directly from domain packages for better discoverability.</p> <ul> <li><code>fsspeckit.utils.datetime</code> - Date and time utilities</li> <li><code>fsspeckit.utils.logging</code> - Logging configuration and utilities</li> <li><code>fsspeckit.utils.misc</code> - Miscellaneous utility functions</li> <li><code>fsspeckit.utils.polars</code> - Polars DataFrame utilities</li> <li><code>fsspeckit.utils.pyarrow</code> - PyArrow utilities and integrations</li> <li><code>fsspeckit.utils.sql</code> - SQL query and filter utilities</li> <li><code>fsspeckit.utils.types</code> - Type definitions and utilities</li> </ul> <p>Migration Tip: For new code, prefer importing directly from domain packages: - <code>from fsspeckit.datasets import DuckDBParquetHandler</code> instead of <code>from fsspeckit.utils import DuckDBParquetHandler</code> - <code>from fsspeckit.common.logging import setup_logging</code> instead of <code>from fsspeckit.utils import setup_logging</code></p>"},{"location":"api/fsspeckit.common/","title":"<code>fsspeckit.common</code> API Reference","text":""},{"location":"api/fsspeckit.common/#fsspeckit.common","title":"common","text":"<p>Cross-cutting utilities for fsspeckit.</p> <p>This package contains utilities that are shared across different components: - Datetime parsing and manipulation utilities - Logging configuration and helpers - General purpose utility functions - Polars DataFrame optimization and manipulation - Type conversion and data transformation utilities</p>"},{"location":"api/fsspeckit.common/#fsspeckit.common-functions","title":"Functions","text":""},{"location":"api/fsspeckit.common/#fsspeckit.common.dict_to_dataframe","title":"fsspeckit.common.dict_to_dataframe","text":"<pre><code>dict_to_dataframe(\n    data: Union[dict, list[dict]],\n    unique: Union[bool, list[str], str] = False,\n) -&gt; Any\n</code></pre> <p>Convert a dictionary or list of dictionaries to a Polars DataFrame.</p> <p>Handles various input formats: - Single dict with list values \u2192 DataFrame rows - Single dict with scalar values \u2192 Single row DataFrame - List of dicts with scalar values \u2192 Multi-row DataFrame - List of dicts with list values \u2192 DataFrame with list columns</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[dict, list[dict]]</code> <p>Dictionary or list of dictionaries to convert.</p> required <code>unique</code> <code>Union[bool, list[str], str]</code> <p>If True, remove duplicate rows. Can also specify columns.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>Polars DataFrame containing the converted data.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Single dict with list values\n&gt;&gt;&gt; data = {'a': [1, 2, 3], 'b': [4, 5, 6]}\n&gt;&gt;&gt; dict_to_dataframe(data)\nshape: (3, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 b   \u2502\n\u2502 --- \u2506 --- \u2502\n\u2502 i64 \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 4   \u2502\n\u2502 2   \u2506 5   \u2502\n\u2502 3   \u2506 6   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <pre><code>&gt;&gt;&gt; # Single dict with scalar values\n&gt;&gt;&gt; data = {'a': 1, 'b': 2}\n&gt;&gt;&gt; dict_to_dataframe(data)\nshape: (1, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 b   \u2502\n\u2502 --- \u2506 --- \u2502\n\u2502 i64 \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 2   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <pre><code>&gt;&gt;&gt; # List of dicts with scalar values\n&gt;&gt;&gt; data = [{'a': 1, 'b': 2}, {'a': 3, 'b': 4}]\n&gt;&gt;&gt; dict_to_dataframe(data)\nshape: (2, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 b   \u2502\n\u2502 --- \u2506 --- \u2502\n\u2502 i64 \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 2   \u2502\n\u2502 3   \u2506 4   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> Source code in <code>src/fsspeckit/common/types.py</code> <pre><code>def dict_to_dataframe(\n    data: Union[dict, list[dict]], unique: Union[bool, list[str], str] = False\n) -&gt; Any:\n    \"\"\"Convert a dictionary or list of dictionaries to a Polars DataFrame.\n\n    Handles various input formats:\n    - Single dict with list values \u2192 DataFrame rows\n    - Single dict with scalar values \u2192 Single row DataFrame\n    - List of dicts with scalar values \u2192 Multi-row DataFrame\n    - List of dicts with list values \u2192 DataFrame with list columns\n\n    Args:\n        data: Dictionary or list of dictionaries to convert.\n        unique: If True, remove duplicate rows. Can also specify columns.\n\n    Returns:\n        Polars DataFrame containing the converted data.\n\n    Examples:\n        &gt;&gt;&gt; # Single dict with list values\n        &gt;&gt;&gt; data = {'a': [1, 2, 3], 'b': [4, 5, 6]}\n        &gt;&gt;&gt; dict_to_dataframe(data)\n        shape: (3, 2)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 a   \u2506 b   \u2502\n        \u2502 --- \u2506 --- \u2502\n        \u2502 i64 \u2506 i64 \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 1   \u2506 4   \u2502\n        \u2502 2   \u2506 5   \u2502\n        \u2502 3   \u2506 6   \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n\n        &gt;&gt;&gt; # Single dict with scalar values\n        &gt;&gt;&gt; data = {'a': 1, 'b': 2}\n        &gt;&gt;&gt; dict_to_dataframe(data)\n        shape: (1, 2)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 a   \u2506 b   \u2502\n        \u2502 --- \u2506 --- \u2502\n        \u2502 i64 \u2506 i64 \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 1   \u2506 2   \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n\n        &gt;&gt;&gt; # List of dicts with scalar values\n        &gt;&gt;&gt; data = [{'a': 1, 'b': 2}, {'a': 3, 'b': 4}]\n        &gt;&gt;&gt; dict_to_dataframe(data)\n        shape: (2, 2)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 a   \u2506 b   \u2502\n        \u2502 --- \u2506 --- \u2502\n        \u2502 i64 \u2506 i64 \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 1   \u2506 2   \u2502\n        \u2502 3   \u2506 4   \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n    \"\"\"\n    from fsspeckit.common.optional import _import_polars\n\n    pl = _import_polars()\n\n    if isinstance(data, list):\n        # If it's a single-element list, just use the first element\n        if len(data) == 1:\n            data = data[0]\n        # If it's a list of dicts\n        else:\n            first_item = data[0]\n            # Check if the dict values are lists/tuples\n            if any(isinstance(v, (list, tuple)) for v in first_item.values()):\n                # Each dict becomes a row with list/tuple values\n                data = pl.DataFrame(data)\n            else:\n                # If values are scalars, convert list of dicts to DataFrame\n                data = pl.DataFrame(data)\n\n            if unique:\n                data = data.unique(\n                    subset=None if not isinstance(unique, (str, list)) else unique,\n                    maintain_order=True,\n                )\n            return data\n\n    # If it's a single dict\n    if isinstance(data, dict):\n        # Check if values are lists/tuples\n        if any(isinstance(v, (list, tuple)) for v in data.values()):\n            # Get the length of any list value (assuming all lists have same length)\n            length = len(next(v for v in data.values() if isinstance(v, (list, tuple))))\n            # Convert to DataFrame where each list element becomes a row\n            data = pl.DataFrame(\n                {\n                    k: v if isinstance(v, (list, tuple)) else [v] * length\n                    for k, v in data.items()\n                }\n            )\n        else:\n            # If values are scalars, wrap them in a list to create a single row\n            data = pl.DataFrame({k: [v] for k, v in data.items()})\n\n        if unique:\n            data = data.unique(\n                subset=None if not isinstance(unique, (str, list)) else unique,\n                maintain_order=True,\n            )\n        return data\n\n    raise ValueError(\"Input must be a dictionary or list of dictionaries\")\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.get_logger","title":"fsspeckit.common.get_logger","text":"<pre><code>get_logger(name: str = 'fsspeckit') -&gt; logger\n</code></pre> <p>Get a logger instance for the given name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Logger name, typically the module name.</p> <code>'fsspeckit'</code> <p>Returns:</p> Type Description <code>logger</code> <p>Configured logger instance.</p> Example <pre><code>logger = get_logger(__name__)\nlogger.info(\"This is a log message\")\n</code></pre> Source code in <code>src/fsspeckit/common/logging.py</code> <pre><code>def get_logger(name: str = \"fsspeckit\") -&gt; \"logger\":\n    \"\"\"Get a logger instance for the given name.\n\n    Args:\n        name: Logger name, typically the module name.\n\n    Returns:\n        Configured logger instance.\n\n    Example:\n        ```python\n        logger = get_logger(__name__)\n        logger.info(\"This is a log message\")\n        ```\n    \"\"\"\n    return logger.bind(name=name)\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.get_partitions_from_path","title":"fsspeckit.common.get_partitions_from_path","text":"<pre><code>get_partitions_from_path(\n    path: str,\n    partitioning: Union[str, list[str], None] = None,\n) -&gt; list[tuple]\n</code></pre> <p>Extract dataset partitions from a file path.</p> <p>Parses file paths to extract partition information based on different partitioning schemes. This is the canonical implementation used across all fsspeckit backends.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>File path potentially containing partition information.</p> required <code>partitioning</code> <code>Union[str, list[str], None]</code> <p>Partitioning scheme: - \"hive\": Hive-style partitioning (key=value) - str: Single partition column name - list[str]: Multiple partition column names - None: Return empty list</p> <code>None</code> <p>Returns:</p> Type Description <code>list[tuple]</code> <p>List of tuples containing (column, value) pairs.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Hive-style partitioning\n&gt;&gt;&gt; get_partitions_from_path(\"data/year=2023/month=01/file.parquet\", \"hive\")\n[('year', '2023'), ('month', '01')]\n</code></pre> <pre><code>&gt;&gt;&gt; # Single partition column\n&gt;&gt;&gt; get_partitions_from_path(\"data/2023/01/file.parquet\", \"year\")\n[('year', '2023')]\n</code></pre> <pre><code>&gt;&gt;&gt; # Multiple partition columns\n&gt;&gt;&gt; get_partitions_from_path(\"data/2023/01/file.parquet\", [\"year\", \"month\"])\n[('year', '2023'), ('month', '01')]\n</code></pre> <pre><code>&gt;&gt;&gt; # No partitioning\n&gt;&gt;&gt; get_partitions_from_path(\"data/file.parquet\", None)\n[]\n</code></pre> Source code in <code>src/fsspeckit/common/partitions.py</code> <pre><code>def get_partitions_from_path(\n    path: str, partitioning: Union[str, list[str], None] = None\n) -&gt; list[tuple]:\n    \"\"\"\n    Extract dataset partitions from a file path.\n\n    Parses file paths to extract partition information based on\n    different partitioning schemes. This is the canonical implementation\n    used across all fsspeckit backends.\n\n    Args:\n        path: File path potentially containing partition information.\n        partitioning: Partitioning scheme:\n            - \"hive\": Hive-style partitioning (key=value)\n            - str: Single partition column name\n            - list[str]: Multiple partition column names\n            - None: Return empty list\n\n    Returns:\n        List of tuples containing (column, value) pairs.\n\n    Examples:\n        &gt;&gt;&gt; # Hive-style partitioning\n        &gt;&gt;&gt; get_partitions_from_path(\"data/year=2023/month=01/file.parquet\", \"hive\")\n        [('year', '2023'), ('month', '01')]\n\n        &gt;&gt;&gt; # Single partition column\n        &gt;&gt;&gt; get_partitions_from_path(\"data/2023/01/file.parquet\", \"year\")\n        [('year', '2023')]\n\n        &gt;&gt;&gt; # Multiple partition columns\n        &gt;&gt;&gt; get_partitions_from_path(\"data/2023/01/file.parquet\", [\"year\", \"month\"])\n        [('year', '2023'), ('month', '01')]\n\n        &gt;&gt;&gt; # No partitioning\n        &gt;&gt;&gt; get_partitions_from_path(\"data/file.parquet\", None)\n        []\n    \"\"\"\n    if \".\" in path:\n        path = os.path.dirname(path)\n\n    parts = path.split(\"/\")\n\n    if isinstance(partitioning, str):\n        if partitioning == \"hive\":\n            return [tuple(p.split(\"=\")) for p in parts if \"=\" in p]\n        else:\n            # Single partition column - take the first directory that looks like a value\n            # This is a simple heuristic for cases like data/2023/file.parquet\n            if parts:\n                return [(partitioning, parts[0])]\n            return []\n    elif isinstance(partitioning, list):\n        # Multiple partition columns - map column names to path parts from right to left\n        if not parts:\n            return []\n\n        # Take the last N parts where N is the number of partition columns\n        partition_parts = (\n            parts[-len(partitioning) :] if len(parts) &gt;= len(partitioning) else parts\n        )\n        return list(zip(partitioning, partition_parts))\n    else:\n        return []\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.get_timedelta_str","title":"fsspeckit.common.get_timedelta_str","text":"<pre><code>get_timedelta_str(\n    timedelta_string: str, to: str = \"polars\"\n) -&gt; str\n</code></pre> <p>Convert timedelta strings between different formats.</p> <p>Converts timedelta strings between Polars and DuckDB formats, with graceful fallback for unknown units. Never raises errors for unknown units - instead returns a reasonable string representation.</p> <p>Parameters:</p> Name Type Description Default <code>timedelta_string</code> <code>str</code> <p>Input timedelta string (e.g., \"1h\", \"2d\", \"5invalid\").</p> required <code>to</code> <code>str</code> <p>Target format - \"polars\" or \"duckdb\". Defaults to \"polars\".</p> <code>'polars'</code> <p>Returns:</p> Type Description <code>str</code> <p>String in the target format. For unknown units, returns \"value unit\"</p> <code>str</code> <p>format without raising errors.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Valid Polars units\n&gt;&gt;&gt; get_timedelta_str(\"1h\", to=\"polars\")\n'1h'\n&gt;&gt;&gt; get_timedelta_str(\"1d\", to=\"polars\")\n'1d'\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Convert to DuckDB format\n&gt;&gt;&gt; get_timedelta_str(\"1h\", to=\"duckdb\")\n'1 hour'\n&gt;&gt;&gt; get_timedelta_str(\"1s\", to=\"duckdb\")\n'1 second'\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Unknown units - graceful fallback\n&gt;&gt;&gt; get_timedelta_str(\"1invalid\", to=\"polars\")\n'1 invalid'\n&gt;&gt;&gt; get_timedelta_str(\"5unknown\", to=\"duckdb\")\n'5 unknown'\n</code></pre> Source code in <code>src/fsspeckit/common/datetime.py</code> <pre><code>def get_timedelta_str(timedelta_string: str, to: str = \"polars\") -&gt; str:\n    \"\"\"Convert timedelta strings between different formats.\n\n    Converts timedelta strings between Polars and DuckDB formats, with graceful\n    fallback for unknown units. Never raises errors for unknown units - instead\n    returns a reasonable string representation.\n\n    Args:\n        timedelta_string: Input timedelta string (e.g., \"1h\", \"2d\", \"5invalid\").\n        to: Target format - \"polars\" or \"duckdb\". Defaults to \"polars\".\n\n    Returns:\n        String in the target format. For unknown units, returns \"value unit\"\n        format without raising errors.\n\n    Examples:\n        &gt;&gt;&gt; # Valid Polars units\n        &gt;&gt;&gt; get_timedelta_str(\"1h\", to=\"polars\")\n        '1h'\n        &gt;&gt;&gt; get_timedelta_str(\"1d\", to=\"polars\")\n        '1d'\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Convert to DuckDB format\n        &gt;&gt;&gt; get_timedelta_str(\"1h\", to=\"duckdb\")\n        '1 hour'\n        &gt;&gt;&gt; get_timedelta_str(\"1s\", to=\"duckdb\")\n        '1 second'\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Unknown units - graceful fallback\n        &gt;&gt;&gt; get_timedelta_str(\"1invalid\", to=\"polars\")\n        '1 invalid'\n        &gt;&gt;&gt; get_timedelta_str(\"5unknown\", to=\"duckdb\")\n        '5 unknown'\n    \"\"\"\n    polars_timedelta_units = [\n        \"ns\",\n        \"us\",\n        \"ms\",\n        \"s\",\n        \"m\",\n        \"h\",\n        \"d\",\n        \"w\",\n        \"mo\",\n        \"y\",\n    ]\n    duckdb_timedelta_units = [\n        \"nanosecond\",\n        \"microsecond\",\n        \"millisecond\",\n        \"second\",\n        \"minute\",\n        \"hour\",\n        \"day\",\n        \"week\",\n        \"month\",\n        \"year\",\n    ]\n\n    unit = re.sub(\"[0-9]\", \"\", timedelta_string).strip()\n    val = timedelta_string.replace(unit, \"\").strip()\n    base_unit = re.sub(\"s$\", \"\", unit)\n\n    if to == \"polars\":\n        # Already a valid Polars unit\n        if unit in polars_timedelta_units:\n            return timedelta_string\n        # Try to map known DuckDB units to Polars units\n        mapping = dict(zip(duckdb_timedelta_units, polars_timedelta_units))\n        target = mapping.get(base_unit)\n        if target is not None:\n            return f\"{val}{target}\"\n        # Fallback for unknown units: preserve value and unit as-is\n        return f\"{val} {unit}\".strip()\n\n    # DuckDB branch\n    if unit in polars_timedelta_units:\n        mapping = dict(zip(polars_timedelta_units, duckdb_timedelta_units))\n        return f\"{val} {mapping[unit]}\"\n\n    # Unknown Polars-style unit when targeting DuckDB: keep unit without trailing \"s\"\n    return f\"{val} {base_unit}\".strip()\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.get_timestamp_column","title":"fsspeckit.common.get_timestamp_column","text":"<pre><code>get_timestamp_column(df: Any) -&gt; Union[str, list[str]]\n</code></pre> <p>Get timestamp column names from a DataFrame or PyArrow Table.</p> <p>Automatically detects and normalizes different DataFrame types to work with pandas DataFrames, Polars DataFrames/LazyFrames, and PyArrow Tables.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>A Polars DataFrame/LazyFrame, PyArrow Table, or pandas DataFrame. The function automatically converts pandas DataFrames and PyArrow Tables to Polars LazyFrames for consistent timestamp detection.</p> required <p>Returns:</p> Type Description <code>Union[str, list[str]]</code> <p>List of strings containing timestamp column names. Returns an empty list</p> <code>Union[str, list[str]]</code> <p>if no timestamp columns are found.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import polars as pl\n&gt;&gt;&gt; import pyarrow as pa\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Works with pandas DataFrames\n&gt;&gt;&gt; df_pd = pd.DataFrame({\"ts\": pd.date_range(\"2023-01-01\", periods=3)})\n&gt;&gt;&gt; get_timestamp_column(df_pd)\n['ts']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Works with Polars DataFrames\n&gt;&gt;&gt; df_pl = pl.DataFrame({\"ts\": [datetime(2023, 1, 1)]})\n&gt;&gt;&gt; get_timestamp_column(df_pl)\n['ts']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Works with PyArrow Tables\n&gt;&gt;&gt; table = pa.table({\"ts\": pa.array([datetime(2023, 1, 1)])})\n&gt;&gt;&gt; get_timestamp_column(table)\n['ts']\n</code></pre> Source code in <code>src/fsspeckit/common/datetime.py</code> <pre><code>def get_timestamp_column(df: Any) -&gt; Union[str, list[str]]:\n    \"\"\"Get timestamp column names from a DataFrame or PyArrow Table.\n\n    Automatically detects and normalizes different DataFrame types to work with\n    pandas DataFrames, Polars DataFrames/LazyFrames, and PyArrow Tables.\n\n    Args:\n        df: A Polars DataFrame/LazyFrame, PyArrow Table, or pandas DataFrame.\n            The function automatically converts pandas DataFrames and PyArrow Tables\n            to Polars LazyFrames for consistent timestamp detection.\n\n    Returns:\n        List of strings containing timestamp column names. Returns an empty list\n        if no timestamp columns are found.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import polars as pl\n        &gt;&gt;&gt; import pyarrow as pa\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Works with pandas DataFrames\n        &gt;&gt;&gt; df_pd = pd.DataFrame({\"ts\": pd.date_range(\"2023-01-01\", periods=3)})\n        &gt;&gt;&gt; get_timestamp_column(df_pd)\n        ['ts']\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Works with Polars DataFrames\n        &gt;&gt;&gt; df_pl = pl.DataFrame({\"ts\": [datetime(2023, 1, 1)]})\n        &gt;&gt;&gt; get_timestamp_column(df_pl)\n        ['ts']\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Works with PyArrow Tables\n        &gt;&gt;&gt; table = pa.table({\"ts\": pa.array([datetime(2023, 1, 1)])})\n        &gt;&gt;&gt; get_timestamp_column(table)\n        ['ts']\n    \"\"\"\n    from fsspeckit.common.optional import (\n        _import_pandas,\n        _import_polars,\n        _import_pyarrow,\n    )\n\n    pl = _import_polars()\n    pa = _import_pyarrow()\n    pd = _import_pandas()\n\n    # Import polars.selectors at runtime\n    import polars.selectors as cs\n\n    # Normalise supported input types to a Polars LazyFrame\n    if isinstance(df, pa.Table):\n        df = pl.from_arrow(df).lazy()\n    elif isinstance(df, pd.DataFrame):\n        df = pl.from_pandas(df).lazy()\n\n    return df.select(cs.datetime() | cs.date()).collect_schema().names()\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.opt_dtype_pl","title":"fsspeckit.common.opt_dtype_pl","text":"<pre><code>opt_dtype_pl(\n    df: DataFrame,\n    include: str | list[str] | None = None,\n    exclude: str | list[str] | None = None,\n    time_zone: str | None = None,\n    shrink_numerics: bool = False,\n    allow_unsigned: bool = True,\n    allow_null: bool = True,\n    sample_size: int | None = 1024,\n    sample_method: SampleMethod = \"first\",\n    strict: bool = False,\n    *,\n    force_timezone: str | None = None,\n) -&gt; DataFrame\n</code></pre> <p>Optimize data types of a Polars DataFrame for performance and memory efficiency.</p> <p>This function analyzes each column and converts it to the most appropriate data type based on content, handling string-to-type conversions and numeric type downcasting.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The Polars DataFrame to optimize.</p> required <code>include</code> <code>str | list[str] | None</code> <p>Column(s) to include in optimization (default: all columns).</p> <code>None</code> <code>exclude</code> <code>str | list[str] | None</code> <p>Column(s) to exclude from optimization.</p> <code>None</code> <code>time_zone</code> <code>str | None</code> <p>Optional time zone hint during datetime parsing.</p> <code>None</code> <code>shrink_numerics</code> <code>bool</code> <p>Whether to downcast numeric types when possible.</p> <code>False</code> <code>allow_unsigned</code> <code>bool</code> <p>Whether to allow unsigned integer types.</p> <code>True</code> <code>allow_null</code> <code>bool</code> <p>Whether to allow columns with all null values to be cast to Null type.</p> <code>True</code> <code>sample_size</code> <code>int | None</code> <p>Maximum number of cleaned values to inspect for regex-based inference. Use None to inspect the entire column.</p> <code>1024</code> <code>sample_method</code> <code>SampleMethod</code> <p>Which subset to inspect (<code>\"first\"</code> or <code>\"random\"</code>).</p> <code>'first'</code> <code>strict</code> <code>bool</code> <p>If True, will raise an error if any column cannot be optimized.</p> <code>False</code> <code>force_timezone</code> <code>str | None</code> <p>If set, ensure all parsed datetime columns end up with this timezone.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with optimized data types.</p> Source code in <code>src/fsspeckit/common/polars.py</code> <pre><code>def opt_dtype(\n    df: pl.DataFrame,\n    include: str | list[str] | None = None,\n    exclude: str | list[str] | None = None,\n    time_zone: str | None = None,\n    shrink_numerics: bool = False,\n    allow_unsigned: bool = True,\n    allow_null: bool = True,\n    sample_size: int | None = 1024,\n    sample_method: SampleMethod = \"first\",\n    strict: bool = False,\n    *,\n    force_timezone: str | None = None,\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Optimize data types of a Polars DataFrame for performance and memory efficiency.\n\n    This function analyzes each column and converts it to the most appropriate\n    data type based on content, handling string-to-type conversions and\n    numeric type downcasting.\n\n    Args:\n        df: The Polars DataFrame to optimize.\n        include: Column(s) to include in optimization (default: all columns).\n        exclude: Column(s) to exclude from optimization.\n        time_zone: Optional time zone hint during datetime parsing.\n        shrink_numerics: Whether to downcast numeric types when possible.\n        allow_unsigned: Whether to allow unsigned integer types.\n        allow_null: Whether to allow columns with all null values to be cast to Null type.\n        sample_size: Maximum number of cleaned values to inspect for regex-based inference. Use None to inspect the entire column.\n        sample_method: Which subset to inspect (`\"first\"` or `\"random\"`).\n        strict: If True, will raise an error if any column cannot be optimized.\n        force_timezone: If set, ensure all parsed datetime columns end up with this timezone.\n\n    Returns:\n        DataFrame with optimized data types.\n    \"\"\"\n    if sample_method not in (\"first\", \"random\"):\n        raise ValueError(\"sample_method must be 'first' or 'random'\")\n\n    if isinstance(df, pl.LazyFrame):\n        return opt_dtype(\n            df.collect(),\n            include=include,\n            exclude=exclude,\n            time_zone=time_zone,\n            shrink_numerics=shrink_numerics,\n            allow_unsigned=allow_unsigned,\n            allow_null=allow_null,\n            sample_size=sample_size,\n            sample_method=sample_method,\n            strict=strict,\n            force_timezone=force_timezone,\n        ).lazy()\n\n    # Normalize include/exclude parameters\n    if isinstance(include, str):\n        include = [include]\n    if isinstance(exclude, str):\n        exclude = [exclude]\n\n    # Determine columns to process\n    cols_to_process = df.columns\n    if include:\n        cols_to_process = [col for col in include if col in df.columns]\n    if exclude:\n        cols_to_process = [col for col in cols_to_process if col not in exclude]\n\n    # Generate optimization expressions for all columns\n    expressions = []\n    for col_name in cols_to_process:\n        try:\n            expressions.append(\n                _get_column_expr(\n                    df,\n                    col_name,\n                    shrink_numerics,\n                    allow_unsigned,\n                    allow_null,\n                    time_zone,\n                    force_timezone,\n                    sample_size,\n                    sample_method,\n                    strict,\n                )\n            )\n        except Exception as e:\n            if strict:\n                raise e\n            # If strict mode is off, just keep the original column\n            continue\n\n    # Apply all transformations at once if any exist\n    return df if not expressions else df.with_columns(expressions)\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.safe_format_error","title":"fsspeckit.common.safe_format_error","text":"<pre><code>safe_format_error(\n    operation: str,\n    path: str | None = None,\n    error: BaseException | None = None,\n    **context: Any,\n) -&gt; str\n</code></pre> <p>Format an error message with credentials scrubbed.</p> <p>Parameters:</p> Name Type Description Default <code>operation</code> <code>str</code> <p>Description of the operation that failed.</p> required <code>path</code> <code>str | None</code> <p>Optional path involved in the operation.</p> <code>None</code> <code>error</code> <code>BaseException | None</code> <p>Optional exception that occurred.</p> <code>None</code> <code>**context</code> <code>Any</code> <p>Additional context key-value pairs.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>A formatted, credential-scrubbed error message.</p> Source code in <code>src/fsspeckit/common/security.py</code> <pre><code>def safe_format_error(\n    operation: str,\n    path: str | None = None,\n    error: BaseException | None = None,\n    **context: Any,\n) -&gt; str:\n    \"\"\"Format an error message with credentials scrubbed.\n\n    Args:\n        operation: Description of the operation that failed.\n        path: Optional path involved in the operation.\n        error: Optional exception that occurred.\n        **context: Additional context key-value pairs.\n\n    Returns:\n        A formatted, credential-scrubbed error message.\n    \"\"\"\n    parts = [f\"Failed to {operation}\"]\n\n    if path:\n        parts.append(f\"at '{path}'\")\n\n    if error:\n        parts.append(f\": {scrub_exception(error)}\")\n\n    if context:\n        context_str = \", \".join(f\"{k}={scrub_credentials(str(v))}\" for k, v in context.items())\n        parts.append(f\" ({context_str})\")\n\n    return \" \".join(parts)\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.scrub_credentials","title":"fsspeckit.common.scrub_credentials","text":"<pre><code>scrub_credentials(message: str) -&gt; str\n</code></pre> <p>Remove or mask credential-like values from a string.</p> <p>This is intended for use before logging error messages that might contain sensitive information like access keys or tokens.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The string to scrub.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The string with credential-like values replaced with [REDACTED].</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; scrub_credentials(\"Error: access_key_id=AKIAIOSFODNN7EXAMPLE\")\n'Error: access_key_id=[REDACTED]'\n</code></pre> <pre><code>&gt;&gt;&gt; scrub_credentials(\"Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...\")\n'[REDACTED]'\n</code></pre> Source code in <code>src/fsspeckit/common/security.py</code> <pre><code>def scrub_credentials(message: str) -&gt; str:\n    \"\"\"Remove or mask credential-like values from a string.\n\n    This is intended for use before logging error messages that might\n    contain sensitive information like access keys or tokens.\n\n    Args:\n        message: The string to scrub.\n\n    Returns:\n        The string with credential-like values replaced with [REDACTED].\n\n    Examples:\n        &gt;&gt;&gt; scrub_credentials(\"Error: access_key_id=AKIAIOSFODNN7EXAMPLE\")\n        'Error: access_key_id=[REDACTED]'\n\n        &gt;&gt;&gt; scrub_credentials(\"Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...\")\n        '[REDACTED]'\n    \"\"\"\n    if not message:\n        return message\n\n    result = message\n\n    for pattern in _CREDENTIAL_PATTERNS:\n        # Replace matched groups with [REDACTED]\n        def redact_match(match: re.Match) -&gt; str:\n            groups = match.groups()\n            if len(groups) &gt;= 2:\n                # Pattern with key=value format - keep the key, redact the value\n                return match.group(0).replace(groups[-1], \"[REDACTED]\")\n            else:\n                # Single match - redact entire thing\n                return \"[REDACTED]\"\n\n        result = pattern.sub(redact_match, result)\n\n    return result\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.scrub_exception","title":"fsspeckit.common.scrub_exception","text":"<pre><code>scrub_exception(exc: BaseException) -&gt; str\n</code></pre> <p>Scrub credentials from an exception's string representation.</p> <p>Parameters:</p> Name Type Description Default <code>exc</code> <code>BaseException</code> <p>The exception to scrub.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A scrubbed string representation of the exception.</p> Source code in <code>src/fsspeckit/common/security.py</code> <pre><code>def scrub_exception(exc: BaseException) -&gt; str:\n    \"\"\"Scrub credentials from an exception's string representation.\n\n    Args:\n        exc: The exception to scrub.\n\n    Returns:\n        A scrubbed string representation of the exception.\n    \"\"\"\n    return scrub_credentials(str(exc))\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.setup_logging","title":"fsspeckit.common.setup_logging","text":"<pre><code>setup_logging(\n    level: Optional[str] = None,\n    disable: bool = False,\n    format_string: Optional[str] = None,\n) -&gt; None\n</code></pre> <p>Configure the Loguru logger for fsspeckit.</p> <p>Removes the default handler and adds a new one targeting stderr with customizable level and format.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>Optional[str]</code> <p>Log level (DEBUG, INFO, WARNING, ERROR, CRITICAL).    If None, uses fsspeckit_LOG_LEVEL environment variable    or defaults to \"INFO\".</p> <code>None</code> <code>disable</code> <code>bool</code> <p>Whether to disable logging for fsspeckit package.</p> <code>False</code> <code>format_string</code> <code>Optional[str]</code> <p>Custom format string for log messages.           If None, uses a default comprehensive format.</p> <code>None</code> Example <pre><code># Basic setup\nsetup_logging()\n\n# Custom level and format\nsetup_logging(level=\"DEBUG\", format_string=\"{time} | {level} | {message}\")\n\n# Disable logging\nsetup_logging(disable=True)\n</code></pre> Source code in <code>src/fsspeckit/common/logging.py</code> <pre><code>def setup_logging(\n    level: Optional[str] = None,\n    disable: bool = False,\n    format_string: Optional[str] = None,\n) -&gt; None:\n    \"\"\"Configure the Loguru logger for fsspeckit.\n\n    Removes the default handler and adds a new one targeting stderr\n    with customizable level and format.\n\n    Args:\n        level: Log level (DEBUG, INFO, WARNING, ERROR, CRITICAL).\n               If None, uses fsspeckit_LOG_LEVEL environment variable\n               or defaults to \"INFO\".\n        disable: Whether to disable logging for fsspeckit package.\n        format_string: Custom format string for log messages.\n                      If None, uses a default comprehensive format.\n\n    Example:\n        ```python\n        # Basic setup\n        setup_logging()\n\n        # Custom level and format\n        setup_logging(level=\"DEBUG\", format_string=\"{time} | {level} | {message}\")\n\n        # Disable logging\n        setup_logging(disable=True)\n        ```\n    \"\"\"\n    # Determine log level\n    if level is None:\n        level = os.getenv(\"fsspeckit_LOG_LEVEL\", \"INFO\")\n\n    # Default format if none provided\n    if format_string is None:\n        format_string = (\n            \"&lt;green&gt;{time:YYYY-MM-DD HH:mm:ss.SSS}&lt;/green&gt; | \"\n            \"&lt;level&gt;{level: &lt;8}&lt;/level&gt; | \"\n            \"&lt;cyan&gt;{name}&lt;/cyan&gt;:&lt;cyan&gt;{function}&lt;/cyan&gt;:&lt;cyan&gt;{line}&lt;/cyan&gt; - \"\n            \"&lt;level&gt;{message}&lt;/level&gt;\"\n        )\n\n    # Remove the default handler added by Loguru\n    logger.remove()\n\n    # Add new handler with custom configuration\n    logger.add(\n        sys.stderr,\n        level=level.upper(),\n        format=format_string,\n    )\n\n    # Optionally disable logging for this package\n    if disable:\n        logger.disable(\"fsspeckit\")\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.sync_dir","title":"fsspeckit.common.sync_dir","text":"<pre><code>sync_dir(\n    src_fs: AbstractFileSystem,\n    dst_fs: AbstractFileSystem,\n    src_path: str = \"\",\n    dst_path: str = \"\",\n    server_side: bool = True,\n    chunk_size: int = 8 * 1024 * 1024,\n    parallel: bool = False,\n    n_jobs: int = -1,\n    verbose: bool = True,\n) -&gt; dict[str, list[str]]\n</code></pre> <p>Sync two directories between different filesystems.</p> <p>Compares files in the source and destination directories, copies new or updated files from source to destination, and deletes stale files from destination.</p> <p>Parameters:</p> Name Type Description Default <code>src_fs</code> <code>AbstractFileSystem</code> <p>Source filesystem (fsspec AbstractFileSystem)</p> required <code>dst_fs</code> <code>AbstractFileSystem</code> <p>Destination filesystem (fsspec AbstractFileSystem)</p> required <code>src_path</code> <code>str</code> <p>Path in source filesystem to sync. Default is root ('').</p> <code>''</code> <code>dst_path</code> <code>str</code> <p>Path in destination filesystem to sync. Default is root ('').</p> <code>''</code> <code>chunk_size</code> <code>int</code> <p>Size of chunks to read/write files (in bytes). Default is 8MB.</p> <code>8 * 1024 * 1024</code> <code>parallel</code> <code>bool</code> <p>Whether to perform copy/delete operations in parallel. Default is False.</p> <code>False</code> <code>n_jobs</code> <code>int</code> <p>Number of parallel jobs if parallel=True. Default is -1 (all cores).</p> <code>-1</code> <code>verbose</code> <code>bool</code> <p>Whether to show progress bars. Default is True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, list[str]]</code> <p>Summary of added and deleted files</p> Source code in <code>src/fsspeckit/common/misc.py</code> <pre><code>def sync_dir(\n    src_fs: AbstractFileSystem,\n    dst_fs: AbstractFileSystem,\n    src_path: str = \"\",\n    dst_path: str = \"\",\n    server_side: bool = True,\n    chunk_size: int = 8 * 1024 * 1024,\n    parallel: bool = False,\n    n_jobs: int = -1,\n    verbose: bool = True,\n) -&gt; dict[str, list[str]]:\n    \"\"\"Sync two directories between different filesystems.\n\n    Compares files in the source and destination directories, copies new or updated files from source to destination,\n    and deletes stale files from destination.\n\n    Args:\n        src_fs: Source filesystem (fsspec AbstractFileSystem)\n        dst_fs: Destination filesystem (fsspec AbstractFileSystem)\n        src_path: Path in source filesystem to sync. Default is root ('').\n        dst_path: Path in destination filesystem to sync. Default is root ('').\n        chunk_size: Size of chunks to read/write files (in bytes). Default is 8MB.\n        parallel: Whether to perform copy/delete operations in parallel. Default is False.\n        n_jobs: Number of parallel jobs if parallel=True. Default is -1 (all cores).\n        verbose: Whether to show progress bars. Default is True.\n\n    Returns:\n        dict: Summary of added and deleted files\n    \"\"\"\n\n    src_mapper = src_fs.get_mapper(src_path)\n    dst_mapper = dst_fs.get_mapper(dst_path)\n\n    add_files = sorted(src_mapper.keys() - dst_mapper.keys())\n    delete_files = sorted(dst_mapper.keys() - src_mapper.keys())\n\n    return sync_files(\n        add_files=add_files,\n        delete_files=delete_files,\n        src_fs=src_fs,\n        dst_fs=dst_fs,\n        src_path=src_path,\n        dst_path=dst_path,\n        chunk_size=chunk_size,\n        server_side=server_side,\n        parallel=parallel,\n        n_jobs=n_jobs,\n        verbose=verbose,\n    )\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.sync_files","title":"fsspeckit.common.sync_files","text":"<pre><code>sync_files(\n    add_files: list[str],\n    delete_files: list[str],\n    src_fs: AbstractFileSystem,\n    dst_fs: AbstractFileSystem,\n    src_path: str = \"\",\n    dst_path: str = \"\",\n    server_side: bool = False,\n    chunk_size: int = 8 * 1024 * 1024,\n    parallel: bool = False,\n    n_jobs: int = -1,\n    verbose: bool = True,\n) -&gt; dict[str, list[str]]\n</code></pre> <p>Sync files between two filesystems by copying new files and deleting old ones.</p> <p>Parameters:</p> Name Type Description Default <code>add_files</code> <code>list[str]</code> <p>List of file paths to add (copy from source to destination)</p> required <code>delete_files</code> <code>list[str]</code> <p>List of file paths to delete from destination</p> required <code>src_fs</code> <code>AbstractFileSystem</code> <p>Source filesystem (fsspec AbstractFileSystem)</p> required <code>dst_fs</code> <code>AbstractFileSystem</code> <p>Destination filesystem (fsspec AbstractFileSystem)</p> required <code>src_path</code> <code>str</code> <p>Base path in source filesystem. Default is root ('').</p> <code>''</code> <code>dst_path</code> <code>str</code> <p>Base path in destination filesystem. Default is root ('').</p> <code>''</code> <code>server_side</code> <code>bool</code> <p>Whether to use server-side copy if supported. Default is False.</p> <code>False</code> <code>chunk_size</code> <code>int</code> <p>Size of chunks to read/write files (in bytes). Default is 8MB.</p> <code>8 * 1024 * 1024</code> <code>parallel</code> <code>bool</code> <p>Whether to perform copy/delete operations in parallel. Default is False.</p> <code>False</code> <code>n_jobs</code> <code>int</code> <p>Number of parallel jobs if parallel=True. Default is -1 (all cores).</p> <code>-1</code> <code>verbose</code> <code>bool</code> <p>Whether to show progress bars. Default is True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, list[str]]</code> <p>Summary of added and deleted files</p> Source code in <code>src/fsspeckit/common/misc.py</code> <pre><code>def sync_files(\n    add_files: list[str],\n    delete_files: list[str],\n    src_fs: AbstractFileSystem,\n    dst_fs: AbstractFileSystem,\n    src_path: str = \"\",\n    dst_path: str = \"\",\n    server_side: bool = False,\n    chunk_size: int = 8 * 1024 * 1024,\n    parallel: bool = False,\n    n_jobs: int = -1,\n    verbose: bool = True,\n) -&gt; dict[str, list[str]]:\n    \"\"\"Sync files between two filesystems by copying new files and deleting old ones.\n\n    Args:\n        add_files: List of file paths to add (copy from source to destination)\n        delete_files: List of file paths to delete from destination\n        src_fs: Source filesystem (fsspec AbstractFileSystem)\n        dst_fs: Destination filesystem (fsspec AbstractFileSystem)\n        src_path: Base path in source filesystem. Default is root ('').\n        dst_path: Base path in destination filesystem. Default is root ('').\n        server_side: Whether to use server-side copy if supported. Default is False.\n        chunk_size: Size of chunks to read/write files (in bytes). Default is 8MB.\n        parallel: Whether to perform copy/delete operations in parallel. Default is False.\n        n_jobs: Number of parallel jobs if parallel=True. Default is -1 (all cores).\n        verbose: Whether to show progress bars. Default is True.\n\n    Returns:\n        dict: Summary of added and deleted files\n    \"\"\"\n    CHUNK = chunk_size\n    RETRIES = 3\n\n    server_side = check_fs_identical(src_fs, dst_fs) and server_side\n\n    src_mapper = src_fs.get_mapper(src_path)\n    dst_mapper = dst_fs.get_mapper(dst_path)\n\n    def server_side_copy_file(key, src_mapper, dst_mapper, RETRIES):\n        last_exc = None\n        for attempt in range(1, RETRIES + 1):\n            try:\n                dst_mapper[key] = src_mapper[key]\n                break\n            except (OSError, IOError) as e:\n                last_exc = e\n                if attempt == RETRIES:\n                    logger.error(\n                        \"Failed to copy file %s after %d attempts: %s\",\n                        key,\n                        RETRIES,\n                        str(e),\n                        exc_info=True,\n                    )\n                    raise RuntimeError(f\"Failed to copy file {key} after {RETRIES} attempts\") from e\n            except Exception as e:\n                last_exc = e\n                if attempt == RETRIES:\n                    logger.error(\n                        \"Unexpected error copying file %s: %s\",\n                        key,\n                        str(e),\n                        exc_info=True,\n                    )\n                    raise RuntimeError(f\"Unexpected error copying file {key}: {e}\") from e\n\n    def copy_file(key, src_fs, dst_fs, src_path, dst_path, CHUNK, RETRIES):\n        last_exc = None\n        for attempt in range(1, RETRIES + 1):\n            try:\n                with (\n                    src_fs.open(posixpath.join(src_path, key), \"rb\") as r,\n                    dst_fs.open(posixpath.join(dst_path, key), \"wb\") as w,\n                ):\n                    while True:\n                        chunk = r.read(CHUNK)\n                        if not chunk:\n                            break\n                        w.write(chunk)\n                break\n            except (OSError, IOError) as e:\n                last_exc = e\n                if attempt == RETRIES:\n                    logger.error(\n                        \"Failed to copy file %s after %d attempts: %s\",\n                        key,\n                        RETRIES,\n                        str(e),\n                        exc_info=True,\n                    )\n                    raise RuntimeError(f\"Failed to copy file {key} after {RETRIES} attempts\") from e\n            except Exception as e:\n                last_exc = e\n                if attempt == RETRIES:\n                    logger.error(\n                        \"Unexpected error copying file %s: %s\",\n                        key,\n                        str(e),\n                        exc_info=True,\n                    )\n                    raise RuntimeError(f\"Unexpected error copying file {key}: {e}\") from e\n\n    def delete_file(key, dst_fs, dst_path, RETRIES):\n        last_exc = None\n        for attempt in range(1, RETRIES + 1):\n            try:\n                dst_fs.rm(posixpath.join(dst_path, key))\n                break\n            except (OSError, IOError) as e:\n                last_exc = e\n                if attempt == RETRIES:\n                    logger.error(\n                        \"Failed to delete file %s after %d attempts: %s\",\n                        key,\n                        RETRIES,\n                        str(e),\n                        exc_info=True,\n                    )\n                    raise RuntimeError(f\"Failed to delete file {key} after {RETRIES} attempts\") from e\n            except Exception as e:\n                last_exc = e\n                if attempt == RETRIES:\n                    logger.error(\n                        \"Unexpected error deleting file %s: %s\",\n                        key,\n                        str(e),\n                        exc_info=True,\n                    )\n                    raise RuntimeError(f\"Unexpected error deleting file {key}: {e}\") from e\n\n    if len(add_files):\n        # Copy new files\n        if parallel:\n            if server_side:\n                try:\n                    run_parallel(\n                        server_side_copy_file,\n                        add_files,\n                        src_mapper=src_mapper,\n                        dst_mapper=dst_mapper,\n                        RETRIES=RETRIES,\n                        n_jobs=n_jobs,\n                        verbose=verbose,\n                    )\n                except (RuntimeError, OSError) as e:\n                    logger.warning(\n                        \"Server-side copy failed for some files, falling back to client-side: %s\",\n                        str(e),\n                    )\n                    # Fallback to client-side copy if server-side fails\n                    run_parallel(\n                        copy_file,\n                        add_files,\n                        src_fs=src_fs,\n                        dst_fs=dst_fs,\n                        src_path=src_path,\n                        dst_path=dst_path,\n                        CHUNK=CHUNK,\n                        RETRIES=RETRIES,\n                        n_jobs=n_jobs,\n                        verbose=verbose,\n                    )\n\n            else:\n                run_parallel(\n                    copy_file,\n                    add_files,\n                    src_fs=src_fs,\n                    dst_fs=dst_fs,\n                    src_path=src_path,\n                    dst_path=dst_path,\n                    CHUNK=CHUNK,\n                    RETRIES=RETRIES,\n                    n_jobs=n_jobs,\n                    verbose=verbose,\n                )\n        else:\n            if verbose:\n                from rich.progress import track\n\n                for key in track(\n                    add_files,\n                    description=\"Copying new files...\",\n                    total=len(add_files),\n                ):\n                    if server_side:\n                        try:\n                            server_side_copy_file(key, src_mapper, dst_mapper, RETRIES)\n                        except (RuntimeError, OSError):\n                            copy_file(\n                                key, src_fs, dst_fs, src_path, dst_path, CHUNK, RETRIES\n                            )\n                    else:\n                        copy_file(\n                            key, src_fs, dst_fs, src_path, dst_path, CHUNK, RETRIES\n                        )\n            else:\n                for key in add_files:\n                    if server_side:\n                        try:\n                            server_side_copy_file(key, src_mapper, dst_mapper, RETRIES)\n                        except (RuntimeError, OSError):\n                            copy_file(\n                                key, src_fs, dst_fs, src_path, dst_path, CHUNK, RETRIES\n                            )\n                    else:\n                        copy_file(\n                            key, src_fs, dst_fs, src_path, dst_path, CHUNK, RETRIES\n                        )\n\n    if len(delete_files):\n        # Delete old files from destination\n        if parallel:\n            run_parallel(\n                delete_file,\n                delete_files,\n                dst_fs=dst_fs,\n                dst_path=dst_path,\n                RETRIES=RETRIES,\n                n_jobs=n_jobs,\n                verbose=verbose,\n            )\n        else:\n            if verbose:\n                from rich.progress import track\n\n                for key in track(\n                    delete_files,\n                    description=\"Deleting stale files...\",\n                    total=len(delete_files),\n                ):\n                    delete_file(key, dst_fs, dst_path, RETRIES)\n            else:\n                for key in delete_files:\n                    delete_file(key, dst_fs, dst_path, RETRIES)\n\n    return {\"added_files\": add_files, \"deleted_files\": delete_files}\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.timestamp_from_string","title":"fsspeckit.common.timestamp_from_string  <code>cached</code>","text":"<pre><code>timestamp_from_string(\n    timestamp_str: str,\n    tz: Union[str, None] = None,\n    naive: bool = False,\n) -&gt; Union[datetime, date, time]\n</code></pre> <p>Converts a timestamp string (ISO 8601 format) into a datetime, date, or time object using only standard Python libraries.</p> <p>Handles strings with or without timezone information (e.g., '2023-01-01T10:00:00+02:00', '2023-01-01', '10:00:00'). Supports timezone offsets like '+HH:MM' or '+HHMM'. For named timezones (e.g., 'Europe/Paris'), requires Python 3.9+ and the 'tzdata' package to be installed.</p> <p>Parameters:</p> Name Type Description Default <code>timestamp_str</code> <code>str</code> <p>The string representation of the timestamp (ISO 8601 format).</p> required <code>tz</code> <code>str</code> <p>Target timezone identifier (e.g., 'UTC', '+02:00', 'Europe/Paris'). If provided, the output datetime/time will be localized or converted to this timezone. Defaults to None.</p> <code>None</code> <code>naive</code> <code>bool</code> <p>If True, return a naive datetime/time (no timezone info), even if the input string or <code>tz</code> parameter specifies one. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[datetime, date, time]</code> <p>Union[dt.datetime, dt.date, dt.time]: The parsed datetime, date, or time object.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the timestamp string format is invalid or the timezone is         invalid/unsupported.</p> Source code in <code>src/fsspeckit/common/datetime.py</code> <pre><code>@lru_cache(maxsize=128)\ndef timestamp_from_string(\n    timestamp_str: str,\n    tz: Union[str, None] = None,\n    naive: bool = False,\n) -&gt; Union[dt.datetime, dt.date, dt.time]:\n    \"\"\"\n    Converts a timestamp string (ISO 8601 format) into a datetime, date, or time object\n    using only standard Python libraries.\n\n    Handles strings with or without timezone information (e.g., '2023-01-01T10:00:00+02:00',\n    '2023-01-01', '10:00:00'). Supports timezone offsets like '+HH:MM' or '+HHMM'.\n    For named timezones (e.g., 'Europe/Paris'), requires Python 3.9+ and the 'tzdata'\n    package to be installed.\n\n    Args:\n        timestamp_str (str): The string representation of the timestamp (ISO 8601 format).\n        tz (str, optional): Target timezone identifier (e.g., 'UTC', '+02:00', 'Europe/Paris').\n            If provided, the output datetime/time will be localized or converted to this timezone.\n            Defaults to None.\n        naive (bool, optional): If True, return a naive datetime/time (no timezone info),\n            even if the input string or `tz` parameter specifies one. Defaults to False.\n\n    Returns:\n        Union[dt.datetime, dt.date, dt.time]: The parsed datetime, date, or time object.\n\n    Raises:\n        ValueError: If the timestamp string format is invalid or the timezone is\n                    invalid/unsupported.\n    \"\"\"\n\n    # Regex to parse timezone offsets like +HH:MM or +HHMM\n    _TZ_OFFSET_REGEX = re.compile(r\"([+-])(\\d{2}):?(\\d{2})\")\n\n    def _parse_tz_offset(tz_str: str) -&gt; dt.tzinfo | None:\n        \"\"\"Parses a timezone offset string into a timezone object.\"\"\"\n        match = _TZ_OFFSET_REGEX.fullmatch(tz_str)\n        if match:\n            sign, hours, minutes = match.groups()\n            offset_seconds = (int(hours) * 3600 + int(minutes) * 60) * (\n                -1 if sign == \"-\" else 1\n            )\n            if abs(offset_seconds) &gt;= 24 * 3600:\n                raise ValueError(f\"Invalid timezone offset: {tz_str}\")\n            return dt.timezone(dt.timedelta(seconds=offset_seconds), name=tz_str)\n        return None\n\n    def _get_tzinfo(tz_identifier: str | None) -&gt; dt.tzinfo | None:\n        \"\"\"Gets a tzinfo object from a string (offset or IANA name).\"\"\"\n        if tz_identifier is None:\n            return None\n        if tz_identifier.upper() == \"UTC\":\n            return dt.timezone.utc\n\n        # Try parsing as offset first\n        offset_tz = _parse_tz_offset(tz_identifier)\n        if offset_tz:\n            return offset_tz\n\n        # Try parsing as IANA name using zoneinfo (if available)\n        if ZoneInfo:\n            try:\n                return ZoneInfo(tz_identifier)\n            except ZoneInfoNotFoundError:\n                raise ValueError(\n                    f\"Timezone '{tz_identifier}' not found. Install 'tzdata' or use offset format.\"\n                )\n            except Exception as e:  # Catch other potential zoneinfo errors\n                raise ValueError(f\"Error loading timezone '{tz_identifier}': {e}\")\n        else:\n            # zoneinfo not available\n            raise ValueError(\n                f\"Invalid timezone: '{tz_identifier}'. Use offset format (e.g., '+02:00') \"\n                \"or run Python 3.9+ with 'tzdata' installed for named timezones.\"\n            )\n\n    target_tz: dt.tzinfo | None = _get_tzinfo(tz)\n    parsed_obj: dt.datetime | dt.date | dt.time | None = None\n\n    # Preprocess: Replace space separator, strip whitespace\n    processed_str = timestamp_str.strip().replace(\" \", \"T\")\n\n    # Attempt parsing (datetime, date, time) using fromisoformat\n    try:\n        # Python &lt; 3.11 fromisoformat has limitations (e.g., no Z, no +HHMM offset)\n        # This implementation assumes Python 3.11+ for full ISO 8601 support via fromisoformat\n        # or that input strings use formats compatible with older versions (e.g., +HH:MM)\n        parsed_obj = dt.datetime.fromisoformat(processed_str)\n    except ValueError:\n        try:\n            parsed_obj = dt.date.fromisoformat(processed_str)\n        except ValueError:\n            try:\n                # Time parsing needs care, especially with offsets in older Python\n                parsed_obj = dt.time.fromisoformat(processed_str)\n            except ValueError:\n                # Add fallback for simple HH:MM:SS if needed, though less robust\n                # try:\n                #     parsed_obj = dt.datetime.strptime(processed_str, \"%H:%M:%S\").time()\n                # except ValueError:\n                raise ValueError(f\"Invalid timestamp format: '{timestamp_str}'\")\n\n    # Apply timezone logic if we have a datetime or time object\n    if isinstance(parsed_obj, (dt.datetime, dt.time)):\n        is_aware = (\n            parsed_obj.tzinfo is not None\n            and parsed_obj.tzinfo.utcoffset(\n                parsed_obj if isinstance(parsed_obj, dt.datetime) else None\n            )\n            is not None\n        )\n\n        if target_tz:\n            if is_aware:\n                # Convert existing aware object to target timezone (only for datetime)\n                if isinstance(parsed_obj, dt.datetime):\n                    parsed_obj = parsed_obj.astimezone(target_tz)\n                # else: dt.time cannot be converted without a date context. Keep original tz.\n            else:\n                # Localize naive object to target timezone\n                parsed_obj = parsed_obj.replace(tzinfo=target_tz)\n            is_aware = True  # Object is now considered aware\n\n        # Handle naive flag: remove tzinfo if requested\n        if naive and is_aware:\n            parsed_obj = parsed_obj.replace(tzinfo=None)\n\n    # If it's a date object, tz/naive flags are ignored\n    elif isinstance(parsed_obj, dt.date):\n        pass\n\n    return parsed_obj\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.to_pyarrow_table","title":"fsspeckit.common.to_pyarrow_table","text":"<pre><code>to_pyarrow_table(\n    data: Union[Any, dict, list[Any]],\n    concat: bool = False,\n    unique: Union[bool, list[str], str] = False,\n) -&gt; Any\n</code></pre> <p>Convert various data formats to PyArrow Table.</p> <p>Handles conversion from Polars DataFrames, Pandas DataFrames, dictionaries, and lists of these types to PyArrow Tables.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[Any, dict, list[Any]]</code> <p>Input data to convert.</p> required <code>concat</code> <code>bool</code> <p>Whether to concatenate multiple inputs into single table.</p> <code>False</code> <code>unique</code> <code>Union[bool, list[str], str]</code> <p>Whether to remove duplicates. Can specify columns.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>PyArrow Table containing the converted data.</p> Example <pre><code>df = pl.DataFrame({\"a\": [1, 2], \"b\": [3, 4]})\ntable = to_pyarrow_table(df)\nprint(table.schema)\n# a: int64\n# b: int64\n</code></pre> Source code in <code>src/fsspeckit/common/types.py</code> <pre><code>def to_pyarrow_table(\n    data: Union[\n        Any,  # pl.DataFrame, pl.LazyFrame, pd.DataFrame\n        dict,\n        list[Any],  # list of DataFrames or dicts\n    ],\n    concat: bool = False,\n    unique: Union[bool, list[str], str] = False,\n) -&gt; Any:\n    \"\"\"Convert various data formats to PyArrow Table.\n\n    Handles conversion from Polars DataFrames, Pandas DataFrames,\n    dictionaries, and lists of these types to PyArrow Tables.\n\n    Args:\n        data: Input data to convert.\n        concat: Whether to concatenate multiple inputs into single table.\n        unique: Whether to remove duplicates. Can specify columns.\n\n    Returns:\n        PyArrow Table containing the converted data.\n\n    Example:\n        ```python\n        df = pl.DataFrame({\"a\": [1, 2], \"b\": [3, 4]})\n        table = to_pyarrow_table(df)\n        print(table.schema)\n        # a: int64\n        # b: int64\n        ```\n    \"\"\"\n    from fsspeckit.common.optional import (\n        _import_pandas,\n        _import_polars,\n        _import_pyarrow,\n    )\n    from fsspeckit.datasets.pyarrow import convert_large_types_to_normal\n\n    pl = _import_polars()\n    pd = _import_pandas()\n    pa = _import_pyarrow()\n\n    # Convert dict to DataFrame first\n    if isinstance(data, dict):\n        data = dict_to_dataframe(data)\n    if isinstance(data, list):\n        if isinstance(data[0], dict):\n            data = dict_to_dataframe(data, unique=unique)\n\n    # Ensure data is a list for uniform processing\n    if not isinstance(data, list):\n        data = [data]\n\n    # Collect lazy frames\n    if isinstance(data[0], pl.LazyFrame):\n        data = [dd.collect() for dd in data]\n\n    # Convert based on the first item's type\n    if isinstance(data[0], pl.DataFrame):\n        if concat:\n            data = pl.concat(data, how=\"diagonal_relaxed\")\n            if unique:\n                data = data.unique(\n                    subset=None if not isinstance(unique, (str, list)) else unique,\n                    maintain_order=True,\n                )\n            data = data.to_arrow()\n            data = data.cast(convert_large_types_to_normal(data.schema))\n        else:\n            data = [dd.to_arrow() for dd in data]\n            data = [dd.cast(convert_large_types_to_normal(dd.schema)) for dd in data]\n\n    elif isinstance(data[0], pd.DataFrame):\n        data = [pa.Table.from_pandas(dd, preserve_index=False) for dd in data]\n        if concat:\n            data = pa.concat_tables(data, promote_options=\"permissive\")\n            if unique:\n                data = (\n                    pl.from_arrow(data)\n                    .unique(\n                        subset=None if not isinstance(unique, (str, list)) else unique,\n                        maintain_order=True,\n                    )\n                    .to_arrow()\n                )\n                data = data.cast(convert_large_types_to_normal(data.schema))\n\n    elif isinstance(data[0], (pa.RecordBatch, Generator)):\n        if concat:\n            data = pa.Table.from_batches(data)\n            if unique:\n                data = (\n                    pl.from_arrow(data)\n                    .unique(\n                        subset=None if not isinstance(unique, (str, list)) else unique,\n                        maintain_order=True,\n                    )\n                    .to_arrow()\n                )\n                data = data.cast(convert_large_types_to_normal(data.schema))\n        else:\n            data = [pa.Table.from_batches([dd]) for dd in data]\n\n    return data\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.validate_columns","title":"fsspeckit.common.validate_columns","text":"<pre><code>validate_columns(\n    columns: list[str] | None, valid_columns: list[str]\n) -&gt; list[str] | None\n</code></pre> <p>Validate that requested columns exist in the schema.</p> <p>This is a helper to prevent column injection in SQL-like operations.</p> <p>Parameters:</p> Name Type Description Default <code>columns</code> <code>list[str] | None</code> <p>List of column names to validate, or None.</p> required <code>valid_columns</code> <code>list[str]</code> <p>List of valid column names from the schema.</p> required <p>Returns:</p> Type Description <code>list[str] | None</code> <p>The validated columns list, or None if columns was None.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any column is not in the valid set.</p> Source code in <code>src/fsspeckit/common/security.py</code> <pre><code>def validate_columns(columns: list[str] | None, valid_columns: list[str]) -&gt; list[str] | None:\n    \"\"\"Validate that requested columns exist in the schema.\n\n    This is a helper to prevent column injection in SQL-like operations.\n\n    Args:\n        columns: List of column names to validate, or None.\n        valid_columns: List of valid column names from the schema.\n\n    Returns:\n        The validated columns list, or None if columns was None.\n\n    Raises:\n        ValueError: If any column is not in the valid set.\n    \"\"\"\n    if columns is None:\n        return None\n\n    valid_set = set(valid_columns)\n    invalid = [col for col in columns if col not in valid_set]\n\n    if invalid:\n        raise ValueError(\n            f\"Invalid column(s): {', '.join(invalid)}. \"\n            f\"Valid columns are: {', '.join(sorted(valid_set))}\"\n        )\n\n    return columns\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.validate_compression_codec","title":"fsspeckit.common.validate_compression_codec","text":"<pre><code>validate_compression_codec(codec: str) -&gt; str\n</code></pre> <p>Validate that a compression codec is in the allowed set.</p> <p>This prevents injection of arbitrary values into SQL queries or filesystem operations that accept codec parameters.</p> <p>Parameters:</p> Name Type Description Default <code>codec</code> <code>str</code> <p>The compression codec name to validate.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The validated codec name (lowercased).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the codec is not in the allowed set.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; validate_compression_codec(\"snappy\")\n'snappy'\n</code></pre> <pre><code>&gt;&gt;&gt; validate_compression_codec(\"GZIP\")\n'gzip'\n</code></pre> <pre><code>&gt;&gt;&gt; validate_compression_codec(\"malicious; DROP TABLE\")\nValueError: Invalid compression codec\n</code></pre> Source code in <code>src/fsspeckit/common/security.py</code> <pre><code>def validate_compression_codec(codec: str) -&gt; str:\n    \"\"\"Validate that a compression codec is in the allowed set.\n\n    This prevents injection of arbitrary values into SQL queries or\n    filesystem operations that accept codec parameters.\n\n    Args:\n        codec: The compression codec name to validate.\n\n    Returns:\n        The validated codec name (lowercased).\n\n    Raises:\n        ValueError: If the codec is not in the allowed set.\n\n    Examples:\n        &gt;&gt;&gt; validate_compression_codec(\"snappy\")\n        'snappy'\n\n        &gt;&gt;&gt; validate_compression_codec(\"GZIP\")\n        'gzip'\n\n        &gt;&gt;&gt; validate_compression_codec(\"malicious; DROP TABLE\")\n        ValueError: Invalid compression codec\n    \"\"\"\n    if not codec or not isinstance(codec, str):\n        raise ValueError(\"Compression codec must be a non-empty string\")\n\n    normalized = codec.lower().strip()\n\n    if normalized not in VALID_COMPRESSION_CODECS:\n        valid_list = \", \".join(sorted(VALID_COMPRESSION_CODECS - {\"none\"}))\n        raise ValueError(\n            f\"Invalid compression codec: '{codec}'. \"\n            f\"Must be one of: {valid_list}\"\n        )\n\n    return normalized\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.validate_path","title":"fsspeckit.common.validate_path","text":"<pre><code>validate_path(\n    path: str, base_dir: str | None = None\n) -&gt; str\n</code></pre> <p>Validate a filesystem path for security issues.</p> <p>Checks for: - Embedded null bytes and control characters - Path traversal attempts (../ sequences escaping base_dir) - Empty or whitespace-only paths</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to validate.</p> required <code>base_dir</code> <code>str | None</code> <p>Optional base directory. If provided, the path must resolve to a location within this directory (prevents path traversal).</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The validated path (unchanged if valid).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the path contains forbidden characters, is empty, or escapes the base directory.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; validate_path(\"/data/file.parquet\")\n'/data/file.parquet'\n</code></pre> <pre><code>&gt;&gt;&gt; validate_path(\"../../../etc/passwd\", base_dir=\"/data\")\nValueError: Path escapes base directory\n</code></pre> <pre><code>&gt;&gt;&gt; validate_path(\"file\\x00.parquet\")\nValueError: Path contains forbidden characters\n</code></pre> Source code in <code>src/fsspeckit/common/security.py</code> <pre><code>def validate_path(path: str, base_dir: str | None = None) -&gt; str:\n    \"\"\"Validate a filesystem path for security issues.\n\n    Checks for:\n    - Embedded null bytes and control characters\n    - Path traversal attempts (../ sequences escaping base_dir)\n    - Empty or whitespace-only paths\n\n    Args:\n        path: The path to validate.\n        base_dir: Optional base directory. If provided, the path must resolve\n            to a location within this directory (prevents path traversal).\n\n    Returns:\n        The validated path (unchanged if valid).\n\n    Raises:\n        ValueError: If the path contains forbidden characters, is empty,\n            or escapes the base directory.\n\n    Examples:\n        &gt;&gt;&gt; validate_path(\"/data/file.parquet\")\n        '/data/file.parquet'\n\n        &gt;&gt;&gt; validate_path(\"../../../etc/passwd\", base_dir=\"/data\")\n        ValueError: Path escapes base directory\n\n        &gt;&gt;&gt; validate_path(\"file\\\\x00.parquet\")\n        ValueError: Path contains forbidden characters\n    \"\"\"\n    if not path or not path.strip():\n        raise ValueError(\"Path cannot be empty or whitespace-only\")\n\n    # Check for forbidden control characters\n    for char in path:\n        if char in _FORBIDDEN_PATH_CHARS:\n            raise ValueError(\n                f\"Path contains forbidden control character: {repr(char)}\"\n            )\n\n    # Check for path traversal when base_dir is specified\n    if base_dir is not None:\n        import os\n\n        # Normalize both paths for comparison\n        base_resolved = os.path.normpath(os.path.abspath(base_dir))\n\n        # Handle relative paths by joining with base\n        if not os.path.isabs(path):\n            full_path = os.path.join(base_dir, path)\n        else:\n            full_path = path\n\n        path_resolved = os.path.normpath(os.path.abspath(full_path))\n\n        # Check if resolved path starts with base directory\n        if not path_resolved.startswith(base_resolved + os.sep) and path_resolved != base_resolved:\n            raise ValueError(\n                f\"Path '{path}' escapes base directory '{base_dir}'\"\n            )\n\n    return path\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common-modules","title":"Modules","text":""},{"location":"api/fsspeckit.common/#fsspeckit.common.datetime","title":"fsspeckit.common.datetime","text":""},{"location":"api/fsspeckit.common/#fsspeckit.common.datetime-functions","title":"Functions","text":""},{"location":"api/fsspeckit.common/#fsspeckit.common.datetime.get_timedelta_str","title":"fsspeckit.common.datetime.get_timedelta_str","text":"<pre><code>get_timedelta_str(\n    timedelta_string: str, to: str = \"polars\"\n) -&gt; str\n</code></pre> <p>Convert timedelta strings between different formats.</p> <p>Converts timedelta strings between Polars and DuckDB formats, with graceful fallback for unknown units. Never raises errors for unknown units - instead returns a reasonable string representation.</p> <p>Parameters:</p> Name Type Description Default <code>timedelta_string</code> <code>str</code> <p>Input timedelta string (e.g., \"1h\", \"2d\", \"5invalid\").</p> required <code>to</code> <code>str</code> <p>Target format - \"polars\" or \"duckdb\". Defaults to \"polars\".</p> <code>'polars'</code> <p>Returns:</p> Type Description <code>str</code> <p>String in the target format. For unknown units, returns \"value unit\"</p> <code>str</code> <p>format without raising errors.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Valid Polars units\n&gt;&gt;&gt; get_timedelta_str(\"1h\", to=\"polars\")\n'1h'\n&gt;&gt;&gt; get_timedelta_str(\"1d\", to=\"polars\")\n'1d'\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Convert to DuckDB format\n&gt;&gt;&gt; get_timedelta_str(\"1h\", to=\"duckdb\")\n'1 hour'\n&gt;&gt;&gt; get_timedelta_str(\"1s\", to=\"duckdb\")\n'1 second'\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Unknown units - graceful fallback\n&gt;&gt;&gt; get_timedelta_str(\"1invalid\", to=\"polars\")\n'1 invalid'\n&gt;&gt;&gt; get_timedelta_str(\"5unknown\", to=\"duckdb\")\n'5 unknown'\n</code></pre> Source code in <code>src/fsspeckit/common/datetime.py</code> <pre><code>def get_timedelta_str(timedelta_string: str, to: str = \"polars\") -&gt; str:\n    \"\"\"Convert timedelta strings between different formats.\n\n    Converts timedelta strings between Polars and DuckDB formats, with graceful\n    fallback for unknown units. Never raises errors for unknown units - instead\n    returns a reasonable string representation.\n\n    Args:\n        timedelta_string: Input timedelta string (e.g., \"1h\", \"2d\", \"5invalid\").\n        to: Target format - \"polars\" or \"duckdb\". Defaults to \"polars\".\n\n    Returns:\n        String in the target format. For unknown units, returns \"value unit\"\n        format without raising errors.\n\n    Examples:\n        &gt;&gt;&gt; # Valid Polars units\n        &gt;&gt;&gt; get_timedelta_str(\"1h\", to=\"polars\")\n        '1h'\n        &gt;&gt;&gt; get_timedelta_str(\"1d\", to=\"polars\")\n        '1d'\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Convert to DuckDB format\n        &gt;&gt;&gt; get_timedelta_str(\"1h\", to=\"duckdb\")\n        '1 hour'\n        &gt;&gt;&gt; get_timedelta_str(\"1s\", to=\"duckdb\")\n        '1 second'\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Unknown units - graceful fallback\n        &gt;&gt;&gt; get_timedelta_str(\"1invalid\", to=\"polars\")\n        '1 invalid'\n        &gt;&gt;&gt; get_timedelta_str(\"5unknown\", to=\"duckdb\")\n        '5 unknown'\n    \"\"\"\n    polars_timedelta_units = [\n        \"ns\",\n        \"us\",\n        \"ms\",\n        \"s\",\n        \"m\",\n        \"h\",\n        \"d\",\n        \"w\",\n        \"mo\",\n        \"y\",\n    ]\n    duckdb_timedelta_units = [\n        \"nanosecond\",\n        \"microsecond\",\n        \"millisecond\",\n        \"second\",\n        \"minute\",\n        \"hour\",\n        \"day\",\n        \"week\",\n        \"month\",\n        \"year\",\n    ]\n\n    unit = re.sub(\"[0-9]\", \"\", timedelta_string).strip()\n    val = timedelta_string.replace(unit, \"\").strip()\n    base_unit = re.sub(\"s$\", \"\", unit)\n\n    if to == \"polars\":\n        # Already a valid Polars unit\n        if unit in polars_timedelta_units:\n            return timedelta_string\n        # Try to map known DuckDB units to Polars units\n        mapping = dict(zip(duckdb_timedelta_units, polars_timedelta_units))\n        target = mapping.get(base_unit)\n        if target is not None:\n            return f\"{val}{target}\"\n        # Fallback for unknown units: preserve value and unit as-is\n        return f\"{val} {unit}\".strip()\n\n    # DuckDB branch\n    if unit in polars_timedelta_units:\n        mapping = dict(zip(polars_timedelta_units, duckdb_timedelta_units))\n        return f\"{val} {mapping[unit]}\"\n\n    # Unknown Polars-style unit when targeting DuckDB: keep unit without trailing \"s\"\n    return f\"{val} {base_unit}\".strip()\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.datetime.get_timestamp_column","title":"fsspeckit.common.datetime.get_timestamp_column","text":"<pre><code>get_timestamp_column(df: Any) -&gt; Union[str, list[str]]\n</code></pre> <p>Get timestamp column names from a DataFrame or PyArrow Table.</p> <p>Automatically detects and normalizes different DataFrame types to work with pandas DataFrames, Polars DataFrames/LazyFrames, and PyArrow Tables.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>A Polars DataFrame/LazyFrame, PyArrow Table, or pandas DataFrame. The function automatically converts pandas DataFrames and PyArrow Tables to Polars LazyFrames for consistent timestamp detection.</p> required <p>Returns:</p> Type Description <code>Union[str, list[str]]</code> <p>List of strings containing timestamp column names. Returns an empty list</p> <code>Union[str, list[str]]</code> <p>if no timestamp columns are found.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import polars as pl\n&gt;&gt;&gt; import pyarrow as pa\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Works with pandas DataFrames\n&gt;&gt;&gt; df_pd = pd.DataFrame({\"ts\": pd.date_range(\"2023-01-01\", periods=3)})\n&gt;&gt;&gt; get_timestamp_column(df_pd)\n['ts']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Works with Polars DataFrames\n&gt;&gt;&gt; df_pl = pl.DataFrame({\"ts\": [datetime(2023, 1, 1)]})\n&gt;&gt;&gt; get_timestamp_column(df_pl)\n['ts']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Works with PyArrow Tables\n&gt;&gt;&gt; table = pa.table({\"ts\": pa.array([datetime(2023, 1, 1)])})\n&gt;&gt;&gt; get_timestamp_column(table)\n['ts']\n</code></pre> Source code in <code>src/fsspeckit/common/datetime.py</code> <pre><code>def get_timestamp_column(df: Any) -&gt; Union[str, list[str]]:\n    \"\"\"Get timestamp column names from a DataFrame or PyArrow Table.\n\n    Automatically detects and normalizes different DataFrame types to work with\n    pandas DataFrames, Polars DataFrames/LazyFrames, and PyArrow Tables.\n\n    Args:\n        df: A Polars DataFrame/LazyFrame, PyArrow Table, or pandas DataFrame.\n            The function automatically converts pandas DataFrames and PyArrow Tables\n            to Polars LazyFrames for consistent timestamp detection.\n\n    Returns:\n        List of strings containing timestamp column names. Returns an empty list\n        if no timestamp columns are found.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import polars as pl\n        &gt;&gt;&gt; import pyarrow as pa\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Works with pandas DataFrames\n        &gt;&gt;&gt; df_pd = pd.DataFrame({\"ts\": pd.date_range(\"2023-01-01\", periods=3)})\n        &gt;&gt;&gt; get_timestamp_column(df_pd)\n        ['ts']\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Works with Polars DataFrames\n        &gt;&gt;&gt; df_pl = pl.DataFrame({\"ts\": [datetime(2023, 1, 1)]})\n        &gt;&gt;&gt; get_timestamp_column(df_pl)\n        ['ts']\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Works with PyArrow Tables\n        &gt;&gt;&gt; table = pa.table({\"ts\": pa.array([datetime(2023, 1, 1)])})\n        &gt;&gt;&gt; get_timestamp_column(table)\n        ['ts']\n    \"\"\"\n    from fsspeckit.common.optional import (\n        _import_pandas,\n        _import_polars,\n        _import_pyarrow,\n    )\n\n    pl = _import_polars()\n    pa = _import_pyarrow()\n    pd = _import_pandas()\n\n    # Import polars.selectors at runtime\n    import polars.selectors as cs\n\n    # Normalise supported input types to a Polars LazyFrame\n    if isinstance(df, pa.Table):\n        df = pl.from_arrow(df).lazy()\n    elif isinstance(df, pd.DataFrame):\n        df = pl.from_pandas(df).lazy()\n\n    return df.select(cs.datetime() | cs.date()).collect_schema().names()\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.datetime.timestamp_from_string","title":"fsspeckit.common.datetime.timestamp_from_string  <code>cached</code>","text":"<pre><code>timestamp_from_string(\n    timestamp_str: str,\n    tz: Union[str, None] = None,\n    naive: bool = False,\n) -&gt; Union[datetime, date, time]\n</code></pre> <p>Converts a timestamp string (ISO 8601 format) into a datetime, date, or time object using only standard Python libraries.</p> <p>Handles strings with or without timezone information (e.g., '2023-01-01T10:00:00+02:00', '2023-01-01', '10:00:00'). Supports timezone offsets like '+HH:MM' or '+HHMM'. For named timezones (e.g., 'Europe/Paris'), requires Python 3.9+ and the 'tzdata' package to be installed.</p> <p>Parameters:</p> Name Type Description Default <code>timestamp_str</code> <code>str</code> <p>The string representation of the timestamp (ISO 8601 format).</p> required <code>tz</code> <code>str</code> <p>Target timezone identifier (e.g., 'UTC', '+02:00', 'Europe/Paris'). If provided, the output datetime/time will be localized or converted to this timezone. Defaults to None.</p> <code>None</code> <code>naive</code> <code>bool</code> <p>If True, return a naive datetime/time (no timezone info), even if the input string or <code>tz</code> parameter specifies one. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[datetime, date, time]</code> <p>Union[dt.datetime, dt.date, dt.time]: The parsed datetime, date, or time object.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the timestamp string format is invalid or the timezone is         invalid/unsupported.</p> Source code in <code>src/fsspeckit/common/datetime.py</code> <pre><code>@lru_cache(maxsize=128)\ndef timestamp_from_string(\n    timestamp_str: str,\n    tz: Union[str, None] = None,\n    naive: bool = False,\n) -&gt; Union[dt.datetime, dt.date, dt.time]:\n    \"\"\"\n    Converts a timestamp string (ISO 8601 format) into a datetime, date, or time object\n    using only standard Python libraries.\n\n    Handles strings with or without timezone information (e.g., '2023-01-01T10:00:00+02:00',\n    '2023-01-01', '10:00:00'). Supports timezone offsets like '+HH:MM' or '+HHMM'.\n    For named timezones (e.g., 'Europe/Paris'), requires Python 3.9+ and the 'tzdata'\n    package to be installed.\n\n    Args:\n        timestamp_str (str): The string representation of the timestamp (ISO 8601 format).\n        tz (str, optional): Target timezone identifier (e.g., 'UTC', '+02:00', 'Europe/Paris').\n            If provided, the output datetime/time will be localized or converted to this timezone.\n            Defaults to None.\n        naive (bool, optional): If True, return a naive datetime/time (no timezone info),\n            even if the input string or `tz` parameter specifies one. Defaults to False.\n\n    Returns:\n        Union[dt.datetime, dt.date, dt.time]: The parsed datetime, date, or time object.\n\n    Raises:\n        ValueError: If the timestamp string format is invalid or the timezone is\n                    invalid/unsupported.\n    \"\"\"\n\n    # Regex to parse timezone offsets like +HH:MM or +HHMM\n    _TZ_OFFSET_REGEX = re.compile(r\"([+-])(\\d{2}):?(\\d{2})\")\n\n    def _parse_tz_offset(tz_str: str) -&gt; dt.tzinfo | None:\n        \"\"\"Parses a timezone offset string into a timezone object.\"\"\"\n        match = _TZ_OFFSET_REGEX.fullmatch(tz_str)\n        if match:\n            sign, hours, minutes = match.groups()\n            offset_seconds = (int(hours) * 3600 + int(minutes) * 60) * (\n                -1 if sign == \"-\" else 1\n            )\n            if abs(offset_seconds) &gt;= 24 * 3600:\n                raise ValueError(f\"Invalid timezone offset: {tz_str}\")\n            return dt.timezone(dt.timedelta(seconds=offset_seconds), name=tz_str)\n        return None\n\n    def _get_tzinfo(tz_identifier: str | None) -&gt; dt.tzinfo | None:\n        \"\"\"Gets a tzinfo object from a string (offset or IANA name).\"\"\"\n        if tz_identifier is None:\n            return None\n        if tz_identifier.upper() == \"UTC\":\n            return dt.timezone.utc\n\n        # Try parsing as offset first\n        offset_tz = _parse_tz_offset(tz_identifier)\n        if offset_tz:\n            return offset_tz\n\n        # Try parsing as IANA name using zoneinfo (if available)\n        if ZoneInfo:\n            try:\n                return ZoneInfo(tz_identifier)\n            except ZoneInfoNotFoundError:\n                raise ValueError(\n                    f\"Timezone '{tz_identifier}' not found. Install 'tzdata' or use offset format.\"\n                )\n            except Exception as e:  # Catch other potential zoneinfo errors\n                raise ValueError(f\"Error loading timezone '{tz_identifier}': {e}\")\n        else:\n            # zoneinfo not available\n            raise ValueError(\n                f\"Invalid timezone: '{tz_identifier}'. Use offset format (e.g., '+02:00') \"\n                \"or run Python 3.9+ with 'tzdata' installed for named timezones.\"\n            )\n\n    target_tz: dt.tzinfo | None = _get_tzinfo(tz)\n    parsed_obj: dt.datetime | dt.date | dt.time | None = None\n\n    # Preprocess: Replace space separator, strip whitespace\n    processed_str = timestamp_str.strip().replace(\" \", \"T\")\n\n    # Attempt parsing (datetime, date, time) using fromisoformat\n    try:\n        # Python &lt; 3.11 fromisoformat has limitations (e.g., no Z, no +HHMM offset)\n        # This implementation assumes Python 3.11+ for full ISO 8601 support via fromisoformat\n        # or that input strings use formats compatible with older versions (e.g., +HH:MM)\n        parsed_obj = dt.datetime.fromisoformat(processed_str)\n    except ValueError:\n        try:\n            parsed_obj = dt.date.fromisoformat(processed_str)\n        except ValueError:\n            try:\n                # Time parsing needs care, especially with offsets in older Python\n                parsed_obj = dt.time.fromisoformat(processed_str)\n            except ValueError:\n                # Add fallback for simple HH:MM:SS if needed, though less robust\n                # try:\n                #     parsed_obj = dt.datetime.strptime(processed_str, \"%H:%M:%S\").time()\n                # except ValueError:\n                raise ValueError(f\"Invalid timestamp format: '{timestamp_str}'\")\n\n    # Apply timezone logic if we have a datetime or time object\n    if isinstance(parsed_obj, (dt.datetime, dt.time)):\n        is_aware = (\n            parsed_obj.tzinfo is not None\n            and parsed_obj.tzinfo.utcoffset(\n                parsed_obj if isinstance(parsed_obj, dt.datetime) else None\n            )\n            is not None\n        )\n\n        if target_tz:\n            if is_aware:\n                # Convert existing aware object to target timezone (only for datetime)\n                if isinstance(parsed_obj, dt.datetime):\n                    parsed_obj = parsed_obj.astimezone(target_tz)\n                # else: dt.time cannot be converted without a date context. Keep original tz.\n            else:\n                # Localize naive object to target timezone\n                parsed_obj = parsed_obj.replace(tzinfo=target_tz)\n            is_aware = True  # Object is now considered aware\n\n        # Handle naive flag: remove tzinfo if requested\n        if naive and is_aware:\n            parsed_obj = parsed_obj.replace(tzinfo=None)\n\n    # If it's a date object, tz/naive flags are ignored\n    elif isinstance(parsed_obj, dt.date):\n        pass\n\n    return parsed_obj\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.logging","title":"fsspeckit.common.logging","text":"<p>Logging configuration utilities for fsspeckit.</p>"},{"location":"api/fsspeckit.common/#fsspeckit.common.logging-functions","title":"Functions","text":""},{"location":"api/fsspeckit.common/#fsspeckit.common.logging.get_logger","title":"fsspeckit.common.logging.get_logger","text":"<pre><code>get_logger(name: str = 'fsspeckit') -&gt; logger\n</code></pre> <p>Get a logger instance for the given name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Logger name, typically the module name.</p> <code>'fsspeckit'</code> <p>Returns:</p> Type Description <code>logger</code> <p>Configured logger instance.</p> Example <pre><code>logger = get_logger(__name__)\nlogger.info(\"This is a log message\")\n</code></pre> Source code in <code>src/fsspeckit/common/logging.py</code> <pre><code>def get_logger(name: str = \"fsspeckit\") -&gt; \"logger\":\n    \"\"\"Get a logger instance for the given name.\n\n    Args:\n        name: Logger name, typically the module name.\n\n    Returns:\n        Configured logger instance.\n\n    Example:\n        ```python\n        logger = get_logger(__name__)\n        logger.info(\"This is a log message\")\n        ```\n    \"\"\"\n    return logger.bind(name=name)\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.logging.setup_logging","title":"fsspeckit.common.logging.setup_logging","text":"<pre><code>setup_logging(\n    level: Optional[str] = None,\n    disable: bool = False,\n    format_string: Optional[str] = None,\n) -&gt; None\n</code></pre> <p>Configure the Loguru logger for fsspeckit.</p> <p>Removes the default handler and adds a new one targeting stderr with customizable level and format.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>Optional[str]</code> <p>Log level (DEBUG, INFO, WARNING, ERROR, CRITICAL).    If None, uses fsspeckit_LOG_LEVEL environment variable    or defaults to \"INFO\".</p> <code>None</code> <code>disable</code> <code>bool</code> <p>Whether to disable logging for fsspeckit package.</p> <code>False</code> <code>format_string</code> <code>Optional[str]</code> <p>Custom format string for log messages.           If None, uses a default comprehensive format.</p> <code>None</code> Example <pre><code># Basic setup\nsetup_logging()\n\n# Custom level and format\nsetup_logging(level=\"DEBUG\", format_string=\"{time} | {level} | {message}\")\n\n# Disable logging\nsetup_logging(disable=True)\n</code></pre> Source code in <code>src/fsspeckit/common/logging.py</code> <pre><code>def setup_logging(\n    level: Optional[str] = None,\n    disable: bool = False,\n    format_string: Optional[str] = None,\n) -&gt; None:\n    \"\"\"Configure the Loguru logger for fsspeckit.\n\n    Removes the default handler and adds a new one targeting stderr\n    with customizable level and format.\n\n    Args:\n        level: Log level (DEBUG, INFO, WARNING, ERROR, CRITICAL).\n               If None, uses fsspeckit_LOG_LEVEL environment variable\n               or defaults to \"INFO\".\n        disable: Whether to disable logging for fsspeckit package.\n        format_string: Custom format string for log messages.\n                      If None, uses a default comprehensive format.\n\n    Example:\n        ```python\n        # Basic setup\n        setup_logging()\n\n        # Custom level and format\n        setup_logging(level=\"DEBUG\", format_string=\"{time} | {level} | {message}\")\n\n        # Disable logging\n        setup_logging(disable=True)\n        ```\n    \"\"\"\n    # Determine log level\n    if level is None:\n        level = os.getenv(\"fsspeckit_LOG_LEVEL\", \"INFO\")\n\n    # Default format if none provided\n    if format_string is None:\n        format_string = (\n            \"&lt;green&gt;{time:YYYY-MM-DD HH:mm:ss.SSS}&lt;/green&gt; | \"\n            \"&lt;level&gt;{level: &lt;8}&lt;/level&gt; | \"\n            \"&lt;cyan&gt;{name}&lt;/cyan&gt;:&lt;cyan&gt;{function}&lt;/cyan&gt;:&lt;cyan&gt;{line}&lt;/cyan&gt; - \"\n            \"&lt;level&gt;{message}&lt;/level&gt;\"\n        )\n\n    # Remove the default handler added by Loguru\n    logger.remove()\n\n    # Add new handler with custom configuration\n    logger.add(\n        sys.stderr,\n        level=level.upper(),\n        format=format_string,\n    )\n\n    # Optionally disable logging for this package\n    if disable:\n        logger.disable(\"fsspeckit\")\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.logging_config","title":"fsspeckit.common.logging_config","text":"<p>Logging configuration utilities for fsspeckit using Python's standard logging module.</p>"},{"location":"api/fsspeckit.common/#fsspeckit.common.logging_config-functions","title":"Functions","text":""},{"location":"api/fsspeckit.common/#fsspeckit.common.logging_config.get_logger","title":"fsspeckit.common.logging_config.get_logger","text":"<pre><code>get_logger(name: str) -&gt; Logger\n</code></pre> <p>Get a properly configured logger for a module.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Module name (typically name)</p> required <p>Returns:</p> Type Description <code>Logger</code> <p>Configured logger instance</p> Source code in <code>src/fsspeckit/common/logging_config.py</code> <pre><code>def get_logger(name: str) -&gt; logging.Logger:\n    \"\"\"\n    Get a properly configured logger for a module.\n\n    Args:\n        name: Module name (typically __name__)\n\n    Returns:\n        Configured logger instance\n    \"\"\"\n    # Auto-configure if not already done\n    if not _configured:\n        setup_logging()\n\n    return logging.getLogger(f\"fsspeckit.{name}\")\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.logging_config.setup_logging","title":"fsspeckit.common.logging_config.setup_logging","text":"<pre><code>setup_logging(\n    level: str = \"INFO\",\n    format_string: Optional[str] = None,\n    include_timestamp: bool = True,\n    enable_console: bool = True,\n    enable_file: bool = False,\n    file_path: Optional[str] = None,\n) -&gt; None\n</code></pre> <p>Configure logging for the fsspeckit package.</p> <p>This should be called once at application startup.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>str</code> <p>Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)</p> <code>'INFO'</code> <code>format_string</code> <code>Optional[str]</code> <p>Custom log format string</p> <code>None</code> <code>include_timestamp</code> <code>bool</code> <p>Whether to include timestamp in logs</p> <code>True</code> <code>enable_console</code> <code>bool</code> <p>Whether to output to console</p> <code>True</code> <code>enable_file</code> <code>bool</code> <p>Whether to output to file</p> <code>False</code> <code>file_path</code> <code>Optional[str]</code> <p>Path for log file output</p> <code>None</code> Source code in <code>src/fsspeckit/common/logging_config.py</code> <pre><code>def setup_logging(\n    level: str = \"INFO\",\n    format_string: Optional[str] = None,\n    include_timestamp: bool = True,\n    enable_console: bool = True,\n    enable_file: bool = False,\n    file_path: Optional[str] = None\n) -&gt; None:\n    \"\"\"\n    Configure logging for the fsspeckit package.\n\n    This should be called once at application startup.\n\n    Args:\n        level: Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)\n        format_string: Custom log format string\n        include_timestamp: Whether to include timestamp in logs\n        enable_console: Whether to output to console\n        enable_file: Whether to output to file\n        file_path: Path for log file output\n    \"\"\"\n    global _configured\n\n    if _configured:\n        return\n\n    # Parse level from environment if not provided\n    if not level:\n        level = os.getenv('FSSPECKIT_LOG_LEVEL', 'INFO')\n\n    # Set default format\n    if not format_string:\n        timestamp_part = \"%(asctime)s - \" if include_timestamp else \"\"\n        format_string = f\"{timestamp_part}%(name)s - %(levelname)s - %(message)s\"\n\n    # Configure root logger\n    root_logger = logging.getLogger('fsspeckit')\n    root_logger.setLevel(getattr(logging, level.upper()))\n\n    # Clear existing handlers\n    root_logger.handlers.clear()\n\n    # Console handler\n    if enable_console:\n        console_handler = logging.StreamHandler(sys.stdout)\n        console_handler.setFormatter(logging.Formatter(format_string))\n        root_logger.addHandler(console_handler)\n\n    # File handler\n    if enable_file and file_path:\n        file_handler = logging.FileHandler(file_path)\n        file_handler.setFormatter(logging.Formatter(format_string))\n        root_logger.addHandler(file_handler)\n\n    _configured = True\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.misc","title":"fsspeckit.common.misc","text":"<p>Miscellaneous utility functions for fsspeckit.</p>"},{"location":"api/fsspeckit.common/#fsspeckit.common.misc-functions","title":"Functions","text":""},{"location":"api/fsspeckit.common/#fsspeckit.common.misc.check_fs_identical","title":"fsspeckit.common.misc.check_fs_identical","text":"<pre><code>check_fs_identical(\n    fs1: AbstractFileSystem, fs2: AbstractFileSystem\n) -&gt; bool\n</code></pre> <p>Check if two fsspec filesystems are identical.</p> <p>Parameters:</p> Name Type Description Default <code>fs1</code> <code>AbstractFileSystem</code> <p>First filesystem (fsspec AbstractFileSystem)</p> required <code>fs2</code> <code>AbstractFileSystem</code> <p>Second filesystem (fsspec AbstractFileSystem)</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if filesystems are identical, False otherwise</p> Source code in <code>src/fsspeckit/common/misc.py</code> <pre><code>def check_fs_identical(fs1: AbstractFileSystem, fs2: AbstractFileSystem) -&gt; bool:\n    \"\"\"Check if two fsspec filesystems are identical.\n\n    Args:\n        fs1: First filesystem (fsspec AbstractFileSystem)\n        fs2: Second filesystem (fsspec AbstractFileSystem)\n\n    Returns:\n        bool: True if filesystems are identical, False otherwise\n    \"\"\"\n\n    def _get_root_fs(fs: AbstractFileSystem) -&gt; AbstractFileSystem:\n        while hasattr(fs, \"fs\"):\n            fs = fs.fs\n        return fs\n\n    fs1 = _get_root_fs(fs1)\n    fs2 = _get_root_fs(fs2)\n    return fs1 == fs2\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.misc.get_partitions_from_path","title":"fsspeckit.common.misc.get_partitions_from_path","text":"<pre><code>get_partitions_from_path(\n    path: str, partitioning: Union[str, list, None] = None\n) -&gt; Dict[str, str]\n</code></pre> <p>Extract dataset partitions from a file path.</p> <p>Parses file paths to extract partition information based on different partitioning schemes. By default, uses Hive-style partitioning.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>File path potentially containing partition information.</p> required <code>partitioning</code> <code>Union[str, list, None]</code> <p>Partitioning scheme: - \"hive\": Hive-style partitioning (key=value) - str: Single partition column name - list[str]: Multiple partition column names - None: Default to Hive-style partitioning</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dictionary mapping partition keys to their values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Default Hive-style partitioning\n&gt;&gt;&gt; get_partitions_from_path(\"data/year=2023/month=01/file.parquet\")\n{'year': '2023', 'month': '01'}\n</code></pre> <pre><code>&gt;&gt;&gt; # Explicit Hive-style partitioning\n&gt;&gt;&gt; get_partitions_from_path(\"data/year=2023/month=01/file.parquet\", \"hive\")\n{'year': '2023', 'month': '01'}\n</code></pre> <pre><code>&gt;&gt;&gt; # Single partition column\n&gt;&gt;&gt; get_partitions_from_path(\"data/2023/01/file.parquet\", \"year\")\n{'year': '2023'}\n</code></pre> <pre><code>&gt;&gt;&gt; # Multiple partition columns\n&gt;&gt;&gt; get_partitions_from_path(\"data/2023/01/file.parquet\", [\"year\", \"month\"])\n{'year': '2023', 'month': '01'}\n</code></pre> Source code in <code>src/fsspeckit/common/misc.py</code> <pre><code>def get_partitions_from_path(\n    path: str, partitioning: Union[str, list, None] = None\n) -&gt; Dict[str, str]:\n    \"\"\"Extract dataset partitions from a file path.\n\n    Parses file paths to extract partition information based on\n    different partitioning schemes. By default, uses Hive-style partitioning.\n\n    Args:\n        path: File path potentially containing partition information.\n        partitioning: Partitioning scheme:\n            - \"hive\": Hive-style partitioning (key=value)\n            - str: Single partition column name\n            - list[str]: Multiple partition column names\n            - None: Default to Hive-style partitioning\n\n    Returns:\n        Dictionary mapping partition keys to their values.\n\n    Examples:\n        &gt;&gt;&gt; # Default Hive-style partitioning\n        &gt;&gt;&gt; get_partitions_from_path(\"data/year=2023/month=01/file.parquet\")\n        {'year': '2023', 'month': '01'}\n\n        &gt;&gt;&gt; # Explicit Hive-style partitioning\n        &gt;&gt;&gt; get_partitions_from_path(\"data/year=2023/month=01/file.parquet\", \"hive\")\n        {'year': '2023', 'month': '01'}\n\n        &gt;&gt;&gt; # Single partition column\n        &gt;&gt;&gt; get_partitions_from_path(\"data/2023/01/file.parquet\", \"year\")\n        {'year': '2023'}\n\n        &gt;&gt;&gt; # Multiple partition columns\n        &gt;&gt;&gt; get_partitions_from_path(\"data/2023/01/file.parquet\", [\"year\", \"month\"])\n        {'year': '2023', 'month': '01'}\n    \"\"\"\n    # Normalize path to handle Windows and relative paths\n    normalized_path = Path(path).as_posix().replace(\"\\\\\", \"/\")\n\n    # Remove filename if present\n    if \".\" in normalized_path:\n        normalized_path = str(Path(normalized_path).parent)\n\n    parts = normalized_path.split(\"/\")\n\n    # Default to Hive-style partitioning when partitioning is None\n    if partitioning is None or partitioning == \"hive\":\n        partitions = {}\n        for part in parts:\n            if \"=\" in part:\n                key, value = part.split(\"=\", 1)  # Split only on first =\n                partitions[key] = value\n        return partitions\n    elif isinstance(partitioning, str):\n        # Single partition column\n        return {partitioning: parts[0]} if parts else {}\n    elif isinstance(partitioning, list):\n        # Multiple partition columns\n        result = {}\n        for i, col_name in enumerate(partitioning):\n            if i &lt; len(parts):\n                result[col_name] = parts[-(len(partitioning) - i)]\n        return result\n    else:\n        return {}\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.misc.path_to_glob","title":"fsspeckit.common.misc.path_to_glob","text":"<pre><code>path_to_glob(\n    path: str, format: Union[str, None] = None\n) -&gt; str\n</code></pre> <p>Convert a path to a glob pattern for file matching.</p> <p>Intelligently converts paths to glob patterns that match files of the specified format, handling various directory and wildcard patterns.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Base path to convert. Can include wildcards (* or ). Examples: \"data/\", \"data/*.json\", \"data/\"</p> required <code>format</code> <code>Union[str, None]</code> <p>File format to match (without dot). If None, inferred from path. Examples: \"json\", \"csv\", \"parquet\"</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Glob pattern that matches files of specified format. Examples: \"data/**/.json\", \"data/.csv\"</p> Example <pre><code># Basic directory\nprint(path_to_glob(\"data\", \"json\"))\n# 'data/**/*.json'\n\n# With wildcards\nprint(path_to_glob(\"data/**\", \"csv\"))\n# 'data/**/*.csv'\n\n# Format inference\nprint(path_to_glob(\"data/file.parquet\"))\n# 'data/file.parquet'\n</code></pre> Source code in <code>src/fsspeckit/common/misc.py</code> <pre><code>def path_to_glob(path: str, format: Union[str, None] = None) -&gt; str:\n    \"\"\"Convert a path to a glob pattern for file matching.\n\n    Intelligently converts paths to glob patterns that match files of the specified\n    format, handling various directory and wildcard patterns.\n\n    Args:\n        path: Base path to convert. Can include wildcards (* or **).\n            Examples: \"data/\", \"data/*.json\", \"data/**\"\n        format: File format to match (without dot). If None, inferred from path.\n            Examples: \"json\", \"csv\", \"parquet\"\n\n    Returns:\n        str: Glob pattern that matches files of specified format.\n            Examples: \"data/**/*.json\", \"data/*.csv\"\n\n    Example:\n        ```python\n        # Basic directory\n        print(path_to_glob(\"data\", \"json\"))\n        # 'data/**/*.json'\n\n        # With wildcards\n        print(path_to_glob(\"data/**\", \"csv\"))\n        # 'data/**/*.csv'\n\n        # Format inference\n        print(path_to_glob(\"data/file.parquet\"))\n        # 'data/file.parquet'\n        ```\n    \"\"\"\n    path = path.rstrip(\"/\")\n    if format is None:\n        if \".json\" in path:\n            format = \"json\"\n        elif \".csv\" in path:\n            format = \"csv\"\n        elif \".parquet\" in path:\n            format = \"parquet\"\n\n    if format is not None and format in path:\n        return path\n    else:\n        if path.endswith(\"**\"):\n            return posixpath.join(path, f\"*.{format}\")\n        elif path.endswith(\"*\"):\n            if path.endswith(\"*/*\"):\n                return path + f\".{format}\"\n            return posixpath.join(path.rstrip(\"/*\"), f\"*.{format}\")\n        return posixpath.join(path, f\"**/*.{format}\")\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.misc.sync_dir","title":"fsspeckit.common.misc.sync_dir","text":"<pre><code>sync_dir(\n    src_fs: AbstractFileSystem,\n    dst_fs: AbstractFileSystem,\n    src_path: str = \"\",\n    dst_path: str = \"\",\n    server_side: bool = True,\n    chunk_size: int = 8 * 1024 * 1024,\n    parallel: bool = False,\n    n_jobs: int = -1,\n    verbose: bool = True,\n) -&gt; dict[str, list[str]]\n</code></pre> <p>Sync two directories between different filesystems.</p> <p>Compares files in the source and destination directories, copies new or updated files from source to destination, and deletes stale files from destination.</p> <p>Parameters:</p> Name Type Description Default <code>src_fs</code> <code>AbstractFileSystem</code> <p>Source filesystem (fsspec AbstractFileSystem)</p> required <code>dst_fs</code> <code>AbstractFileSystem</code> <p>Destination filesystem (fsspec AbstractFileSystem)</p> required <code>src_path</code> <code>str</code> <p>Path in source filesystem to sync. Default is root ('').</p> <code>''</code> <code>dst_path</code> <code>str</code> <p>Path in destination filesystem to sync. Default is root ('').</p> <code>''</code> <code>chunk_size</code> <code>int</code> <p>Size of chunks to read/write files (in bytes). Default is 8MB.</p> <code>8 * 1024 * 1024</code> <code>parallel</code> <code>bool</code> <p>Whether to perform copy/delete operations in parallel. Default is False.</p> <code>False</code> <code>n_jobs</code> <code>int</code> <p>Number of parallel jobs if parallel=True. Default is -1 (all cores).</p> <code>-1</code> <code>verbose</code> <code>bool</code> <p>Whether to show progress bars. Default is True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, list[str]]</code> <p>Summary of added and deleted files</p> Source code in <code>src/fsspeckit/common/misc.py</code> <pre><code>def sync_dir(\n    src_fs: AbstractFileSystem,\n    dst_fs: AbstractFileSystem,\n    src_path: str = \"\",\n    dst_path: str = \"\",\n    server_side: bool = True,\n    chunk_size: int = 8 * 1024 * 1024,\n    parallel: bool = False,\n    n_jobs: int = -1,\n    verbose: bool = True,\n) -&gt; dict[str, list[str]]:\n    \"\"\"Sync two directories between different filesystems.\n\n    Compares files in the source and destination directories, copies new or updated files from source to destination,\n    and deletes stale files from destination.\n\n    Args:\n        src_fs: Source filesystem (fsspec AbstractFileSystem)\n        dst_fs: Destination filesystem (fsspec AbstractFileSystem)\n        src_path: Path in source filesystem to sync. Default is root ('').\n        dst_path: Path in destination filesystem to sync. Default is root ('').\n        chunk_size: Size of chunks to read/write files (in bytes). Default is 8MB.\n        parallel: Whether to perform copy/delete operations in parallel. Default is False.\n        n_jobs: Number of parallel jobs if parallel=True. Default is -1 (all cores).\n        verbose: Whether to show progress bars. Default is True.\n\n    Returns:\n        dict: Summary of added and deleted files\n    \"\"\"\n\n    src_mapper = src_fs.get_mapper(src_path)\n    dst_mapper = dst_fs.get_mapper(dst_path)\n\n    add_files = sorted(src_mapper.keys() - dst_mapper.keys())\n    delete_files = sorted(dst_mapper.keys() - src_mapper.keys())\n\n    return sync_files(\n        add_files=add_files,\n        delete_files=delete_files,\n        src_fs=src_fs,\n        dst_fs=dst_fs,\n        src_path=src_path,\n        dst_path=dst_path,\n        chunk_size=chunk_size,\n        server_side=server_side,\n        parallel=parallel,\n        n_jobs=n_jobs,\n        verbose=verbose,\n    )\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.misc.sync_files","title":"fsspeckit.common.misc.sync_files","text":"<pre><code>sync_files(\n    add_files: list[str],\n    delete_files: list[str],\n    src_fs: AbstractFileSystem,\n    dst_fs: AbstractFileSystem,\n    src_path: str = \"\",\n    dst_path: str = \"\",\n    server_side: bool = False,\n    chunk_size: int = 8 * 1024 * 1024,\n    parallel: bool = False,\n    n_jobs: int = -1,\n    verbose: bool = True,\n) -&gt; dict[str, list[str]]\n</code></pre> <p>Sync files between two filesystems by copying new files and deleting old ones.</p> <p>Parameters:</p> Name Type Description Default <code>add_files</code> <code>list[str]</code> <p>List of file paths to add (copy from source to destination)</p> required <code>delete_files</code> <code>list[str]</code> <p>List of file paths to delete from destination</p> required <code>src_fs</code> <code>AbstractFileSystem</code> <p>Source filesystem (fsspec AbstractFileSystem)</p> required <code>dst_fs</code> <code>AbstractFileSystem</code> <p>Destination filesystem (fsspec AbstractFileSystem)</p> required <code>src_path</code> <code>str</code> <p>Base path in source filesystem. Default is root ('').</p> <code>''</code> <code>dst_path</code> <code>str</code> <p>Base path in destination filesystem. Default is root ('').</p> <code>''</code> <code>server_side</code> <code>bool</code> <p>Whether to use server-side copy if supported. Default is False.</p> <code>False</code> <code>chunk_size</code> <code>int</code> <p>Size of chunks to read/write files (in bytes). Default is 8MB.</p> <code>8 * 1024 * 1024</code> <code>parallel</code> <code>bool</code> <p>Whether to perform copy/delete operations in parallel. Default is False.</p> <code>False</code> <code>n_jobs</code> <code>int</code> <p>Number of parallel jobs if parallel=True. Default is -1 (all cores).</p> <code>-1</code> <code>verbose</code> <code>bool</code> <p>Whether to show progress bars. Default is True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, list[str]]</code> <p>Summary of added and deleted files</p> Source code in <code>src/fsspeckit/common/misc.py</code> <pre><code>def sync_files(\n    add_files: list[str],\n    delete_files: list[str],\n    src_fs: AbstractFileSystem,\n    dst_fs: AbstractFileSystem,\n    src_path: str = \"\",\n    dst_path: str = \"\",\n    server_side: bool = False,\n    chunk_size: int = 8 * 1024 * 1024,\n    parallel: bool = False,\n    n_jobs: int = -1,\n    verbose: bool = True,\n) -&gt; dict[str, list[str]]:\n    \"\"\"Sync files between two filesystems by copying new files and deleting old ones.\n\n    Args:\n        add_files: List of file paths to add (copy from source to destination)\n        delete_files: List of file paths to delete from destination\n        src_fs: Source filesystem (fsspec AbstractFileSystem)\n        dst_fs: Destination filesystem (fsspec AbstractFileSystem)\n        src_path: Base path in source filesystem. Default is root ('').\n        dst_path: Base path in destination filesystem. Default is root ('').\n        server_side: Whether to use server-side copy if supported. Default is False.\n        chunk_size: Size of chunks to read/write files (in bytes). Default is 8MB.\n        parallel: Whether to perform copy/delete operations in parallel. Default is False.\n        n_jobs: Number of parallel jobs if parallel=True. Default is -1 (all cores).\n        verbose: Whether to show progress bars. Default is True.\n\n    Returns:\n        dict: Summary of added and deleted files\n    \"\"\"\n    CHUNK = chunk_size\n    RETRIES = 3\n\n    server_side = check_fs_identical(src_fs, dst_fs) and server_side\n\n    src_mapper = src_fs.get_mapper(src_path)\n    dst_mapper = dst_fs.get_mapper(dst_path)\n\n    def server_side_copy_file(key, src_mapper, dst_mapper, RETRIES):\n        last_exc = None\n        for attempt in range(1, RETRIES + 1):\n            try:\n                dst_mapper[key] = src_mapper[key]\n                break\n            except (OSError, IOError) as e:\n                last_exc = e\n                if attempt == RETRIES:\n                    logger.error(\n                        \"Failed to copy file %s after %d attempts: %s\",\n                        key,\n                        RETRIES,\n                        str(e),\n                        exc_info=True,\n                    )\n                    raise RuntimeError(f\"Failed to copy file {key} after {RETRIES} attempts\") from e\n            except Exception as e:\n                last_exc = e\n                if attempt == RETRIES:\n                    logger.error(\n                        \"Unexpected error copying file %s: %s\",\n                        key,\n                        str(e),\n                        exc_info=True,\n                    )\n                    raise RuntimeError(f\"Unexpected error copying file {key}: {e}\") from e\n\n    def copy_file(key, src_fs, dst_fs, src_path, dst_path, CHUNK, RETRIES):\n        last_exc = None\n        for attempt in range(1, RETRIES + 1):\n            try:\n                with (\n                    src_fs.open(posixpath.join(src_path, key), \"rb\") as r,\n                    dst_fs.open(posixpath.join(dst_path, key), \"wb\") as w,\n                ):\n                    while True:\n                        chunk = r.read(CHUNK)\n                        if not chunk:\n                            break\n                        w.write(chunk)\n                break\n            except (OSError, IOError) as e:\n                last_exc = e\n                if attempt == RETRIES:\n                    logger.error(\n                        \"Failed to copy file %s after %d attempts: %s\",\n                        key,\n                        RETRIES,\n                        str(e),\n                        exc_info=True,\n                    )\n                    raise RuntimeError(f\"Failed to copy file {key} after {RETRIES} attempts\") from e\n            except Exception as e:\n                last_exc = e\n                if attempt == RETRIES:\n                    logger.error(\n                        \"Unexpected error copying file %s: %s\",\n                        key,\n                        str(e),\n                        exc_info=True,\n                    )\n                    raise RuntimeError(f\"Unexpected error copying file {key}: {e}\") from e\n\n    def delete_file(key, dst_fs, dst_path, RETRIES):\n        last_exc = None\n        for attempt in range(1, RETRIES + 1):\n            try:\n                dst_fs.rm(posixpath.join(dst_path, key))\n                break\n            except (OSError, IOError) as e:\n                last_exc = e\n                if attempt == RETRIES:\n                    logger.error(\n                        \"Failed to delete file %s after %d attempts: %s\",\n                        key,\n                        RETRIES,\n                        str(e),\n                        exc_info=True,\n                    )\n                    raise RuntimeError(f\"Failed to delete file {key} after {RETRIES} attempts\") from e\n            except Exception as e:\n                last_exc = e\n                if attempt == RETRIES:\n                    logger.error(\n                        \"Unexpected error deleting file %s: %s\",\n                        key,\n                        str(e),\n                        exc_info=True,\n                    )\n                    raise RuntimeError(f\"Unexpected error deleting file {key}: {e}\") from e\n\n    if len(add_files):\n        # Copy new files\n        if parallel:\n            if server_side:\n                try:\n                    run_parallel(\n                        server_side_copy_file,\n                        add_files,\n                        src_mapper=src_mapper,\n                        dst_mapper=dst_mapper,\n                        RETRIES=RETRIES,\n                        n_jobs=n_jobs,\n                        verbose=verbose,\n                    )\n                except (RuntimeError, OSError) as e:\n                    logger.warning(\n                        \"Server-side copy failed for some files, falling back to client-side: %s\",\n                        str(e),\n                    )\n                    # Fallback to client-side copy if server-side fails\n                    run_parallel(\n                        copy_file,\n                        add_files,\n                        src_fs=src_fs,\n                        dst_fs=dst_fs,\n                        src_path=src_path,\n                        dst_path=dst_path,\n                        CHUNK=CHUNK,\n                        RETRIES=RETRIES,\n                        n_jobs=n_jobs,\n                        verbose=verbose,\n                    )\n\n            else:\n                run_parallel(\n                    copy_file,\n                    add_files,\n                    src_fs=src_fs,\n                    dst_fs=dst_fs,\n                    src_path=src_path,\n                    dst_path=dst_path,\n                    CHUNK=CHUNK,\n                    RETRIES=RETRIES,\n                    n_jobs=n_jobs,\n                    verbose=verbose,\n                )\n        else:\n            if verbose:\n                from rich.progress import track\n\n                for key in track(\n                    add_files,\n                    description=\"Copying new files...\",\n                    total=len(add_files),\n                ):\n                    if server_side:\n                        try:\n                            server_side_copy_file(key, src_mapper, dst_mapper, RETRIES)\n                        except (RuntimeError, OSError):\n                            copy_file(\n                                key, src_fs, dst_fs, src_path, dst_path, CHUNK, RETRIES\n                            )\n                    else:\n                        copy_file(\n                            key, src_fs, dst_fs, src_path, dst_path, CHUNK, RETRIES\n                        )\n            else:\n                for key in add_files:\n                    if server_side:\n                        try:\n                            server_side_copy_file(key, src_mapper, dst_mapper, RETRIES)\n                        except (RuntimeError, OSError):\n                            copy_file(\n                                key, src_fs, dst_fs, src_path, dst_path, CHUNK, RETRIES\n                            )\n                    else:\n                        copy_file(\n                            key, src_fs, dst_fs, src_path, dst_path, CHUNK, RETRIES\n                        )\n\n    if len(delete_files):\n        # Delete old files from destination\n        if parallel:\n            run_parallel(\n                delete_file,\n                delete_files,\n                dst_fs=dst_fs,\n                dst_path=dst_path,\n                RETRIES=RETRIES,\n                n_jobs=n_jobs,\n                verbose=verbose,\n            )\n        else:\n            if verbose:\n                from rich.progress import track\n\n                for key in track(\n                    delete_files,\n                    description=\"Deleting stale files...\",\n                    total=len(delete_files),\n                ):\n                    delete_file(key, dst_fs, dst_path, RETRIES)\n            else:\n                for key in delete_files:\n                    delete_file(key, dst_fs, dst_path, RETRIES)\n\n    return {\"added_files\": add_files, \"deleted_files\": delete_files}\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.optional","title":"fsspeckit.common.optional","text":"<p>Optional dependency management utilities.</p> <p>This module provides utilities for managing optional dependencies in fsspeckit, implementing lazy loading patterns that allow core functionality to work without requiring all optional dependencies to be installed.</p>"},{"location":"api/fsspeckit.common/#fsspeckit.common.optional-functions","title":"Functions","text":""},{"location":"api/fsspeckit.common/#fsspeckit.common.optional.check_optional_dependency","title":"fsspeckit.common.optional.check_optional_dependency","text":"<pre><code>check_optional_dependency(\n    package_name: str, feature_name: Optional[str] = None\n) -&gt; None\n</code></pre> <p>Check if an optional dependency is available and raise helpful error if not.</p> <p>Parameters:</p> Name Type Description Default <code>package_name</code> <code>str</code> <p>Name of the required package</p> required <code>feature_name</code> <code>Optional[str]</code> <p>Name of the feature that requires this package (optional)</p> <code>None</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>If the package is not available</p> Source code in <code>src/fsspeckit/common/optional.py</code> <pre><code>def check_optional_dependency(\n    package_name: str, feature_name: Optional[str] = None\n) -&gt; None:\n    \"\"\"Check if an optional dependency is available and raise helpful error if not.\n\n    Args:\n        package_name: Name of the required package\n        feature_name: Name of the feature that requires this package (optional)\n\n    Raises:\n        ImportError: If the package is not available\n    \"\"\"\n    if not importlib.util.find_spec(package_name):\n        extra = _get_install_extra(package_name)\n        feature_msg = f\" for {feature_name}\" if feature_name else \"\"\n        raise ImportError(\n            f\"{package_name} is required{feature_msg}. \"\n            f\"Install with: pip install fsspeckit[{extra}]\"\n        )\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.partitions","title":"fsspeckit.common.partitions","text":"<p>Shared partition utilities for fsspeckit.</p> <p>This module provides canonical implementations for partition parsing and related operations across all backends. It consolidates partition-related logic that was previously scattered across different modules.</p> <p>Key responsibilities: 1. Partition extraction from file paths 2. Support for multiple partitioning schemes (Hive, directory-based) 3. Partition validation and normalization 4. Path manipulation for partitioned datasets</p> <p>Architecture: - Functions are designed to work with string paths and fsspec filesystems - Support for common partitioning patterns used in data lakes - Consistent behavior across all backends - Extensible design for custom partitioning schemes</p> <p>Usage: Backend implementations should delegate to this module rather than implementing their own partition parsing logic. This ensures consistent behavior across DuckDB, PyArrow, and future backends.</p>"},{"location":"api/fsspeckit.common/#fsspeckit.common.partitions-functions","title":"Functions","text":""},{"location":"api/fsspeckit.common/#fsspeckit.common.partitions.apply_partition_pruning","title":"fsspeckit.common.partitions.apply_partition_pruning","text":"<pre><code>apply_partition_pruning(\n    paths: list[str],\n    partition_filters: dict[str, Any],\n    partitioning: str | list[str] | None = None,\n) -&gt; list[str]\n</code></pre> <p>Apply partition pruning to reduce the set of files to scan.</p> <p>This is an optimization that eliminates files based on partition values before any I/O operations.</p> <p>Parameters:</p> Name Type Description Default <code>paths</code> <code>list[str]</code> <p>List of all file paths.</p> required <code>partition_filters</code> <code>dict[str, Any]</code> <p>Dictionary of partition filters to apply.</p> required <code>partitioning</code> <code>str | list[str] | None</code> <p>Partitioning scheme.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>Pruned list of paths.</p> Source code in <code>src/fsspeckit/common/partitions.py</code> <pre><code>def apply_partition_pruning(\n    paths: list[str],\n    partition_filters: dict[str, Any],\n    partitioning: str | list[str] | None = None,\n) -&gt; list[str]:\n    \"\"\"\n    Apply partition pruning to reduce the set of files to scan.\n\n    This is an optimization that eliminates files based on partition\n    values before any I/O operations.\n\n    Args:\n        paths: List of all file paths.\n        partition_filters: Dictionary of partition filters to apply.\n        partitioning: Partitioning scheme.\n\n    Returns:\n        Pruned list of paths.\n    \"\"\"\n    if not partition_filters:\n        return paths\n\n    return filter_paths_by_partitions(paths, partition_filters, partitioning)\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.partitions.build_partition_path","title":"fsspeckit.common.partitions.build_partition_path","text":"<pre><code>build_partition_path(\n    base_path: str,\n    partitions: list[tuple[str, str]],\n    partitioning: str = \"hive\",\n) -&gt; str\n</code></pre> <p>Build a file path with partition directories.</p> <p>Parameters:</p> Name Type Description Default <code>base_path</code> <code>str</code> <p>Base directory path.</p> required <code>partitions</code> <code>list[tuple[str, str]]</code> <p>List of (column, value) tuples.</p> required <code>partitioning</code> <code>str</code> <p>Partitioning scheme (\"hive\" or \"directory\").</p> <code>'hive'</code> <p>Returns:</p> Type Description <code>str</code> <p>Path string with partition directories.</p> Source code in <code>src/fsspeckit/common/partitions.py</code> <pre><code>def build_partition_path(\n    base_path: str, partitions: list[tuple[str, str]], partitioning: str = \"hive\"\n) -&gt; str:\n    \"\"\"\n    Build a file path with partition directories.\n\n    Args:\n        base_path: Base directory path.\n        partitions: List of (column, value) tuples.\n        partitioning: Partitioning scheme (\"hive\" or \"directory\").\n\n    Returns:\n        Path string with partition directories.\n    \"\"\"\n    if not partitions:\n        return base_path\n\n    if partitioning == \"hive\":\n        # Hive-style: column=value/column=value\n        partition_dirs = [f\"{col}={val}\" for col, val in partitions]\n    else:\n        # Directory-style: value/value (order matters)\n        partition_dirs = [val for _, val in partitions]\n\n    return \"/\".join([base_path.rstrip(\"/\")] + partition_dirs)\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.partitions.create_partition_expression","title":"fsspeckit.common.partitions.create_partition_expression","text":"<pre><code>create_partition_expression(\n    partitions: list[tuple[str, str]],\n    backend: str = \"pyarrow\",\n) -&gt; Any\n</code></pre> <p>Create a partition filter expression for different backends.</p> <p>Parameters:</p> Name Type Description Default <code>partitions</code> <code>list[tuple[str, str]]</code> <p>List of (column, value) tuples.</p> required <code>backend</code> <code>str</code> <p>Target backend (\"pyarrow\", \"duckdb\").</p> <code>'pyarrow'</code> <p>Returns:</p> Type Description <code>Any</code> <p>Backend-specific filter expression.</p> Source code in <code>src/fsspeckit/common/partitions.py</code> <pre><code>def create_partition_expression(\n    partitions: list[tuple[str, str]], backend: str = \"pyarrow\"\n) -&gt; Any:\n    \"\"\"\n    Create a partition filter expression for different backends.\n\n    Args:\n        partitions: List of (column, value) tuples.\n        backend: Target backend (\"pyarrow\", \"duckdb\").\n\n    Returns:\n        Backend-specific filter expression.\n    \"\"\"\n    if not partitions:\n        return None\n\n    if backend == \"pyarrow\":\n        import pyarrow.dataset as ds\n\n        # Build PyArrow dataset filter expression\n        expressions = []\n        for col, val in partitions:\n            expressions.append(ds.field(col) == val)\n\n        # Combine with AND logic\n        result = expressions[0]\n        for expr in expressions[1:]:\n            result = result &amp; expr\n        return result\n\n    elif backend == \"duckdb\":\n        # Build DuckDB WHERE clause\n        conditions = []\n        for col, val in partitions:\n            if isinstance(val, str):\n                conditions.append(f\"\\\"{col}\\\" = '{val}'\")\n            else:\n                conditions.append(f'\"{col}\" = {val}')\n\n        return \" AND \".join(conditions)\n\n    else:\n        raise ValueError(f\"Unsupported backend: {backend}\")\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.partitions.extract_partition_filters","title":"fsspeckit.common.partitions.extract_partition_filters","text":"<pre><code>extract_partition_filters(\n    paths: list[str],\n    partitioning: str | list[str] | None = None,\n) -&gt; dict[str, set[str]]\n</code></pre> <p>Extract unique partition values from a list of paths.</p> <p>Parameters:</p> Name Type Description Default <code>paths</code> <code>list[str]</code> <p>List of file paths.</p> required <code>partitioning</code> <code>str | list[str] | None</code> <p>Partitioning scheme.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, set[str]]</code> <p>Dictionary mapping column names to sets of unique values.</p> Source code in <code>src/fsspeckit/common/partitions.py</code> <pre><code>def extract_partition_filters(\n    paths: list[str], partitioning: str | list[str] | None = None\n) -&gt; dict[str, set[str]]:\n    \"\"\"\n    Extract unique partition values from a list of paths.\n\n    Args:\n        paths: List of file paths.\n        partitioning: Partitioning scheme.\n\n    Returns:\n        Dictionary mapping column names to sets of unique values.\n    \"\"\"\n    partition_values = {}\n\n    for path in paths:\n        partitions = get_partitions_from_path(path, partitioning)\n        for col, val in partitions:\n            if col not in partition_values:\n                partition_values[col] = set()\n            partition_values[col].add(val)\n\n    return partition_values\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.partitions.filter_paths_by_partitions","title":"fsspeckit.common.partitions.filter_paths_by_partitions","text":"<pre><code>filter_paths_by_partitions(\n    paths: list[str],\n    partition_filters: dict[str, str | list[str]],\n    partitioning: str | list[str] | None = None,\n) -&gt; list[str]\n</code></pre> <p>Filter paths based on partition values.</p> <p>Parameters:</p> Name Type Description Default <code>paths</code> <code>list[str]</code> <p>List of file paths to filter.</p> required <code>partition_filters</code> <code>dict[str, str | list[str]]</code> <p>Dictionary mapping column names to filter values.</p> required <code>partitioning</code> <code>str | list[str] | None</code> <p>Partitioning scheme.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>Filtered list of paths.</p> Source code in <code>src/fsspeckit/common/partitions.py</code> <pre><code>def filter_paths_by_partitions(\n    paths: list[str],\n    partition_filters: dict[str, str | list[str]],\n    partitioning: str | list[str] | None = None,\n) -&gt; list[str]:\n    \"\"\"\n    Filter paths based on partition values.\n\n    Args:\n        paths: List of file paths to filter.\n        partition_filters: Dictionary mapping column names to filter values.\n        partitioning: Partitioning scheme.\n\n    Returns:\n        Filtered list of paths.\n    \"\"\"\n    filtered_paths = []\n\n    for path in paths:\n        partitions = dict(get_partitions_from_path(path, partitioning))\n\n        # Check if path matches all filters\n        matches = True\n        for col, filter_val in partition_filters.items():\n            if col not in partitions:\n                matches = False\n                break\n\n            path_val = partitions[col]\n\n            # Handle list of allowed values\n            if isinstance(filter_val, list):\n                if path_val not in filter_val:\n                    matches = False\n                    break\n            else:\n                # Single value comparison\n                if path_val != filter_val:\n                    matches = False\n                    break\n\n        if matches:\n            filtered_paths.append(path)\n\n    return filtered_paths\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.partitions.get_partition_columns_from_paths","title":"fsspeckit.common.partitions.get_partition_columns_from_paths","text":"<pre><code>get_partition_columns_from_paths(\n    paths: list[str],\n    partitioning: str | list[str] | None = None,\n) -&gt; list[str]\n</code></pre> <p>Get all unique partition column names from a list of paths.</p> <p>Parameters:</p> Name Type Description Default <code>paths</code> <code>list[str]</code> <p>List of file paths.</p> required <code>partitioning</code> <code>str | list[str] | None</code> <p>Partitioning scheme.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of unique partition column names.</p> Source code in <code>src/fsspeckit/common/partitions.py</code> <pre><code>def get_partition_columns_from_paths(\n    paths: list[str], partitioning: str | list[str] | None = None\n) -&gt; list[str]:\n    \"\"\"\n    Get all unique partition column names from a list of paths.\n\n    Args:\n        paths: List of file paths.\n        partitioning: Partitioning scheme.\n\n    Returns:\n        List of unique partition column names.\n    \"\"\"\n    columns = set()\n\n    for path in paths:\n        partitions = get_partitions_from_path(path, partitioning)\n        for col, _ in partitions:\n            columns.add(col)\n\n    return sorted(list(columns))\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.partitions.get_partitions_from_path","title":"fsspeckit.common.partitions.get_partitions_from_path","text":"<pre><code>get_partitions_from_path(\n    path: str,\n    partitioning: Union[str, list[str], None] = None,\n) -&gt; list[tuple]\n</code></pre> <p>Extract dataset partitions from a file path.</p> <p>Parses file paths to extract partition information based on different partitioning schemes. This is the canonical implementation used across all fsspeckit backends.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>File path potentially containing partition information.</p> required <code>partitioning</code> <code>Union[str, list[str], None]</code> <p>Partitioning scheme: - \"hive\": Hive-style partitioning (key=value) - str: Single partition column name - list[str]: Multiple partition column names - None: Return empty list</p> <code>None</code> <p>Returns:</p> Type Description <code>list[tuple]</code> <p>List of tuples containing (column, value) pairs.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Hive-style partitioning\n&gt;&gt;&gt; get_partitions_from_path(\"data/year=2023/month=01/file.parquet\", \"hive\")\n[('year', '2023'), ('month', '01')]\n</code></pre> <pre><code>&gt;&gt;&gt; # Single partition column\n&gt;&gt;&gt; get_partitions_from_path(\"data/2023/01/file.parquet\", \"year\")\n[('year', '2023')]\n</code></pre> <pre><code>&gt;&gt;&gt; # Multiple partition columns\n&gt;&gt;&gt; get_partitions_from_path(\"data/2023/01/file.parquet\", [\"year\", \"month\"])\n[('year', '2023'), ('month', '01')]\n</code></pre> <pre><code>&gt;&gt;&gt; # No partitioning\n&gt;&gt;&gt; get_partitions_from_path(\"data/file.parquet\", None)\n[]\n</code></pre> Source code in <code>src/fsspeckit/common/partitions.py</code> <pre><code>def get_partitions_from_path(\n    path: str, partitioning: Union[str, list[str], None] = None\n) -&gt; list[tuple]:\n    \"\"\"\n    Extract dataset partitions from a file path.\n\n    Parses file paths to extract partition information based on\n    different partitioning schemes. This is the canonical implementation\n    used across all fsspeckit backends.\n\n    Args:\n        path: File path potentially containing partition information.\n        partitioning: Partitioning scheme:\n            - \"hive\": Hive-style partitioning (key=value)\n            - str: Single partition column name\n            - list[str]: Multiple partition column names\n            - None: Return empty list\n\n    Returns:\n        List of tuples containing (column, value) pairs.\n\n    Examples:\n        &gt;&gt;&gt; # Hive-style partitioning\n        &gt;&gt;&gt; get_partitions_from_path(\"data/year=2023/month=01/file.parquet\", \"hive\")\n        [('year', '2023'), ('month', '01')]\n\n        &gt;&gt;&gt; # Single partition column\n        &gt;&gt;&gt; get_partitions_from_path(\"data/2023/01/file.parquet\", \"year\")\n        [('year', '2023')]\n\n        &gt;&gt;&gt; # Multiple partition columns\n        &gt;&gt;&gt; get_partitions_from_path(\"data/2023/01/file.parquet\", [\"year\", \"month\"])\n        [('year', '2023'), ('month', '01')]\n\n        &gt;&gt;&gt; # No partitioning\n        &gt;&gt;&gt; get_partitions_from_path(\"data/file.parquet\", None)\n        []\n    \"\"\"\n    if \".\" in path:\n        path = os.path.dirname(path)\n\n    parts = path.split(\"/\")\n\n    if isinstance(partitioning, str):\n        if partitioning == \"hive\":\n            return [tuple(p.split(\"=\")) for p in parts if \"=\" in p]\n        else:\n            # Single partition column - take the first directory that looks like a value\n            # This is a simple heuristic for cases like data/2023/file.parquet\n            if parts:\n                return [(partitioning, parts[0])]\n            return []\n    elif isinstance(partitioning, list):\n        # Multiple partition columns - map column names to path parts from right to left\n        if not parts:\n            return []\n\n        # Take the last N parts where N is the number of partition columns\n        partition_parts = (\n            parts[-len(partitioning) :] if len(parts) &gt;= len(partitioning) else parts\n        )\n        return list(zip(partitioning, partition_parts))\n    else:\n        return []\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.partitions.infer_partitioning_scheme","title":"fsspeckit.common.partitions.infer_partitioning_scheme","text":"<pre><code>infer_partitioning_scheme(\n    paths: list[str], max_samples: int = 100\n) -&gt; dict[str, Any]\n</code></pre> <p>Infer the partitioning scheme from a sample of paths.</p> <p>Parameters:</p> Name Type Description Default <code>paths</code> <code>list[str]</code> <p>List of file paths to analyze.</p> required <code>max_samples</code> <code>int</code> <p>Maximum number of paths to sample.</p> <code>100</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with inferred scheme information.</p> Source code in <code>src/fsspeckit/common/partitions.py</code> <pre><code>def infer_partitioning_scheme(\n    paths: list[str], max_samples: int = 100\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Infer the partitioning scheme from a sample of paths.\n\n    Args:\n        paths: List of file paths to analyze.\n        max_samples: Maximum number of paths to sample.\n\n    Returns:\n        Dictionary with inferred scheme information.\n    \"\"\"\n    if not paths:\n        return {\"scheme\": None, \"confidence\": 0.0}\n\n    # Sample paths for analysis\n    sample_paths = paths[:max_samples] if len(paths) &gt; max_samples else paths\n\n    # Check for Hive-style partitioning\n    hive_partitions = []\n    directory_partitions = []\n\n    for path in sample_paths:\n        # Remove filename and get directory parts\n        dir_path = os.path.dirname(path)\n        parts = dir_path.split(\"/\")\n\n        # Look for key=value patterns\n        hive_parts = [p for p in parts if \"=\" in p and p.split(\"=\")[0].strip()]\n        if hive_parts:\n            hive_partitions.append(len(hive_parts))\n\n        # Look for directory-style partitions (numeric dates, etc.)\n        dir_parts = [\n            p\n            for p in parts\n            if p.replace(\"/\", \"\").replace(\"-\", \"\").replace(\"_\", \"\").isdigit()\n        ]\n        if dir_parts:\n            directory_partitions.append(len(dir_parts))\n\n    # Determine most likely scheme\n    result = {\"scheme\": None, \"confidence\": 0.0}\n\n    if hive_partitions:\n        avg_hive_parts = sum(hive_partitions) / len(hive_partitions)\n        if avg_hive_parts &gt;= 1:  # At least 1 partition level on average\n            result[\"scheme\"] = \"hive\"\n            result[\"confidence\"] = min(\n                1.0, avg_hive_parts / 3.0\n            )  # Normalize by expected max\n            result[\"avg_partitions\"] = avg_hive_parts\n            return result\n\n    if directory_partitions:\n        avg_dir_parts = sum(directory_partitions) / len(directory_partitions)\n        if avg_dir_parts &gt;= 1:  # At least 1 partition level on average\n            result[\"scheme\"] = \"directory\"\n            result[\"confidence\"] = min(\n                1.0, avg_dir_parts / 3.0\n            )  # Normalize by expected max\n            result[\"avg_partitions\"] = avg_dir_parts\n            return result\n\n    # No clear partitioning detected\n    return result\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.partitions.normalize_partition_value","title":"fsspeckit.common.partitions.normalize_partition_value","text":"<pre><code>normalize_partition_value(value: str) -&gt; str\n</code></pre> <p>Normalize a partition value for consistent comparison.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>Raw partition value from path.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Normalized partition value.</p> Source code in <code>src/fsspeckit/common/partitions.py</code> <pre><code>def normalize_partition_value(value: str) -&gt; str:\n    \"\"\"\n    Normalize a partition value for consistent comparison.\n\n    Args:\n        value: Raw partition value from path.\n\n    Returns:\n        Normalized partition value.\n    \"\"\"\n    return value.strip().strip(\"\\\"'\").replace(\"\\\\\", \"\")\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.partitions.validate_partition_columns","title":"fsspeckit.common.partitions.validate_partition_columns","text":"<pre><code>validate_partition_columns(\n    partitions: list[tuple[str, str]],\n    expected_columns: list[str] | None = None,\n) -&gt; bool\n</code></pre> <p>Validate partition columns against expected schema.</p> <p>Parameters:</p> Name Type Description Default <code>partitions</code> <code>list[tuple[str, str]]</code> <p>List of (column, value) tuples.</p> required <code>expected_columns</code> <code>list[str] | None</code> <p>Optional list of expected column names.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if partitions are valid, False otherwise.</p> Source code in <code>src/fsspeckit/common/partitions.py</code> <pre><code>def validate_partition_columns(\n    partitions: list[tuple[str, str]], expected_columns: list[str] | None = None\n) -&gt; bool:\n    \"\"\"\n    Validate partition columns against expected schema.\n\n    Args:\n        partitions: List of (column, value) tuples.\n        expected_columns: Optional list of expected column names.\n\n    Returns:\n        True if partitions are valid, False otherwise.\n    \"\"\"\n    if not partitions:\n        return True\n\n    if expected_columns is not None:\n        partition_columns = {col for col, _ in partitions}\n        expected_set = set(expected_columns)\n\n        # Check if all partition columns are expected\n        if not partition_columns.issubset(expected_set):\n            return False\n\n        # Check if all expected columns are present (if strict validation needed)\n        # This is optional - some datasets might have missing partitions\n        # return partition_columns == expected_set\n\n    # Validate that no column names are empty\n    for col, val in partitions:\n        if not col or not col.strip():\n            return False\n\n    return True\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.polars","title":"fsspeckit.common.polars","text":""},{"location":"api/fsspeckit.common/#fsspeckit.common.polars-functions","title":"Functions","text":""},{"location":"api/fsspeckit.common/#fsspeckit.common.polars.drop_null_columns","title":"fsspeckit.common.polars.drop_null_columns","text":"<pre><code>drop_null_columns(\n    df: DataFrame | LazyFrame,\n) -&gt; DataFrame | LazyFrame\n</code></pre> <p>Remove columns with all null values from the DataFrame.</p> Source code in <code>src/fsspeckit/common/polars.py</code> <pre><code>def drop_null_columns(df: pl.DataFrame | pl.LazyFrame) -&gt; pl.DataFrame | pl.LazyFrame:\n    \"\"\"Remove columns with all null values from the DataFrame.\"\"\"\n    return df.select([col for col in df.columns if df[col].null_count() &lt; df.height])\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.polars.opt_dtype","title":"fsspeckit.common.polars.opt_dtype","text":"<pre><code>opt_dtype(\n    df: DataFrame,\n    include: str | list[str] | None = None,\n    exclude: str | list[str] | None = None,\n    time_zone: str | None = None,\n    shrink_numerics: bool = False,\n    allow_unsigned: bool = True,\n    allow_null: bool = True,\n    sample_size: int | None = 1024,\n    sample_method: SampleMethod = \"first\",\n    strict: bool = False,\n    *,\n    force_timezone: str | None = None,\n) -&gt; DataFrame\n</code></pre> <p>Optimize data types of a Polars DataFrame for performance and memory efficiency.</p> <p>This function analyzes each column and converts it to the most appropriate data type based on content, handling string-to-type conversions and numeric type downcasting.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The Polars DataFrame to optimize.</p> required <code>include</code> <code>str | list[str] | None</code> <p>Column(s) to include in optimization (default: all columns).</p> <code>None</code> <code>exclude</code> <code>str | list[str] | None</code> <p>Column(s) to exclude from optimization.</p> <code>None</code> <code>time_zone</code> <code>str | None</code> <p>Optional time zone hint during datetime parsing.</p> <code>None</code> <code>shrink_numerics</code> <code>bool</code> <p>Whether to downcast numeric types when possible.</p> <code>False</code> <code>allow_unsigned</code> <code>bool</code> <p>Whether to allow unsigned integer types.</p> <code>True</code> <code>allow_null</code> <code>bool</code> <p>Whether to allow columns with all null values to be cast to Null type.</p> <code>True</code> <code>sample_size</code> <code>int | None</code> <p>Maximum number of cleaned values to inspect for regex-based inference. Use None to inspect the entire column.</p> <code>1024</code> <code>sample_method</code> <code>SampleMethod</code> <p>Which subset to inspect (<code>\"first\"</code> or <code>\"random\"</code>).</p> <code>'first'</code> <code>strict</code> <code>bool</code> <p>If True, will raise an error if any column cannot be optimized.</p> <code>False</code> <code>force_timezone</code> <code>str | None</code> <p>If set, ensure all parsed datetime columns end up with this timezone.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with optimized data types.</p> Source code in <code>src/fsspeckit/common/polars.py</code> <pre><code>def opt_dtype(\n    df: pl.DataFrame,\n    include: str | list[str] | None = None,\n    exclude: str | list[str] | None = None,\n    time_zone: str | None = None,\n    shrink_numerics: bool = False,\n    allow_unsigned: bool = True,\n    allow_null: bool = True,\n    sample_size: int | None = 1024,\n    sample_method: SampleMethod = \"first\",\n    strict: bool = False,\n    *,\n    force_timezone: str | None = None,\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Optimize data types of a Polars DataFrame for performance and memory efficiency.\n\n    This function analyzes each column and converts it to the most appropriate\n    data type based on content, handling string-to-type conversions and\n    numeric type downcasting.\n\n    Args:\n        df: The Polars DataFrame to optimize.\n        include: Column(s) to include in optimization (default: all columns).\n        exclude: Column(s) to exclude from optimization.\n        time_zone: Optional time zone hint during datetime parsing.\n        shrink_numerics: Whether to downcast numeric types when possible.\n        allow_unsigned: Whether to allow unsigned integer types.\n        allow_null: Whether to allow columns with all null values to be cast to Null type.\n        sample_size: Maximum number of cleaned values to inspect for regex-based inference. Use None to inspect the entire column.\n        sample_method: Which subset to inspect (`\"first\"` or `\"random\"`).\n        strict: If True, will raise an error if any column cannot be optimized.\n        force_timezone: If set, ensure all parsed datetime columns end up with this timezone.\n\n    Returns:\n        DataFrame with optimized data types.\n    \"\"\"\n    if sample_method not in (\"first\", \"random\"):\n        raise ValueError(\"sample_method must be 'first' or 'random'\")\n\n    if isinstance(df, pl.LazyFrame):\n        return opt_dtype(\n            df.collect(),\n            include=include,\n            exclude=exclude,\n            time_zone=time_zone,\n            shrink_numerics=shrink_numerics,\n            allow_unsigned=allow_unsigned,\n            allow_null=allow_null,\n            sample_size=sample_size,\n            sample_method=sample_method,\n            strict=strict,\n            force_timezone=force_timezone,\n        ).lazy()\n\n    # Normalize include/exclude parameters\n    if isinstance(include, str):\n        include = [include]\n    if isinstance(exclude, str):\n        exclude = [exclude]\n\n    # Determine columns to process\n    cols_to_process = df.columns\n    if include:\n        cols_to_process = [col for col in include if col in df.columns]\n    if exclude:\n        cols_to_process = [col for col in cols_to_process if col not in exclude]\n\n    # Generate optimization expressions for all columns\n    expressions = []\n    for col_name in cols_to_process:\n        try:\n            expressions.append(\n                _get_column_expr(\n                    df,\n                    col_name,\n                    shrink_numerics,\n                    allow_unsigned,\n                    allow_null,\n                    time_zone,\n                    force_timezone,\n                    sample_size,\n                    sample_method,\n                    strict,\n                )\n            )\n        except Exception as e:\n            if strict:\n                raise e\n            # If strict mode is off, just keep the original column\n            continue\n\n    # Apply all transformations at once if any exist\n    return df if not expressions else df.with_columns(expressions)\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.schema","title":"fsspeckit.common.schema","text":"<p>Shared schema utilities for fsspeckit.</p> <p>This module provides canonical implementations for schema compatibility, unification, timezone handling, and type optimization across all backends. It consolidates schema-related logic that was previously scattered across dataset-specific modules.</p> <p>Key responsibilities: 1. Schema unification with intelligent conflict resolution 2. Timezone standardization and detection 3. Large type conversion to standard types 4. Schema casting and validation 5. Data type optimization 6. Empty column handling</p> <p>Architecture: - Functions are designed to work with PyArrow schemas and tables - All operations preserve metadata when possible - Timezone handling supports multiple strategies (auto, explicit, removal) - Type optimization includes safety checks and fallback strategies - Schema unification uses multiple fallback strategies for maximum compatibility</p> <p>Usage: Backend implementations should delegate to this module rather than implementing their own schema logic. This ensures consistent behavior across DuckDB, PyArrow, and future backends.</p>"},{"location":"api/fsspeckit.common/#fsspeckit.common.schema-functions","title":"Functions","text":""},{"location":"api/fsspeckit.common/#fsspeckit.common.schema.cast_schema","title":"fsspeckit.common.schema.cast_schema","text":"<pre><code>cast_schema(table: Table, schema: Schema) -&gt; Table\n</code></pre> <p>Cast a PyArrow table to a given schema, updating the schema to match the table's columns.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>Table</code> <p>The PyArrow table to cast.</p> required <code>schema</code> <code>Schema</code> <p>The target schema to cast table to.</p> required <p>Returns:</p> Type Description <code>Table</code> <p>pa.Table: A new PyArrow table with the specified schema.</p> Source code in <code>src/fsspeckit/common/schema.py</code> <pre><code>def cast_schema(table: pa.Table, schema: pa.Schema) -&gt; pa.Table:\n    \"\"\"\n    Cast a PyArrow table to a given schema, updating the schema to match the table's columns.\n\n    Args:\n        table (pa.Table): The PyArrow table to cast.\n        schema (pa.Schema): The target schema to cast table to.\n\n    Returns:\n        pa.Table: A new PyArrow table with the specified schema.\n    \"\"\"\n    table_columns = set(table.schema.names)\n    for field in schema:\n        if field.name not in table_columns:\n            table = table.append_column(\n                field.name, pa.nulls(table.num_rows, type=field.type)\n            )\n    return table.cast(schema)\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.schema.convert_large_types_to_normal","title":"fsspeckit.common.schema.convert_large_types_to_normal","text":"<pre><code>convert_large_types_to_normal(schema: Schema) -&gt; Schema\n</code></pre> <p>Convert large types in a PyArrow schema to their standard types.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>Schema</code> <p>The PyArrow schema to convert.</p> required <p>Returns:</p> Type Description <code>Schema</code> <p>pa.Schema: A new PyArrow schema with large types converted to standard types.</p> Source code in <code>src/fsspeckit/common/schema.py</code> <pre><code>def convert_large_types_to_normal(schema: pa.Schema) -&gt; pa.Schema:\n    \"\"\"\n    Convert large types in a PyArrow schema to their standard types.\n\n    Args:\n        schema (pa.Schema): The PyArrow schema to convert.\n\n    Returns:\n        pa.Schema: A new PyArrow schema with large types converted to standard types.\n    \"\"\"\n    # Define mapping of large types to standard types\n    type_mapping = {\n        pa.large_string(): pa.string(),\n        pa.large_binary(): pa.binary(),\n        pa.large_utf8(): pa.utf8(),\n        pa.large_list(pa.null()): pa.list_(pa.null()),\n        pa.large_list_view(pa.null()): pa.list_view(pa.null()),\n    }\n    # Convert fields\n    new_fields = []\n    for field in schema:\n        field_type = field.type\n        # Check if type exists in mapping\n        if field_type in type_mapping:\n            new_field = pa.field(\n                name=field.name,\n                type=type_mapping[field_type],\n                nullable=field.nullable,\n                metadata=field.metadata,\n            )\n            new_fields.append(new_field)\n        # Handle large lists with nested types\n        elif isinstance(field_type, pa.LargeListType):\n            new_field = pa.field(\n                name=field.name,\n                type=pa.list_(\n                    type_mapping[field_type.value_type]\n                    if field_type.value_type in type_mapping\n                    else field_type.value_type\n                ),\n                nullable=field.nullable,\n                metadata=field.metadata,\n            )\n            new_fields.append(new_field)\n        # Handle dictionary with large_string, large_utf8, or large_binary values\n        elif isinstance(field_type, pa.DictionaryType):\n            new_field = pa.field(\n                name=field.name,\n                type=pa.dictionary(\n                    field_type.index_type,\n                    type_mapping[field_type.value_type]\n                    if field_type.value_type in type_mapping\n                    else field_type.value_type,\n                    field_type.ordered,\n                ),\n                metadata=field.metadata,\n            )\n            new_fields.append(new_field)\n        else:\n            new_fields.append(field)\n\n    return pa.schema(new_fields)\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.schema.dominant_timezone_per_column","title":"fsspeckit.common.schema.dominant_timezone_per_column","text":"<pre><code>dominant_timezone_per_column(\n    schemas: list[Schema],\n) -&gt; dict[str, tuple[str | None, str | None]]\n</code></pre> <p>For each timestamp column (by name) across all schemas, detect the most frequent timezone (including None). If None and a timezone are tied, prefer the timezone. Returns a dict: {column_name: dominant_timezone}</p> Source code in <code>src/fsspeckit/common/schema.py</code> <pre><code>def dominant_timezone_per_column(\n    schemas: list[pa.Schema],\n) -&gt; dict[str, tuple[str | None, str | None]]:\n    \"\"\"\n    For each timestamp column (by name) across all schemas, detect the most frequent timezone (including None).\n    If None and a timezone are tied, prefer the timezone.\n    Returns a dict: {column_name: dominant_timezone}\n    \"\"\"\n    tz_counts: defaultdict[str, Counter[str | None]] = defaultdict(Counter)\n    units: dict[str, str | None] = {}\n\n    for schema in schemas:\n        for field in schema:\n            if pa.types.is_timestamp(field.type):\n                tz = field.type.tz\n                name = field.name\n                tz_counts[name][tz] += 1\n                # Track unit for each column (assume consistent)\n                if name not in units:\n                    units[name] = field.type.unit\n\n    dominant = {}\n    for name, counter in tz_counts.items():\n        most_common = counter.most_common()\n        if not most_common:\n            continue\n        top_count = most_common[0][1]\n        # Find all with top_count\n        top_tzs = [tz for tz, cnt in most_common if cnt == top_count]\n        # If tie and one is not None, prefer not-None\n        if len(top_tzs) &gt; 1 and any(tz is not None for tz in top_tzs):\n            tz = next(tz for tz in top_tzs if tz is not None)\n        else:\n            tz = most_common[0][0]\n        dominant[name] = (units[name], tz)\n    return dominant\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.schema.remove_empty_columns","title":"fsspeckit.common.schema.remove_empty_columns","text":"<pre><code>remove_empty_columns(table: Table) -&gt; Table\n</code></pre> <p>Remove columns that are entirely empty from a PyArrow table.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>Table</code> <p>The PyArrow table to process.</p> required <p>Returns:</p> Type Description <code>Table</code> <p>pa.Table: A new PyArrow table with empty columns removed.</p> Source code in <code>src/fsspeckit/common/schema.py</code> <pre><code>def remove_empty_columns(table: pa.Table) -&gt; pa.Table:\n    \"\"\"Remove columns that are entirely empty from a PyArrow table.\n\n    Args:\n        table (pa.Table): The PyArrow table to process.\n\n    Returns:\n        pa.Table: A new PyArrow table with empty columns removed.\n    \"\"\"\n    if table.num_rows == 0:\n        return table\n\n    empty_cols = []\n    for col_name in table.column_names:\n        column = table.column(col_name)\n        if column.null_count == table.num_rows:\n            empty_cols.append(col_name)\n\n    if not empty_cols:\n        return table\n    return table.drop(empty_cols)\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.schema.standardize_schema_timezones","title":"fsspeckit.common.schema.standardize_schema_timezones","text":"<pre><code>standardize_schema_timezones(\n    schemas: Schema | list[Schema],\n    timezone: str | None = None,\n) -&gt; Schema | list[Schema]\n</code></pre> <p>Standardize timezone info for all timestamp columns in a list of PyArrow schemas.</p> <p>Parameters:</p> Name Type Description Default <code>schemas</code> <code>list of pa.Schema</code> <p>List of PyArrow schemas.</p> required <code>timezone</code> <code>str or None</code> <p>If None, remove timezone from all timestamp columns.                     If str, set this timezone for all timestamp columns.                     If \"auto\", use the most frequent timezone across schemas.</p> <code>None</code> <p>Returns:</p> Type Description <code>Schema | list[Schema]</code> <p>list of pa.Schema: New schemas with standardized timezone info.</p> Source code in <code>src/fsspeckit/common/schema.py</code> <pre><code>def standardize_schema_timezones(\n    schemas: pa.Schema | list[pa.Schema], timezone: str | None = None\n) -&gt; pa.Schema | list[pa.Schema]:\n    \"\"\"\n    Standardize timezone info for all timestamp columns in a list of PyArrow schemas.\n\n    Args:\n        schemas (list of pa.Schema): List of PyArrow schemas.\n        timezone (str or None): If None, remove timezone from all timestamp columns.\n                                If str, set this timezone for all timestamp columns.\n                                If \"auto\", use the most frequent timezone across schemas.\n\n    Returns:\n        list of pa.Schema: New schemas with standardized timezone info.\n    \"\"\"\n    if isinstance(schemas, pa.Schema):\n        single_input = True\n        schema_list: list[pa.Schema] = [schemas]\n    else:\n        single_input = False\n        schema_list = list(schemas)\n    if timezone == \"auto\":\n        majority_schema = standardize_schema_timezones_by_majority(schema_list)\n        result_list = [majority_schema for _ in schema_list]\n        return majority_schema if single_input else result_list\n    new_schemas = []\n    for schema in schema_list:\n        fields = []\n        for field in schema:\n            if pa.types.is_timestamp(field.type):\n                fields.append(\n                    pa.field(\n                        field.name,\n                        pa.timestamp(field.type.unit, timezone),\n                        field.nullable,\n                        field.metadata,\n                    )\n                )\n            else:\n                fields.append(field)\n        new_schemas.append(pa.schema(fields, schema.metadata))\n    return new_schemas[0] if single_input else new_schemas\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.schema.standardize_schema_timezones_by_majority","title":"fsspeckit.common.schema.standardize_schema_timezones_by_majority","text":"<pre><code>standardize_schema_timezones_by_majority(\n    schemas: list[Schema],\n) -&gt; Schema\n</code></pre> <p>For each timestamp column (by name) across all schemas, set the timezone to the most frequent (with tie-breaking). Returns a new list of schemas with updated timestamp timezones.</p> Source code in <code>src/fsspeckit/common/schema.py</code> <pre><code>def standardize_schema_timezones_by_majority(\n    schemas: list[pa.Schema],\n) -&gt; pa.Schema:\n    \"\"\"\n    For each timestamp column (by name) across all schemas, set the timezone to the most frequent (with tie-breaking).\n    Returns a new list of schemas with updated timestamp timezones.\n    \"\"\"\n    dom = dominant_timezone_per_column(schemas)\n    if not schemas:\n        return pa.schema([])\n\n    seen: set[str] = set()\n    fields: list[pa.Field] = []\n    for schema in schemas:\n        for field in schema:\n            if field.name in seen:\n                continue\n            seen.add(field.name)\n            if pa.types.is_timestamp(field.type) and field.name in dom:\n                unit, tz = dom[field.name]\n                fields.append(\n                    pa.field(\n                        field.name,\n                        pa.timestamp(unit, tz),\n                        field.nullable,\n                        field.metadata,\n                    )\n                )\n            else:\n                fields.append(field)\n    return pa.schema(fields, schemas[0].metadata)\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.schema.unify_schemas","title":"fsspeckit.common.schema.unify_schemas","text":"<pre><code>unify_schemas(\n    schemas: list[Schema],\n    use_large_dtypes: bool = False,\n    timezone: str | None = None,\n    standardize_timezones: bool = True,\n    verbose: bool = False,\n    remove_conflicting_columns: bool = False,\n) -&gt; Schema\n</code></pre> <p>Unify a list of PyArrow schemas into a single schema using intelligent conflict resolution.</p> <p>Parameters:</p> Name Type Description Default <code>schemas</code> <code>list[Schema]</code> <p>List of PyArrow schemas to unify.</p> required <code>use_large_dtypes</code> <code>bool</code> <p>If True, keep large types like large_string.</p> <code>False</code> <code>timezone</code> <code>str | None</code> <p>If specified, standardize all timestamp columns to this timezone. If \"auto\", use the most frequent timezone across schemas. If None, remove timezone from all timestamp columns.</p> <code>None</code> <code>standardize_timezones</code> <code>bool</code> <p>If True, standardize all timestamp columns to most frequent timezone.</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>If True, print conflict resolution details for debugging.</p> <code>False</code> <code>remove_conflicting_columns</code> <code>bool</code> <p>If True, allows removal of columns with type conflicts as a fallback strategy instead of converting them. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Schema</code> <p>pa.Schema: A unified PyArrow schema.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no schemas are provided.</p> Source code in <code>src/fsspeckit/common/schema.py</code> <pre><code>def unify_schemas(\n    schemas: list[pa.Schema],\n    use_large_dtypes: bool = False,\n    timezone: str | None = None,\n    standardize_timezones: bool = True,\n    verbose: bool = False,\n    remove_conflicting_columns: bool = False,\n) -&gt; pa.Schema:\n    \"\"\"\n    Unify a list of PyArrow schemas into a single schema using intelligent conflict resolution.\n\n    Args:\n        schemas (list[pa.Schema]): List of PyArrow schemas to unify.\n        use_large_dtypes (bool): If True, keep large types like large_string.\n        timezone (str | None): If specified, standardize all timestamp columns to this timezone.\n            If \"auto\", use the most frequent timezone across schemas.\n            If None, remove timezone from all timestamp columns.\n        standardize_timezones (bool): If True, standardize all timestamp columns to most frequent timezone.\n        verbose (bool): If True, print conflict resolution details for debugging.\n        remove_conflicting_columns (bool): If True, allows removal of columns with type conflicts as a fallback\n            strategy instead of converting them. Defaults to False.\n\n    Returns:\n        pa.Schema: A unified PyArrow schema.\n\n    Raises:\n        ValueError: If no schemas are provided.\n    \"\"\"\n    if not schemas:\n        raise ValueError(\"At least one schema must be provided for unification\")\n\n    # Early exit for single schema\n    unique_schemas = _unique_schemas(schemas)\n    if len(unique_schemas) == 1:\n        result_schema = unique_schemas[0]\n        if standardize_timezones:\n            result_schema = standardize_schema_timezones([result_schema], timezone)[0]\n        return (\n            result_schema\n            if use_large_dtypes\n            else convert_large_types_to_normal(result_schema)\n        )\n\n    # Step 1: Find and resolve conflicts first\n    conflicts = _find_conflicting_fields(unique_schemas)\n    if conflicts and verbose:\n        _log_conflict_summary(conflicts, verbose)\n\n    if conflicts:\n        # Normalize schemas using intelligent promotion rules\n        unique_schemas = _normalize_schema_types(unique_schemas, conflicts)\n\n    # Step 2: Attempt unification with conflict-resolved schemas\n    try:\n        unified_schema = pa.unify_schemas(unique_schemas, promote_options=\"permissive\")\n\n        # Step 3: Apply timezone standardization to the unified result\n        if standardize_timezones:\n            unified_schema = standardize_schema_timezones([unified_schema], timezone)[0]\n\n        return (\n            unified_schema\n            if use_large_dtypes\n            else convert_large_types_to_normal(unified_schema)\n        )\n\n    except (pa.ArrowInvalid, pa.ArrowTypeError) as e:\n        # Step 4: Intelligent fallback strategies\n        if verbose:\n            logger.debug(\"Primary unification failed: %s\", e)\n            logger.debug(\"Attempting fallback strategies...\")\n\n        # Fallback 1: Try aggressive string conversion for remaining conflicts\n        try:\n            fallback_schema = _aggressive_fallback_unification(unique_schemas)\n            if standardize_timezones:\n                fallback_schema = standardize_schema_timezones(\n                    [fallback_schema], timezone\n                )[0]\n            if verbose:\n                logger.debug(\"\u2713 Aggressive fallback succeeded\")\n            return (\n                fallback_schema\n                if use_large_dtypes\n                else convert_large_types_to_normal(fallback_schema)\n            )\n\n        except (pa.ArrowInvalid, pa.ArrowTypeError, ValueError) as e:\n            if verbose:\n                logger.debug(\"\u2717 Aggressive fallback failed: %s\", str(e), exc_info=True)\n\n        # Fallback 2: Remove conflicting fields (if enabled)\n        if remove_conflicting_columns:\n            try:\n                non_conflicting_schema = _remove_conflicting_fields(unique_schemas)\n                if standardize_timezones:\n                    non_conflicting_schema = standardize_schema_timezones(\n                        [non_conflicting_schema], timezone\n                    )[0]\n                if verbose:\n                    logger.debug(\"\u2713 Remove conflicting fields fallback succeeded\")\n                return (\n                    non_conflicting_schema\n                    if use_large_dtypes\n                    else convert_large_types_to_normal(non_conflicting_schema)\n                )\n\n            except (pa.ArrowInvalid, pa.ArrowTypeError, ValueError) as e:\n                if verbose:\n                    logger.debug(\"\u2717 Remove conflicting fields fallback failed: %s\", str(e), exc_info=True)\n\n        # Fallback 3: Remove problematic fields that can't be unified\n        try:\n            minimal_schema = _remove_problematic_fields(unique_schemas)\n            if standardize_timezones:\n                minimal_schema = standardize_schema_timezones(\n                    [minimal_schema], timezone\n                )[0]\n            if verbose:\n                logger.debug(\"\u2713 Minimal schema (removed problematic fields) succeeded\")\n            return (\n                minimal_schema\n                if use_large_dtypes\n                else convert_large_types_to_normal(minimal_schema)\n            )\n\n        except (pa.ArrowInvalid, pa.ArrowTypeError, ValueError) as e:\n            if verbose:\n                logger.debug(\"\u2717 Minimal schema fallback failed: %s\", str(e), exc_info=True)\n\n        # Fallback 4: Return first schema as last resort\n        if verbose:\n            logger.debug(\"\u2717 All fallback strategies failed, returning first schema\")\n\n        first_schema = unique_schemas[0]\n        if standardize_timezones:\n            first_schema = standardize_schema_timezones([first_schema], timezone)[0]\n        return (\n            first_schema\n            if use_large_dtypes\n            else convert_large_types_to_normal(first_schema)\n        )\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.security","title":"fsspeckit.common.security","text":"<p>Security validation helpers for fsspeckit.</p> <p>This module provides basic validation for paths, codecs, and credential scrubbing to prevent common security issues like path traversal and credential leakage in logs.</p>"},{"location":"api/fsspeckit.common/#fsspeckit.common.security-functions","title":"Functions","text":""},{"location":"api/fsspeckit.common/#fsspeckit.common.security.safe_format_error","title":"fsspeckit.common.security.safe_format_error","text":"<pre><code>safe_format_error(\n    operation: str,\n    path: str | None = None,\n    error: BaseException | None = None,\n    **context: Any,\n) -&gt; str\n</code></pre> <p>Format an error message with credentials scrubbed.</p> <p>Parameters:</p> Name Type Description Default <code>operation</code> <code>str</code> <p>Description of the operation that failed.</p> required <code>path</code> <code>str | None</code> <p>Optional path involved in the operation.</p> <code>None</code> <code>error</code> <code>BaseException | None</code> <p>Optional exception that occurred.</p> <code>None</code> <code>**context</code> <code>Any</code> <p>Additional context key-value pairs.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>A formatted, credential-scrubbed error message.</p> Source code in <code>src/fsspeckit/common/security.py</code> <pre><code>def safe_format_error(\n    operation: str,\n    path: str | None = None,\n    error: BaseException | None = None,\n    **context: Any,\n) -&gt; str:\n    \"\"\"Format an error message with credentials scrubbed.\n\n    Args:\n        operation: Description of the operation that failed.\n        path: Optional path involved in the operation.\n        error: Optional exception that occurred.\n        **context: Additional context key-value pairs.\n\n    Returns:\n        A formatted, credential-scrubbed error message.\n    \"\"\"\n    parts = [f\"Failed to {operation}\"]\n\n    if path:\n        parts.append(f\"at '{path}'\")\n\n    if error:\n        parts.append(f\": {scrub_exception(error)}\")\n\n    if context:\n        context_str = \", \".join(f\"{k}={scrub_credentials(str(v))}\" for k, v in context.items())\n        parts.append(f\" ({context_str})\")\n\n    return \" \".join(parts)\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.security.scrub_credentials","title":"fsspeckit.common.security.scrub_credentials","text":"<pre><code>scrub_credentials(message: str) -&gt; str\n</code></pre> <p>Remove or mask credential-like values from a string.</p> <p>This is intended for use before logging error messages that might contain sensitive information like access keys or tokens.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The string to scrub.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The string with credential-like values replaced with [REDACTED].</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; scrub_credentials(\"Error: access_key_id=AKIAIOSFODNN7EXAMPLE\")\n'Error: access_key_id=[REDACTED]'\n</code></pre> <pre><code>&gt;&gt;&gt; scrub_credentials(\"Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...\")\n'[REDACTED]'\n</code></pre> Source code in <code>src/fsspeckit/common/security.py</code> <pre><code>def scrub_credentials(message: str) -&gt; str:\n    \"\"\"Remove or mask credential-like values from a string.\n\n    This is intended for use before logging error messages that might\n    contain sensitive information like access keys or tokens.\n\n    Args:\n        message: The string to scrub.\n\n    Returns:\n        The string with credential-like values replaced with [REDACTED].\n\n    Examples:\n        &gt;&gt;&gt; scrub_credentials(\"Error: access_key_id=AKIAIOSFODNN7EXAMPLE\")\n        'Error: access_key_id=[REDACTED]'\n\n        &gt;&gt;&gt; scrub_credentials(\"Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...\")\n        '[REDACTED]'\n    \"\"\"\n    if not message:\n        return message\n\n    result = message\n\n    for pattern in _CREDENTIAL_PATTERNS:\n        # Replace matched groups with [REDACTED]\n        def redact_match(match: re.Match) -&gt; str:\n            groups = match.groups()\n            if len(groups) &gt;= 2:\n                # Pattern with key=value format - keep the key, redact the value\n                return match.group(0).replace(groups[-1], \"[REDACTED]\")\n            else:\n                # Single match - redact entire thing\n                return \"[REDACTED]\"\n\n        result = pattern.sub(redact_match, result)\n\n    return result\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.security.scrub_exception","title":"fsspeckit.common.security.scrub_exception","text":"<pre><code>scrub_exception(exc: BaseException) -&gt; str\n</code></pre> <p>Scrub credentials from an exception's string representation.</p> <p>Parameters:</p> Name Type Description Default <code>exc</code> <code>BaseException</code> <p>The exception to scrub.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A scrubbed string representation of the exception.</p> Source code in <code>src/fsspeckit/common/security.py</code> <pre><code>def scrub_exception(exc: BaseException) -&gt; str:\n    \"\"\"Scrub credentials from an exception's string representation.\n\n    Args:\n        exc: The exception to scrub.\n\n    Returns:\n        A scrubbed string representation of the exception.\n    \"\"\"\n    return scrub_credentials(str(exc))\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.security.validate_columns","title":"fsspeckit.common.security.validate_columns","text":"<pre><code>validate_columns(\n    columns: list[str] | None, valid_columns: list[str]\n) -&gt; list[str] | None\n</code></pre> <p>Validate that requested columns exist in the schema.</p> <p>This is a helper to prevent column injection in SQL-like operations.</p> <p>Parameters:</p> Name Type Description Default <code>columns</code> <code>list[str] | None</code> <p>List of column names to validate, or None.</p> required <code>valid_columns</code> <code>list[str]</code> <p>List of valid column names from the schema.</p> required <p>Returns:</p> Type Description <code>list[str] | None</code> <p>The validated columns list, or None if columns was None.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any column is not in the valid set.</p> Source code in <code>src/fsspeckit/common/security.py</code> <pre><code>def validate_columns(columns: list[str] | None, valid_columns: list[str]) -&gt; list[str] | None:\n    \"\"\"Validate that requested columns exist in the schema.\n\n    This is a helper to prevent column injection in SQL-like operations.\n\n    Args:\n        columns: List of column names to validate, or None.\n        valid_columns: List of valid column names from the schema.\n\n    Returns:\n        The validated columns list, or None if columns was None.\n\n    Raises:\n        ValueError: If any column is not in the valid set.\n    \"\"\"\n    if columns is None:\n        return None\n\n    valid_set = set(valid_columns)\n    invalid = [col for col in columns if col not in valid_set]\n\n    if invalid:\n        raise ValueError(\n            f\"Invalid column(s): {', '.join(invalid)}. \"\n            f\"Valid columns are: {', '.join(sorted(valid_set))}\"\n        )\n\n    return columns\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.security.validate_compression_codec","title":"fsspeckit.common.security.validate_compression_codec","text":"<pre><code>validate_compression_codec(codec: str) -&gt; str\n</code></pre> <p>Validate that a compression codec is in the allowed set.</p> <p>This prevents injection of arbitrary values into SQL queries or filesystem operations that accept codec parameters.</p> <p>Parameters:</p> Name Type Description Default <code>codec</code> <code>str</code> <p>The compression codec name to validate.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The validated codec name (lowercased).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the codec is not in the allowed set.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; validate_compression_codec(\"snappy\")\n'snappy'\n</code></pre> <pre><code>&gt;&gt;&gt; validate_compression_codec(\"GZIP\")\n'gzip'\n</code></pre> <pre><code>&gt;&gt;&gt; validate_compression_codec(\"malicious; DROP TABLE\")\nValueError: Invalid compression codec\n</code></pre> Source code in <code>src/fsspeckit/common/security.py</code> <pre><code>def validate_compression_codec(codec: str) -&gt; str:\n    \"\"\"Validate that a compression codec is in the allowed set.\n\n    This prevents injection of arbitrary values into SQL queries or\n    filesystem operations that accept codec parameters.\n\n    Args:\n        codec: The compression codec name to validate.\n\n    Returns:\n        The validated codec name (lowercased).\n\n    Raises:\n        ValueError: If the codec is not in the allowed set.\n\n    Examples:\n        &gt;&gt;&gt; validate_compression_codec(\"snappy\")\n        'snappy'\n\n        &gt;&gt;&gt; validate_compression_codec(\"GZIP\")\n        'gzip'\n\n        &gt;&gt;&gt; validate_compression_codec(\"malicious; DROP TABLE\")\n        ValueError: Invalid compression codec\n    \"\"\"\n    if not codec or not isinstance(codec, str):\n        raise ValueError(\"Compression codec must be a non-empty string\")\n\n    normalized = codec.lower().strip()\n\n    if normalized not in VALID_COMPRESSION_CODECS:\n        valid_list = \", \".join(sorted(VALID_COMPRESSION_CODECS - {\"none\"}))\n        raise ValueError(\n            f\"Invalid compression codec: '{codec}'. \"\n            f\"Must be one of: {valid_list}\"\n        )\n\n    return normalized\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.security.validate_path","title":"fsspeckit.common.security.validate_path","text":"<pre><code>validate_path(\n    path: str, base_dir: str | None = None\n) -&gt; str\n</code></pre> <p>Validate a filesystem path for security issues.</p> <p>Checks for: - Embedded null bytes and control characters - Path traversal attempts (../ sequences escaping base_dir) - Empty or whitespace-only paths</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to validate.</p> required <code>base_dir</code> <code>str | None</code> <p>Optional base directory. If provided, the path must resolve to a location within this directory (prevents path traversal).</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The validated path (unchanged if valid).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the path contains forbidden characters, is empty, or escapes the base directory.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; validate_path(\"/data/file.parquet\")\n'/data/file.parquet'\n</code></pre> <pre><code>&gt;&gt;&gt; validate_path(\"../../../etc/passwd\", base_dir=\"/data\")\nValueError: Path escapes base directory\n</code></pre> <pre><code>&gt;&gt;&gt; validate_path(\"file\\x00.parquet\")\nValueError: Path contains forbidden characters\n</code></pre> Source code in <code>src/fsspeckit/common/security.py</code> <pre><code>def validate_path(path: str, base_dir: str | None = None) -&gt; str:\n    \"\"\"Validate a filesystem path for security issues.\n\n    Checks for:\n    - Embedded null bytes and control characters\n    - Path traversal attempts (../ sequences escaping base_dir)\n    - Empty or whitespace-only paths\n\n    Args:\n        path: The path to validate.\n        base_dir: Optional base directory. If provided, the path must resolve\n            to a location within this directory (prevents path traversal).\n\n    Returns:\n        The validated path (unchanged if valid).\n\n    Raises:\n        ValueError: If the path contains forbidden characters, is empty,\n            or escapes the base directory.\n\n    Examples:\n        &gt;&gt;&gt; validate_path(\"/data/file.parquet\")\n        '/data/file.parquet'\n\n        &gt;&gt;&gt; validate_path(\"../../../etc/passwd\", base_dir=\"/data\")\n        ValueError: Path escapes base directory\n\n        &gt;&gt;&gt; validate_path(\"file\\\\x00.parquet\")\n        ValueError: Path contains forbidden characters\n    \"\"\"\n    if not path or not path.strip():\n        raise ValueError(\"Path cannot be empty or whitespace-only\")\n\n    # Check for forbidden control characters\n    for char in path:\n        if char in _FORBIDDEN_PATH_CHARS:\n            raise ValueError(\n                f\"Path contains forbidden control character: {repr(char)}\"\n            )\n\n    # Check for path traversal when base_dir is specified\n    if base_dir is not None:\n        import os\n\n        # Normalize both paths for comparison\n        base_resolved = os.path.normpath(os.path.abspath(base_dir))\n\n        # Handle relative paths by joining with base\n        if not os.path.isabs(path):\n            full_path = os.path.join(base_dir, path)\n        else:\n            full_path = path\n\n        path_resolved = os.path.normpath(os.path.abspath(full_path))\n\n        # Check if resolved path starts with base directory\n        if not path_resolved.startswith(base_resolved + os.sep) and path_resolved != base_resolved:\n            raise ValueError(\n                f\"Path '{path}' escapes base directory '{base_dir}'\"\n            )\n\n    return path\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.types","title":"fsspeckit.common.types","text":"<p>Type conversion and data transformation utilities.</p>"},{"location":"api/fsspeckit.common/#fsspeckit.common.types-functions","title":"Functions","text":""},{"location":"api/fsspeckit.common/#fsspeckit.common.types.dict_to_dataframe","title":"fsspeckit.common.types.dict_to_dataframe","text":"<pre><code>dict_to_dataframe(\n    data: Union[dict, list[dict]],\n    unique: Union[bool, list[str], str] = False,\n) -&gt; Any\n</code></pre> <p>Convert a dictionary or list of dictionaries to a Polars DataFrame.</p> <p>Handles various input formats: - Single dict with list values \u2192 DataFrame rows - Single dict with scalar values \u2192 Single row DataFrame - List of dicts with scalar values \u2192 Multi-row DataFrame - List of dicts with list values \u2192 DataFrame with list columns</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[dict, list[dict]]</code> <p>Dictionary or list of dictionaries to convert.</p> required <code>unique</code> <code>Union[bool, list[str], str]</code> <p>If True, remove duplicate rows. Can also specify columns.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>Polars DataFrame containing the converted data.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Single dict with list values\n&gt;&gt;&gt; data = {'a': [1, 2, 3], 'b': [4, 5, 6]}\n&gt;&gt;&gt; dict_to_dataframe(data)\nshape: (3, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 b   \u2502\n\u2502 --- \u2506 --- \u2502\n\u2502 i64 \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 4   \u2502\n\u2502 2   \u2506 5   \u2502\n\u2502 3   \u2506 6   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <pre><code>&gt;&gt;&gt; # Single dict with scalar values\n&gt;&gt;&gt; data = {'a': 1, 'b': 2}\n&gt;&gt;&gt; dict_to_dataframe(data)\nshape: (1, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 b   \u2502\n\u2502 --- \u2506 --- \u2502\n\u2502 i64 \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 2   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <pre><code>&gt;&gt;&gt; # List of dicts with scalar values\n&gt;&gt;&gt; data = [{'a': 1, 'b': 2}, {'a': 3, 'b': 4}]\n&gt;&gt;&gt; dict_to_dataframe(data)\nshape: (2, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 b   \u2502\n\u2502 --- \u2506 --- \u2502\n\u2502 i64 \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 2   \u2502\n\u2502 3   \u2506 4   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> Source code in <code>src/fsspeckit/common/types.py</code> <pre><code>def dict_to_dataframe(\n    data: Union[dict, list[dict]], unique: Union[bool, list[str], str] = False\n) -&gt; Any:\n    \"\"\"Convert a dictionary or list of dictionaries to a Polars DataFrame.\n\n    Handles various input formats:\n    - Single dict with list values \u2192 DataFrame rows\n    - Single dict with scalar values \u2192 Single row DataFrame\n    - List of dicts with scalar values \u2192 Multi-row DataFrame\n    - List of dicts with list values \u2192 DataFrame with list columns\n\n    Args:\n        data: Dictionary or list of dictionaries to convert.\n        unique: If True, remove duplicate rows. Can also specify columns.\n\n    Returns:\n        Polars DataFrame containing the converted data.\n\n    Examples:\n        &gt;&gt;&gt; # Single dict with list values\n        &gt;&gt;&gt; data = {'a': [1, 2, 3], 'b': [4, 5, 6]}\n        &gt;&gt;&gt; dict_to_dataframe(data)\n        shape: (3, 2)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 a   \u2506 b   \u2502\n        \u2502 --- \u2506 --- \u2502\n        \u2502 i64 \u2506 i64 \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 1   \u2506 4   \u2502\n        \u2502 2   \u2506 5   \u2502\n        \u2502 3   \u2506 6   \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n\n        &gt;&gt;&gt; # Single dict with scalar values\n        &gt;&gt;&gt; data = {'a': 1, 'b': 2}\n        &gt;&gt;&gt; dict_to_dataframe(data)\n        shape: (1, 2)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 a   \u2506 b   \u2502\n        \u2502 --- \u2506 --- \u2502\n        \u2502 i64 \u2506 i64 \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 1   \u2506 2   \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n\n        &gt;&gt;&gt; # List of dicts with scalar values\n        &gt;&gt;&gt; data = [{'a': 1, 'b': 2}, {'a': 3, 'b': 4}]\n        &gt;&gt;&gt; dict_to_dataframe(data)\n        shape: (2, 2)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 a   \u2506 b   \u2502\n        \u2502 --- \u2506 --- \u2502\n        \u2502 i64 \u2506 i64 \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 1   \u2506 2   \u2502\n        \u2502 3   \u2506 4   \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n    \"\"\"\n    from fsspeckit.common.optional import _import_polars\n\n    pl = _import_polars()\n\n    if isinstance(data, list):\n        # If it's a single-element list, just use the first element\n        if len(data) == 1:\n            data = data[0]\n        # If it's a list of dicts\n        else:\n            first_item = data[0]\n            # Check if the dict values are lists/tuples\n            if any(isinstance(v, (list, tuple)) for v in first_item.values()):\n                # Each dict becomes a row with list/tuple values\n                data = pl.DataFrame(data)\n            else:\n                # If values are scalars, convert list of dicts to DataFrame\n                data = pl.DataFrame(data)\n\n            if unique:\n                data = data.unique(\n                    subset=None if not isinstance(unique, (str, list)) else unique,\n                    maintain_order=True,\n                )\n            return data\n\n    # If it's a single dict\n    if isinstance(data, dict):\n        # Check if values are lists/tuples\n        if any(isinstance(v, (list, tuple)) for v in data.values()):\n            # Get the length of any list value (assuming all lists have same length)\n            length = len(next(v for v in data.values() if isinstance(v, (list, tuple))))\n            # Convert to DataFrame where each list element becomes a row\n            data = pl.DataFrame(\n                {\n                    k: v if isinstance(v, (list, tuple)) else [v] * length\n                    for k, v in data.items()\n                }\n            )\n        else:\n            # If values are scalars, wrap them in a list to create a single row\n            data = pl.DataFrame({k: [v] for k, v in data.items()})\n\n        if unique:\n            data = data.unique(\n                subset=None if not isinstance(unique, (str, list)) else unique,\n                maintain_order=True,\n            )\n        return data\n\n    raise ValueError(\"Input must be a dictionary or list of dictionaries\")\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.types.to_pyarrow_table","title":"fsspeckit.common.types.to_pyarrow_table","text":"<pre><code>to_pyarrow_table(\n    data: Union[Any, dict, list[Any]],\n    concat: bool = False,\n    unique: Union[bool, list[str], str] = False,\n) -&gt; Any\n</code></pre> <p>Convert various data formats to PyArrow Table.</p> <p>Handles conversion from Polars DataFrames, Pandas DataFrames, dictionaries, and lists of these types to PyArrow Tables.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[Any, dict, list[Any]]</code> <p>Input data to convert.</p> required <code>concat</code> <code>bool</code> <p>Whether to concatenate multiple inputs into single table.</p> <code>False</code> <code>unique</code> <code>Union[bool, list[str], str]</code> <p>Whether to remove duplicates. Can specify columns.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>PyArrow Table containing the converted data.</p> Example <pre><code>df = pl.DataFrame({\"a\": [1, 2], \"b\": [3, 4]})\ntable = to_pyarrow_table(df)\nprint(table.schema)\n# a: int64\n# b: int64\n</code></pre> Source code in <code>src/fsspeckit/common/types.py</code> <pre><code>def to_pyarrow_table(\n    data: Union[\n        Any,  # pl.DataFrame, pl.LazyFrame, pd.DataFrame\n        dict,\n        list[Any],  # list of DataFrames or dicts\n    ],\n    concat: bool = False,\n    unique: Union[bool, list[str], str] = False,\n) -&gt; Any:\n    \"\"\"Convert various data formats to PyArrow Table.\n\n    Handles conversion from Polars DataFrames, Pandas DataFrames,\n    dictionaries, and lists of these types to PyArrow Tables.\n\n    Args:\n        data: Input data to convert.\n        concat: Whether to concatenate multiple inputs into single table.\n        unique: Whether to remove duplicates. Can specify columns.\n\n    Returns:\n        PyArrow Table containing the converted data.\n\n    Example:\n        ```python\n        df = pl.DataFrame({\"a\": [1, 2], \"b\": [3, 4]})\n        table = to_pyarrow_table(df)\n        print(table.schema)\n        # a: int64\n        # b: int64\n        ```\n    \"\"\"\n    from fsspeckit.common.optional import (\n        _import_pandas,\n        _import_polars,\n        _import_pyarrow,\n    )\n    from fsspeckit.datasets.pyarrow import convert_large_types_to_normal\n\n    pl = _import_polars()\n    pd = _import_pandas()\n    pa = _import_pyarrow()\n\n    # Convert dict to DataFrame first\n    if isinstance(data, dict):\n        data = dict_to_dataframe(data)\n    if isinstance(data, list):\n        if isinstance(data[0], dict):\n            data = dict_to_dataframe(data, unique=unique)\n\n    # Ensure data is a list for uniform processing\n    if not isinstance(data, list):\n        data = [data]\n\n    # Collect lazy frames\n    if isinstance(data[0], pl.LazyFrame):\n        data = [dd.collect() for dd in data]\n\n    # Convert based on the first item's type\n    if isinstance(data[0], pl.DataFrame):\n        if concat:\n            data = pl.concat(data, how=\"diagonal_relaxed\")\n            if unique:\n                data = data.unique(\n                    subset=None if not isinstance(unique, (str, list)) else unique,\n                    maintain_order=True,\n                )\n            data = data.to_arrow()\n            data = data.cast(convert_large_types_to_normal(data.schema))\n        else:\n            data = [dd.to_arrow() for dd in data]\n            data = [dd.cast(convert_large_types_to_normal(dd.schema)) for dd in data]\n\n    elif isinstance(data[0], pd.DataFrame):\n        data = [pa.Table.from_pandas(dd, preserve_index=False) for dd in data]\n        if concat:\n            data = pa.concat_tables(data, promote_options=\"permissive\")\n            if unique:\n                data = (\n                    pl.from_arrow(data)\n                    .unique(\n                        subset=None if not isinstance(unique, (str, list)) else unique,\n                        maintain_order=True,\n                    )\n                    .to_arrow()\n                )\n                data = data.cast(convert_large_types_to_normal(data.schema))\n\n    elif isinstance(data[0], (pa.RecordBatch, Generator)):\n        if concat:\n            data = pa.Table.from_batches(data)\n            if unique:\n                data = (\n                    pl.from_arrow(data)\n                    .unique(\n                        subset=None if not isinstance(unique, (str, list)) else unique,\n                        maintain_order=True,\n                    )\n                    .to_arrow()\n                )\n                data = data.cast(convert_large_types_to_normal(data.schema))\n        else:\n            data = [pa.Table.from_batches([dd]) for dd in data]\n\n    return data\n</code></pre>"},{"location":"api/fsspeckit.core.base/","title":"<code>fsspeckit.core.base</code> API Documentation","text":"<p>This module provides core filesystem functionalities and utilities, including custom cache mappers, enhanced cached filesystems, and a GitLab filesystem implementation.</p>"},{"location":"api/fsspeckit.core.base/#filenamecachemapper","title":"<code>FileNameCacheMapper</code>","text":"<p>Maps remote file paths to local cache paths while preserving directory structure.</p> <p>This cache mapper maintains the original file path structure in the cache directory, creating necessary subdirectories as needed.</p> <p>Attributes:</p> <ul> <li><code>directory</code> (<code>str</code>): Base directory for cached files</li> </ul> <p>Example:</p> <pre><code>from fsspeckit.core.base import FileNameCacheMapper\n\n# Create cache mapper for S3 files\nmapper = FileNameCacheMapper(\"/tmp/cache\")\n\n# Map remote path to cache path\ncache_path = mapper(\"bucket/data/file.csv\")\nprint(cache_path)  # Preserves structure\n</code></pre>"},{"location":"api/fsspeckit.core.base/#__init__","title":"<code>__init__()</code>","text":"<p>Initialize cache mapper with base directory.</p> Parameter Type Description <code>directory</code> <code>str</code> Base directory where cached files will be stored"},{"location":"api/fsspeckit.core.base/#__call__","title":"<code>__call__()</code>","text":"<p>Map remote file path to cache file path.</p> <p>Creates necessary subdirectories in the cache directory to maintain the original path structure.</p> Parameter Type Description <code>path</code> <code>str</code> Original file path from remote filesystem Returns Type Description <code>str</code> <code>str</code> Cache file path that preserves original structure <p>Example:</p> <pre><code>from fsspeckit.core.base import FileNameCacheMapper\n\nmapper = FileNameCacheMapper(\"/tmp/cache\")\n# Maps maintain directory structure\nprint(mapper(\"data/nested/file.txt\"))\n</code></pre>"},{"location":"api/fsspeckit.core.base/#monitoredsimplecachefilesystem","title":"<code>MonitoredSimpleCacheFileSystem</code>","text":"<p>Enhanced caching filesystem with monitoring and improved path handling.</p> <p>This filesystem extends <code>SimpleCacheFileSystem</code> to provide:</p> <ul> <li>Verbose logging of cache operations</li> <li>Improved path mapping for cache files</li> <li>Enhanced synchronization capabilities</li> <li>Better handling of parallel operations</li> </ul> <p>Attributes:</p> <ul> <li><code>_verbose</code> (<code>bool</code>): Whether to print verbose cache operations</li> <li><code>_mapper</code> (<code>FileNameCacheMapper</code>): Maps remote paths to cache paths</li> <li><code>storage</code> (<code>list[str]</code>): List of cache storage locations</li> <li><code>fs</code> (<code>AbstractFileSystem</code>): Underlying filesystem being cached</li> </ul> <p>Example:</p> <pre><code>from fsspec import filesystem\nfrom fsspeckit.core.base import MonitoredSimpleCacheFileSystem\n\ns3_fs = filesystem(\"s3\")\ncached_fs = MonitoredSimpleCacheFileSystem(\n    fs=s3_fs,\n    cache_storage=\"/tmp/cache\",\n    verbose=True\n)\n# Use cached_fs like any other filesystem\nfiles = cached_fs.ls(\"my-bucket/\")\n</code></pre>"},{"location":"api/fsspeckit.core.base/#__init___1","title":"<code>__init__()</code>","text":"<p>Initialize monitored cache filesystem.</p> Parameter Type Description <code>fs</code> <code>Optional[fsspec.AbstractFileSystem]</code> Underlying filesystem to cache. If None, creates a local filesystem. <code>cache_storage</code> <code>Union[str, list[str]]</code> Cache storage location(s). Can be string path or list of paths. <code>verbose</code> <code>bool</code> Whether to enable verbose logging of cache operations. <code>**kwargs</code> <code>Any</code> Additional arguments passed to <code>SimpleCacheFileSystem</code>. <p>Example:</p> <pre><code># Cache S3 filesystem\ns3_fs = filesystem(\"s3\")\ncached = MonitoredSimpleCacheFileSystem(\n    fs=s3_fs,\n    cache_storage=\"/tmp/s3_cache\",\n    verbose=True\n)\n</code></pre>"},{"location":"api/fsspeckit.core.base/#_check_cache","title":"<code>_check_cache()</code>","text":"<p>Check if file exists in cache and return cache path if found.</p> Parameter Type Description <code>path</code> <code>str</code> Remote file path to check Returns Type Description <code>Optional[str]</code> <code>str</code> or <code>None</code> Cache file path if found, None otherwise <p>Example:</p> <pre><code>from fsspeckit.core.base import MonitoredSimpleCacheFileSystem\nfrom fsspec import filesystem\n\ncached_fs = MonitoredSimpleCacheFileSystem(fs=filesystem(\"s3\"), cache_storage=\"/tmp/cache\")\n# Check if a file is in cache\ncache_path = cached_fs._check_cache(\"my-bucket/data/file.txt\")\nif cache_path:\n    print(f\"File found in cache at: {cache_path}\")\nelse:\n    print(\"File not in cache.\")\n</code></pre>"},{"location":"api/fsspeckit.core.base/#_check_file","title":"<code>_check_file()</code>","text":"<p>Ensure file is in cache, downloading if necessary.</p> Parameter Type Description <code>path</code> <code>str</code> Remote file path Returns Type Description <code>str</code> <code>str</code> Local cache path for the file <p>Example:</p> <pre><code>from fsspeckit.core.base import MonitoredSimpleCacheFileSystem\nfrom fsspec import filesystem\n\ncached_fs = MonitoredSimpleCacheFileSystem(fs=filesystem(\"s3\"), cache_storage=\"/tmp/cache\")\n# Ensure file is in cache (downloads if not present)\nlocal_path = cached_fs._check_file(\"my-bucket/data/large_file.parquet\")\nprint(f\"File available locally at: {local_path}\")\n</code></pre>"},{"location":"api/fsspeckit.core.base/#gitlabfilesystem","title":"<code>GitLabFileSystem</code>","text":"<p>Filesystem interface for GitLab repositories.</p> <p>Provides read-only access to files in GitLab repositories, including:</p> <ul> <li>Public and private repositories</li> <li>Self-hosted GitLab instances</li> <li>Branch/tag/commit selection</li> <li>Token-based authentication</li> </ul> <p>Attributes:</p> <ul> <li><code>protocol</code> (<code>str</code>): Always \"gitlab\"</li> <li><code>base_url</code> (<code>str</code>): GitLab instance URL</li> <li><code>project_id</code> (<code>str</code>): Project ID</li> <li><code>project_name</code> (<code>str</code>): Project name/path</li> <li><code>ref</code> (<code>str</code>): Git reference (branch, tag, commit)</li> <li><code>token</code> (<code>str</code>): Access token</li> <li><code>api_version</code> (<code>str</code>): API version</li> </ul> <p>Example:</p> <pre><code># Public repository\nfs = GitLabFileSystem(\n    project_name=\"group/project\",\n    ref=\"main\"\n)\nfiles = fs.ls(\"/\")\n\n# Private repository with token\nfs = GitLabFileSystem(\n    project_id=\"12345\",\n    token=\"glpat_xxxx\",\n    ref=\"develop\"\n)\ncontent = fs.cat(\"README.md\")\n</code></pre>"},{"location":"api/fsspeckit.core.base/#__init___2","title":"<code>__init__()</code>","text":"<p>Initialize GitLab filesystem.</p> Parameter Type Description <code>base_url</code> <code>str</code> GitLab instance URL <code>project_id</code> <code>Optional[Union[str, int]]</code> Project ID number <code>project_name</code> <code>Optional[str]</code> Project name/path (alternative to project_id) <code>ref</code> <code>str</code> Git reference (branch, tag, or commit SHA) <code>token</code> <code>Optional[str]</code> GitLab personal access token <code>api_version</code> <code>str</code> API version to use <p>| <code>**kwargs</code> | <code>Any</code> | Additional filesystem arguments |</p> Raises Type Description <code>ValueError</code> <code>ValueError</code> If neither <code>project_id</code> nor <code>project_name</code> is provided <p>Example:</p> <pre><code>from fsspeckit.core.base import GitLabFileSystem\n\n# Access a public repository\nfs_public = GitLabFileSystem(\n    project_name=\"gitlab-org/gitlab\",\n    ref=\"master\"\n)\nprint(fs_public.ls(\"README.md\"))\n\n# Access a private repository (replace with your token and project info)\n# fs_private = GitLabFileSystem(\n#     project_id=\"12345\",\n# #    token=\"your_private_token\",\n#     ref=\"main\"\n# )\n# print(fs_private.ls(\"/\"))\n</code></pre>"},{"location":"api/fsspeckit.core.base/#_get_file_content","title":"<code>_get_file_content()</code>","text":"<p>Get file content from GitLab API.</p> Parameter Type Description <code>path</code> <code>str</code> File path in repository Returns Type Description <code>bytes</code> <code>bytes</code> File content as bytes <p>Example:</p> <pre><code>from fsspeckit.core.base import GitLabFileSystem\n\nfs = GitLabFileSystem(project_name=\"gitlab-org/gitlab\")\ncontent = fs.cat(\"README.md\")\nprint(content[:50])\n</code></pre> Raises Type Description <code>FileNotFoundError</code> <code>FileNotFoundError</code> If file doesn't exist <code>requests.HTTPError</code> <code>requests.HTTPError</code> For other HTTP errors"},{"location":"api/fsspeckit.core.base/#_open","title":"<code>_open()</code>","text":"<p>Open file for reading.</p> Parameter Type Description <code>path</code> <code>str</code> File path to open <code>mode</code> <code>str</code> File mode (only 'rb' and 'r' supported) <code>block_size</code> <code>Optional[int]</code> Block size for reading (unused) <code>cache_options</code> <code>Optional[dict]</code> Cache options (unused) <code>**kwargs</code> <code>Any</code> Additional options Returns Type Description File-like object File-like object File-like object for reading Raises Type Description <code>ValueError</code> <code>ValueError</code> If mode is not supported"},{"location":"api/fsspeckit.core.base/#cat","title":"<code>cat()</code>","text":"<p>Get file contents as bytes.</p> Parameter Type Description <code>path</code> <code>str</code> File path <code>**kwargs</code> <code>Any</code> Additional options Returns Type Description <code>bytes</code> <code>bytes</code> File content as bytes"},{"location":"api/fsspeckit.core.base/#ls","title":"<code>ls()</code>","text":"<p>List directory contents.</p> Parameter Type Description <code>path</code> <code>str</code> Directory path to list <code>detail</code> <code>bool</code> Whether to return detailed information <code>**kwargs</code> <code>Any</code> Additional options Returns Type Description <code>list</code> <code>list</code> List of files/directories or their details"},{"location":"api/fsspeckit.core.base/#exists","title":"<code>exists()</code>","text":"<p>Check if file or directory exists.</p> Parameter Type Description <code>path</code> <code>str</code> Path to check <code>**kwargs</code> <code>Any</code> Additional options Returns Type Description <code>bool</code> <code>bool</code> True if path exists, False otherwise"},{"location":"api/fsspeckit.core.base/#info","title":"<code>info()</code>","text":"<p>Get file information.</p> Parameter Type Description <code>path</code> <code>str</code> File path <code>**kwargs</code> <code>Any</code> Additional options Returns Type Description <code>dict</code> <code>dict</code> Dictionary with file information Raises Type Description <code>FileNotFoundError</code> <code>FileNotFoundError</code> If file not found"},{"location":"api/fsspeckit.core.base/#filesystem","title":"<code>filesystem()</code>","text":"<p>Get filesystem instance with enhanced configuration options.</p> <p>Creates filesystem instances with support for storage options classes, intelligent caching, and protocol inference from paths.</p> Parameter Type Description <code>protocol_or_path</code> <code>str</code> Filesystem protocol (e.g., \"s3\", \"file\") or path with protocol prefix <code>storage_options</code> <code>Optional[Union[BaseStorageOptions, dict]]</code> Storage configuration as <code>BaseStorageOptions</code> instance or dict <code>cached</code> <code>bool</code> Whether to wrap filesystem in caching layer <code>cache_storage</code> <code>Optional[str]</code> Cache directory path (if <code>cached=True</code>) <code>verbose</code> <code>bool</code> Enable verbose logging for cache operations <code>dirfs</code> <code>bool</code> Whether to wrap the filesystem in a <code>DirFileSystem</code>. Defaults to <code>True</code>. <code>base_fs</code> <code>AbstractFileSystem</code> An existing filesystem to wrap. <code>**kwargs</code> <code>Any</code> Additional filesystem arguments Returns Type Description <code>AbstractFileSystem</code> <code>fsspec.AbstractFileSystem</code> Configured filesystem instance <p>Example:</p> <pre><code># Basic local filesystem\nfs = filesystem(\"file\")\n\n# S3 with storage options\nfrom fsspeckit.storage_options.cloud import AwsStorageOptions\nopts = AwsStorageOptions(region=\"us-west-2\")\nfs = filesystem(\"s3\", storage_options=opts, cached=True)\n\n# Infer protocol from path\nfs = filesystem(\"s3://my-bucket/\", cached=True)\n\n# GitLab filesystem\nfs = filesystem(\"gitlab\", storage_options={\n    \"project_name\": \"group/project\",\n    \"token\": \"glpat_xxxx\"\n})\n</code></pre>"},{"location":"api/fsspeckit.core.base/#get_filesystem","title":"<code>get_filesystem()</code>","text":"<p>Get filesystem instance with enhanced configuration options.</p> <p>Deprecated</p> <p>Use <code>filesystem</code> instead. This function will be removed in a future version.</p> <p>Creates filesystem instances with support for storage options classes, intelligent caching, and protocol inference from paths.</p> Parameter Type Description <code>protocol_or_path</code> <code>str</code> Filesystem protocol (e.g., \"s3\", \"file\") or path with protocol prefix <code>storage_options</code> <code>Optional[Union[BaseStorageOptions, dict]]</code> Storage configuration as <code>BaseStorageOptions</code> instance or dict <code>cached</code> <code>bool</code> Whether to wrap filesystem in caching layer <code>cache_storage</code> <code>Optional[str]</code> Cache directory path (if <code>cached=True</code>) <code>verbose</code> <code>bool</code> Enable verbose logging for cache operations <code>**kwargs</code> <code>Any</code> Additional filesystem arguments Returns Type Description <code>fsspec.AbstractFileSystem</code> <code>fsspec.AbstractFileSystem</code> Configured filesystem instance <p>Example:</p> <pre><code># Basic local filesystem\nfs = get_filesystem(\"file\")\n\n# S3 with storage options\nfrom fsspeckit.storage_options.cloud import AwsStorageOptions\nopts = AwsStorageOptions(region=\"us-west-2\")\nfs = get_filesystem(\"s3\", storage_options=opts, cached=True)\n\n# Infer protocol from path\nfs = get_filesystem(\"s3://my-bucket/\", cached=True)\n\n# GitLab filesystem\nfs = get_filesystem(\"gitlab\", storage_options={\n    \"project_name\": \"group/project\",\n    \"token\": \"glpat_xxxx\"\n})\n</code></pre>"},{"location":"api/fsspeckit.core.ext/","title":"<code>fsspeckit.core.ext</code> API Documentation","text":"<p>This module provides extended functionalities for <code>fsspec.AbstractFileSystem</code>, including methods for reading and writing various file formats (JSON, CSV, Parquet) with advanced options like batch processing, parallelization, and data type optimization. It also includes functions for creating PyArrow datasets.</p>"},{"location":"api/fsspeckit.core.ext/#path_to_glob","title":"<code>path_to_glob()</code>","text":"<p>Convert a path to a glob pattern for file matching.</p> <p>Intelligently converts paths to glob patterns that match files of the specified format, handling various directory and wildcard patterns.</p> Parameter Type Description <code>path</code> <code>str</code> Base path to convert. Can include wildcards (<code>*</code> or <code>**</code>). <code>format</code> <code>str \\| None</code> File format to match (without dot). If <code>None</code>, inferred from path. <p>Common examples:</p> <ul> <li>Paths: <code>\\\"data/\\\"</code>, <code>\\\"data/*.json\\\"</code>, <code>\\\"data/**\\\"</code></li> <li>Formats: <code>\\\"json\\\"</code>, <code>\\\"csv\\\"</code>, <code>\\\"parquet\\\"</code></li> </ul> Returns Type Description <code>str</code> <code>str</code> Glob pattern that matches files of the specified format. <p>Example:</p> <pre><code># Basic directory\npath_to_glob(\"data\", \"json\")\n# 'data/**/*.json'\n\n# With wildcards\npath_to_glob(\"data/**\", \"csv\")\n# 'data/**/*.csv'\n\n# Format inference\npath_to_glob(\"data/file.parquet\")\n# 'data/file.parquet'\n</code></pre>"},{"location":"api/fsspeckit.core.ext/#read_json_file","title":"<code>read_json_file()</code>","text":"<p>Read a single JSON file from any filesystem.</p> <p>A public wrapper around <code>_read_json_file</code> providing a clean interface for reading individual JSON files.</p> Parameter Type Description <code>self</code> <code>AbstractFileSystem</code> Filesystem instance to use for reading <code>path</code> <code>str</code> Path to JSON file to read <code>include_file_path</code> <code>bool</code> Whether to return dict with filepath as key <code>jsonlines</code> <code>bool</code> Whether to read as JSON Lines format Returns Type Description <code>dict</code> or <code>list[dict]</code> <code>dict</code> or <code>list[dict]</code> Parsed JSON data. For regular JSON, returns a dict. For JSON Lines, returns a list of dicts. If <code>include_file_path=True</code>, returns <code>{filepath: data}</code>. <p>Example:</p> <pre><code>from fsspec.implementations.local import LocalFileSystem\n\nfs = LocalFileSystem()\n# Read regular JSON\ndata = fs.read_json_file(\"config.json\")\nprint(data[\"setting\"])\n# 'value'\n\n# Read JSON Lines with filepath\ndata = fs.read_json_file(\n    \"logs.jsonl\",\n    include_file_path=True,\n    jsonlines=True\n)\nprint(list(data.keys())[0])\n# 'logs.jsonl'\n</code></pre>"},{"location":"api/fsspeckit.core.ext/#read_json","title":"<code>read_json()</code>","text":"<p>Read JSON data from one or more files with powerful options.</p> <p>Provides a flexible interface for reading JSON data with support for:</p> <ul> <li>Single file or multiple files</li> <li>Regular JSON or JSON Lines format</li> <li>Batch processing for large datasets</li> <li>Parallel processing</li> <li>DataFrame conversion</li> <li>File path tracking</li> </ul> Parameter Type Description <code>path</code> <code>str</code> or <code>list[str]</code> Path(s) to JSON file(s). Can be: - Single path string (globs supported) - List of path strings <code>batch_size</code> <code>int | None</code> If set, enables batch reading with this many files per batch <code>include_file_path</code> <code>bool</code> Include source filepath in output <code>jsonlines</code> <code>bool</code> Whether to read as JSON Lines format <code>as_dataframe</code> <code>bool</code> Convert output to Polars DataFrame(s) <code>concat</code> <code>bool</code> Combine multiple files/batches into single result <code>use_threads</code> <code>bool</code> Enable parallel file reading <code>verbose</code> <code>bool</code> Print progress information <code>opt_dtypes</code> <code>bool</code> Optimize DataFrame dtypes for performance <code>**kwargs</code> <code>Any</code> Additional arguments passed to DataFrame conversion Returns Type Description <code>dict</code> or <code>list[dict]</code> or <code>pl.DataFrame</code> or <code>list[pl.DataFrame]</code> or <code>Generator</code> Various types depending on arguments: - <code>dict</code>: Single JSON file as dictionary - <code>list[dict]</code>: Multiple JSON files as list of dictionaries - <code>pl.DataFrame</code>: Single or concatenated DataFrame - <code>list[pl.DataFrame]</code>: List of Dataframes (if <code>concat=False</code>) - <code>Generator</code>: If <code>batch_size</code> set, yields batches of above types <p>Example:</p> <pre><code>from fsspec.implementations.local import LocalFileSystem\n\nfs = LocalFileSystem()\n# Read all JSON files in directory\ndf = fs.read_json(\n    \"data/*.json\",\n    as_dataframe=True,\n    concat=True\n)\nprint(df.shape)\n# (1000, 5)  # Combined data from all files\n\n# Batch process large dataset\nfor batch_df in fs.read_json(\n    \"logs/*.jsonl\",\n    batch_size=100,\n    jsonlines=True,\n    include_file_path=True\n):\n    print(f\"Processing {len(batch_df)} records\")\n\n# Parallel read with custom options\ndfs = fs.read_json(\n    [\"file1.json\", \"file2.json\"],\n    use_threads=True,\n    concat=False,\n    verbose=True\n)\nprint(f\"Read {len(dfs)} files\")\n</code></pre>"},{"location":"api/fsspeckit.core.ext/#read_csv_file","title":"<code>read_csv_file()</code>","text":"<p>Read a single CSV file from any filesystem.</p> <p>Internal function that handles reading individual CSV files and optionally adds the source filepath as a column.</p> Parameter Type Description <code>self</code> <code>AbstractFileSystem</code> Filesystem instance to use for reading <code>path</code> <code>str</code> Path to CSV file <code>include_file_path</code> <code>bool</code> Add source filepath as a column <code>opt_dtypes</code> <code>bool</code> Optimize DataFrame dtypes <code>**kwargs</code> <code>Any</code> Additional arguments passed to <code>pl.read_csv()</code> Returns Type Description <code>pl.DataFrame</code> <code>pl.DataFrame</code> DataFrame containing CSV data <p>Example:</p> <pre><code>from fsspec.implementations.local import LocalFileSystem\n\nfs = LocalFileSystem()\n# This example assumes _read_csv_file is an internal method or needs to be called differently.\n# For public use, you would typically use fs.read_csv().\n# df = fs.read_csv_file(\n#     \"data.csv\",\n#     include_file_path=True,\n#     delimiter=\"|\"\n# )\n# print(\"file_path\" in df.columns)\n# True\n</code></pre>"},{"location":"api/fsspeckit.core.ext/#read_csv","title":"<code>read_csv()</code>","text":"<p>Read CSV data from one or more files with powerful options.</p> <p>Provides a flexible interface for reading CSV files with support for:</p> <ul> <li>Single file or multiple files</li> <li>Batch processing for large datasets</li> <li>Parallel</li> <li>File path tracking</li> <li>Polars DataFrame output</li> </ul> Parameter Type Description <code>path</code> <code>str</code> or <code>list[str]</code> Path(s) to CSV file(s). Can be: - Single path string (globs supported) - List of path strings <code>batch_size</code> <code>int | None</code> If set, enables batch reading with this many files per batch <code>include_file_path</code> <code>bool</code> Add source filepath as a column <code>concat</code> <code>bool</code> Combine multiple files/batches into single DataFrame <code>use_threads</code> <code>bool</code> Enable parallel file reading <code>verbose</code> <code>bool</code> Print progress information <code>**kwargs</code> <code>Any</code> Additional arguments passed to <code>pl.read_csv()</code> Returns Type Description <code>pl.DataFrame</code> or <code>list[pl.DataFrame]</code> or <code>Generator</code> Various types depending on arguments: - <code>pl.DataFrame</code>: Single or concatenated DataFrame - <code>list[pl.DataFrame]</code>: List of DataFrames (if <code>concat=False</code>) - <code>Generator</code>: If <code>batch_size</code> set, yields batches of above types <p>Example:</p> <pre><code>from fsspec.implementations.local import LocalFileSystem\n\nfs = LocalFileSystem()\n# Read all CSVs in directory\ndf = fs.read_csv(\n    \"data/*.csv\",\n    include_file_path=True\n)\nprint(df.columns)\n# ['file_path', 'col1', 'col2', ...]\n\n# Batch process large dataset\nfor batch_df in fs.read_csv(\n    \"logs/*.csv\",\n    batch_size=100,\n    use_threads=True,\n    verbose=True\n):\n    print(f\"Processing {len(batch_df)} rows\")\n\n# Multiple files without concatenation\ndfs = fs.read_csv(\n    [\"file1.csv\", \"file2.csv\"],\n    concat=False,\n    use_threads=True\n)\nprint(f\"Read {len(dfs)} files\")\n</code></pre>"},{"location":"api/fsspeckit.core.ext/#read_parquet_file","title":"<code>read_parquet_file()</code>","text":"<p>Read a single Parquet file from any filesystem.</p> <p>Internal function that handles reading individual Parquet files and optionally adds the source filepath as a column.</p> Parameter Type Description <code>self</code> <code>AbstractFileSystem</code> Filesystem instance to use for reading <code>path</code> <code>str</code> Path to Parquet file <code>include_file_path</code> <code>bool</code> Add source filepath as a column <code>opt_dtypes</code> <code>bool</code> Optimize DataFrame dtypes <code>**kwargs</code> <code>Any</code> Additional arguments passed to <code>pq.read_table()</code> Returns Type Description <code>pa.Table</code> <code>pa.Table</code> PyArrow Table containing Parquet data <p>Example:</p> <pre><code>from fsspec.implementations.local import LocalFileSystem\n\nfs = LocalFileSystem()\n# This example assumes _read_parquet_file is an internal method or needs to be called differently.\n# For public use, you would typically use fs.read_parquet().\n# table = fs.read_parquet_file(\n#     \"data.parquet\",\n#     include_file_path=True,\n#     use_threads=True\n# )\n# print(\"file_path\" in table.column_names)\n# True\n</code></pre>"},{"location":"api/fsspeckit.core.ext/#read_parquet","title":"<code>read_parquet()</code>","text":"<p>Read Parquet data with advanced features and optimizations.</p> <p>Provides a high-performance interface for reading Parquet files with support for:</p> <ul> <li>Single file or multiple files</li> <li>Batch processing for large datasets</li> <li>Parallel processing</li> <li>File path tracking</li> <li>Automatic concatenation</li> <li>PyArrow Table output</li> </ul> <p>The function automatically uses optimal reading strategies:</p> <ul> <li>Direct dataset reading for simple cases</li> <li>Parallel processing for multiple files</li> <li>Batched reading for memory efficiency</li> </ul> Parameter Type Description <code>path</code> <code>str</code> or <code>list[str]</code> Path(s) to Parquet file(s). Can be: - Single path string (globs supported) - List of path strings - Directory containing _metadata file <code>batch_size</code> <code>int | None</code> If set, enables batch reading with this many files per batch <code>include_file_path</code> <code>bool</code> Add source filepath as a column <code>concat</code> <code>bool</code> Combine multiple files/batches into single Table <code>use_threads</code> <code>bool</code> Enable parallel file reading <code>verbose</code> <code>bool</code> Print progress information <code>opt_dtypes</code> <code>bool</code> Optimize Table dtypes for performance <code>**kwargs</code> <code>Any</code> Additional arguments passed to <code>pq.read_table()</code> Returns Type Description <code>pa.Table</code> or <code>list[pa.Table]</code> or <code>Generator</code> Various types depending on arguments: - <code>pa.Table</code>: Single or concatenated Table - <code>list[pa.Table]</code>: List of Tables (if <code>concat=False</code>) - <code>Generator</code>: If <code>batch_size</code> set, yields batches of above types <p>Example:</p> <pre><code>from fsspec.implementations.local import LocalFileSystem\n\nfs = LocalFileSystem()\n# Read all Parquet files in directory\ntable = fs.read_parquet(\n    \"data/*.parquet\",\n    include_file_path=True\n)\nprint(table.column_names)\n# ['file_path', 'col1', 'col2', ...]\n\n# Batch process large dataset\nfor batch in fs.read_parquet(\n    \"data/*.parquet\",\n    batch_size=100,\n    use_threads=True\n):\n    print(f\"Processing {batch.num_rows} rows\")\n\n# Read from directory with metadata\ntable = fs.read_parquet(\n    \"data/\",  # Contains _metadata\n    use_threads=True\n)\nprint(f\"Total rows: {table.num_rows}\")\n</code></pre>"},{"location":"api/fsspeckit.core.ext/#read_files","title":"<code>read_files()</code>","text":"<p>Universal interface for reading data files of any supported format.</p> <p>A unified API that automatically delegates to the appropriate reading function based on file format, while preserving all advanced features like:</p> <ul> <li>Batch processing</li> <li>Parallel reading</li> <li>File path tracking</li> <li>Format-specific optimizations</li> </ul> Parameter Type Description <code>path</code> <code>str</code> or <code>list[str]</code> Path(s) to data file(s). Can be: - Single path string (globs supported) - List of path strings <code>format</code> <code>str</code> File format to read. Supported values: - \"json\": Regular JSON or JSON Lines - \"csv\": CSV files - \"parquet\": Parquet files <code>batch_size</code> <code>int | None</code> If set, enables batch reading with this many files per batch <code>include_file_path</code> <code>bool</code> Add source filepath as column/field <code>concat</code> <code>bool</code> Combine multiple files/batches into single result <code>jsonlines</code> <code>bool</code> For JSON format, whether to read as JSON Lines <code>use_threads</code> <code>bool</code> Enable parallel file reading <code>verbose</code> <code>bool</code> Print progress information <code>opt_dtypes</code> <code>bool</code> Optimize DataFrame/Arrow Table dtypes for performance <code>**kwargs</code> <code>Any</code> Additional format-specific arguments Returns Type Description <code>pl.DataFrame</code> or <code>pa.Table</code> or <code>list[pl.DataFrame]</code> or <code>list[pa.Table]</code> or <code>Generator</code> Various types depending on format and arguments: - <code>pl.DataFrame</code>: For CSV and optionally JSON - <code>pa.Table</code>: For Parquet - <code>list[pl.DataFrame</code> or <code>pa.Table]</code>: Without concatenation - <code>Generator</code>: If <code>batch_size</code> set, yields batches <p>Example:</p> <pre><code>from fsspec.implementations.local import LocalFileSystem\n\nfs = LocalFileSystem()\n# Read CSV files\ndf = fs.read_files(\n    \"data/*.csv\",\n    format=\"csv\",\n    include_file_path=True\n)\nprint(type(df))\n# &lt;class 'polars.DataFrame'&gt;\n\n# Batch process Parquet files\nfor batch in fs.read_files(\n    \"data/*.parquet\",\n    format=\"parquet\",\n    batch_size=100,\n    use_threads=True\n):\n    print(f\"Batch type: {type(batch)}\")\n\n# Read JSON Lines\ndf = fs.read_files(\n    \"logs/*.jsonl\",\n    format=\"json\",\n    jsonlines=True,\n    concat=True\n)\nprint(df.columns)\n</code></pre>"},{"location":"api/fsspeckit.core.ext/#pyarrow_dataset","title":"<code>pyarrow_dataset()</code>","text":"<p>Create a PyArrow dataset from files in any supported format.</p> <p>Creates a dataset that provides optimized reading and querying capabilities including:</p> <ul> <li>Schema inference and enforcement</li> <li>Partition discovery and pruning</li> <li>Predicate pushdown</li> <li>Column projection</li> </ul> Parameter Type Description <code>path</code> <code>str</code> Base path to dataset files <code>format</code> <code>str</code> File format. Currently supports: - \"parquet\" (default) - \"csv\" - \"json\" (experimental) <code>schema</code> <code>pa.Schema | None</code> Optional schema to enforce. If None, inferred from data. <code>partitioning</code> <code>str</code> or <code>list[str]</code> or <code>pds.Partitioning</code> How the dataset is partitioned. Can be: - <code>str</code>: Single partition field - <code>list[str]</code>: Multiple partition fields - <code>pds.Partitioning</code>: Custom partitioning scheme <code>**kwargs</code> <code>Any</code> Additional arguments for dataset creation Returns Type Description <code>pds.Dataset</code> <code>pds.Dataset</code> PyArrow dataset instance <p>Example:</p> <pre><code>from fsspec.implementations.local import LocalFileSystem\n\nfs = LocalFileSystem()\n# Simple Parquet dataset\nds = fs.pyarrow_dataset(\"data/\")\nprint(ds.schema)\n\n# Partitioned dataset\nds = fs.pyarrow_dataset(\n    \"events/\",\n    partitioning=[\"year\", \"month\"]\n)\n# Query with partition pruning\ntable = ds.to_table(\n    filter=(ds.field(\"year\") == 2024)\n)\n\n# CSV with schema\nds = fs.pyarrow_dataset(\n    \"logs/\",\n    format=\"csv\",\n    schema=pa.schema([\n        (\"timestamp\", pa.timestamp(\"s\")),\n        (\"level\", pa.string()),\n        (\"message\", pa.string())\n    ])\n)\n</code></pre>"},{"location":"api/fsspeckit.core.ext/#pyarrow_parquet_dataset","title":"<code>pyarrow_parquet_dataset()</code>","text":"<p>Create a PyArrow dataset optimized for Parquet files.</p> <p>Creates a dataset specifically for Parquet data, automatically handling <code>_metadata</code> files for optimized reading.</p> <p>This function is particularly useful for:</p> <ul> <li>Datasets with existing <code>_metadata</code> files</li> <li>Multi-file datasets that should be treated as one</li> <li>Partitioned Parquet datasets</li> </ul> Parameter Type Description <code>path</code> <code>str</code> Path to dataset directory or <code>_metadata</code> file <code>schema</code> <code>pa.Schema | None</code> Optional schema to enforce. If None, inferred from data. <code>partitioning</code> <code>str</code> or <code>list[str]</code> or <code>pds.Partitioning</code> How the dataset is partitioned. Can be: - <code>str</code>: Single partition field - <code>list[str]</code>: Multiple partition fields - <code>pds.Partitioning</code>: Custom partitioning scheme <code>**kwargs</code> <code>Any</code> Additional dataset arguments Returns Type Description <code>pds.Dataset</code> <code>pds.Dataset</code> PyArrow dataset instance <p>Example:</p> <pre><code>from fsspec.implementations.local import LocalFileSystem\n\nfs = LocalFileSystem()\n# Dataset with _metadata\nds = fs.pyarrow_parquet_dataset(\"data/_metadata\")\nprint(ds.files)  # Shows all data files\n\n# Partitioned dataset directory\nds = fs.pyarrow_parquet_dataset(\n    \"sales/\",\n    partitioning=[\"year\", \"region\"]\n)\n# Query with partition pruning\ntable = ds.to_table(\n    filter=(\n        (ds.field(\"year\") == 2024) &amp;\n        (ds.field(\"region\") == \"EMEA\")\n    )\n)\n</code></pre>"},{"location":"api/fsspeckit.core.filesystem/","title":"fsspeckit.core.filesystem","text":""},{"location":"api/fsspeckit.core.filesystem/#fsspeckit.core.filesystem","title":"filesystem","text":"<p>Core filesystem functionality - focused on factory functions and high-level APIs.</p> <p>This module has been refactored to be more focused: - Main factory functions (filesystem, get_filesystem) - High-level filesystem types (GitLabFileSystem) - Path helpers imported from filesystem_paths - Cache classes imported from filesystem_cache</p> <p>Internal implementation details have been moved to: - filesystem_paths: Path manipulation and protocol detection - filesystem_cache: Cache mapper and monitored cache filesystem</p>"},{"location":"api/fsspeckit.core.filesystem/#fsspeckit.core.filesystem-classes","title":"Classes","text":""},{"location":"api/fsspeckit.core.filesystem/#fsspeckit.core.filesystem.GitLabFileSystem","title":"fsspeckit.core.filesystem.GitLabFileSystem","text":"<pre><code>GitLabFileSystem(\n    base_url: str = \"https://gitlab.com\",\n    project_id: Optional[Union[str, int]] = None,\n    project_name: Optional[str] = None,\n    ref: str = \"main\",\n    token: Optional[str] = None,\n    api_version: str = \"v4\",\n    **kwargs: Any,\n)\n</code></pre> <p>               Bases: <code>AbstractFileSystem</code></p> <p>Filesystem interface for GitLab repositories.</p> <p>Provides read-only access to files in GitLab repositories, including: - Public and private repositories - Self-hosted GitLab instances - Branch/tag/commit selection - Token-based authentication</p> <p>Attributes:</p> Name Type Description <code>protocol</code> <code>str</code> <p>Always \"gitlab\"</p> <code>base_url</code> <code>str</code> <p>GitLab instance URL</p> <code>project_id</code> <code>str</code> <p>Project ID</p> <code>project_name</code> <code>str</code> <p>Project name/path</p> <code>ref</code> <code>str</code> <p>Git reference (branch, tag, commit)</p> <code>token</code> <code>str</code> <p>Access token</p> <code>api_version</code> <code>str</code> <p>API version</p> Example <pre><code># Public repository\nfs = GitLabFileSystem(\n    project_name=\"group/project\",\n    ref=\"main\",\n)\nfiles = fs.ls(\"/\")\n\n# Private repository with token\nfs = GitLabFileSystem(\n    project_id=\"12345\",\n    token=\"glpat_xxxx\",\n    ref=\"develop\",\n)\ncontent = fs.cat(\"README.md\")\n</code></pre> <p>Initialize GitLab filesystem.</p> <p>Parameters:</p> Name Type Description Default <code>base_url</code> <code>str</code> <p>GitLab instance URL</p> <code>'https://gitlab.com'</code> <code>project_id</code> <code>Optional[Union[str, int]]</code> <p>Project ID number</p> <code>None</code> <code>project_name</code> <code>Optional[str]</code> <p>Project name/path (alternative to project_id)</p> <code>None</code> <code>ref</code> <code>str</code> <p>Git reference (branch, tag, or commit SHA)</p> <code>'main'</code> <code>token</code> <code>Optional[str]</code> <p>GitLab personal access token</p> <code>None</code> <code>api_version</code> <code>str</code> <p>API version to use</p> <code>'v4'</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments</p> <code>{}</code> Source code in <code>src/fsspeckit/core/filesystem.py</code> <pre><code>def __init__(\n    self,\n    base_url: str = \"https://gitlab.com\",\n    project_id: Optional[Union[str, int]] = None,\n    project_name: Optional[str] = None,\n    ref: str = \"main\",\n    token: Optional[str] = None,\n    api_version: str = \"v4\",\n    **kwargs: Any,\n):\n    \"\"\"Initialize GitLab filesystem.\n\n    Args:\n        base_url: GitLab instance URL\n        project_id: Project ID number\n        project_name: Project name/path (alternative to project_id)\n        ref: Git reference (branch, tag, or commit SHA)\n        token: GitLab personal access token\n        api_version: API version to use\n        **kwargs: Additional arguments\n    \"\"\"\n    super().__init__(**kwargs)\n\n    self.base_url = base_url.rstrip(\"/\")\n    self.project_id = project_id\n    self.project_name = project_name\n    self.ref = ref\n    self.token = token\n    self.api_version = api_version\n\n    if not project_id and not project_name:\n        raise ValueError(\"Either project_id or project_name must be provided\")\n</code></pre>"},{"location":"api/fsspeckit.core.filesystem/#fsspeckit.core.filesystem.GitLabFileSystem-functions","title":"Functions","text":""},{"location":"api/fsspeckit.core.filesystem/#fsspeckit.core.filesystem.GitLabFileSystem.cat_file","title":"fsspeckit.core.filesystem.GitLabFileSystem.cat_file","text":"<pre><code>cat_file(path: str, **kwargs: Any) -&gt; bytes\n</code></pre> <p>Get file content.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>File path</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>bytes</code> <p>File content</p> Source code in <code>src/fsspeckit/core/filesystem.py</code> <pre><code>def cat_file(self, path: str, **kwargs: Any) -&gt; bytes:\n    \"\"\"Get file content.\n\n    Args:\n        path: File path\n        **kwargs: Additional arguments\n\n    Returns:\n        File content\n    \"\"\"\n    params = {\"ref\": self.ref}\n\n    response = self._make_request(f\"repository/files/{path}\", params)\n    data = response.json()\n\n    import base64\n\n    return base64.b64decode(data[\"content\"])\n</code></pre>"},{"location":"api/fsspeckit.core.filesystem/#fsspeckit.core.filesystem.GitLabFileSystem.exists","title":"fsspeckit.core.filesystem.GitLabFileSystem.exists","text":"<pre><code>exists(path: str, **kwargs: Any) -&gt; bool\n</code></pre> <p>Check if file exists.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>File path</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if file exists</p> Source code in <code>src/fsspeckit/core/filesystem.py</code> <pre><code>def exists(self, path: str, **kwargs: Any) -&gt; bool:\n    \"\"\"Check if file exists.\n\n    Args:\n        path: File path\n        **kwargs: Additional arguments\n\n    Returns:\n        True if file exists\n    \"\"\"\n    try:\n        self.info(path, **kwargs)\n        return True\n    except requests.HTTPError:\n        return False\n</code></pre>"},{"location":"api/fsspeckit.core.filesystem/#fsspeckit.core.filesystem.GitLabFileSystem.info","title":"fsspeckit.core.filesystem.GitLabFileSystem.info","text":"<pre><code>info(path: str, **kwargs: Any) -&gt; dict\n</code></pre> <p>Get file information.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>File path</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict</code> <p>File information</p> Source code in <code>src/fsspeckit/core/filesystem.py</code> <pre><code>def info(self, path: str, **kwargs: Any) -&gt; dict:\n    \"\"\"Get file information.\n\n    Args:\n        path: File path\n        **kwargs: Additional arguments\n\n    Returns:\n        File information\n    \"\"\"\n    params = {\"ref\": self.ref}\n\n    response = self._make_request(f\"repository/files/{path}\", params)\n    return response.json()\n</code></pre>"},{"location":"api/fsspeckit.core.filesystem/#fsspeckit.core.filesystem.GitLabFileSystem.ls","title":"fsspeckit.core.filesystem.GitLabFileSystem.ls","text":"<pre><code>ls(\n    path: str = \"\", detail: bool = False, **kwargs: Any\n) -&gt; Union[List[Any], Any]\n</code></pre> <p>List files in repository.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Directory path</p> <code>''</code> <code>detail</code> <code>bool</code> <p>Whether to return detailed information</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[List[Any], Any]</code> <p>List of files</p> Source code in <code>src/fsspeckit/core/filesystem.py</code> <pre><code>def ls(self, path: str = \"\", detail: bool = False, **kwargs: Any) -&gt; Union[List[Any], Any]:\n    \"\"\"List files in repository.\n\n    Args:\n        path: Directory path\n        detail: Whether to return detailed information\n        **kwargs: Additional arguments\n\n    Returns:\n        List of files\n    \"\"\"\n    params = {\"ref\": self.ref, \"per_page\": 100}\n\n    if path:\n        params[\"path\"] = path.lstrip(\"/\")\n\n    response = self._make_request(\"repository/tree\", params)\n    files = response.json()\n\n    if detail:\n        return files\n    else:\n        return [item[\"name\"] for item in files]\n</code></pre>"},{"location":"api/fsspeckit.core.filesystem/#fsspeckit.core.filesystem-functions","title":"Functions","text":""},{"location":"api/fsspeckit.core.filesystem/#fsspeckit.core.filesystem.dir_ls_p","title":"fsspeckit.core.filesystem.dir_ls_p","text":"<pre><code>dir_ls_p(\n    self, path: str, detail: bool = False, **kwargs: Any\n) -&gt; Union[List[Any], Any]\n</code></pre> <p>List directory contents with path handling.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Directory path</p> required <code>detail</code> <code>bool</code> <p>Whether to return detailed information</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[List[Any], Any]</code> <p>Directory listing</p> Source code in <code>src/fsspeckit/core/filesystem.py</code> <pre><code>def dir_ls_p(self, path: str, detail: bool = False, **kwargs: Any) -&gt; Union[List[Any], Any]:\n    \"\"\"List directory contents with path handling.\n\n    Args:\n        path: Directory path\n        detail: Whether to return detailed information\n        **kwargs: Additional arguments\n\n    Returns:\n        Directory listing\n    \"\"\"\n    path = self._strip_protocol(path)\n    return self.fs.ls(path, detail=detail, **kwargs)\n</code></pre>"},{"location":"api/fsspeckit.core.filesystem/#fsspeckit.core.filesystem.filesystem","title":"fsspeckit.core.filesystem.filesystem","text":"<pre><code>filesystem(\n    protocol_or_path: str | None = \"\",\n    storage_options: Optional[\n        Union[BaseStorageOptions, dict]\n    ] = None,\n    cached: bool = False,\n    cache_storage: Optional[str] = None,\n    verbose: bool = False,\n    dirfs: bool = True,\n    base_fs: AbstractFileSystem = None,\n    use_listings_cache: bool = True,\n    skip_instance_cache: bool = False,\n    **kwargs: Any,\n) -&gt; AbstractFileSystem\n</code></pre> <p>Get filesystem instance with enhanced configuration options.</p> <p>Creates filesystem instances with support for storage options classes, intelligent caching, and protocol inference from paths.</p> <p>Parameters:</p> Name Type Description Default <code>protocol_or_path</code> <code>str | None</code> <p>Filesystem protocol (e.g., \"s3\", \"file\") or path with protocol prefix</p> <code>''</code> <code>storage_options</code> <code>Optional[Union[BaseStorageOptions, dict]]</code> <p>Storage configuration as BaseStorageOptions instance or dict</p> <code>None</code> <code>cached</code> <code>bool</code> <p>Whether to wrap filesystem in caching layer</p> <code>False</code> <code>cache_storage</code> <code>Optional[str]</code> <p>Cache directory path (if cached=True)</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Enable verbose logging for cache operations</p> <code>False</code> <code>dirfs</code> <code>bool</code> <p>Whether to wrap filesystem in DirFileSystem</p> <code>True</code> <code>base_fs</code> <code>AbstractFileSystem</code> <p>Base filesystem instance to use</p> <code>None</code> <code>use_listings_cache</code> <code>bool</code> <p>Whether to enable directory-listing cache</p> <code>True</code> <code>skip_instance_cache</code> <code>bool</code> <p>Whether to skip fsspec instance caching</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional filesystem arguments</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>AbstractFileSystem</code> <code>AbstractFileSystem</code> <p>Configured filesystem instance</p> Example <pre><code># Basic local filesystem\nfs = filesystem(\"file\")\n\n# S3 with storage options\nfrom fsspeckit.storage_options import AwsStorageOptions\nopts = AwsStorageOptions(region=\"us-west-2\")\nfs = filesystem(\"s3\", storage_options=opts, cached=True)\n\n# Infer protocol from path\nfs = filesystem(\"s3://my-bucket/\", cached=True)\n\n# GitLab filesystem\nfs = filesystem(\n    \"gitlab\",\n    storage_options={\n        \"project_name\": \"group/project\",\n        \"token\": \"glpat_xxxx\",\n    },\n)\n</code></pre> Source code in <code>src/fsspeckit/core/filesystem.py</code> <pre><code>def filesystem(\n    protocol_or_path: str | None = \"\",\n    storage_options: Optional[Union[BaseStorageOptions, dict]] = None,\n    cached: bool = False,\n    cache_storage: Optional[str] = None,\n    verbose: bool = False,\n    dirfs: bool = True,\n    base_fs: AbstractFileSystem = None,\n    use_listings_cache: bool = True,  # \u2190 disable directory-listing cache\n    skip_instance_cache: bool = False,\n    **kwargs: Any,\n) -&gt; AbstractFileSystem:\n    \"\"\"Get filesystem instance with enhanced configuration options.\n\n    Creates filesystem instances with support for storage options classes,\n    intelligent caching, and protocol inference from paths.\n\n    Args:\n        protocol_or_path: Filesystem protocol (e.g., \"s3\", \"file\") or path with protocol prefix\n        storage_options: Storage configuration as BaseStorageOptions instance or dict\n        cached: Whether to wrap filesystem in caching layer\n        cache_storage: Cache directory path (if cached=True)\n        verbose: Enable verbose logging for cache operations\n        dirfs: Whether to wrap filesystem in DirFileSystem\n        base_fs: Base filesystem instance to use\n        use_listings_cache: Whether to enable directory-listing cache\n        skip_instance_cache: Whether to skip fsspec instance caching\n        **kwargs: Additional filesystem arguments\n\n    Returns:\n        AbstractFileSystem: Configured filesystem instance\n\n    Example:\n        ```python\n        # Basic local filesystem\n        fs = filesystem(\"file\")\n\n        # S3 with storage options\n        from fsspeckit.storage_options import AwsStorageOptions\n        opts = AwsStorageOptions(region=\"us-west-2\")\n        fs = filesystem(\"s3\", storage_options=opts, cached=True)\n\n        # Infer protocol from path\n        fs = filesystem(\"s3://my-bucket/\", cached=True)\n\n        # GitLab filesystem\n        fs = filesystem(\n            \"gitlab\",\n            storage_options={\n                \"project_name\": \"group/project\",\n                \"token\": \"glpat_xxxx\",\n            },\n        )\n        ```\n    \"\"\"\n    if isinstance(protocol_or_path, Path):\n        protocol_or_path = protocol_or_path.as_posix()\n\n    raw_input = _ensure_string(protocol_or_path)\n    protocol_from_kwargs = kwargs.pop(\"protocol\", None)\n\n    provided_protocol: str | None = None\n    base_path_input: str = \"\"\n\n    if raw_input:\n        provided_protocol, remainder = split_protocol(raw_input)\n        if provided_protocol:\n            base_path_input = remainder or \"\"\n        else:\n            base_path_input = remainder or raw_input\n            if base_fs is None and base_path_input in known_implementations:\n                provided_protocol = base_path_input\n                base_path_input = \"\"\n    else:\n        base_path_input = \"\"\n\n    base_path_input = base_path_input.replace(\"\\\\\", \"/\")\n\n    if (\n        base_fs is None\n        and base_path_input\n        and (provided_protocol or protocol_from_kwargs) in {None, \"file\", \"local\"}\n    ):\n        detected_parent, is_file = _detect_local_file_path(base_path_input)\n        if is_file:\n            base_path_input = detected_parent\n\n    base_path = _normalize_path(base_path_input)\n    cache_path_hint = base_path\n\n    if base_fs is not None:\n        if not dirfs:\n            raise ValueError(\"dirfs must be True when providing base_fs\")\n\n        base_is_dir = isinstance(base_fs, DirFileSystem)\n        underlying_fs = base_fs.fs if base_is_dir else base_fs\n        underlying_protocols = _protocol_set(underlying_fs.protocol)\n        requested_protocol = provided_protocol or protocol_from_kwargs\n\n        if requested_protocol and not _protocol_matches(\n            requested_protocol, underlying_protocols\n        ):\n            raise ValueError(\n                f\"Protocol '{requested_protocol}' does not match base filesystem protocol \"\n                f\"{sorted(underlying_protocols)}\"\n            )\n\n        sep = getattr(underlying_fs, \"sep\", \"/\") or \"/\"\n        base_root = base_fs.path if base_is_dir else \"\"\n        base_root_norm = _normalize_path(base_root, sep)\n        cache_path_hint = base_root_norm\n\n        fs: AbstractFileSystem\n        path_for_cache = base_root_norm\n\n        if requested_protocol:\n            absolute_target = _strip_for_fs(underlying_fs, raw_input)\n            absolute_target = _normalize_path(absolute_target, sep)\n\n            if (\n                base_is_dir\n                and base_root_norm\n                and not _is_within(base_root_norm, absolute_target, sep)\n            ):\n                raise ValueError(\n                    f\"Requested path '{absolute_target}' is outside the base directory \"\n                    f\"'{base_root_norm}'\"\n                )\n\n            if base_is_dir and absolute_target == base_root_norm:\n                fs = base_fs\n            else:\n                fs = DirFileSystem(path=absolute_target, fs=underlying_fs)\n\n            path_for_cache = absolute_target\n        else:\n            rel_input = base_path\n            if rel_input:\n                segments = [segment for segment in rel_input.split(sep) if segment]\n                if any(segment == \"..\" for segment in segments):\n                    raise ValueError(\n                        \"Relative paths must not escape the base filesystem root\"\n                    )\n\n                candidate = _normalize_path(rel_input, sep)\n                absolute_target = _smart_join(base_root_norm, candidate, sep)\n\n                if (\n                    base_is_dir\n                    and base_root_norm\n                    and not _is_within(base_root_norm, absolute_target, sep)\n                ):\n                    raise ValueError(\n                        f\"Resolved path '{absolute_target}' is outside the base \"\n                        f\"directory '{base_root_norm}'\"\n                    )\n\n                if base_is_dir and absolute_target == base_root_norm:\n                    fs = base_fs\n                else:\n                    fs = DirFileSystem(path=absolute_target, fs=underlying_fs)\n\n                path_for_cache = absolute_target\n            else:\n                fs = base_fs\n                path_for_cache = base_root_norm\n\n        cache_path_hint = path_for_cache\n\n        if cached:\n            if getattr(fs, \"is_cache_fs\", False):\n                return fs\n            storage = cache_storage\n            if storage is None:\n                storage = _default_cache_storage(cache_path_hint or None)\n            cached_fs = MonitoredSimpleCacheFileSystem(\n                fs=fs, cache_storage=storage, verbose=verbose\n            )\n            cached_fs.is_cache_fs = True\n            return cached_fs\n\n        if not hasattr(fs, \"is_cache_fs\"):\n            fs.is_cache_fs = False\n        return fs\n\n    protocol = provided_protocol or protocol_from_kwargs\n    if protocol is None:\n        if isinstance(storage_options, dict):\n            protocol = storage_options.get(\"protocol\")\n        else:\n            protocol = getattr(storage_options, \"protocol\", None)\n\n    protocol = protocol or \"file\"\n    protocol = protocol.lower()\n\n    if protocol in {\"file\", \"local\"}:\n        fs = fsspec_filesystem(\n            protocol,\n            use_listings_cache=use_listings_cache,\n            skip_instance_cache=skip_instance_cache,\n        )\n        if dirfs:\n            dir_path: str | Path = base_path or Path.cwd()\n            fs = DirFileSystem(path=dir_path, fs=fs)\n            cache_path_hint = _ensure_string(dir_path)\n\n        if cached:\n            if getattr(fs, \"is_cache_fs\", False):\n                return fs\n            storage = cache_storage\n            if storage is None:\n                storage = _default_cache_storage(cache_path_hint or None)\n            cached_fs = MonitoredSimpleCacheFileSystem(\n                fs=fs, cache_storage=storage, verbose=verbose\n            )\n            cached_fs.is_cache_fs = True\n            return cached_fs\n\n        if not hasattr(fs, \"is_cache_fs\"):\n            fs.is_cache_fs = False\n        return fs\n\n    protocol_for_instance_cache = protocol\n    kwargs[\"protocol\"] = protocol\n\n    fs = fsspec_filesystem(\n        protocol,\n        **kwargs,\n        use_listings_cache=use_listings_cache,\n        skip_instance_cache=skip_instance_cache,\n    )\n\n    if cached:\n        if getattr(fs, \"is_cache_fs\", False):\n            return fs\n        storage = cache_storage\n        if storage is None:\n            storage = _default_cache_storage(cache_path_hint or None)\n        cached_fs = MonitoredSimpleCacheFileSystem(\n            fs=fs, cache_storage=storage, verbose=verbose\n        )\n        cached_fs.is_cache_fs = True\n        return cached_fs\n\n    if not hasattr(fs, \"is_cache_fs\"):\n        fs.is_cache_fs = False\n    return fs\n</code></pre>"},{"location":"api/fsspeckit.core.filesystem/#fsspeckit.core.filesystem.get_filesystem","title":"fsspeckit.core.filesystem.get_filesystem","text":"<pre><code>get_filesystem(\n    protocol_or_path: str | None = \"\",\n    storage_options: Optional[\n        Union[BaseStorageOptions, dict]\n    ] = None,\n    **kwargs: Any,\n) -&gt; AbstractFileSystem\n</code></pre> <p>Get filesystem instance (simple version).</p> <p>This is a simplified version of filesystem() for backward compatibility. See filesystem() for full documentation.</p> <p>Parameters:</p> Name Type Description Default <code>protocol_or_path</code> <code>str | None</code> <p>Filesystem protocol or path</p> <code>''</code> <code>storage_options</code> <code>Optional[Union[BaseStorageOptions, dict]]</code> <p>Storage configuration</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>AbstractFileSystem</code> <code>AbstractFileSystem</code> <p>Filesystem instance</p> Source code in <code>src/fsspeckit/core/filesystem.py</code> <pre><code>def get_filesystem(\n    protocol_or_path: str | None = \"\",\n    storage_options: Optional[Union[BaseStorageOptions, dict]] = None,\n    **kwargs: Any,\n) -&gt; AbstractFileSystem:\n    \"\"\"Get filesystem instance (simple version).\n\n    This is a simplified version of filesystem() for backward compatibility.\n    See filesystem() for full documentation.\n\n    Args:\n        protocol_or_path: Filesystem protocol or path\n        storage_options: Storage configuration\n        **kwargs: Additional arguments\n\n    Returns:\n        AbstractFileSystem: Filesystem instance\n    \"\"\"\n    return filesystem(\n        protocol_or_path=protocol_or_path,\n        storage_options=storage_options,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/fsspeckit.core.filesystem/#fsspeckit.core.filesystem.mscf_ls_p","title":"fsspeckit.core.filesystem.mscf_ls_p","text":"<pre><code>mscf_ls_p(\n    self, path: str, detail: bool = False, **kwargs: Any\n) -&gt; Union[List[Any], Any]\n</code></pre> <p>List directory for monitored cache filesystem.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Directory path</p> required <code>detail</code> <code>bool</code> <p>Whether to return detailed information</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[List[Any], Any]</code> <p>Directory listing</p> Source code in <code>src/fsspeckit/core/filesystem.py</code> <pre><code>def mscf_ls_p(self, path: str, detail: bool = False, **kwargs: Any) -&gt; Union[List[Any], Any]:\n    \"\"\"List directory for monitored cache filesystem.\n\n    Args:\n        path: Directory path\n        detail: Whether to return detailed information\n        **kwargs: Additional arguments\n\n    Returns:\n        Directory listing\n    \"\"\"\n    return self.fs.ls(path, detail=detail, **kwargs)\n</code></pre>"},{"location":"api/fsspeckit.core.filesystem/#fsspeckit.core.filesystem.setup_filesystem_logging","title":"fsspeckit.core.filesystem.setup_filesystem_logging","text":"<pre><code>setup_filesystem_logging() -&gt; None\n</code></pre> <p>Setup filesystem logging configuration.</p> Source code in <code>src/fsspeckit/core/filesystem.py</code> <pre><code>def setup_filesystem_logging() -&gt; None:\n    \"\"\"Setup filesystem logging configuration.\"\"\"\n    # This is a placeholder for any filesystem-specific logging setup\n    # Currently, logging is handled by the common logging module\n    pass\n</code></pre>"},{"location":"api/fsspeckit.core.filesystem/#fsspeckit.core.filesystem-modules","title":"Modules","text":""},{"location":"api/fsspeckit.core.maintenance/","title":"fsspeckit.core.maintenance","text":""},{"location":"api/fsspeckit.core.maintenance/#fsspeckit.core.maintenance","title":"maintenance","text":"<p>Backend-neutral maintenance layer for parquet dataset operations.</p> <p>This module provides shared functionality for dataset discovery, statistics, and grouping algorithms used by both DuckDB and PyArrow maintenance operations. It serves as the authoritative implementation for maintenance planning, ensuring consistent behavior across different backends.</p> <p>Key responsibilities: 1. Dataset discovery and file-level statistics 2. Compaction grouping algorithms with streaming execution 3. Optimization planning with z-order validation 4. Canonical statistics structures 5. Partition filtering and edge case handling</p> <p>Architecture: - Functions accept both dict format (legacy) and FileInfo objects for backward compatibility - All planning functions return structured results with canonical MaintenanceStats - Backend implementations delegate to this core for consistent behavior - Streaming design avoids materializing entire datasets in memory</p> <p>Core components: - FileInfo: Canonical file information with validation - MaintenanceStats: Canonical statistics structure across backends - CompactionGroup: Logical grouping of files for processing - collect_dataset_stats: Dataset discovery with partition filtering - plan_compaction_groups: Shared compaction planning algorithm - plan_optimize_groups: Shared optimization planning with z-order validation</p> <p>Usage: Backend functions should delegate to this module rather than implementing their own discovery and planning logic. This ensures that DuckDB and PyArrow produce identical grouping decisions and statistics structures.</p>"},{"location":"api/fsspeckit.core.maintenance/#fsspeckit.core.maintenance-classes","title":"Classes","text":""},{"location":"api/fsspeckit.core.maintenance/#fsspeckit.core.maintenance.CompactionGroup","title":"fsspeckit.core.maintenance.CompactionGroup  <code>dataclass</code>","text":"<pre><code>CompactionGroup(files: list[FileInfo])\n</code></pre> <p>A group of files to be compacted or optimized together.</p> <p>This dataclass represents a logical grouping of files that will be processed together during maintenance operations. It enables streaming execution by bounding the amount of data processed at once.</p> <p>Attributes:</p> Name Type Description <code>files</code> <code>list[FileInfo]</code> <p>List of FileInfo objects in this group.</p> <code>total_size_bytes</code> <code>int</code> <p>Total size of all files in this group (computed).</p> <code>total_rows</code> <code>int</code> <p>Total rows across all files in this group (computed).</p> Note <p>Must contain at least one file. The total_size_bytes and total_rows are computed during initialization and used for planning decisions. This structure enables per-group streaming processing without materializing entire datasets.</p>"},{"location":"api/fsspeckit.core.maintenance/#fsspeckit.core.maintenance.FileInfo","title":"fsspeckit.core.maintenance.FileInfo  <code>dataclass</code>","text":"<pre><code>FileInfo(path: str, size_bytes: int, num_rows: int)\n</code></pre> <p>Information about a single parquet file with validation.</p> <p>This canonical data structure represents file metadata across all backends. It enables consistent file information handling and size-based planning.</p> <p>Attributes:</p> Name Type Description <code>path</code> <code>str</code> <p>File path relative to the dataset root.</p> <code>size_bytes</code> <code>int</code> <p>File size in bytes; must be &gt;= 0.</p> <code>num_rows</code> <code>int</code> <p>Number of rows in the file; must be &gt;= 0.</p> Note <p>The size_bytes and num_rows values are validated to be non-negative. This class is used throughout the maintenance planning pipeline for consistent file metadata representation.</p>"},{"location":"api/fsspeckit.core.maintenance/#fsspeckit.core.maintenance.MaintenanceStats","title":"fsspeckit.core.maintenance.MaintenanceStats  <code>dataclass</code>","text":"<pre><code>MaintenanceStats(\n    before_file_count: int,\n    after_file_count: int,\n    before_total_bytes: int,\n    after_total_bytes: int,\n    compacted_file_count: int,\n    rewritten_bytes: int,\n    compression_codec: Union[str, None] = None,\n    dry_run: bool = False,\n    zorder_columns: list[str] | None = None,\n    planned_groups: list[list[str]] | None = None,\n)\n</code></pre> <p>Canonical statistics structure for maintenance operations.</p> <p>This dataclass provides the authoritative statistics format for all maintenance operations across DuckDB and PyArrow backends. It ensures consistent reporting and enables unified testing and validation.</p> <p>Attributes:</p> Name Type Description <code>before_file_count</code> <code>int</code> <p>Number of files before the operation.</p> <code>after_file_count</code> <code>int</code> <p>Number of files after the operation.</p> <code>before_total_bytes</code> <code>int</code> <p>Total bytes before the operation.</p> <code>after_total_bytes</code> <code>int</code> <p>Total bytes after the operation.</p> <code>compacted_file_count</code> <code>int</code> <p>Number of files that were compacted/rewritten.</p> <code>rewritten_bytes</code> <code>int</code> <p>Total bytes rewritten during the operation.</p> <code>compression_codec</code> <code>Union[str, None]</code> <p>Compression codec used (None if unchanged).</p> <code>dry_run</code> <code>bool</code> <p>Whether this was a dry run operation.</p> <code>zorder_columns</code> <code>list[str] | None</code> <p>Z-order columns used (for optimization operations).</p> <code>planned_groups</code> <code>list[list[str]] | None</code> <p>File groupings planned during dry run.</p> Note <p>All numeric fields are validated to be non-negative. The to_dict() method provides backward compatibility with existing code expecting dictionary format.</p>"},{"location":"api/fsspeckit.core.maintenance/#fsspeckit.core.maintenance.MaintenanceStats-functions","title":"Functions","text":""},{"location":"api/fsspeckit.core.maintenance/#fsspeckit.core.maintenance.MaintenanceStats.to_dict","title":"fsspeckit.core.maintenance.MaintenanceStats.to_dict","text":"<pre><code>to_dict() -&gt; dict[str, Any]\n</code></pre> <p>Convert to dictionary format for backward compatibility.</p> Source code in <code>src/fsspeckit/core/maintenance.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert to dictionary format for backward compatibility.\"\"\"\n    result = {\n        \"before_file_count\": self.before_file_count,\n        \"after_file_count\": self.after_file_count,\n        \"before_total_bytes\": self.before_total_bytes,\n        \"after_total_bytes\": self.after_total_bytes,\n        \"compacted_file_count\": self.compacted_file_count,\n        \"rewritten_bytes\": self.rewritten_bytes,\n        \"compression_codec\": self.compression_codec,\n        \"dry_run\": self.dry_run,\n    }\n\n    if self.zorder_columns is not None:\n        result[\"zorder_columns\"] = self.zorder_columns\n    if self.planned_groups is not None:\n        result[\"planned_groups\"] = self.planned_groups\n\n    return result\n</code></pre>"},{"location":"api/fsspeckit.core.maintenance/#fsspeckit.core.maintenance-functions","title":"Functions","text":""},{"location":"api/fsspeckit.core.maintenance/#fsspeckit.core.maintenance.collect_dataset_stats","title":"fsspeckit.core.maintenance.collect_dataset_stats","text":"<pre><code>collect_dataset_stats(\n    path: str,\n    filesystem: AbstractFileSystem | None = None,\n    partition_filter: list[str] | None = None,\n) -&gt; dict[str, Any]\n</code></pre> <p>Collect file-level statistics for a parquet dataset.</p> <p>This function walks the given dataset directory on the provided filesystem, discovers parquet files (recursively), and returns basic statistics.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Root directory of the parquet dataset.</p> required <code>filesystem</code> <code>AbstractFileSystem | None</code> <p>Optional fsspec filesystem. If omitted, a local \"file\" filesystem is used.</p> <code>None</code> <code>partition_filter</code> <code>list[str] | None</code> <p>Optional list of partition prefix filters (e.g. [\"date=2025-11-04\"]). Only files whose path relative to <code>path</code> starts with one of these prefixes are included.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict with keys:</p> <code>dict[str, Any]</code> <ul> <li><code>files</code>: list of <code>{\"path\", \"size_bytes\", \"num_rows\"}</code> dicts</li> </ul> <code>dict[str, Any]</code> <ul> <li><code>total_bytes</code>: sum of file sizes</li> </ul> <code>dict[str, Any]</code> <ul> <li><code>total_rows</code>: sum of row counts</li> </ul> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the path does not exist or no parquet files match the optional partition filter.</p> Source code in <code>src/fsspeckit/core/maintenance.py</code> <pre><code>def collect_dataset_stats(\n    path: str,\n    filesystem: AbstractFileSystem | None = None,\n    partition_filter: list[str] | None = None,\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Collect file-level statistics for a parquet dataset.\n\n    This function walks the given dataset directory on the provided filesystem,\n    discovers parquet files (recursively), and returns basic statistics.\n\n    Args:\n        path: Root directory of the parquet dataset.\n        filesystem: Optional fsspec filesystem. If omitted, a local \"file\"\n            filesystem is used.\n        partition_filter: Optional list of partition prefix filters\n            (e.g. [\"date=2025-11-04\"]). Only files whose path relative to\n            ``path`` starts with one of these prefixes are included.\n\n    Returns:\n        Dict with keys:\n        - ``files``: list of ``{\"path\", \"size_bytes\", \"num_rows\"}`` dicts\n        - ``total_bytes``: sum of file sizes\n        - ``total_rows``: sum of row counts\n\n    Raises:\n        FileNotFoundError: If the path does not exist or no parquet files\n            match the optional partition filter.\n    \"\"\"\n    import pyarrow.parquet as pq\n\n    fs = filesystem or fsspec_filesystem(\"file\")\n\n    if not fs.exists(path):\n        raise FileNotFoundError(f\"Dataset path '{path}' does not exist\")\n\n    root = Path(path)\n\n    # Discover parquet files recursively via a manual stack walk so we can\n    # respect partition_filter prefixes on the logical relative path.\n    files: list[str] = []\n    stack: list[str] = [path]\n    while stack:\n        current_dir = stack.pop()\n        try:\n            entries = fs.ls(current_dir, detail=False)\n        except (OSError, PermissionError) as e:\n            logger.warning(\"Failed to list directory '%s': %s\", current_dir, e)\n            continue\n\n        for entry in entries:\n            if entry.endswith(\".parquet\"):\n                files.append(entry)\n            else:\n                try:\n                    if fs.isdir(entry):\n                        stack.append(entry)\n                except (OSError, PermissionError) as e:\n                    logger.warning(\"Failed to check if entry '%s' is a directory: %s\", entry, e)\n                    continue\n\n    if partition_filter:\n        normalized_filters = [p.rstrip(\"/\") for p in partition_filter]\n        filtered_files: list[str] = []\n        for filename in files:\n            rel = Path(filename).relative_to(root).as_posix()\n            if any(rel.startswith(prefix) for prefix in normalized_filters):\n                filtered_files.append(filename)\n        files = filtered_files\n\n    if not files:\n        raise FileNotFoundError(\n            f\"No parquet files found under '{path}' matching filter\"\n        )\n\n    file_infos: list[dict[str, Any]] = []\n    total_bytes = 0\n    total_rows = 0\n\n    for filename in files:\n        size_bytes = 0\n        try:\n            info = fs.info(filename)\n            if isinstance(info, dict):\n                size_bytes = int(info.get(\"size\", 0))\n        except (OSError, PermissionError) as e:\n            logger.warning(\"Failed to get file info for '%s': %s\", filename, e)\n            size_bytes = 0\n\n        num_rows = 0\n        try:\n            with fs.open(filename, \"rb\") as fh:\n                pf = pq.ParquetFile(fh)\n                num_rows = pf.metadata.num_rows\n        except (OSError, PermissionError, RuntimeError, ValueError) as e:\n            # As a fallback, attempt a minimal table read to estimate rows.\n            logger.debug(\"Failed to read parquet metadata from '%s', trying fallback: %s\", filename, e)\n            try:\n                with fs.open(filename, \"rb\") as fh:\n                    table = pq.read_table(fh)\n                num_rows = table.num_rows\n            except (OSError, PermissionError, RuntimeError, ValueError) as e:\n                logger.debug(\"Fallback table read failed for '%s': %s\", filename, e)\n                num_rows = 0\n\n        total_bytes += size_bytes\n        total_rows += num_rows\n        file_infos.append(\n            {\"path\": filename, \"size_bytes\": size_bytes, \"num_rows\": num_rows}\n        )\n\n    return {\"files\": file_infos, \"total_bytes\": total_bytes, \"total_rows\": total_rows}\n</code></pre>"},{"location":"api/fsspeckit.core.maintenance/#fsspeckit.core.maintenance.plan_compaction_groups","title":"fsspeckit.core.maintenance.plan_compaction_groups","text":"<pre><code>plan_compaction_groups(\n    file_infos: list[dict[str, Any]] | list[FileInfo],\n    target_mb_per_file: int | None,\n    target_rows_per_file: int | None,\n) -&gt; dict[str, Any]\n</code></pre> <p>Plan compaction groups based on size and row thresholds.</p> <p>Parameters:</p> Name Type Description Default <code>file_infos</code> <code>list[dict[str, Any]] | list[FileInfo]</code> <p>List of file information dictionaries or FileInfo objects.</p> required <code>target_mb_per_file</code> <code>int | None</code> <p>Target size in megabytes per output file.</p> required <code>target_rows_per_file</code> <code>int | None</code> <p>Target number of rows per output file.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with:</p> <code>dict[str, Any]</code> <ul> <li>groups: List of CompactionGroup objects to be compacted</li> </ul> <code>dict[str, Any]</code> <ul> <li>untouched_files: List of FileInfo objects not requiring compaction</li> </ul> <code>dict[str, Any]</code> <ul> <li>planned_stats: MaintenanceStats object for the planned operation</li> </ul> <code>dict[str, Any]</code> <ul> <li>planned_groups: List of file paths per group (for backward compatibility)</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If both target_mb_per_file and target_rows_per_file are None or &lt;= 0.</p> Source code in <code>src/fsspeckit/core/maintenance.py</code> <pre><code>def plan_compaction_groups(\n    file_infos: list[dict[str, Any]] | list[FileInfo],\n    target_mb_per_file: int | None,\n    target_rows_per_file: int | None,\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Plan compaction groups based on size and row thresholds.\n\n    Args:\n        file_infos: List of file information dictionaries or FileInfo objects.\n        target_mb_per_file: Target size in megabytes per output file.\n        target_rows_per_file: Target number of rows per output file.\n\n    Returns:\n        Dictionary with:\n        - groups: List of CompactionGroup objects to be compacted\n        - untouched_files: List of FileInfo objects not requiring compaction\n        - planned_stats: MaintenanceStats object for the planned operation\n        - planned_groups: List of file paths per group (for backward compatibility)\n\n    Raises:\n        ValueError: If both target_mb_per_file and target_rows_per_file are None or &lt;= 0.\n    \"\"\"\n    # Validate inputs\n    if target_mb_per_file is None and target_rows_per_file is None:\n        raise ValueError(\n            \"Must provide at least one of target_mb_per_file or target_rows_per_file\"\n        )\n    if target_mb_per_file is not None and target_mb_per_file &lt;= 0:\n        raise ValueError(\"target_mb_per_file must be &gt; 0\")\n    if target_rows_per_file is not None and target_rows_per_file &lt;= 0:\n        raise ValueError(\"target_rows_per_file must be &gt; 0\")\n\n    # Convert to FileInfo objects if needed\n    if file_infos and isinstance(file_infos[0], dict):\n        files = [FileInfo(fi[\"path\"], fi[\"size_bytes\"], fi[\"num_rows\"]) for fi in file_infos]\n    else:\n        files = file_infos  # type: ignore\n\n    size_threshold_bytes = (\n        target_mb_per_file * 1024 * 1024 if target_mb_per_file is not None else None\n    )\n\n    # Separate candidate files (eligible for compaction) from large files.\n    candidates: list[FileInfo] = []\n    large_files: list[FileInfo] = []\n    for file_info in files:\n        size_bytes = file_info.size_bytes\n        if size_threshold_bytes is None or size_bytes &lt; size_threshold_bytes:\n            candidates.append(file_info)\n        else:\n            large_files.append(file_info)\n\n    # Build groups based on thresholds.\n    groups: list[list[FileInfo]] = []\n    current_group: list[FileInfo] = []\n    current_size = 0\n    current_rows = 0\n\n    def flush_group() -&gt; None:\n        nonlocal current_group, current_size, current_rows\n        if current_group:\n            groups.append(current_group)\n            current_group = []\n            current_size = 0\n            current_rows = 0\n\n    for file_info in sorted(candidates, key=lambda x: x.size_bytes):\n        size_bytes = file_info.size_bytes\n        num_rows = file_info.num_rows\n        would_exceed_size = (\n            size_threshold_bytes is not None\n            and current_size + size_bytes &gt; size_threshold_bytes\n            and current_group\n        )\n        would_exceed_rows = (\n            target_rows_per_file is not None\n            and current_rows + num_rows &gt; target_rows_per_file\n            and current_group\n        )\n        if would_exceed_size or would_exceed_rows:\n            flush_group()\n        current_group.append(file_info)\n        current_size += size_bytes\n        current_rows += num_rows\n    flush_group()\n\n    # Only compact groups that contain more than one file; singleton groups\n    # would just rewrite an existing file.\n    finalized_groups: list[CompactionGroup] = [\n        CompactionGroup(files=group) for group in groups if len(group) &gt; 1\n    ]\n\n    # Calculate statistics\n    before_file_count = len(files)\n    before_total_bytes = sum(f.size_bytes for f in files)\n\n    compacted_file_count = sum(len(group.files) for group in finalized_groups)\n    untouched_files = large_files + [\n        file_info for file_info in candidates\n        if not any(file_info in group.files for group in finalized_groups)\n    ]\n\n    after_file_count = len(untouched_files) + len(finalized_groups)\n\n    # Estimate after_total_bytes (assume minimal compression change for planning)\n    compacted_bytes = sum(group.total_size_bytes for group in finalized_groups)\n    untouched_bytes = sum(f.size_bytes for f in untouched_files)\n    after_total_bytes = untouched_bytes + compacted_bytes  # Rough estimate\n\n    rewritten_bytes = compacted_bytes\n\n    # Create compatibility structures\n    planned_groups = [group.file_paths() for group in finalized_groups]\n\n    planned_stats = MaintenanceStats(\n        before_file_count=before_file_count,\n        after_file_count=after_file_count,\n        before_total_bytes=before_total_bytes,\n        after_total_bytes=after_total_bytes,\n        compacted_file_count=compacted_file_count,\n        rewritten_bytes=rewritten_bytes,\n        compression_codec=None,  # Will be set by backend\n        dry_run=True,\n        planned_groups=planned_groups,\n    )\n\n    return {\n        \"groups\": finalized_groups,\n        \"untouched_files\": untouched_files,\n        \"planned_stats\": planned_stats,\n        \"planned_groups\": planned_groups,\n    }\n</code></pre>"},{"location":"api/fsspeckit.core.maintenance/#fsspeckit.core.maintenance.plan_optimize_groups","title":"fsspeckit.core.maintenance.plan_optimize_groups","text":"<pre><code>plan_optimize_groups(\n    file_infos: list[dict[str, Any]] | list[FileInfo],\n    zorder_columns: list[str],\n    target_mb_per_file: int | None = None,\n    target_rows_per_file: int | None = None,\n    sample_schema: Any = None,\n) -&gt; dict[str, Any]\n</code></pre> <p>Plan optimization groups with z-order validation.</p> <p>Parameters:</p> Name Type Description Default <code>file_infos</code> <code>list[dict[str, Any]] | list[FileInfo]</code> <p>List of file information dictionaries or FileInfo objects.</p> required <code>zorder_columns</code> <code>list[str]</code> <p>List of columns to use for z-order clustering.</p> required <code>target_mb_per_file</code> <code>int | None</code> <p>Target size in megabytes per output file.</p> <code>None</code> <code>target_rows_per_file</code> <code>int | None</code> <p>Target number of rows per output file.</p> <code>None</code> <code>sample_schema</code> <code>Any</code> <p>PyArrow schema or object with column_names method for validation.           If None, schema validation will be skipped.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with:</p> <code>dict[str, Any]</code> <ul> <li>groups: List of CompactionGroup objects to be optimized</li> </ul> <code>dict[str, Any]</code> <ul> <li>untouched_files: List of FileInfo objects not requiring optimization</li> </ul> <code>dict[str, Any]</code> <ul> <li>planned_stats: MaintenanceStats object for the planned operation</li> </ul> <code>dict[str, Any]</code> <ul> <li>planned_groups: List of file paths per group (for backward compatibility)</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If thresholds are invalid or zorder_columns is empty.</p> Source code in <code>src/fsspeckit/core/maintenance.py</code> <pre><code>def plan_optimize_groups(\n    file_infos: list[dict[str, Any]] | list[FileInfo],\n    zorder_columns: list[str],\n    target_mb_per_file: int | None = None,\n    target_rows_per_file: int | None = None,\n    sample_schema: Any = None,\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Plan optimization groups with z-order validation.\n\n    Args:\n        file_infos: List of file information dictionaries or FileInfo objects.\n        zorder_columns: List of columns to use for z-order clustering.\n        target_mb_per_file: Target size in megabytes per output file.\n        target_rows_per_file: Target number of rows per output file.\n        sample_schema: PyArrow schema or object with column_names method for validation.\n                      If None, schema validation will be skipped.\n\n    Returns:\n        Dictionary with:\n        - groups: List of CompactionGroup objects to be optimized\n        - untouched_files: List of FileInfo objects not requiring optimization\n        - planned_stats: MaintenanceStats object for the planned operation\n        - planned_groups: List of file paths per group (for backward compatibility)\n\n    Raises:\n        ValueError: If thresholds are invalid or zorder_columns is empty.\n    \"\"\"\n    # Validate inputs\n    if not zorder_columns:\n        raise ValueError(\"zorder_columns must be a non-empty list\")\n    if target_mb_per_file is not None and target_mb_per_file &lt;= 0:\n        raise ValueError(\"target_mb_per_file must be &gt; 0\")\n    if target_rows_per_file is not None and target_rows_per_file &lt;= 0:\n        raise ValueError(\"target_rows_per_file must be &gt; 0\")\n\n    # Validate zorder columns against schema if provided\n    if sample_schema is not None:\n        try:\n            available_cols = set(sample_schema.column_names)\n            missing = [col for col in zorder_columns if col not in available_cols]\n            if missing:\n                raise ValueError(\n                    f\"Missing z-order columns: {', '.join(missing)}. \"\n                    f\"Available columns: {', '.join(sorted(available_cols))}\"\n                )\n        except AttributeError:\n            # sample_schema doesn't have column_names, skip validation\n            pass\n\n    # Convert to FileInfo objects if needed\n    if file_infos and isinstance(file_infos[0], dict):\n        files = [FileInfo(fi[\"path\"], fi[\"size_bytes\"], fi[\"num_rows\"]) for fi in file_infos]\n    else:\n        files = file_infos  # type: ignore\n\n    # For optimization, we typically want to process all files unless they're\n    # already large enough to be left alone\n    size_threshold_bytes = (\n        target_mb_per_file * 1024 * 1024 if target_mb_per_file is not None else None\n    )\n\n    # Separate candidate files from large files\n    candidates: list[FileInfo] = []\n    large_files: list[FileInfo] = []\n    for file_info in files:\n        size_bytes = file_info.size_bytes\n        if size_threshold_bytes is None or size_bytes &lt; size_threshold_bytes:\n            candidates.append(file_info)\n        else:\n            large_files.append(file_info)\n\n    # Group files for optimization - similar to compaction but more aggressive\n    # since optimization typically rewrites all eligible files\n    groups: list[list[FileInfo]] = []\n    current_group: list[FileInfo] = []\n    current_size = 0\n    current_rows = 0\n\n    def flush_group() -&gt; None:\n        nonlocal current_group, current_size, current_rows\n        if current_group:\n            groups.append(current_group)\n            current_group = []\n            current_size = 0\n            current_rows = 0\n\n    # Sort files for more consistent optimization\n    for file_info in sorted(candidates, key=lambda x: x.size_bytes):\n        size_bytes = file_info.size_bytes\n        num_rows = file_info.num_rows\n        would_exceed_size = (\n            size_threshold_bytes is not None\n            and current_size + size_bytes &gt; size_threshold_bytes\n            and current_group\n        )\n        would_exceed_rows = (\n            target_rows_per_file is not None\n            and current_rows + num_rows &gt; target_rows_per_file\n            and current_group\n        )\n        if would_exceed_size or would_exceed_rows:\n            flush_group()\n        current_group.append(file_info)\n        current_size += size_bytes\n        current_rows += num_rows\n    flush_group()\n\n    # Include single-file groups for optimization (unlike compaction)\n    # because optimization needs to reorder all eligible files\n    finalized_groups: list[CompactionGroup] = []\n    for group in groups:\n        if len(group) &gt; 0:  # Include single files too\n            finalized_groups.append(CompactionGroup(files=group))\n\n    # Calculate statistics\n    before_file_count = len(files)\n    before_total_bytes = sum(f.size_bytes for f in files)\n\n    optimized_file_count = sum(len(group.files) for group in finalized_groups)\n    untouched_files = large_files  # Only large files are left untouched in optimization\n\n    after_file_count = len(untouched_files) + len(finalized_groups)\n\n    # Estimate after_total_bytes (optimization may improve compression)\n    optimized_bytes = sum(group.total_size_bytes for group in finalized_groups)\n    untouched_bytes = sum(f.size_bytes for f in untouched_files)\n    after_total_bytes = untouched_bytes + optimized_bytes  # Rough estimate\n\n    rewritten_bytes = optimized_bytes\n\n    # Create compatibility structures\n    planned_groups = [group.file_paths() for group in finalized_groups]\n\n    planned_stats = MaintenanceStats(\n        before_file_count=before_file_count,\n        after_file_count=after_file_count,\n        before_total_bytes=before_total_bytes,\n        after_total_bytes=after_total_bytes,\n        compacted_file_count=optimized_file_count,\n        rewritten_bytes=rewritten_bytes,\n        compression_codec=None,  # Will be set by backend\n        dry_run=True,\n        zorder_columns=zorder_columns,\n        planned_groups=planned_groups,\n    )\n\n    return {\n        \"groups\": finalized_groups,\n        \"untouched_files\": untouched_files,\n        \"planned_stats\": planned_stats,\n        \"planned_groups\": planned_groups,\n    }\n</code></pre>"},{"location":"api/fsspeckit.core.merge/","title":"fsspeckit.core.merge","text":""},{"location":"api/fsspeckit.core.merge/#fsspeckit.core.merge","title":"merge","text":"<p>Backend-neutral merge layer for parquet dataset operations.</p> <p>This module provides shared functionality for merge operations used by both DuckDB and PyArrow merge implementations.</p> <p>Key responsibilities: 1. Merge validation and normalization 2. Strategy semantics and definitions 3. Key validation and schema compatibility checking 4. Shared statistics calculation 5. NULL-key detection and edge case handling</p>"},{"location":"api/fsspeckit.core.merge/#fsspeckit.core.merge-classes","title":"Classes","text":""},{"location":"api/fsspeckit.core.merge/#fsspeckit.core.merge.MergePlan","title":"fsspeckit.core.merge.MergePlan  <code>dataclass</code>","text":"<pre><code>MergePlan(\n    strategy: MergeStrategy,\n    key_columns: list[str],\n    source_count: int,\n    target_exists: bool,\n    dedup_order_by: list[str] | None = None,\n    key_columns_valid: bool = True,\n    schema_compatible: bool = True,\n    null_keys_detected: bool = False,\n    allow_target_empty: bool = True,\n    allow_source_empty: bool = True,\n)\n</code></pre> <p>Plan for executing a merge operation.</p>"},{"location":"api/fsspeckit.core.merge/#fsspeckit.core.merge.MergeStats","title":"fsspeckit.core.merge.MergeStats  <code>dataclass</code>","text":"<pre><code>MergeStats(\n    strategy: MergeStrategy,\n    source_count: int,\n    target_count_before: int,\n    target_count_after: int,\n    inserted: int,\n    updated: int,\n    deleted: int,\n    total_processed: int = 0,\n)\n</code></pre> <p>Canonical statistics structure for merge operations.</p>"},{"location":"api/fsspeckit.core.merge/#fsspeckit.core.merge.MergeStats-functions","title":"Functions","text":""},{"location":"api/fsspeckit.core.merge/#fsspeckit.core.merge.MergeStats.to_dict","title":"fsspeckit.core.merge.MergeStats.to_dict","text":"<pre><code>to_dict() -&gt; dict[str, Any]\n</code></pre> <p>Convert to dictionary format for backward compatibility.</p> Source code in <code>src/fsspeckit/core/merge.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert to dictionary format for backward compatibility.\"\"\"\n    return {\n        \"inserted\": self.inserted,\n        \"updated\": self.updated,\n        \"deleted\": self.deleted,\n        \"total\": self.target_count_after,\n        \"source_count\": self.source_count,\n        \"target_count_before\": self.target_count_before,\n        \"target_count_after\": self.target_count_after,\n        \"total_processed\": self.total_processed,\n        \"strategy\": self.strategy.value,\n    }\n</code></pre>"},{"location":"api/fsspeckit.core.merge/#fsspeckit.core.merge.MergeStrategy","title":"fsspeckit.core.merge.MergeStrategy","text":"<p>               Bases: <code>Enum</code></p> <p>Supported merge strategies with consistent semantics across backends.</p>"},{"location":"api/fsspeckit.core.merge/#fsspeckit.core.merge.MergeStrategy-attributes","title":"Attributes","text":""},{"location":"api/fsspeckit.core.merge/#fsspeckit.core.merge.MergeStrategy.DEDUPLICATE","title":"fsspeckit.core.merge.MergeStrategy.DEDUPLICATE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DEDUPLICATE = 'deduplicate'\n</code></pre> <p>Remove duplicates from source, then upsert.</p>"},{"location":"api/fsspeckit.core.merge/#fsspeckit.core.merge.MergeStrategy.FULL_MERGE","title":"fsspeckit.core.merge.MergeStrategy.FULL_MERGE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FULL_MERGE = 'full_merge'\n</code></pre> <p>Insert, update, and delete (full sync with source).</p>"},{"location":"api/fsspeckit.core.merge/#fsspeckit.core.merge.MergeStrategy.INSERT","title":"fsspeckit.core.merge.MergeStrategy.INSERT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>INSERT = 'insert'\n</code></pre> <p>Insert only new records, ignore existing records.</p>"},{"location":"api/fsspeckit.core.merge/#fsspeckit.core.merge.MergeStrategy.UPDATE","title":"fsspeckit.core.merge.MergeStrategy.UPDATE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>UPDATE = 'update'\n</code></pre> <p>Update only existing records, ignore new records.</p>"},{"location":"api/fsspeckit.core.merge/#fsspeckit.core.merge.MergeStrategy.UPSERT","title":"fsspeckit.core.merge.MergeStrategy.UPSERT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>UPSERT = 'upsert'\n</code></pre> <p>Insert new records, update existing records.</p>"},{"location":"api/fsspeckit.core.merge/#fsspeckit.core.merge-functions","title":"Functions","text":""},{"location":"api/fsspeckit.core.merge/#fsspeckit.core.merge.calculate_merge_stats","title":"fsspeckit.core.merge.calculate_merge_stats","text":"<pre><code>calculate_merge_stats(\n    strategy: MergeStrategy,\n    source_count: int,\n    target_count_before: int,\n    target_count_after: int,\n) -&gt; MergeStats\n</code></pre> <p>Calculate merge operation statistics.</p> <p>Parameters:</p> Name Type Description Default <code>strategy</code> <code>MergeStrategy</code> <p>Merge strategy that was used.</p> required <code>source_count</code> <code>int</code> <p>Number of rows in source data.</p> required <code>target_count_before</code> <code>int</code> <p>Number of rows in target before merge.</p> required <code>target_count_after</code> <code>int</code> <p>Number of rows in target after merge.</p> required <p>Returns:</p> Type Description <code>MergeStats</code> <p>MergeStats with calculated statistics.</p> Source code in <code>src/fsspeckit/core/merge.py</code> <pre><code>def calculate_merge_stats(\n    strategy: MergeStrategy,\n    source_count: int,\n    target_count_before: int,\n    target_count_after: int,\n) -&gt; MergeStats:\n    \"\"\"\n    Calculate merge operation statistics.\n\n    Args:\n        strategy: Merge strategy that was used.\n        source_count: Number of rows in source data.\n        target_count_before: Number of rows in target before merge.\n        target_count_after: Number of rows in target after merge.\n\n    Returns:\n        MergeStats with calculated statistics.\n    \"\"\"\n    stats = MergeStats(\n        strategy=strategy,\n        source_count=source_count,\n        target_count_before=target_count_before,\n        target_count_after=target_count_after,\n        inserted=0,\n        updated=0,\n        deleted=0,\n    )\n\n    if strategy == MergeStrategy.INSERT:\n        # INSERT: only additions, no updates or deletes\n        stats.inserted = target_count_after - target_count_before\n        stats.updated = 0\n        stats.deleted = 0\n\n    elif strategy == MergeStrategy.UPDATE:\n        # UPDATE: no additions or deletes (all existing potentially updated)\n        stats.inserted = 0\n        stats.updated = target_count_before if target_count_before &gt; 0 else 0\n        stats.deleted = 0\n\n    elif strategy == MergeStrategy.FULL_MERGE:\n        # FULL_MERGE: source replaces target completely\n        stats.inserted = source_count\n        stats.updated = 0\n        stats.deleted = target_count_before\n\n    else:  # UPSERT or DEDUPLICATE\n        # UPSERT/DEDUPLICATE: additions and updates\n        net_change = target_count_after - target_count_before\n        stats.inserted = max(0, net_change)\n        stats.updated = source_count - stats.inserted\n        stats.deleted = 0\n\n    # Update total_processed\n    stats.total_processed = stats.inserted + stats.updated\n\n    return stats\n</code></pre>"},{"location":"api/fsspeckit.core.merge/#fsspeckit.core.merge.check_null_keys","title":"fsspeckit.core.merge.check_null_keys","text":"<pre><code>check_null_keys(\n    source_table: Table,\n    target_table: Table | None,\n    key_columns: list[str],\n) -&gt; None\n</code></pre> <p>Check for NULL values in key columns.</p> <p>Parameters:</p> Name Type Description Default <code>source_table</code> <code>Table</code> <p>Source data table.</p> required <code>target_table</code> <code>Table | None</code> <p>Target data table, None if target doesn't exist.</p> required <code>key_columns</code> <code>list[str]</code> <p>List of key column names.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If NULL values found in key columns.</p> Source code in <code>src/fsspeckit/core/merge.py</code> <pre><code>def check_null_keys(\n    source_table: pa.Table,\n    target_table: pa.Table | None,\n    key_columns: list[str],\n) -&gt; None:\n    \"\"\"\n    Check for NULL values in key columns.\n\n    Args:\n        source_table: Source data table.\n        target_table: Target data table, None if target doesn't exist.\n        key_columns: List of key column names.\n\n    Raises:\n        ValueError: If NULL values found in key columns.\n    \"\"\"\n    # Check source for NULL keys\n    for key_col in key_columns:\n        source_col = source_table.column(key_col)\n        if source_col.null_count &gt; 0:\n            raise ValueError(\n                f\"Key column '{key_col}' contains {source_col.null_count} NULL values in source. \"\n                f\"Key columns must not have NULLs.\"\n            )\n\n    # Check target for NULL keys if it exists\n    if target_table is not None:\n        for key_col in key_columns:\n            target_col = target_table.column(key_col)\n            if target_col.null_count &gt; 0:\n                raise ValueError(\n                    f\"Key column '{key_col}' contains {target_col.null_count} NULL values in target. \"\n                    f\"Key columns must not have NULLs.\"\n                )\n</code></pre>"},{"location":"api/fsspeckit.core.merge/#fsspeckit.core.merge.get_canonical_merge_strategies","title":"fsspeckit.core.merge.get_canonical_merge_strategies","text":"<pre><code>get_canonical_merge_strategies() -&gt; list[str]\n</code></pre> <p>Get the list of canonical merge strategy names.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of strategy names in canonical order.</p> Source code in <code>src/fsspeckit/core/merge.py</code> <pre><code>def get_canonical_merge_strategies() -&gt; list[str]:\n    \"\"\"\n    Get the list of canonical merge strategy names.\n\n    Returns:\n        List of strategy names in canonical order.\n    \"\"\"\n    return [strategy.value for strategy in MergeStrategy]\n</code></pre>"},{"location":"api/fsspeckit.core.merge/#fsspeckit.core.merge.normalize_key_columns","title":"fsspeckit.core.merge.normalize_key_columns","text":"<pre><code>normalize_key_columns(\n    key_columns: list[str] | str,\n) -&gt; list[str]\n</code></pre> <p>Normalize key columns to a consistent list format.</p> <p>Parameters:</p> Name Type Description Default <code>key_columns</code> <code>list[str] | str</code> <p>Key column(s) as string or list of strings.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of key column names.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If key_columns is empty or contains invalid values.</p> Source code in <code>src/fsspeckit/core/merge.py</code> <pre><code>def normalize_key_columns(key_columns: list[str] | str) -&gt; list[str]:\n    \"\"\"\n    Normalize key columns to a consistent list format.\n\n    Args:\n        key_columns: Key column(s) as string or list of strings.\n\n    Returns:\n        List of key column names.\n\n    Raises:\n        ValueError: If key_columns is empty or contains invalid values.\n    \"\"\"\n    if isinstance(key_columns, str):\n        if not key_columns.strip():\n            raise ValueError(\"key_columns cannot be empty string\")\n        return [key_columns.strip()]\n\n    if not key_columns:\n        raise ValueError(\"key_columns cannot be empty\")\n\n    # Filter and validate each column name\n    normalized = []\n    for col in key_columns:\n        if not isinstance(col, str):\n            raise ValueError(f\"key_columns must be strings, got {type(col)}\")\n        stripped = col.strip()\n        if not stripped:\n            raise ValueError(\"key_columns cannot contain empty strings\")\n        normalized.append(stripped)\n\n    if not normalized:\n        raise ValueError(\"key_columns cannot be empty after normalization\")\n\n    return normalized\n</code></pre>"},{"location":"api/fsspeckit.core.merge/#fsspeckit.core.merge.validate_merge_inputs","title":"fsspeckit.core.merge.validate_merge_inputs","text":"<pre><code>validate_merge_inputs(\n    source_schema: Schema,\n    target_schema: Schema | None,\n    key_columns: list[str],\n    strategy: MergeStrategy,\n) -&gt; MergePlan\n</code></pre> <p>Validate merge inputs and create a merge plan.</p> <p>Parameters:</p> Name Type Description Default <code>source_schema</code> <code>Schema</code> <p>Schema of the source data.</p> required <code>target_schema</code> <code>Schema | None</code> <p>Schema of the target dataset, None if target doesn't exist.</p> required <code>key_columns</code> <code>list[str]</code> <p>List of key column names for matching records.</p> required <code>strategy</code> <code>MergeStrategy</code> <p>Merge strategy to use.</p> required <p>Returns:</p> Type Description <code>MergePlan</code> <p>MergePlan with validation results and execution details.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If validation fails with specific error messages.</p> Source code in <code>src/fsspeckit/core/merge.py</code> <pre><code>def validate_merge_inputs(\n    source_schema: pa.Schema,\n    target_schema: pa.Schema | None,\n    key_columns: list[str],\n    strategy: MergeStrategy,\n) -&gt; MergePlan:\n    \"\"\"\n    Validate merge inputs and create a merge plan.\n\n    Args:\n        source_schema: Schema of the source data.\n        target_schema: Schema of the target dataset, None if target doesn't exist.\n        key_columns: List of key column names for matching records.\n        strategy: Merge strategy to use.\n\n    Returns:\n        MergePlan with validation results and execution details.\n\n    Raises:\n        ValueError: If validation fails with specific error messages.\n    \"\"\"\n    # Normalize key columns\n    normalized_keys = normalize_key_columns(key_columns)\n\n    # Check key columns exist in source\n    source_columns = set(source_schema.names)\n    missing_source_keys = [col for col in normalized_keys if col not in source_columns]\n    if missing_source_keys:\n        raise ValueError(\n            f\"Key column(s) missing from source: {', '.join(missing_source_keys)}. \"\n            f\"Available columns: {', '.join(sorted(source_columns))}\"\n        )\n\n    # Initialize validation flags\n    keys_valid = True\n    schema_compatible = True\n    null_keys_possible = False\n\n    # Check target schema if it exists\n    target_exists = target_schema is not None\n    if target_exists:\n        target_columns = set(target_schema.names)\n\n        # Check key columns exist in target\n        missing_target_keys = [\n            col for col in normalized_keys if col not in target_columns\n        ]\n        if missing_target_keys:\n            raise ValueError(\n                f\"Key column(s) missing from target: {', '.join(missing_target_keys)}. \"\n                f\"Available columns: {', '.join(sorted(target_columns))}\"\n            )\n\n        # Check schema compatibility\n        for field in source_schema:\n            if field.name in target_columns:\n                target_field = target_schema.field(field.name)\n                if field.type != target_field.type:\n                    schema_compatible = False\n                    break\n\n        # Check for column mismatches\n        source_only = source_columns - target_columns\n        target_only = target_columns - source_columns\n        if source_only or target_only:\n            schema_compatible = False\n\n    # Check if NULL keys are possible based on schema nullability\n    for key_col in normalized_keys:\n        source_field = source_schema.field(key_col)\n        if source_field.nullable:\n            null_keys_possible = True\n            break\n\n    # Determine if empty target/source are allowed based on strategy\n    allow_target_empty = True  # All strategies allow empty target\n    allow_source_empty = strategy != MergeStrategy.UPDATE  # UPDATE needs source records\n\n    return MergePlan(\n        strategy=strategy,\n        key_columns=normalized_keys,\n        source_count=0,  # Will be set by caller\n        target_exists=target_exists,\n        dedup_order_by=None,  # Will be set by caller if needed\n        key_columns_valid=keys_valid,\n        schema_compatible=schema_compatible,\n        null_keys_detected=null_keys_possible,\n        allow_target_empty=allow_target_empty,\n        allow_source_empty=allow_source_empty,\n    )\n</code></pre>"},{"location":"api/fsspeckit.core.merge/#fsspeckit.core.merge.validate_strategy_compatibility","title":"fsspeckit.core.merge.validate_strategy_compatibility","text":"<pre><code>validate_strategy_compatibility(\n    strategy: MergeStrategy,\n    source_count: int,\n    target_exists: bool,\n) -&gt; None\n</code></pre> <p>Validate that the chosen strategy is compatible with the data state.</p> <p>Parameters:</p> Name Type Description Default <code>strategy</code> <code>MergeStrategy</code> <p>Merge strategy to validate.</p> required <code>source_count</code> <code>int</code> <p>Number of rows in source data.</p> required <code>target_exists</code> <code>bool</code> <p>Whether target dataset exists.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If strategy is incompatible with the data state.</p> Source code in <code>src/fsspeckit/core/merge.py</code> <pre><code>def validate_strategy_compatibility(\n    strategy: MergeStrategy,\n    source_count: int,\n    target_exists: bool,\n) -&gt; None:\n    \"\"\"\n    Validate that the chosen strategy is compatible with the data state.\n\n    Args:\n        strategy: Merge strategy to validate.\n        source_count: Number of rows in source data.\n        target_exists: Whether target dataset exists.\n\n    Raises:\n        ValueError: If strategy is incompatible with the data state.\n    \"\"\"\n    if strategy == MergeStrategy.UPDATE and source_count == 0:\n        # UPDATE strategy with empty source doesn't make sense\n        raise ValueError(\"UPDATE strategy requires non-empty source data\")\n\n    if strategy == MergeStrategy.FULL_MERGE and not target_exists:\n        # FULL_MERGE on non-existent target is equivalent to just writing source\n        # This is more of a warning situation, but we'll allow it\n        pass\n\n    # Other strategies are generally compatible with any state\n    pass\n</code></pre>"},{"location":"api/fsspeckit.datasets/","title":"<code>fsspeckit.datasets</code> API Reference","text":""},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets","title":"datasets","text":"<p>Dataset-level operations for fsspeckit.</p> <p>This package contains dataset-specific functionality including: - DuckDB parquet handlers for high-performance dataset operations - PyArrow utilities for schema management and type conversion - Dataset merging and optimization tools</p>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets-classes","title":"Classes","text":""},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.DuckDBParquetHandler","title":"fsspeckit.datasets.DuckDBParquetHandler","text":"<pre><code>DuckDBParquetHandler(\n    storage_options: Optional[\n        Union[BaseStorageOptions, dict]\n    ] = None,\n    filesystem: Optional[AbstractFileSystem] = None,\n)\n</code></pre> <p>               Bases: <code>DuckDBDatasetIO</code></p> <p>Backward compatibility wrapper for DuckDBParquetHandler.</p> <p>This class has been refactored into: - DuckDBConnection: for connection management - DuckDBDatasetIO: for dataset I/O operations</p> <p>For new code, consider using DuckDBConnection and DuckDBDatasetIO directly.</p> <p>Initialize DuckDB parquet handler.</p> <p>Parameters:</p> Name Type Description Default <code>storage_options</code> <code>Optional[Union[BaseStorageOptions, dict]]</code> <p>Storage configuration options (deprecated)</p> <code>None</code> <code>filesystem</code> <code>Optional[AbstractFileSystem]</code> <p>Filesystem instance (deprecated)</p> <code>None</code> Source code in <code>src/fsspeckit/datasets/duckdb.py</code> <pre><code>def __init__(\n    self,\n    storage_options: Optional[Union[BaseStorageOptions, dict]] = None,\n    filesystem: Optional[AbstractFileSystem] = None,\n):\n    \"\"\"Initialize DuckDB parquet handler.\n\n    Args:\n        storage_options: Storage configuration options (deprecated)\n        filesystem: Filesystem instance (deprecated)\n    \"\"\"\n    from fsspeckit.datasets.duckdb_connection import create_duckdb_connection\n\n    # Create connection from filesystem\n    self._connection = create_duckdb_connection(filesystem=filesystem)\n\n    # Initialize the IO handler\n    super().__init__(self._connection)\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.DuckDBParquetHandler-functions","title":"Functions","text":""},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.DuckDBParquetHandler.__del__","title":"fsspeckit.datasets.DuckDBParquetHandler.__del__","text":"<pre><code>__del__()\n</code></pre> <p>Destructor (deprecated).</p> Source code in <code>src/fsspeckit/datasets/duckdb.py</code> <pre><code>def __del__(self):\n    \"\"\"Destructor (deprecated).\"\"\"\n    self.close()\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.DuckDBParquetHandler.__enter__","title":"fsspeckit.datasets.DuckDBParquetHandler.__enter__","text":"<pre><code>__enter__()\n</code></pre> <p>Enter context manager (deprecated).</p> Source code in <code>src/fsspeckit/datasets/duckdb.py</code> <pre><code>def __enter__(self):\n    \"\"\"Enter context manager (deprecated).\"\"\"\n    return self\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.DuckDBParquetHandler.__exit__","title":"fsspeckit.datasets.DuckDBParquetHandler.__exit__","text":"<pre><code>__exit__(exc_type, exc_val, exc_tb)\n</code></pre> <p>Exit context manager (deprecated).</p> Source code in <code>src/fsspeckit/datasets/duckdb.py</code> <pre><code>def __exit__(self, exc_type, exc_val, exc_tb):\n    \"\"\"Exit context manager (deprecated).\"\"\"\n    self.close()\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.DuckDBParquetHandler.close","title":"fsspeckit.datasets.DuckDBParquetHandler.close","text":"<pre><code>close()\n</code></pre> <p>Close connection (deprecated, use connection.close).</p> Source code in <code>src/fsspeckit/datasets/duckdb.py</code> <pre><code>def close(self):\n    \"\"\"Close connection (deprecated, use connection.close).\"\"\"\n    self._connection.close()\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.DuckDBParquetHandler.execute_sql","title":"fsspeckit.datasets.DuckDBParquetHandler.execute_sql","text":"<pre><code>execute_sql(query: str, parameters=None)\n</code></pre> <p>Execute SQL query (deprecated, use connection.execute_sql).</p> Source code in <code>src/fsspeckit/datasets/duckdb.py</code> <pre><code>def execute_sql(self, query: str, parameters=None):\n    \"\"\"Execute SQL query (deprecated, use connection.execute_sql).\"\"\"\n    return self._connection.execute_sql(query, parameters)\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets-functions","title":"Functions","text":""},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.cast_schema","title":"fsspeckit.datasets.cast_schema","text":"<pre><code>cast_schema(table: Table, schema: Schema) -&gt; Table\n</code></pre> <p>Cast a PyArrow table to a target schema.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>Table</code> <p>Source table</p> required <code>schema</code> <code>Schema</code> <p>Target schema</p> required <p>Returns:</p> Type Description <code>Table</code> <p>Table cast to target schema</p> Source code in <code>src/fsspeckit/datasets/pyarrow_schema.py</code> <pre><code>def cast_schema(table: pa.Table, schema: pa.Schema) -&gt; pa.Table:\n    \"\"\"Cast a PyArrow table to a target schema.\n\n    Args:\n        table: Source table\n        schema: Target schema\n\n    Returns:\n        Table cast to target schema\n    \"\"\"\n    # Filter schema to only include columns present in the table\n    table_schema = table.schema\n    valid_fields = []\n\n    for field in schema:\n        if field.name in table_schema.names:\n            valid_fields.append(field)\n\n    target_schema = pa.schema(valid_fields)\n\n    # Cast the table\n    return table.cast(target_schema)\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.collect_dataset_stats_pyarrow","title":"fsspeckit.datasets.collect_dataset_stats_pyarrow","text":"<pre><code>collect_dataset_stats_pyarrow(\n    path: str,\n    filesystem: AbstractFileSystem | None = None,\n    partition_filter: list[str] | None = None,\n) -&gt; dict[str, Any]\n</code></pre> <p>Collect file-level statistics for a parquet dataset using shared core logic.</p> <p>This function delegates to the shared <code>fsspeckit.core.maintenance.collect_dataset_stats</code> function, ensuring consistent dataset discovery and statistics across both DuckDB and PyArrow backends.</p> <p>The helper walks the given dataset directory on the provided filesystem, discovers parquet files (recursively), and returns basic statistics:</p> <ul> <li>Per-file path, size in bytes, and number of rows</li> <li>Aggregated total bytes and total rows</li> </ul> <p>The function is intentionally streaming/metadata-driven and never materializes the full dataset as a single :class:<code>pyarrow.Table</code>.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Root directory of the parquet dataset.</p> required <code>filesystem</code> <code>AbstractFileSystem | None</code> <p>Optional fsspec filesystem. If omitted, a local \"file\" filesystem is used.</p> <code>None</code> <code>partition_filter</code> <code>list[str] | None</code> <p>Optional list of partition prefix filters (e.g. [\"date=2025-11-04\"]). Only files whose path relative to <code>path</code> starts with one of these prefixes are included.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict with keys:</p> <code>dict[str, Any]</code> <ul> <li><code>files</code>: list of <code>{\"path\", \"size_bytes\", \"num_rows\"}</code> dicts</li> </ul> <code>dict[str, Any]</code> <ul> <li><code>total_bytes</code>: sum of file sizes</li> </ul> <code>dict[str, Any]</code> <ul> <li><code>total_rows</code>: sum of row counts</li> </ul> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the path does not exist or no parquet files match the optional partition filter.</p> Note <p>This is a thin wrapper around the shared core function. See :func:<code>fsspeckit.core.maintenance.collect_dataset_stats</code> for the authoritative implementation.</p> Source code in <code>src/fsspeckit/datasets/pyarrow_dataset.py</code> <pre><code>def collect_dataset_stats_pyarrow(\n    path: str,\n    filesystem: AbstractFileSystem | None = None,\n    partition_filter: list[str] | None = None,\n) -&gt; dict[str, Any]:\n    \"\"\"Collect file-level statistics for a parquet dataset using shared core logic.\n\n    This function delegates to the shared ``fsspeckit.core.maintenance.collect_dataset_stats``\n    function, ensuring consistent dataset discovery and statistics across both DuckDB\n    and PyArrow backends.\n\n    The helper walks the given dataset directory on the provided filesystem,\n    discovers parquet files (recursively), and returns basic statistics:\n\n    - Per-file path, size in bytes, and number of rows\n    - Aggregated total bytes and total rows\n\n    The function is intentionally streaming/metadata-driven and never\n    materializes the full dataset as a single :class:`pyarrow.Table`.\n\n    Args:\n        path: Root directory of the parquet dataset.\n        filesystem: Optional fsspec filesystem. If omitted, a local \"file\"\n            filesystem is used.\n        partition_filter: Optional list of partition prefix filters\n            (e.g. [\"date=2025-11-04\"]). Only files whose path relative to\n            ``path`` starts with one of these prefixes are included.\n\n    Returns:\n        Dict with keys:\n\n        - ``files``: list of ``{\"path\", \"size_bytes\", \"num_rows\"}`` dicts\n        - ``total_bytes``: sum of file sizes\n        - ``total_rows``: sum of row counts\n\n    Raises:\n        FileNotFoundError: If the path does not exist or no parquet files\n            match the optional partition filter.\n\n    Note:\n        This is a thin wrapper around the shared core function. See\n        :func:`fsspeckit.core.maintenance.collect_dataset_stats` for the\n        authoritative implementation.\n    \"\"\"\n    from fsspeckit.core.maintenance import collect_dataset_stats\n\n    return collect_dataset_stats(\n        path=path,\n        filesystem=filesystem,\n        partition_filter=partition_filter,\n    )\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.compact_parquet_dataset_pyarrow","title":"fsspeckit.datasets.compact_parquet_dataset_pyarrow","text":"<pre><code>compact_parquet_dataset_pyarrow(\n    path: str,\n    target_mb_per_file: int | None = None,\n    target_rows_per_file: int | None = None,\n    partition_filter: list[str] | None = None,\n    compression: str | None = None,\n    dry_run: bool = False,\n    filesystem: AbstractFileSystem | None = None,\n) -&gt; dict[str, Any]\n</code></pre> <p>Compact a parquet dataset directory into fewer larger files using PyArrow and shared planning.</p> <p>Groups small files based on size (MB) and/or row thresholds, rewrites grouped files into new parquet files, and optionally changes compression. Supports a dry-run mode that returns the compaction plan without modifying files.</p> <p>The implementation uses the shared core planning algorithm for consistent behavior across backends. It processes data in a group-based, streaming fashion: it reads only the files in a given group into memory when processing that group and never materializes the entire dataset as a single table.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Dataset root directory (local path or fsspec URL).</p> required <code>target_mb_per_file</code> <code>int | None</code> <p>Optional max output size per file; must be &gt; 0.</p> <code>None</code> <code>target_rows_per_file</code> <code>int | None</code> <p>Optional max rows per output file; must be &gt; 0.</p> <code>None</code> <code>partition_filter</code> <code>list[str] | None</code> <p>Optional list of partition prefixes (e.g. <code>[\"date=2025-11-15\"]</code>) used to limit both stats collection and rewrites to matching paths.</p> <code>None</code> <code>compression</code> <code>str | None</code> <p>Optional parquet compression codec; defaults to <code>\"snappy\"</code>.</p> <code>None</code> <code>dry_run</code> <code>bool</code> <p>When <code>True</code> the function returns a plan + before/after stats without reading or writing any parquet data.</p> <code>False</code> <code>filesystem</code> <code>AbstractFileSystem | None</code> <p>Optional <code>fsspec.AbstractFileSystem</code> to reuse existing FS clients.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A stats dictionary describing before/after file counts, total bytes,</p> <code>dict[str, Any]</code> <p>rewritten bytes, and optional <code>planned_groups</code> when <code>dry_run</code> is enabled.</p> <code>dict[str, Any]</code> <p>The structure follows the canonical <code>MaintenanceStats</code> format from the shared core.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If thresholds are invalid or no files match partition filter.</p> <code>FileNotFoundError</code> <p>If the path does not exist.</p> Example <pre><code>result = compact_parquet_dataset_pyarrow(\n    \"/path/to/dataset\",\n    target_mb_per_file=64,\n    dry_run=True,\n)\nprint(f\"Files before: {result['before_file_count']}\")\nprint(f\"Files after: {result['after_file_count']}\")\n</code></pre> Note <p>This function delegates dataset discovery and compaction planning to the shared <code>fsspeckit.core.maintenance</code> module, ensuring consistent behavior across DuckDB and PyArrow backends.</p> Source code in <code>src/fsspeckit/datasets/pyarrow_dataset.py</code> <pre><code>def compact_parquet_dataset_pyarrow(\n    path: str,\n    target_mb_per_file: int | None = None,\n    target_rows_per_file: int | None = None,\n    partition_filter: list[str] | None = None,\n    compression: str | None = None,\n    dry_run: bool = False,\n    filesystem: AbstractFileSystem | None = None,\n) -&gt; dict[str, Any]:\n    \"\"\"Compact a parquet dataset directory into fewer larger files using PyArrow and shared planning.\n\n    Groups small files based on size (MB) and/or row thresholds, rewrites grouped\n    files into new parquet files, and optionally changes compression. Supports a\n    dry-run mode that returns the compaction plan without modifying files.\n\n    The implementation uses the shared core planning algorithm for consistent\n    behavior across backends. It processes data in a group-based, streaming fashion:\n    it reads only the files in a given group into memory when processing that group\n    and never materializes the entire dataset as a single table.\n\n    Args:\n        path: Dataset root directory (local path or fsspec URL).\n        target_mb_per_file: Optional max output size per file; must be &gt; 0.\n        target_rows_per_file: Optional max rows per output file; must be &gt; 0.\n        partition_filter: Optional list of partition prefixes (e.g. ``[\"date=2025-11-15\"]``)\n            used to limit both stats collection and rewrites to matching paths.\n        compression: Optional parquet compression codec; defaults to ``\"snappy\"``.\n        dry_run: When ``True`` the function returns a plan + before/after stats\n            without reading or writing any parquet data.\n        filesystem: Optional ``fsspec.AbstractFileSystem`` to reuse existing FS clients.\n\n    Returns:\n        A stats dictionary describing before/after file counts, total bytes,\n        rewritten bytes, and optional ``planned_groups`` when ``dry_run`` is enabled.\n        The structure follows the canonical ``MaintenanceStats`` format from the shared core.\n\n    Raises:\n        ValueError: If thresholds are invalid or no files match partition filter.\n        FileNotFoundError: If the path does not exist.\n\n    Example:\n        ```python\n        result = compact_parquet_dataset_pyarrow(\n            \"/path/to/dataset\",\n            target_mb_per_file=64,\n            dry_run=True,\n        )\n        print(f\"Files before: {result['before_file_count']}\")\n        print(f\"Files after: {result['after_file_count']}\")\n        ```\n\n    Note:\n        This function delegates dataset discovery and compaction planning to the\n        shared ``fsspeckit.core.maintenance`` module, ensuring consistent behavior\n        across DuckDB and PyArrow backends.\n    \"\"\"\n    from fsspeckit.core.maintenance import plan_compaction_groups, MaintenanceStats\n\n    # Get dataset stats using shared logic\n    stats = collect_dataset_stats_pyarrow(\n        path=path, filesystem=filesystem, partition_filter=partition_filter\n    )\n    files = stats[\"files\"]\n\n    # Use shared compaction planning\n    plan_result = plan_compaction_groups(\n        file_infos=files,\n        target_mb_per_file=target_mb_per_file,\n        target_rows_per_file=target_rows_per_file,\n    )\n\n    groups = plan_result[\"groups\"]\n    planned_stats = plan_result[\"planned_stats\"]\n\n    # Update planned stats with compression info\n    planned_stats.compression_codec = compression\n    planned_stats.dry_run = dry_run\n\n    # If dry run, return the plan\n    if dry_run:\n        result = planned_stats.to_dict()\n        result[\"planned_groups\"] = groups\n        return result\n\n    # Execute compaction\n    if not groups:\n        return planned_stats.to_dict()\n\n    # Execute the compaction\n    for group in groups:\n        # Read all files in this group\n        tables = []\n        for file_info in group[\"files\"]:\n            file_path = file_info[\"path\"]\n            table = pq.read_table(\n                file_path,\n                filesystem=filesystem,\n            )\n            tables.append(table)\n\n        # Concatenate tables\n        if len(tables) &gt; 1:\n            combined = pa.concat_tables(tables, promote_options=\"permissive\")\n        else:\n            combined = tables[0]\n\n        # Write to output file\n        output_path = group[\"output_path\"]\n        pq.write_table(\n            combined,\n            output_path,\n            filesystem=filesystem,\n            compression=compression or \"snappy\",\n        )\n\n    # Remove original files\n    for group in groups:\n        for file_info in group[\"files\"]:\n            file_path = file_info[\"path\"]\n            filesystem.rm(file_path)\n\n    return planned_stats.to_dict()\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.convert_large_types_to_normal","title":"fsspeckit.datasets.convert_large_types_to_normal","text":"<pre><code>convert_large_types_to_normal(schema: Schema) -&gt; Schema\n</code></pre> <p>Convert large types (like large_string) to normal types.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>Schema</code> <p>PyArrow schema</p> required <p>Returns:</p> Type Description <code>Schema</code> <p>Schema with large types converted</p> Source code in <code>src/fsspeckit/datasets/pyarrow_schema.py</code> <pre><code>def convert_large_types_to_normal(schema: pa.Schema) -&gt; pa.Schema:\n    \"\"\"Convert large types (like large_string) to normal types.\n\n    Args:\n        schema: PyArrow schema\n\n    Returns:\n        Schema with large types converted\n    \"\"\"\n    fields = []\n    for field in schema:\n        if pa.types.is_large_string(field.type):\n            field = field.with_type(pa.string())\n        elif pa.types.is_large_utf8(field.type):\n            field = field.with_type(pa.utf8())\n        elif pa.types.is_large_list(field.type):\n            field = field.with_type(pa.list_(field.type.value_type))\n        fields.append(field)\n\n    return pa.schema(fields)\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.opt_dtype_pa","title":"fsspeckit.datasets.opt_dtype_pa","text":"<pre><code>opt_dtype_pa(\n    table: Table,\n    strict: bool = False,\n    columns: list[str] | None = None,\n) -&gt; Table\n</code></pre> <p>Optimize dtypes in a PyArrow table based on data analysis.</p> <p>This function analyzes the data in each column and attempts to downcast to more appropriate types (e.g., int64 -&gt; int32, float64 -&gt; float32, string -&gt; int/bool where applicable).</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>Table</code> <p>PyArrow table to optimize</p> required <code>strict</code> <code>bool</code> <p>Whether to use strict type checking</p> <code>False</code> <code>columns</code> <code>list[str] | None</code> <p>List of columns to optimize (None for all)</p> <code>None</code> <p>Returns:</p> Type Description <code>Table</code> <p>Table with optimized dtypes</p> Example <pre><code>import pyarrow as pa\n\ntable = pa.table(\n    {\n        \"a\": pa.array([1, 2, 3], type=pa.int64()),\n        \"b\": pa.array([1.0, 2.0, 3.0], type=pa.float64()),\n    },\n)\noptimized = opt_dtype(table)\nprint(optimized.column(0).type)  # DataType(int32)\nprint(optimized.column(1).type)  # DataType(float32)\n</code></pre> Source code in <code>src/fsspeckit/datasets/pyarrow_schema.py</code> <pre><code>def opt_dtype(\n    table: pa.Table,\n    strict: bool = False,\n    columns: list[str] | None = None,\n) -&gt; pa.Table:\n    \"\"\"Optimize dtypes in a PyArrow table based on data analysis.\n\n    This function analyzes the data in each column and attempts to downcast\n    to more appropriate types (e.g., int64 -&gt; int32, float64 -&gt; float32,\n    string -&gt; int/bool where applicable).\n\n    Args:\n        table: PyArrow table to optimize\n        strict: Whether to use strict type checking\n        columns: List of columns to optimize (None for all)\n\n    Returns:\n        Table with optimized dtypes\n\n    Example:\n        ```python\n        import pyarrow as pa\n\n        table = pa.table(\n            {\n                \"a\": pa.array([1, 2, 3], type=pa.int64()),\n                \"b\": pa.array([1.0, 2.0, 3.0], type=pa.float64()),\n            },\n        )\n        optimized = opt_dtype(table)\n        print(optimized.column(0).type)  # DataType(int32)\n        print(optimized.column(1).type)  # DataType(float32)\n        ```\n    \"\"\"\n    from fsspeckit.common.misc import run_parallel\n\n    if columns is None:\n        columns = table.column_names\n\n    # Process columns in parallel\n    results = run_parallel(\n        _process_column_for_opt_dtype,\n        [(table, col, strict) for col in columns],\n        backend=\"threading\",\n        n_jobs=-1,\n    )\n\n    # Build new table with optimized columns\n    new_columns = {}\n    for col_name, optimized_array in results:\n        new_columns[col_name] = optimized_array\n\n    # Keep non-optimized columns as-is\n    for col_name in table.column_names:\n        if col_name not in new_columns:\n            new_columns[col_name] = table.column(col_name)\n\n    return pa.table(new_columns)\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.optimize_parquet_dataset_pyarrow","title":"fsspeckit.datasets.optimize_parquet_dataset_pyarrow","text":"<pre><code>optimize_parquet_dataset_pyarrow(\n    path: str,\n    target_mb_per_file: int | None = None,\n    target_rows_per_file: int | None = None,\n    partition_filter: list[str] | None = None,\n    compression: str | None = None,\n    filesystem: AbstractFileSystem | None = None,\n    verbose: bool = False,\n) -&gt; dict[str, Any]\n</code></pre> <p>Optimize a parquet dataset through compaction and optional statistics recalculation.</p> <p>This is a convenience function that combines compaction with optional statistics recalculation. It's particularly useful after many small write operations have created a large number of small files.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Dataset root directory</p> required <code>target_mb_per_file</code> <code>int | None</code> <p>Target size per file in MB</p> <code>None</code> <code>target_rows_per_file</code> <code>int | None</code> <p>Target rows per file</p> <code>None</code> <code>partition_filter</code> <code>list[str] | None</code> <p>Optional partition filters</p> <code>None</code> <code>compression</code> <code>str | None</code> <p>Compression codec to use</p> <code>None</code> <code>filesystem</code> <code>AbstractFileSystem | None</code> <p>Optional filesystem instance</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Print progress information</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Optimization statistics</p> Example <pre><code>stats = optimize_parquet_dataset_pyarrow(\n    \"dataset/\",\n    target_mb_per_file=64,\n    compression=\"zstd\",\n)\nprint(\n    f\"Reduced from {stats['before_file_count']} \"\n    f\"to {stats['after_file_count']} files\",\n)\n</code></pre> Source code in <code>src/fsspeckit/datasets/pyarrow_dataset.py</code> <pre><code>def optimize_parquet_dataset_pyarrow(\n    path: str,\n    target_mb_per_file: int | None = None,\n    target_rows_per_file: int | None = None,\n    partition_filter: list[str] | None = None,\n    compression: str | None = None,\n    filesystem: AbstractFileSystem | None = None,\n    verbose: bool = False,\n) -&gt; dict[str, Any]:\n    \"\"\"Optimize a parquet dataset through compaction and optional statistics recalculation.\n\n    This is a convenience function that combines compaction with optional statistics\n    recalculation. It's particularly useful after many small write operations have\n    created a large number of small files.\n\n    Args:\n        path: Dataset root directory\n        target_mb_per_file: Target size per file in MB\n        target_rows_per_file: Target rows per file\n        partition_filter: Optional partition filters\n        compression: Compression codec to use\n        filesystem: Optional filesystem instance\n        verbose: Print progress information\n\n    Returns:\n        Optimization statistics\n\n    Example:\n        ```python\n        stats = optimize_parquet_dataset_pyarrow(\n            \"dataset/\",\n            target_mb_per_file=64,\n            compression=\"zstd\",\n        )\n        print(\n            f\"Reduced from {stats['before_file_count']} \"\n            f\"to {stats['after_file_count']} files\",\n        )\n        ```\n    \"\"\"\n    # Use compaction\n    result = compact_parquet_dataset_pyarrow(\n        path=path,\n        target_mb_per_file=target_mb_per_file,\n        target_rows_per_file=target_rows_per_file,\n        partition_filter=partition_filter,\n        compression=compression,\n        dry_run=False,\n        filesystem=filesystem,\n    )\n\n    if verbose:\n        logger.info(\"Optimization complete: %s\", result)\n\n    return result\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.unify_schemas_pa","title":"fsspeckit.datasets.unify_schemas_pa","text":"<pre><code>unify_schemas_pa(\n    schemas: list[Schema],\n    standardize_timezones: bool = True,\n    verbose: bool = False,\n) -&gt; Schema\n</code></pre> <p>Unify multiple PyArrow schemas into a single schema.</p> <p>This function handles type conflicts by: 1. Finding fields with conflicting types 2. Attempting to normalize compatible types 3. Using fallback strategies for incompatible types 4. Removing problematic fields if necessary</p> <p>Parameters:</p> Name Type Description Default <code>schemas</code> <code>list[Schema]</code> <p>List of schemas to unify</p> required <code>standardize_timezones</code> <code>bool</code> <p>Whether to standardize timezone info</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>Whether to print conflict information</p> <code>False</code> <p>Returns:</p> Type Description <code>Schema</code> <p>Unified PyArrow schema</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If schemas cannot be unified</p> Source code in <code>src/fsspeckit/datasets/pyarrow_schema.py</code> <pre><code>def unify_schemas(\n    schemas: list[pa.Schema],\n    standardize_timezones: bool = True,\n    verbose: bool = False,\n) -&gt; pa.Schema:\n    \"\"\"Unify multiple PyArrow schemas into a single schema.\n\n    This function handles type conflicts by:\n    1. Finding fields with conflicting types\n    2. Attempting to normalize compatible types\n    3. Using fallback strategies for incompatible types\n    4. Removing problematic fields if necessary\n\n    Args:\n        schemas: List of schemas to unify\n        standardize_timezones: Whether to standardize timezone info\n        verbose: Whether to print conflict information\n\n    Returns:\n        Unified PyArrow schema\n\n    Raises:\n        ValueError: If schemas cannot be unified\n    \"\"\"\n    if not schemas:\n        raise ValueError(\"Cannot unify empty list of schemas\")\n\n    if len(schemas) == 1:\n        return schemas[0]\n\n    # Remove duplicate schemas\n    schemas = _unique_schemas(schemas)\n\n    # Standardize timezones if requested\n    if standardize_timezones:\n        schemas = standardize_schema_timezones(schemas, standardize_timezones)\n\n    # Find conflicts\n    conflicts = _find_conflicting_fields(schemas)\n\n    if not conflicts:\n        # No conflicts, concatenate all fields\n        all_fields = []\n        for schema in schemas:\n            all_fields.extend(schema)\n        return pa.schema(all_fields)\n\n    if verbose:\n        _log_conflict_summary(conflicts, verbose)\n\n    # Try to normalize types\n    try:\n        normalized = _normalize_schema_types(schemas, conflicts)\n\n        # Check if normalization resolved conflicts\n        remaining_conflicts = _find_conflicting_fields(normalized)\n\n        if not remaining_conflicts:\n            # Normalization successful\n            all_fields = []\n            for schema in normalized:\n                all_fields.extend(schema)\n            return pa.schema(all_fields)\n\n        # Fall through to next strategy\n        conflicts = remaining_conflicts\n\n    except (pa.ArrowInvalid, pa.ArrowTypeError, pa.ArrowNotImplementedError) as e:\n        # Normalization failed, log and continue to fallback\n        logger.debug(\n            \"Schema type normalization failed: %s. Trying aggressive fallback.\",\n            str(e)\n        )\n\n    # Try aggressive fallback\n    try:\n        return _aggressive_fallback_unification(schemas)\n    except (pa.ArrowInvalid, pa.ArrowTypeError, pa.ArrowNotImplementedError) as e:\n        # Aggressive fallback failed, log and try last resort\n        logger.debug(\n            \"Aggressive fallback unification failed: %s. Trying last resort cleanup.\",\n            str(e)\n        )\n\n    # Last resort: remove problematic fields\n    cleaned = _remove_problematic_fields(schemas)\n    all_fields = []\n    for schema in cleaned:\n        all_fields.extend(schema)\n\n    if verbose and conflicts:\n        logger.debug(\"Removed %d conflicting fields during unification\", len(conflicts))\n\n    return pa.schema(all_fields)\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets-modules","title":"Modules","text":""},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.duckdb","title":"fsspeckit.datasets.duckdb","text":"<p>Re-export module for backward compatibility.</p> <p>This module has been decomposed into focused submodules: - duckdb_connection: Connection management and filesystem registration - duckdb_dataset: Dataset I/O and maintenance operations</p> <p>All public APIs are re-exported here to maintain backward compatibility. New code should import directly from the submodules for better organization.</p>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.duckdb-classes","title":"Classes","text":""},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.duckdb.DuckDBConnection","title":"fsspeckit.datasets.duckdb.DuckDBConnection","text":"<pre><code>DuckDBConnection(\n    filesystem: AbstractFileSystem | None = None,\n)\n</code></pre> <p>Manages DuckDB connection lifecycle and filesystem registration.</p> <p>This class is responsible for: - Creating and managing DuckDB connections - Registering fsspec filesystems with DuckDB - Connection cleanup</p> <p>Parameters:</p> Name Type Description Default <code>filesystem</code> <code>AbstractFileSystem | None</code> <p>fsspec filesystem instance to use</p> <code>None</code> <p>Initialize DuckDB connection manager.</p> <p>Parameters:</p> Name Type Description Default <code>filesystem</code> <code>AbstractFileSystem | None</code> <p>Filesystem to use. Defaults to local filesystem.</p> <code>None</code> Source code in <code>src/fsspeckit/datasets/duckdb_connection.py</code> <pre><code>def __init__(self, filesystem: AbstractFileSystem | None = None) -&gt; None:\n    \"\"\"Initialize DuckDB connection manager.\n\n    Args:\n        filesystem: Filesystem to use. Defaults to local filesystem.\n    \"\"\"\n    self._connection: duckdb.DuckDBPyConnection | None = None\n    self._filesystem = filesystem or fsspec_filesystem(\"file\")\n</code></pre> Attributes\u00b6 <code></code> fsspeckit.datasets.duckdb.DuckDBConnection.connection <code>property</code> \u00b6 <pre><code>connection: Any\n</code></pre> <p>Get active DuckDB connection, creating it if necessary.</p> <p>Returns:</p> Type Description <code>Any</code> <p>Active DuckDB connection</p> <code></code> fsspeckit.datasets.duckdb.DuckDBConnection.filesystem <code>property</code> \u00b6 <pre><code>filesystem: AbstractFileSystem\n</code></pre> <p>Get the filesystem instance.</p> <p>Returns:</p> Type Description <code>AbstractFileSystem</code> <p>Filesystem instance</p> Functions\u00b6 <code></code> fsspeckit.datasets.duckdb.DuckDBConnection.__del__ \u00b6 <pre><code>__del__() -&gt; None\n</code></pre> <p>Destructor to ensure connection is closed.</p> Source code in <code>src/fsspeckit/datasets/duckdb_connection.py</code> <pre><code>def __del__(self) -&gt; None:\n    \"\"\"Destructor to ensure connection is closed.\"\"\"\n    self.close()\n</code></pre> <code></code> fsspeckit.datasets.duckdb.DuckDBConnection.__enter__ \u00b6 <pre><code>__enter__() -&gt; 'DuckDBConnection'\n</code></pre> <p>Enter context manager.</p> <p>Returns:</p> Type Description <code>'DuckDBConnection'</code> <p>self</p> Source code in <code>src/fsspeckit/datasets/duckdb_connection.py</code> <pre><code>def __enter__(self) -&gt; \"DuckDBConnection\":\n    \"\"\"Enter context manager.\n\n    Returns:\n        self\n    \"\"\"\n    return self\n</code></pre> <code></code> fsspeckit.datasets.duckdb.DuckDBConnection.__exit__ \u00b6 <pre><code>__exit__(exc_type: Any, exc_val: Any, exc_tb: Any) -&gt; None\n</code></pre> <p>Exit context manager and close connection.</p> Source code in <code>src/fsspeckit/datasets/duckdb_connection.py</code> <pre><code>def __exit__(\n    self,\n    exc_type: Any,\n    exc_val: Any,\n    exc_tb: Any,\n) -&gt; None:\n    \"\"\"Exit context manager and close connection.\"\"\"\n    self.close()\n</code></pre> <code></code> fsspeckit.datasets.duckdb.DuckDBConnection.close \u00b6 <pre><code>close() -&gt; None\n</code></pre> <p>Close the connection and clean up resources.</p> Source code in <code>src/fsspeckit/datasets/duckdb_connection.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Close the connection and clean up resources.\"\"\"\n    if self._connection is not None:\n        try:\n            self._connection.close()\n        except (_DUCKDB_EXCEPTIONS.get(\"ConnectionException\"), _DUCKDB_EXCEPTIONS.get(\"OperationalException\")) as e:\n            logger.warning(\"Error closing DuckDB connection: %s\", e)\n        finally:\n            self._connection = None\n</code></pre> <code></code> fsspeckit.datasets.duckdb.DuckDBConnection.execute_sql \u00b6 <pre><code>execute_sql(\n    query: str, parameters: list[Any] | None = None\n) -&gt; Any\n</code></pre> <p>Execute a SQL query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>SQL query to execute</p> required <code>parameters</code> <code>list[Any] | None</code> <p>Optional query parameters</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>Query result</p> Source code in <code>src/fsspeckit/datasets/duckdb_connection.py</code> <pre><code>def execute_sql(\n    self,\n    query: str,\n    parameters: list[Any] | None = None,\n) -&gt; Any:\n    \"\"\"Execute a SQL query.\n\n    Args:\n        query: SQL query to execute\n        parameters: Optional query parameters\n\n    Returns:\n        Query result\n    \"\"\"\n    conn = self.connection\n\n    if parameters:\n        return conn.execute(query, parameters)\n    else:\n        return conn.execute(query)\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.duckdb.DuckDBDatasetIO","title":"fsspeckit.datasets.duckdb.DuckDBDatasetIO","text":"<pre><code>DuckDBDatasetIO(connection: DuckDBConnection)\n</code></pre> <p>DuckDB-based dataset I/O operations.</p> <p>This class provides methods for reading and writing parquet files and datasets using DuckDB's high-performance parquet engine.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>DuckDBConnection</code> <p>DuckDB connection manager</p> required <p>Initialize DuckDB dataset I/O.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>DuckDBConnection</code> <p>DuckDB connection manager</p> required Source code in <code>src/fsspeckit/datasets/duckdb_dataset.py</code> <pre><code>def __init__(self, connection: DuckDBConnection) -&gt; None:\n    \"\"\"Initialize DuckDB dataset I/O.\n\n    Args:\n        connection: DuckDB connection manager\n    \"\"\"\n    self._connection = connection\n</code></pre> Functions\u00b6 <code></code> fsspeckit.datasets.duckdb.DuckDBDatasetIO.compact_parquet_dataset \u00b6 <pre><code>compact_parquet_dataset(\n    path: str,\n    target_mb_per_file: int | None = None,\n    target_rows_per_file: int | None = None,\n    partition_filter: list[str] | None = None,\n    compression: str | None = None,\n    dry_run: bool = False,\n    verbose: bool = False,\n) -&gt; dict[str, Any]\n</code></pre> <p>Compact a parquet dataset using DuckDB.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Dataset path</p> required <code>target_mb_per_file</code> <code>int | None</code> <p>Target size per file</p> <code>None</code> <code>target_rows_per_file</code> <code>int | None</code> <p>Target rows per file</p> <code>None</code> <code>partition_filter</code> <code>list[str] | None</code> <p>Optional partition filters</p> <code>None</code> <code>compression</code> <code>str | None</code> <p>Compression codec</p> <code>None</code> <code>dry_run</code> <code>bool</code> <p>Whether to perform a dry run</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>Print progress information</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Compaction statistics</p> Source code in <code>src/fsspeckit/datasets/duckdb_dataset.py</code> <pre><code>def compact_parquet_dataset(\n    self,\n    path: str,\n    target_mb_per_file: int | None = None,\n    target_rows_per_file: int | None = None,\n    partition_filter: list[str] | None = None,\n    compression: str | None = None,\n    dry_run: bool = False,\n    verbose: bool = False,\n) -&gt; dict[str, Any]:\n    \"\"\"Compact a parquet dataset using DuckDB.\n\n    Args:\n        path: Dataset path\n        target_mb_per_file: Target size per file\n        target_rows_per_file: Target rows per file\n        partition_filter: Optional partition filters\n        compression: Compression codec\n        dry_run: Whether to perform a dry run\n        verbose: Print progress information\n\n    Returns:\n        Compaction statistics\n    \"\"\"\n    from fsspeckit.core.maintenance import plan_compaction_groups, MaintenanceStats\n\n    # Collect stats\n    stats = self._collect_dataset_stats(path, partition_filter)\n    files = stats[\"files\"]\n\n    # Plan compaction\n    plan_result = plan_compaction_groups(\n        file_infos=files,\n        target_mb_per_file=target_mb_per_file,\n        target_rows_per_file=target_rows_per_file,\n    )\n\n    groups = plan_result[\"groups\"]\n    planned_stats = plan_result[\"planned_stats\"]\n\n    if dry_run:\n        result = planned_stats.to_dict()\n        result[\"planned_groups\"] = groups\n        return result\n\n    # Execute compaction\n    if not groups:\n        return planned_stats.to_dict()\n\n    conn = self._connection.connection\n\n    for group in groups:\n        # Read all files in this group into DuckDB\n        tables = []\n        for file_info in group[\"files\"]:\n            file_path = file_info[\"path\"]\n            table = conn.execute(\n                f\"SELECT * FROM parquet_scan('{file_path}')\"\n            ).fetch_arrow_table()\n            tables.append(table)\n\n        # Concatenate tables\n        if len(tables) &gt; 1:\n            combined = pa.concat_tables(tables, promote_options=\"permissive\")\n        else:\n            combined = tables[0]\n\n        # Write to output\n        output_path = group[\"output_path\"]\n        self.write_parquet(combined, output_path, compression=compression)\n\n    # Remove original files\n    for group in groups:\n        for file_info in group[\"files\"]:\n            file_path = file_info[\"path\"]\n            self._connection.filesystem.rm(file_path)\n\n    return planned_stats.to_dict()\n</code></pre> <code></code> fsspeckit.datasets.duckdb.DuckDBDatasetIO.merge_parquet_dataset \u00b6 <pre><code>merge_parquet_dataset(\n    sources: list[str],\n    output_path: str,\n    target: str | None = None,\n    strategy: str | MergeStrategy = \"deduplicate\",\n    key_columns: list[str] | str | None = None,\n    compression: str | None = None,\n    verbose: bool = False,\n    **kwargs: Any,\n) -&gt; MergeStats\n</code></pre> <p>Merge multiple parquet datasets using DuckDB.</p> <p>Parameters:</p> Name Type Description Default <code>sources</code> <code>list[str]</code> <p>List of source dataset paths</p> required <code>output_path</code> <code>str</code> <p>Path for merged output</p> required <code>target</code> <code>str | None</code> <p>Target dataset path (for upsert/update strategies)</p> <code>None</code> <code>strategy</code> <code>str | MergeStrategy</code> <p>Merge strategy to use</p> <code>'deduplicate'</code> <code>key_columns</code> <code>list[str] | str | None</code> <p>Key columns for merging</p> <code>None</code> <code>compression</code> <code>str | None</code> <p>Output compression codec</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Print progress information</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>MergeStats</code> <p>MergeStats with merge statistics</p> Example <pre><code>from fsspeckit.datasets.duckdb_connection import create_duckdb_connection\nfrom fsspeckit.datasets.duckdb_dataset import DuckDBDatasetIO\n\nconn = create_duckdb_connection()\nio = DuckDBDatasetIO(conn)\nstats = io.merge_parquet_dataset(\n    sources=[\"dataset1/\", \"dataset2/\"],\n    output_path=\"merged/\",\n    strategy=\"deduplicate\",\n    key_columns=[\"id\"],\n)\n</code></pre> Source code in <code>src/fsspeckit/datasets/duckdb_dataset.py</code> <pre><code>def merge_parquet_dataset(\n    self,\n    sources: list[str],\n    output_path: str,\n    target: str | None = None,\n    strategy: str | CoreMergeStrategy = \"deduplicate\",\n    key_columns: list[str] | str | None = None,\n    compression: str | None = None,\n    verbose: bool = False,\n    **kwargs: Any,\n) -&gt; MergeStats:\n    \"\"\"Merge multiple parquet datasets using DuckDB.\n\n    Args:\n        sources: List of source dataset paths\n        output_path: Path for merged output\n        target: Target dataset path (for upsert/update strategies)\n        strategy: Merge strategy to use\n        key_columns: Key columns for merging\n        compression: Output compression codec\n        verbose: Print progress information\n        **kwargs: Additional arguments\n\n    Returns:\n        MergeStats with merge statistics\n\n    Example:\n        ```python\n        from fsspeckit.datasets.duckdb_connection import create_duckdb_connection\n        from fsspeckit.datasets.duckdb_dataset import DuckDBDatasetIO\n\n        conn = create_duckdb_connection()\n        io = DuckDBDatasetIO(conn)\n        stats = io.merge_parquet_dataset(\n            sources=[\"dataset1/\", \"dataset2/\"],\n            output_path=\"merged/\",\n            strategy=\"deduplicate\",\n            key_columns=[\"id\"],\n        )\n        ```\n    \"\"\"\n    # Validate inputs using shared core logic\n    validate_merge_inputs(\n        sources=sources,\n        strategy=strategy,\n        key_columns=key_columns,\n        target=target,\n    )\n\n    validate_strategy_compatibility(strategy, key_columns, target)\n\n    # Normalize parameters\n    if key_columns is not None:\n        key_columns = normalize_key_columns(key_columns)\n\n    # Process merge using DuckDB\n    result = self._execute_merge_strategy(\n        sources=sources,\n        output_path=output_path,\n        target=target,\n        strategy=strategy,\n        key_columns=key_columns,\n        compression=compression,\n        verbose=verbose,\n    )\n\n    return result\n</code></pre> <code></code> fsspeckit.datasets.duckdb.DuckDBDatasetIO.optimize_parquet_dataset \u00b6 <pre><code>optimize_parquet_dataset(\n    path: str,\n    target_mb_per_file: int | None = None,\n    target_rows_per_file: int | None = None,\n    partition_filter: list[str] | None = None,\n    compression: str | None = None,\n    verbose: bool = False,\n) -&gt; dict[str, Any]\n</code></pre> <p>Optimize a parquet dataset.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Dataset path</p> required <code>target_mb_per_file</code> <code>int | None</code> <p>Target size per file</p> <code>None</code> <code>target_rows_per_file</code> <code>int | None</code> <p>Target rows per file</p> <code>None</code> <code>partition_filter</code> <code>list[str] | None</code> <p>Optional partition filters</p> <code>None</code> <code>compression</code> <code>str | None</code> <p>Compression codec</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Print progress information</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Optimization statistics</p> Source code in <code>src/fsspeckit/datasets/duckdb_dataset.py</code> <pre><code>def optimize_parquet_dataset(\n    self,\n    path: str,\n    target_mb_per_file: int | None = None,\n    target_rows_per_file: int | None = None,\n    partition_filter: list[str] | None = None,\n    compression: str | None = None,\n    verbose: bool = False,\n) -&gt; dict[str, Any]:\n    \"\"\"Optimize a parquet dataset.\n\n    Args:\n        path: Dataset path\n        target_mb_per_file: Target size per file\n        target_rows_per_file: Target rows per file\n        partition_filter: Optional partition filters\n        compression: Compression codec\n        verbose: Print progress information\n\n    Returns:\n        Optimization statistics\n    \"\"\"\n    # Use compaction for optimization\n    result = self.compact_parquet_dataset(\n        path=path,\n        target_mb_per_file=target_mb_per_file,\n        target_rows_per_file=target_rows_per_file,\n        partition_filter=partition_filter,\n        compression=compression,\n        dry_run=False,\n        verbose=verbose,\n    )\n\n    if verbose:\n        logger.info(\"Optimization complete: %s\", result)\n\n    return result\n</code></pre> <code></code> fsspeckit.datasets.duckdb.DuckDBDatasetIO.read_parquet \u00b6 <pre><code>read_parquet(\n    path: str,\n    columns: list[str] | None = None,\n    filter: str | None = None,\n    use_threads: bool = True,\n) -&gt; Table\n</code></pre> <p>Read parquet file(s) using DuckDB.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to parquet file or directory</p> required <code>columns</code> <code>list[str] | None</code> <p>Optional list of columns to read</p> <code>None</code> <code>filter</code> <code>str | None</code> <p>Optional SQL WHERE clause</p> <code>None</code> <code>use_threads</code> <code>bool</code> <p>Whether to use parallel reading</p> <code>True</code> <p>Returns:</p> Type Description <code>Table</code> <p>PyArrow table containing the data</p> Example <pre><code>from fsspeckit.datasets.duckdb_connection import create_duckdb_connection\nfrom fsspeckit.datasets.duckdb_dataset import DuckDBDatasetIO\n\nconn = create_duckdb_connection()\nio = DuckDBDatasetIO(conn)\ntable = io.read_parquet(\"/path/to/file.parquet\")\n</code></pre> Source code in <code>src/fsspeckit/datasets/duckdb_dataset.py</code> <pre><code>def read_parquet(\n    self,\n    path: str,\n    columns: list[str] | None = None,\n    filter: str | None = None,\n    use_threads: bool = True,\n) -&gt; pa.Table:\n    \"\"\"Read parquet file(s) using DuckDB.\n\n    Args:\n        path: Path to parquet file or directory\n        columns: Optional list of columns to read\n        filter: Optional SQL WHERE clause\n        use_threads: Whether to use parallel reading\n\n    Returns:\n        PyArrow table containing the data\n\n    Example:\n        ```python\n        from fsspeckit.datasets.duckdb_connection import create_duckdb_connection\n        from fsspeckit.datasets.duckdb_dataset import DuckDBDatasetIO\n\n        conn = create_duckdb_connection()\n        io = DuckDBDatasetIO(conn)\n        table = io.read_parquet(\"/path/to/file.parquet\")\n        ```\n    \"\"\"\n    validate_path(path)\n\n    conn = self._connection.connection\n    fs = self._connection.filesystem\n\n    # Build the query\n    query = \"SELECT * FROM parquet_scan(?)\"\n\n    params = [path]\n\n    if columns:\n        # Escape column names and build select list\n        quoted_cols = [f'\"{col}\"' for col in columns]\n        select_list = \", \".join(quoted_cols)\n        query = f\"SELECT {select_list} FROM parquet_scan(?)\"\n\n    if filter:\n        query += f\" WHERE {filter}\"\n\n    try:\n        # Execute query\n        if use_threads:\n            result = conn.execute(query, params).fetch_arrow_table()\n        else:\n            result = conn.execute(query, params).fetch_arrow_table()\n\n        return result\n\n    except (_DUCKDB_EXCEPTIONS.get(\"IOException\"), _DUCKDB_EXCEPTIONS.get(\"InvalidInputException\"), _DUCKDB_EXCEPTIONS.get(\"ParserException\")) as e:\n        raise RuntimeError(\n            f\"Failed to read parquet from {path}: {safe_format_error(e)}\"\n        ) from e\n</code></pre> <code></code> fsspeckit.datasets.duckdb.DuckDBDatasetIO.write_parquet \u00b6 <pre><code>write_parquet(\n    data: Table | list[Table],\n    path: str,\n    compression: str | None = \"snappy\",\n    row_group_size: int | None = None,\n    use_threads: bool = True,\n) -&gt; None\n</code></pre> <p>Write parquet file using DuckDB.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Table | list[Table]</code> <p>PyArrow table or list of tables to write</p> required <code>path</code> <code>str</code> <p>Output file path</p> required <code>compression</code> <code>str | None</code> <p>Compression codec to use</p> <code>'snappy'</code> <code>row_group_size</code> <code>int | None</code> <p>Rows per row group</p> <code>None</code> <code>use_threads</code> <code>bool</code> <p>Whether to use parallel writing</p> <code>True</code> Example <pre><code>import pyarrow as pa\nfrom fsspeckit.datasets.duckdb_connection import create_duckdb_connection\nfrom fsspeckit.datasets.duckdb_dataset import DuckDBDatasetIO\n\ntable = pa.table({'a': [1, 2, 3], 'b': ['x', 'y', 'z']})\nconn = create_duckdb_connection()\nio = DuckDBDatasetIO(conn)\nio.write_parquet(table, \"/tmp/data.parquet\")\n</code></pre> Source code in <code>src/fsspeckit/datasets/duckdb_dataset.py</code> <pre><code>def write_parquet(\n    self,\n    data: pa.Table | list[pa.Table],\n    path: str,\n    compression: str | None = \"snappy\",\n    row_group_size: int | None = None,\n    use_threads: bool = True,\n) -&gt; None:\n    \"\"\"Write parquet file using DuckDB.\n\n    Args:\n        data: PyArrow table or list of tables to write\n        path: Output file path\n        compression: Compression codec to use\n        row_group_size: Rows per row group\n        use_threads: Whether to use parallel writing\n\n    Example:\n        ```python\n        import pyarrow as pa\n        from fsspeckit.datasets.duckdb_connection import create_duckdb_connection\n        from fsspeckit.datasets.duckdb_dataset import DuckDBDatasetIO\n\n        table = pa.table({'a': [1, 2, 3], 'b': ['x', 'y', 'z']})\n        conn = create_duckdb_connection()\n        io = DuckDBDatasetIO(conn)\n        io.write_parquet(table, \"/tmp/data.parquet\")\n        ```\n    \"\"\"\n    validate_path(path)\n    validate_compression_codec(compression)\n\n    conn = self._connection.connection\n\n    # Register the data as a temporary table\n    table_name = f\"temp_{uuid.uuid4().hex[:16]}\"\n    conn.register(\"data_table\", data)\n\n    try:\n        # Build the COPY command\n        copy_query = f\"COPY data_table TO ?\"\n\n        params = [path]\n\n        if compression:\n            copy_query += f\" (COMPRESSION {compression})\"\n\n        if row_group_size:\n            copy_query += f\" (ROW_GROUP_SIZE {row_group_size})\"\n\n        # Execute the copy\n        if use_threads:\n            conn.execute(copy_query, params)\n        else:\n            conn.execute(copy_query, params)\n\n    finally:\n        # Clean up temporary table\n        try:\n            conn.unregister(\"data_table\")\n        except (_DUCKDB_EXCEPTIONS.get(\"CatalogException\"), _DUCKDB_EXCEPTIONS.get(\"ConnectionException\")) as e:\n            logger.warning(\"Failed to unregister temporary table: %s\", e)\n</code></pre> <code></code> fsspeckit.datasets.duckdb.DuckDBDatasetIO.write_parquet_dataset \u00b6 <pre><code>write_parquet_dataset(\n    data: Table | list[Table],\n    path: str,\n    basename_template: str | None = None,\n    schema: Schema | None = None,\n    partition_by: str | list[str] | None = None,\n    compression: str | None = \"snappy\",\n    max_rows_per_file: int | None = 5000000,\n    row_group_size: int | None = 500000,\n    use_threads: bool = True,\n) -&gt; None\n</code></pre> <p>Write a parquet dataset using DuckDB.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Table | list[Table]</code> <p>PyArrow table or list of tables to write</p> required <code>path</code> <code>str</code> <p>Output directory path</p> required <code>basename_template</code> <code>str | None</code> <p>Template for file names</p> <code>None</code> <code>schema</code> <code>Schema | None</code> <p>Optional schema to enforce</p> <code>None</code> <code>partition_by</code> <code>str | list[str] | None</code> <p>Column(s) to partition by</p> <code>None</code> <code>compression</code> <code>str | None</code> <p>Compression codec</p> <code>'snappy'</code> <code>max_rows_per_file</code> <code>int | None</code> <p>Maximum rows per file</p> <code>5000000</code> <code>row_group_size</code> <code>int | None</code> <p>Rows per row group</p> <code>500000</code> <code>use_threads</code> <code>bool</code> <p>Whether to use parallel writing</p> <code>True</code> Example <pre><code>import pyarrow as pa\nfrom fsspeckit.datasets.duckdb_connection import create_duckdb_connection\nfrom fsspeckit.datasets.duckdb_dataset import DuckDBDatasetIO\n\ntable = pa.table({'a': [1, 2, 3], 'b': ['x', 'y', 'z']})\nconn = create_duckdb_connection()\nio = DuckDBDatasetIO(conn)\nio.write_parquet_dataset(table, \"/tmp/dataset/\")\n</code></pre> Source code in <code>src/fsspeckit/datasets/duckdb_dataset.py</code> <pre><code>def write_parquet_dataset(\n    self,\n    data: pa.Table | list[pa.Table],\n    path: str,\n    basename_template: str | None = None,\n    schema: pa.Schema | None = None,\n    partition_by: str | list[str] | None = None,\n    compression: str | None = \"snappy\",\n    max_rows_per_file: int | None = 5_000_000,\n    row_group_size: int | None = 500_000,\n    use_threads: bool = True,\n) -&gt; None:\n    \"\"\"Write a parquet dataset using DuckDB.\n\n    Args:\n        data: PyArrow table or list of tables to write\n        path: Output directory path\n        basename_template: Template for file names\n        schema: Optional schema to enforce\n        partition_by: Column(s) to partition by\n        compression: Compression codec\n        max_rows_per_file: Maximum rows per file\n        row_group_size: Rows per row group\n        use_threads: Whether to use parallel writing\n\n    Example:\n        ```python\n        import pyarrow as pa\n        from fsspeckit.datasets.duckdb_connection import create_duckdb_connection\n        from fsspeckit.datasets.duckdb_dataset import DuckDBDatasetIO\n\n        table = pa.table({'a': [1, 2, 3], 'b': ['x', 'y', 'z']})\n        conn = create_duckdb_connection()\n        io = DuckDBDatasetIO(conn)\n        io.write_parquet_dataset(table, \"/tmp/dataset/\")\n        ```\n    \"\"\"\n    from fsspeckit.common.optional import _import_pyarrow\n\n    validate_path(path)\n    validate_compression_codec(compression)\n\n    pa_mod = _import_pyarrow()\n\n    conn = self._connection.connection\n\n    # Register the data as a temporary table\n    table_name = f\"temp_{uuid.uuid4().hex[:16]}\"\n    conn.register(\"data_table\", data)\n\n    try:\n        # Build the COPY command for dataset\n        copy_query = \"COPY data_table TO ? (FORMAT PARQUET\"\n\n        params = [path + \"/{i}.parquet\"]\n\n        if compression:\n            copy_query += f\", COMPRESSION {compression}\"\n\n        if max_rows_per_file:\n            copy_query += f\", MAX_ROWS_PER_FILE {max_rows_per_file}\"\n\n        if row_group_size:\n            copy_query += f\", ROW_GROUP_SIZE {row_group_size}\"\n\n        copy_query += \")\"\n\n        # Execute with file numbering\n        conn.execute(copy_query, params)\n\n    finally:\n        # Clean up temporary table\n        try:\n            conn.unregister(\"data_table\")\n        except (_DUCKDB_EXCEPTIONS.get(\"CatalogException\"), _DUCKDB_EXCEPTIONS.get(\"ConnectionException\")) as e:\n            logger.warning(\"Failed to unregister temporary table: %s\", e)\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.duckdb.DuckDBParquetHandler","title":"fsspeckit.datasets.duckdb.DuckDBParquetHandler","text":"<pre><code>DuckDBParquetHandler(\n    storage_options: Optional[\n        Union[BaseStorageOptions, dict]\n    ] = None,\n    filesystem: Optional[AbstractFileSystem] = None,\n)\n</code></pre> <p>               Bases: <code>DuckDBDatasetIO</code></p> <p>Backward compatibility wrapper for DuckDBParquetHandler.</p> <p>This class has been refactored into: - DuckDBConnection: for connection management - DuckDBDatasetIO: for dataset I/O operations</p> <p>For new code, consider using DuckDBConnection and DuckDBDatasetIO directly.</p> <p>Initialize DuckDB parquet handler.</p> <p>Parameters:</p> Name Type Description Default <code>storage_options</code> <code>Optional[Union[BaseStorageOptions, dict]]</code> <p>Storage configuration options (deprecated)</p> <code>None</code> <code>filesystem</code> <code>Optional[AbstractFileSystem]</code> <p>Filesystem instance (deprecated)</p> <code>None</code> Source code in <code>src/fsspeckit/datasets/duckdb.py</code> <pre><code>def __init__(\n    self,\n    storage_options: Optional[Union[BaseStorageOptions, dict]] = None,\n    filesystem: Optional[AbstractFileSystem] = None,\n):\n    \"\"\"Initialize DuckDB parquet handler.\n\n    Args:\n        storage_options: Storage configuration options (deprecated)\n        filesystem: Filesystem instance (deprecated)\n    \"\"\"\n    from fsspeckit.datasets.duckdb_connection import create_duckdb_connection\n\n    # Create connection from filesystem\n    self._connection = create_duckdb_connection(filesystem=filesystem)\n\n    # Initialize the IO handler\n    super().__init__(self._connection)\n</code></pre> Functions\u00b6 <code></code> fsspeckit.datasets.duckdb.DuckDBParquetHandler.__del__ \u00b6 <pre><code>__del__()\n</code></pre> <p>Destructor (deprecated).</p> Source code in <code>src/fsspeckit/datasets/duckdb.py</code> <pre><code>def __del__(self):\n    \"\"\"Destructor (deprecated).\"\"\"\n    self.close()\n</code></pre> <code></code> fsspeckit.datasets.duckdb.DuckDBParquetHandler.__enter__ \u00b6 <pre><code>__enter__()\n</code></pre> <p>Enter context manager (deprecated).</p> Source code in <code>src/fsspeckit/datasets/duckdb.py</code> <pre><code>def __enter__(self):\n    \"\"\"Enter context manager (deprecated).\"\"\"\n    return self\n</code></pre> <code></code> fsspeckit.datasets.duckdb.DuckDBParquetHandler.__exit__ \u00b6 <pre><code>__exit__(exc_type, exc_val, exc_tb)\n</code></pre> <p>Exit context manager (deprecated).</p> Source code in <code>src/fsspeckit/datasets/duckdb.py</code> <pre><code>def __exit__(self, exc_type, exc_val, exc_tb):\n    \"\"\"Exit context manager (deprecated).\"\"\"\n    self.close()\n</code></pre> <code></code> fsspeckit.datasets.duckdb.DuckDBParquetHandler.close \u00b6 <pre><code>close()\n</code></pre> <p>Close connection (deprecated, use connection.close).</p> Source code in <code>src/fsspeckit/datasets/duckdb.py</code> <pre><code>def close(self):\n    \"\"\"Close connection (deprecated, use connection.close).\"\"\"\n    self._connection.close()\n</code></pre> <code></code> fsspeckit.datasets.duckdb.DuckDBParquetHandler.execute_sql \u00b6 <pre><code>execute_sql(query: str, parameters=None)\n</code></pre> <p>Execute SQL query (deprecated, use connection.execute_sql).</p> Source code in <code>src/fsspeckit/datasets/duckdb.py</code> <pre><code>def execute_sql(self, query: str, parameters=None):\n    \"\"\"Execute SQL query (deprecated, use connection.execute_sql).\"\"\"\n    return self._connection.execute_sql(query, parameters)\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.duckdb-functions","title":"Functions","text":""},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.duckdb.create_duckdb_connection","title":"fsspeckit.datasets.duckdb.create_duckdb_connection","text":"<pre><code>create_duckdb_connection(\n    filesystem: AbstractFileSystem | None = None,\n) -&gt; DuckDBConnection\n</code></pre> <p>Create a DuckDB connection manager.</p> <p>Parameters:</p> Name Type Description Default <code>filesystem</code> <code>AbstractFileSystem | None</code> <p>fsspec filesystem to use</p> <code>None</code> <p>Returns:</p> Type Description <code>DuckDBConnection</code> <p>DuckDB connection manager</p> Source code in <code>src/fsspeckit/datasets/duckdb_connection.py</code> <pre><code>def create_duckdb_connection(\n    filesystem: AbstractFileSystem | None = None,\n) -&gt; DuckDBConnection:\n    \"\"\"Create a DuckDB connection manager.\n\n    Args:\n        filesystem: fsspec filesystem to use\n\n    Returns:\n        DuckDB connection manager\n    \"\"\"\n    return DuckDBConnection(filesystem=filesystem)\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.duckdb_cleanup_helpers","title":"fsspeckit.datasets.duckdb_cleanup_helpers","text":""},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.duckdb_connection","title":"fsspeckit.datasets.duckdb_connection","text":"<p>DuckDB connection and filesystem registration helpers.</p> <p>This module contains functions and classes for managing DuckDB connections and integrating with fsspec filesystems.</p>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.duckdb_connection-classes","title":"Classes","text":""},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.duckdb_connection.DuckDBConnection","title":"fsspeckit.datasets.duckdb_connection.DuckDBConnection","text":"<pre><code>DuckDBConnection(\n    filesystem: AbstractFileSystem | None = None,\n)\n</code></pre> <p>Manages DuckDB connection lifecycle and filesystem registration.</p> <p>This class is responsible for: - Creating and managing DuckDB connections - Registering fsspec filesystems with DuckDB - Connection cleanup</p> <p>Parameters:</p> Name Type Description Default <code>filesystem</code> <code>AbstractFileSystem | None</code> <p>fsspec filesystem instance to use</p> <code>None</code> <p>Initialize DuckDB connection manager.</p> <p>Parameters:</p> Name Type Description Default <code>filesystem</code> <code>AbstractFileSystem | None</code> <p>Filesystem to use. Defaults to local filesystem.</p> <code>None</code> Source code in <code>src/fsspeckit/datasets/duckdb_connection.py</code> <pre><code>def __init__(self, filesystem: AbstractFileSystem | None = None) -&gt; None:\n    \"\"\"Initialize DuckDB connection manager.\n\n    Args:\n        filesystem: Filesystem to use. Defaults to local filesystem.\n    \"\"\"\n    self._connection: duckdb.DuckDBPyConnection | None = None\n    self._filesystem = filesystem or fsspec_filesystem(\"file\")\n</code></pre> Attributes\u00b6 <code></code> fsspeckit.datasets.duckdb_connection.DuckDBConnection.connection <code>property</code> \u00b6 <pre><code>connection: Any\n</code></pre> <p>Get active DuckDB connection, creating it if necessary.</p> <p>Returns:</p> Type Description <code>Any</code> <p>Active DuckDB connection</p> <code></code> fsspeckit.datasets.duckdb_connection.DuckDBConnection.filesystem <code>property</code> \u00b6 <pre><code>filesystem: AbstractFileSystem\n</code></pre> <p>Get the filesystem instance.</p> <p>Returns:</p> Type Description <code>AbstractFileSystem</code> <p>Filesystem instance</p> Functions\u00b6 <code></code> fsspeckit.datasets.duckdb_connection.DuckDBConnection.__del__ \u00b6 <pre><code>__del__() -&gt; None\n</code></pre> <p>Destructor to ensure connection is closed.</p> Source code in <code>src/fsspeckit/datasets/duckdb_connection.py</code> <pre><code>def __del__(self) -&gt; None:\n    \"\"\"Destructor to ensure connection is closed.\"\"\"\n    self.close()\n</code></pre> <code></code> fsspeckit.datasets.duckdb_connection.DuckDBConnection.__enter__ \u00b6 <pre><code>__enter__() -&gt; 'DuckDBConnection'\n</code></pre> <p>Enter context manager.</p> <p>Returns:</p> Type Description <code>'DuckDBConnection'</code> <p>self</p> Source code in <code>src/fsspeckit/datasets/duckdb_connection.py</code> <pre><code>def __enter__(self) -&gt; \"DuckDBConnection\":\n    \"\"\"Enter context manager.\n\n    Returns:\n        self\n    \"\"\"\n    return self\n</code></pre> <code></code> fsspeckit.datasets.duckdb_connection.DuckDBConnection.__exit__ \u00b6 <pre><code>__exit__(exc_type: Any, exc_val: Any, exc_tb: Any) -&gt; None\n</code></pre> <p>Exit context manager and close connection.</p> Source code in <code>src/fsspeckit/datasets/duckdb_connection.py</code> <pre><code>def __exit__(\n    self,\n    exc_type: Any,\n    exc_val: Any,\n    exc_tb: Any,\n) -&gt; None:\n    \"\"\"Exit context manager and close connection.\"\"\"\n    self.close()\n</code></pre> <code></code> fsspeckit.datasets.duckdb_connection.DuckDBConnection.close \u00b6 <pre><code>close() -&gt; None\n</code></pre> <p>Close the connection and clean up resources.</p> Source code in <code>src/fsspeckit/datasets/duckdb_connection.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Close the connection and clean up resources.\"\"\"\n    if self._connection is not None:\n        try:\n            self._connection.close()\n        except (_DUCKDB_EXCEPTIONS.get(\"ConnectionException\"), _DUCKDB_EXCEPTIONS.get(\"OperationalException\")) as e:\n            logger.warning(\"Error closing DuckDB connection: %s\", e)\n        finally:\n            self._connection = None\n</code></pre> <code></code> fsspeckit.datasets.duckdb_connection.DuckDBConnection.execute_sql \u00b6 <pre><code>execute_sql(\n    query: str, parameters: list[Any] | None = None\n) -&gt; Any\n</code></pre> <p>Execute a SQL query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>SQL query to execute</p> required <code>parameters</code> <code>list[Any] | None</code> <p>Optional query parameters</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>Query result</p> Source code in <code>src/fsspeckit/datasets/duckdb_connection.py</code> <pre><code>def execute_sql(\n    self,\n    query: str,\n    parameters: list[Any] | None = None,\n) -&gt; Any:\n    \"\"\"Execute a SQL query.\n\n    Args:\n        query: SQL query to execute\n        parameters: Optional query parameters\n\n    Returns:\n        Query result\n    \"\"\"\n    conn = self.connection\n\n    if parameters:\n        return conn.execute(query, parameters)\n    else:\n        return conn.execute(query)\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.duckdb_connection-functions","title":"Functions","text":""},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.duckdb_connection.create_duckdb_connection","title":"fsspeckit.datasets.duckdb_connection.create_duckdb_connection","text":"<pre><code>create_duckdb_connection(\n    filesystem: AbstractFileSystem | None = None,\n) -&gt; DuckDBConnection\n</code></pre> <p>Create a DuckDB connection manager.</p> <p>Parameters:</p> Name Type Description Default <code>filesystem</code> <code>AbstractFileSystem | None</code> <p>fsspec filesystem to use</p> <code>None</code> <p>Returns:</p> Type Description <code>DuckDBConnection</code> <p>DuckDB connection manager</p> Source code in <code>src/fsspeckit/datasets/duckdb_connection.py</code> <pre><code>def create_duckdb_connection(\n    filesystem: AbstractFileSystem | None = None,\n) -&gt; DuckDBConnection:\n    \"\"\"Create a DuckDB connection manager.\n\n    Args:\n        filesystem: fsspec filesystem to use\n\n    Returns:\n        DuckDB connection manager\n    \"\"\"\n    return DuckDBConnection(filesystem=filesystem)\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.duckdb_dataset","title":"fsspeckit.datasets.duckdb_dataset","text":"<p>DuckDB dataset I/O and maintenance operations.</p> <p>This module contains functions for reading, writing, and maintaining parquet datasets using DuckDB.</p>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.duckdb_dataset-classes","title":"Classes","text":""},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.duckdb_dataset.DuckDBDatasetIO","title":"fsspeckit.datasets.duckdb_dataset.DuckDBDatasetIO","text":"<pre><code>DuckDBDatasetIO(connection: DuckDBConnection)\n</code></pre> <p>DuckDB-based dataset I/O operations.</p> <p>This class provides methods for reading and writing parquet files and datasets using DuckDB's high-performance parquet engine.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>DuckDBConnection</code> <p>DuckDB connection manager</p> required <p>Initialize DuckDB dataset I/O.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>DuckDBConnection</code> <p>DuckDB connection manager</p> required Source code in <code>src/fsspeckit/datasets/duckdb_dataset.py</code> <pre><code>def __init__(self, connection: DuckDBConnection) -&gt; None:\n    \"\"\"Initialize DuckDB dataset I/O.\n\n    Args:\n        connection: DuckDB connection manager\n    \"\"\"\n    self._connection = connection\n</code></pre> Functions\u00b6 <code></code> fsspeckit.datasets.duckdb_dataset.DuckDBDatasetIO.compact_parquet_dataset \u00b6 <pre><code>compact_parquet_dataset(\n    path: str,\n    target_mb_per_file: int | None = None,\n    target_rows_per_file: int | None = None,\n    partition_filter: list[str] | None = None,\n    compression: str | None = None,\n    dry_run: bool = False,\n    verbose: bool = False,\n) -&gt; dict[str, Any]\n</code></pre> <p>Compact a parquet dataset using DuckDB.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Dataset path</p> required <code>target_mb_per_file</code> <code>int | None</code> <p>Target size per file</p> <code>None</code> <code>target_rows_per_file</code> <code>int | None</code> <p>Target rows per file</p> <code>None</code> <code>partition_filter</code> <code>list[str] | None</code> <p>Optional partition filters</p> <code>None</code> <code>compression</code> <code>str | None</code> <p>Compression codec</p> <code>None</code> <code>dry_run</code> <code>bool</code> <p>Whether to perform a dry run</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>Print progress information</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Compaction statistics</p> Source code in <code>src/fsspeckit/datasets/duckdb_dataset.py</code> <pre><code>def compact_parquet_dataset(\n    self,\n    path: str,\n    target_mb_per_file: int | None = None,\n    target_rows_per_file: int | None = None,\n    partition_filter: list[str] | None = None,\n    compression: str | None = None,\n    dry_run: bool = False,\n    verbose: bool = False,\n) -&gt; dict[str, Any]:\n    \"\"\"Compact a parquet dataset using DuckDB.\n\n    Args:\n        path: Dataset path\n        target_mb_per_file: Target size per file\n        target_rows_per_file: Target rows per file\n        partition_filter: Optional partition filters\n        compression: Compression codec\n        dry_run: Whether to perform a dry run\n        verbose: Print progress information\n\n    Returns:\n        Compaction statistics\n    \"\"\"\n    from fsspeckit.core.maintenance import plan_compaction_groups, MaintenanceStats\n\n    # Collect stats\n    stats = self._collect_dataset_stats(path, partition_filter)\n    files = stats[\"files\"]\n\n    # Plan compaction\n    plan_result = plan_compaction_groups(\n        file_infos=files,\n        target_mb_per_file=target_mb_per_file,\n        target_rows_per_file=target_rows_per_file,\n    )\n\n    groups = plan_result[\"groups\"]\n    planned_stats = plan_result[\"planned_stats\"]\n\n    if dry_run:\n        result = planned_stats.to_dict()\n        result[\"planned_groups\"] = groups\n        return result\n\n    # Execute compaction\n    if not groups:\n        return planned_stats.to_dict()\n\n    conn = self._connection.connection\n\n    for group in groups:\n        # Read all files in this group into DuckDB\n        tables = []\n        for file_info in group[\"files\"]:\n            file_path = file_info[\"path\"]\n            table = conn.execute(\n                f\"SELECT * FROM parquet_scan('{file_path}')\"\n            ).fetch_arrow_table()\n            tables.append(table)\n\n        # Concatenate tables\n        if len(tables) &gt; 1:\n            combined = pa.concat_tables(tables, promote_options=\"permissive\")\n        else:\n            combined = tables[0]\n\n        # Write to output\n        output_path = group[\"output_path\"]\n        self.write_parquet(combined, output_path, compression=compression)\n\n    # Remove original files\n    for group in groups:\n        for file_info in group[\"files\"]:\n            file_path = file_info[\"path\"]\n            self._connection.filesystem.rm(file_path)\n\n    return planned_stats.to_dict()\n</code></pre> <code></code> fsspeckit.datasets.duckdb_dataset.DuckDBDatasetIO.merge_parquet_dataset \u00b6 <pre><code>merge_parquet_dataset(\n    sources: list[str],\n    output_path: str,\n    target: str | None = None,\n    strategy: str | MergeStrategy = \"deduplicate\",\n    key_columns: list[str] | str | None = None,\n    compression: str | None = None,\n    verbose: bool = False,\n    **kwargs: Any,\n) -&gt; MergeStats\n</code></pre> <p>Merge multiple parquet datasets using DuckDB.</p> <p>Parameters:</p> Name Type Description Default <code>sources</code> <code>list[str]</code> <p>List of source dataset paths</p> required <code>output_path</code> <code>str</code> <p>Path for merged output</p> required <code>target</code> <code>str | None</code> <p>Target dataset path (for upsert/update strategies)</p> <code>None</code> <code>strategy</code> <code>str | MergeStrategy</code> <p>Merge strategy to use</p> <code>'deduplicate'</code> <code>key_columns</code> <code>list[str] | str | None</code> <p>Key columns for merging</p> <code>None</code> <code>compression</code> <code>str | None</code> <p>Output compression codec</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Print progress information</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>MergeStats</code> <p>MergeStats with merge statistics</p> Example <pre><code>from fsspeckit.datasets.duckdb_connection import create_duckdb_connection\nfrom fsspeckit.datasets.duckdb_dataset import DuckDBDatasetIO\n\nconn = create_duckdb_connection()\nio = DuckDBDatasetIO(conn)\nstats = io.merge_parquet_dataset(\n    sources=[\"dataset1/\", \"dataset2/\"],\n    output_path=\"merged/\",\n    strategy=\"deduplicate\",\n    key_columns=[\"id\"],\n)\n</code></pre> Source code in <code>src/fsspeckit/datasets/duckdb_dataset.py</code> <pre><code>def merge_parquet_dataset(\n    self,\n    sources: list[str],\n    output_path: str,\n    target: str | None = None,\n    strategy: str | CoreMergeStrategy = \"deduplicate\",\n    key_columns: list[str] | str | None = None,\n    compression: str | None = None,\n    verbose: bool = False,\n    **kwargs: Any,\n) -&gt; MergeStats:\n    \"\"\"Merge multiple parquet datasets using DuckDB.\n\n    Args:\n        sources: List of source dataset paths\n        output_path: Path for merged output\n        target: Target dataset path (for upsert/update strategies)\n        strategy: Merge strategy to use\n        key_columns: Key columns for merging\n        compression: Output compression codec\n        verbose: Print progress information\n        **kwargs: Additional arguments\n\n    Returns:\n        MergeStats with merge statistics\n\n    Example:\n        ```python\n        from fsspeckit.datasets.duckdb_connection import create_duckdb_connection\n        from fsspeckit.datasets.duckdb_dataset import DuckDBDatasetIO\n\n        conn = create_duckdb_connection()\n        io = DuckDBDatasetIO(conn)\n        stats = io.merge_parquet_dataset(\n            sources=[\"dataset1/\", \"dataset2/\"],\n            output_path=\"merged/\",\n            strategy=\"deduplicate\",\n            key_columns=[\"id\"],\n        )\n        ```\n    \"\"\"\n    # Validate inputs using shared core logic\n    validate_merge_inputs(\n        sources=sources,\n        strategy=strategy,\n        key_columns=key_columns,\n        target=target,\n    )\n\n    validate_strategy_compatibility(strategy, key_columns, target)\n\n    # Normalize parameters\n    if key_columns is not None:\n        key_columns = normalize_key_columns(key_columns)\n\n    # Process merge using DuckDB\n    result = self._execute_merge_strategy(\n        sources=sources,\n        output_path=output_path,\n        target=target,\n        strategy=strategy,\n        key_columns=key_columns,\n        compression=compression,\n        verbose=verbose,\n    )\n\n    return result\n</code></pre> <code></code> fsspeckit.datasets.duckdb_dataset.DuckDBDatasetIO.optimize_parquet_dataset \u00b6 <pre><code>optimize_parquet_dataset(\n    path: str,\n    target_mb_per_file: int | None = None,\n    target_rows_per_file: int | None = None,\n    partition_filter: list[str] | None = None,\n    compression: str | None = None,\n    verbose: bool = False,\n) -&gt; dict[str, Any]\n</code></pre> <p>Optimize a parquet dataset.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Dataset path</p> required <code>target_mb_per_file</code> <code>int | None</code> <p>Target size per file</p> <code>None</code> <code>target_rows_per_file</code> <code>int | None</code> <p>Target rows per file</p> <code>None</code> <code>partition_filter</code> <code>list[str] | None</code> <p>Optional partition filters</p> <code>None</code> <code>compression</code> <code>str | None</code> <p>Compression codec</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Print progress information</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Optimization statistics</p> Source code in <code>src/fsspeckit/datasets/duckdb_dataset.py</code> <pre><code>def optimize_parquet_dataset(\n    self,\n    path: str,\n    target_mb_per_file: int | None = None,\n    target_rows_per_file: int | None = None,\n    partition_filter: list[str] | None = None,\n    compression: str | None = None,\n    verbose: bool = False,\n) -&gt; dict[str, Any]:\n    \"\"\"Optimize a parquet dataset.\n\n    Args:\n        path: Dataset path\n        target_mb_per_file: Target size per file\n        target_rows_per_file: Target rows per file\n        partition_filter: Optional partition filters\n        compression: Compression codec\n        verbose: Print progress information\n\n    Returns:\n        Optimization statistics\n    \"\"\"\n    # Use compaction for optimization\n    result = self.compact_parquet_dataset(\n        path=path,\n        target_mb_per_file=target_mb_per_file,\n        target_rows_per_file=target_rows_per_file,\n        partition_filter=partition_filter,\n        compression=compression,\n        dry_run=False,\n        verbose=verbose,\n    )\n\n    if verbose:\n        logger.info(\"Optimization complete: %s\", result)\n\n    return result\n</code></pre> <code></code> fsspeckit.datasets.duckdb_dataset.DuckDBDatasetIO.read_parquet \u00b6 <pre><code>read_parquet(\n    path: str,\n    columns: list[str] | None = None,\n    filter: str | None = None,\n    use_threads: bool = True,\n) -&gt; Table\n</code></pre> <p>Read parquet file(s) using DuckDB.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to parquet file or directory</p> required <code>columns</code> <code>list[str] | None</code> <p>Optional list of columns to read</p> <code>None</code> <code>filter</code> <code>str | None</code> <p>Optional SQL WHERE clause</p> <code>None</code> <code>use_threads</code> <code>bool</code> <p>Whether to use parallel reading</p> <code>True</code> <p>Returns:</p> Type Description <code>Table</code> <p>PyArrow table containing the data</p> Example <pre><code>from fsspeckit.datasets.duckdb_connection import create_duckdb_connection\nfrom fsspeckit.datasets.duckdb_dataset import DuckDBDatasetIO\n\nconn = create_duckdb_connection()\nio = DuckDBDatasetIO(conn)\ntable = io.read_parquet(\"/path/to/file.parquet\")\n</code></pre> Source code in <code>src/fsspeckit/datasets/duckdb_dataset.py</code> <pre><code>def read_parquet(\n    self,\n    path: str,\n    columns: list[str] | None = None,\n    filter: str | None = None,\n    use_threads: bool = True,\n) -&gt; pa.Table:\n    \"\"\"Read parquet file(s) using DuckDB.\n\n    Args:\n        path: Path to parquet file or directory\n        columns: Optional list of columns to read\n        filter: Optional SQL WHERE clause\n        use_threads: Whether to use parallel reading\n\n    Returns:\n        PyArrow table containing the data\n\n    Example:\n        ```python\n        from fsspeckit.datasets.duckdb_connection import create_duckdb_connection\n        from fsspeckit.datasets.duckdb_dataset import DuckDBDatasetIO\n\n        conn = create_duckdb_connection()\n        io = DuckDBDatasetIO(conn)\n        table = io.read_parquet(\"/path/to/file.parquet\")\n        ```\n    \"\"\"\n    validate_path(path)\n\n    conn = self._connection.connection\n    fs = self._connection.filesystem\n\n    # Build the query\n    query = \"SELECT * FROM parquet_scan(?)\"\n\n    params = [path]\n\n    if columns:\n        # Escape column names and build select list\n        quoted_cols = [f'\"{col}\"' for col in columns]\n        select_list = \", \".join(quoted_cols)\n        query = f\"SELECT {select_list} FROM parquet_scan(?)\"\n\n    if filter:\n        query += f\" WHERE {filter}\"\n\n    try:\n        # Execute query\n        if use_threads:\n            result = conn.execute(query, params).fetch_arrow_table()\n        else:\n            result = conn.execute(query, params).fetch_arrow_table()\n\n        return result\n\n    except (_DUCKDB_EXCEPTIONS.get(\"IOException\"), _DUCKDB_EXCEPTIONS.get(\"InvalidInputException\"), _DUCKDB_EXCEPTIONS.get(\"ParserException\")) as e:\n        raise RuntimeError(\n            f\"Failed to read parquet from {path}: {safe_format_error(e)}\"\n        ) from e\n</code></pre> <code></code> fsspeckit.datasets.duckdb_dataset.DuckDBDatasetIO.write_parquet \u00b6 <pre><code>write_parquet(\n    data: Table | list[Table],\n    path: str,\n    compression: str | None = \"snappy\",\n    row_group_size: int | None = None,\n    use_threads: bool = True,\n) -&gt; None\n</code></pre> <p>Write parquet file using DuckDB.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Table | list[Table]</code> <p>PyArrow table or list of tables to write</p> required <code>path</code> <code>str</code> <p>Output file path</p> required <code>compression</code> <code>str | None</code> <p>Compression codec to use</p> <code>'snappy'</code> <code>row_group_size</code> <code>int | None</code> <p>Rows per row group</p> <code>None</code> <code>use_threads</code> <code>bool</code> <p>Whether to use parallel writing</p> <code>True</code> Example <pre><code>import pyarrow as pa\nfrom fsspeckit.datasets.duckdb_connection import create_duckdb_connection\nfrom fsspeckit.datasets.duckdb_dataset import DuckDBDatasetIO\n\ntable = pa.table({'a': [1, 2, 3], 'b': ['x', 'y', 'z']})\nconn = create_duckdb_connection()\nio = DuckDBDatasetIO(conn)\nio.write_parquet(table, \"/tmp/data.parquet\")\n</code></pre> Source code in <code>src/fsspeckit/datasets/duckdb_dataset.py</code> <pre><code>def write_parquet(\n    self,\n    data: pa.Table | list[pa.Table],\n    path: str,\n    compression: str | None = \"snappy\",\n    row_group_size: int | None = None,\n    use_threads: bool = True,\n) -&gt; None:\n    \"\"\"Write parquet file using DuckDB.\n\n    Args:\n        data: PyArrow table or list of tables to write\n        path: Output file path\n        compression: Compression codec to use\n        row_group_size: Rows per row group\n        use_threads: Whether to use parallel writing\n\n    Example:\n        ```python\n        import pyarrow as pa\n        from fsspeckit.datasets.duckdb_connection import create_duckdb_connection\n        from fsspeckit.datasets.duckdb_dataset import DuckDBDatasetIO\n\n        table = pa.table({'a': [1, 2, 3], 'b': ['x', 'y', 'z']})\n        conn = create_duckdb_connection()\n        io = DuckDBDatasetIO(conn)\n        io.write_parquet(table, \"/tmp/data.parquet\")\n        ```\n    \"\"\"\n    validate_path(path)\n    validate_compression_codec(compression)\n\n    conn = self._connection.connection\n\n    # Register the data as a temporary table\n    table_name = f\"temp_{uuid.uuid4().hex[:16]}\"\n    conn.register(\"data_table\", data)\n\n    try:\n        # Build the COPY command\n        copy_query = f\"COPY data_table TO ?\"\n\n        params = [path]\n\n        if compression:\n            copy_query += f\" (COMPRESSION {compression})\"\n\n        if row_group_size:\n            copy_query += f\" (ROW_GROUP_SIZE {row_group_size})\"\n\n        # Execute the copy\n        if use_threads:\n            conn.execute(copy_query, params)\n        else:\n            conn.execute(copy_query, params)\n\n    finally:\n        # Clean up temporary table\n        try:\n            conn.unregister(\"data_table\")\n        except (_DUCKDB_EXCEPTIONS.get(\"CatalogException\"), _DUCKDB_EXCEPTIONS.get(\"ConnectionException\")) as e:\n            logger.warning(\"Failed to unregister temporary table: %s\", e)\n</code></pre> <code></code> fsspeckit.datasets.duckdb_dataset.DuckDBDatasetIO.write_parquet_dataset \u00b6 <pre><code>write_parquet_dataset(\n    data: Table | list[Table],\n    path: str,\n    basename_template: str | None = None,\n    schema: Schema | None = None,\n    partition_by: str | list[str] | None = None,\n    compression: str | None = \"snappy\",\n    max_rows_per_file: int | None = 5000000,\n    row_group_size: int | None = 500000,\n    use_threads: bool = True,\n) -&gt; None\n</code></pre> <p>Write a parquet dataset using DuckDB.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Table | list[Table]</code> <p>PyArrow table or list of tables to write</p> required <code>path</code> <code>str</code> <p>Output directory path</p> required <code>basename_template</code> <code>str | None</code> <p>Template for file names</p> <code>None</code> <code>schema</code> <code>Schema | None</code> <p>Optional schema to enforce</p> <code>None</code> <code>partition_by</code> <code>str | list[str] | None</code> <p>Column(s) to partition by</p> <code>None</code> <code>compression</code> <code>str | None</code> <p>Compression codec</p> <code>'snappy'</code> <code>max_rows_per_file</code> <code>int | None</code> <p>Maximum rows per file</p> <code>5000000</code> <code>row_group_size</code> <code>int | None</code> <p>Rows per row group</p> <code>500000</code> <code>use_threads</code> <code>bool</code> <p>Whether to use parallel writing</p> <code>True</code> Example <pre><code>import pyarrow as pa\nfrom fsspeckit.datasets.duckdb_connection import create_duckdb_connection\nfrom fsspeckit.datasets.duckdb_dataset import DuckDBDatasetIO\n\ntable = pa.table({'a': [1, 2, 3], 'b': ['x', 'y', 'z']})\nconn = create_duckdb_connection()\nio = DuckDBDatasetIO(conn)\nio.write_parquet_dataset(table, \"/tmp/dataset/\")\n</code></pre> Source code in <code>src/fsspeckit/datasets/duckdb_dataset.py</code> <pre><code>def write_parquet_dataset(\n    self,\n    data: pa.Table | list[pa.Table],\n    path: str,\n    basename_template: str | None = None,\n    schema: pa.Schema | None = None,\n    partition_by: str | list[str] | None = None,\n    compression: str | None = \"snappy\",\n    max_rows_per_file: int | None = 5_000_000,\n    row_group_size: int | None = 500_000,\n    use_threads: bool = True,\n) -&gt; None:\n    \"\"\"Write a parquet dataset using DuckDB.\n\n    Args:\n        data: PyArrow table or list of tables to write\n        path: Output directory path\n        basename_template: Template for file names\n        schema: Optional schema to enforce\n        partition_by: Column(s) to partition by\n        compression: Compression codec\n        max_rows_per_file: Maximum rows per file\n        row_group_size: Rows per row group\n        use_threads: Whether to use parallel writing\n\n    Example:\n        ```python\n        import pyarrow as pa\n        from fsspeckit.datasets.duckdb_connection import create_duckdb_connection\n        from fsspeckit.datasets.duckdb_dataset import DuckDBDatasetIO\n\n        table = pa.table({'a': [1, 2, 3], 'b': ['x', 'y', 'z']})\n        conn = create_duckdb_connection()\n        io = DuckDBDatasetIO(conn)\n        io.write_parquet_dataset(table, \"/tmp/dataset/\")\n        ```\n    \"\"\"\n    from fsspeckit.common.optional import _import_pyarrow\n\n    validate_path(path)\n    validate_compression_codec(compression)\n\n    pa_mod = _import_pyarrow()\n\n    conn = self._connection.connection\n\n    # Register the data as a temporary table\n    table_name = f\"temp_{uuid.uuid4().hex[:16]}\"\n    conn.register(\"data_table\", data)\n\n    try:\n        # Build the COPY command for dataset\n        copy_query = \"COPY data_table TO ? (FORMAT PARQUET\"\n\n        params = [path + \"/{i}.parquet\"]\n\n        if compression:\n            copy_query += f\", COMPRESSION {compression}\"\n\n        if max_rows_per_file:\n            copy_query += f\", MAX_ROWS_PER_FILE {max_rows_per_file}\"\n\n        if row_group_size:\n            copy_query += f\", ROW_GROUP_SIZE {row_group_size}\"\n\n        copy_query += \")\"\n\n        # Execute with file numbering\n        conn.execute(copy_query, params)\n\n    finally:\n        # Clean up temporary table\n        try:\n            conn.unregister(\"data_table\")\n        except (_DUCKDB_EXCEPTIONS.get(\"CatalogException\"), _DUCKDB_EXCEPTIONS.get(\"ConnectionException\")) as e:\n            logger.warning(\"Failed to unregister temporary table: %s\", e)\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.duckdb_dataset-functions","title":"Functions","text":""},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.pyarrow","title":"fsspeckit.datasets.pyarrow","text":"<p>Re-export module for backward compatibility.</p> <p>This module has been decomposed into focused submodules: - pyarrow_schema: Schema unification, type inference, and optimization - pyarrow_dataset: Dataset merge and maintenance operations</p> <p>All public APIs are re-exported here to maintain backward compatibility. New code should import directly from the submodules for better organization.</p>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.pyarrow-functions","title":"Functions","text":""},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.pyarrow.cast_schema","title":"fsspeckit.datasets.pyarrow.cast_schema","text":"<pre><code>cast_schema(table: Table, schema: Schema) -&gt; Table\n</code></pre> <p>Cast a PyArrow table to a target schema.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>Table</code> <p>Source table</p> required <code>schema</code> <code>Schema</code> <p>Target schema</p> required <p>Returns:</p> Type Description <code>Table</code> <p>Table cast to target schema</p> Source code in <code>src/fsspeckit/datasets/pyarrow_schema.py</code> <pre><code>def cast_schema(table: pa.Table, schema: pa.Schema) -&gt; pa.Table:\n    \"\"\"Cast a PyArrow table to a target schema.\n\n    Args:\n        table: Source table\n        schema: Target schema\n\n    Returns:\n        Table cast to target schema\n    \"\"\"\n    # Filter schema to only include columns present in the table\n    table_schema = table.schema\n    valid_fields = []\n\n    for field in schema:\n        if field.name in table_schema.names:\n            valid_fields.append(field)\n\n    target_schema = pa.schema(valid_fields)\n\n    # Cast the table\n    return table.cast(target_schema)\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.pyarrow.collect_dataset_stats_pyarrow","title":"fsspeckit.datasets.pyarrow.collect_dataset_stats_pyarrow","text":"<pre><code>collect_dataset_stats_pyarrow(\n    path: str,\n    filesystem: AbstractFileSystem | None = None,\n    partition_filter: list[str] | None = None,\n) -&gt; dict[str, Any]\n</code></pre> <p>Collect file-level statistics for a parquet dataset using shared core logic.</p> <p>This function delegates to the shared <code>fsspeckit.core.maintenance.collect_dataset_stats</code> function, ensuring consistent dataset discovery and statistics across both DuckDB and PyArrow backends.</p> <p>The helper walks the given dataset directory on the provided filesystem, discovers parquet files (recursively), and returns basic statistics:</p> <ul> <li>Per-file path, size in bytes, and number of rows</li> <li>Aggregated total bytes and total rows</li> </ul> <p>The function is intentionally streaming/metadata-driven and never materializes the full dataset as a single :class:<code>pyarrow.Table</code>.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Root directory of the parquet dataset.</p> required <code>filesystem</code> <code>AbstractFileSystem | None</code> <p>Optional fsspec filesystem. If omitted, a local \"file\" filesystem is used.</p> <code>None</code> <code>partition_filter</code> <code>list[str] | None</code> <p>Optional list of partition prefix filters (e.g. [\"date=2025-11-04\"]). Only files whose path relative to <code>path</code> starts with one of these prefixes are included.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict with keys:</p> <code>dict[str, Any]</code> <ul> <li><code>files</code>: list of <code>{\"path\", \"size_bytes\", \"num_rows\"}</code> dicts</li> </ul> <code>dict[str, Any]</code> <ul> <li><code>total_bytes</code>: sum of file sizes</li> </ul> <code>dict[str, Any]</code> <ul> <li><code>total_rows</code>: sum of row counts</li> </ul> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the path does not exist or no parquet files match the optional partition filter.</p> Note <p>This is a thin wrapper around the shared core function. See :func:<code>fsspeckit.core.maintenance.collect_dataset_stats</code> for the authoritative implementation.</p> Source code in <code>src/fsspeckit/datasets/pyarrow_dataset.py</code> <pre><code>def collect_dataset_stats_pyarrow(\n    path: str,\n    filesystem: AbstractFileSystem | None = None,\n    partition_filter: list[str] | None = None,\n) -&gt; dict[str, Any]:\n    \"\"\"Collect file-level statistics for a parquet dataset using shared core logic.\n\n    This function delegates to the shared ``fsspeckit.core.maintenance.collect_dataset_stats``\n    function, ensuring consistent dataset discovery and statistics across both DuckDB\n    and PyArrow backends.\n\n    The helper walks the given dataset directory on the provided filesystem,\n    discovers parquet files (recursively), and returns basic statistics:\n\n    - Per-file path, size in bytes, and number of rows\n    - Aggregated total bytes and total rows\n\n    The function is intentionally streaming/metadata-driven and never\n    materializes the full dataset as a single :class:`pyarrow.Table`.\n\n    Args:\n        path: Root directory of the parquet dataset.\n        filesystem: Optional fsspec filesystem. If omitted, a local \"file\"\n            filesystem is used.\n        partition_filter: Optional list of partition prefix filters\n            (e.g. [\"date=2025-11-04\"]). Only files whose path relative to\n            ``path`` starts with one of these prefixes are included.\n\n    Returns:\n        Dict with keys:\n\n        - ``files``: list of ``{\"path\", \"size_bytes\", \"num_rows\"}`` dicts\n        - ``total_bytes``: sum of file sizes\n        - ``total_rows``: sum of row counts\n\n    Raises:\n        FileNotFoundError: If the path does not exist or no parquet files\n            match the optional partition filter.\n\n    Note:\n        This is a thin wrapper around the shared core function. See\n        :func:`fsspeckit.core.maintenance.collect_dataset_stats` for the\n        authoritative implementation.\n    \"\"\"\n    from fsspeckit.core.maintenance import collect_dataset_stats\n\n    return collect_dataset_stats(\n        path=path,\n        filesystem=filesystem,\n        partition_filter=partition_filter,\n    )\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.pyarrow.compact_parquet_dataset_pyarrow","title":"fsspeckit.datasets.pyarrow.compact_parquet_dataset_pyarrow","text":"<pre><code>compact_parquet_dataset_pyarrow(\n    path: str,\n    target_mb_per_file: int | None = None,\n    target_rows_per_file: int | None = None,\n    partition_filter: list[str] | None = None,\n    compression: str | None = None,\n    dry_run: bool = False,\n    filesystem: AbstractFileSystem | None = None,\n) -&gt; dict[str, Any]\n</code></pre> <p>Compact a parquet dataset directory into fewer larger files using PyArrow and shared planning.</p> <p>Groups small files based on size (MB) and/or row thresholds, rewrites grouped files into new parquet files, and optionally changes compression. Supports a dry-run mode that returns the compaction plan without modifying files.</p> <p>The implementation uses the shared core planning algorithm for consistent behavior across backends. It processes data in a group-based, streaming fashion: it reads only the files in a given group into memory when processing that group and never materializes the entire dataset as a single table.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Dataset root directory (local path or fsspec URL).</p> required <code>target_mb_per_file</code> <code>int | None</code> <p>Optional max output size per file; must be &gt; 0.</p> <code>None</code> <code>target_rows_per_file</code> <code>int | None</code> <p>Optional max rows per output file; must be &gt; 0.</p> <code>None</code> <code>partition_filter</code> <code>list[str] | None</code> <p>Optional list of partition prefixes (e.g. <code>[\"date=2025-11-15\"]</code>) used to limit both stats collection and rewrites to matching paths.</p> <code>None</code> <code>compression</code> <code>str | None</code> <p>Optional parquet compression codec; defaults to <code>\"snappy\"</code>.</p> <code>None</code> <code>dry_run</code> <code>bool</code> <p>When <code>True</code> the function returns a plan + before/after stats without reading or writing any parquet data.</p> <code>False</code> <code>filesystem</code> <code>AbstractFileSystem | None</code> <p>Optional <code>fsspec.AbstractFileSystem</code> to reuse existing FS clients.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A stats dictionary describing before/after file counts, total bytes,</p> <code>dict[str, Any]</code> <p>rewritten bytes, and optional <code>planned_groups</code> when <code>dry_run</code> is enabled.</p> <code>dict[str, Any]</code> <p>The structure follows the canonical <code>MaintenanceStats</code> format from the shared core.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If thresholds are invalid or no files match partition filter.</p> <code>FileNotFoundError</code> <p>If the path does not exist.</p> Example <pre><code>result = compact_parquet_dataset_pyarrow(\n    \"/path/to/dataset\",\n    target_mb_per_file=64,\n    dry_run=True,\n)\nprint(f\"Files before: {result['before_file_count']}\")\nprint(f\"Files after: {result['after_file_count']}\")\n</code></pre> Note <p>This function delegates dataset discovery and compaction planning to the shared <code>fsspeckit.core.maintenance</code> module, ensuring consistent behavior across DuckDB and PyArrow backends.</p> Source code in <code>src/fsspeckit/datasets/pyarrow_dataset.py</code> <pre><code>def compact_parquet_dataset_pyarrow(\n    path: str,\n    target_mb_per_file: int | None = None,\n    target_rows_per_file: int | None = None,\n    partition_filter: list[str] | None = None,\n    compression: str | None = None,\n    dry_run: bool = False,\n    filesystem: AbstractFileSystem | None = None,\n) -&gt; dict[str, Any]:\n    \"\"\"Compact a parquet dataset directory into fewer larger files using PyArrow and shared planning.\n\n    Groups small files based on size (MB) and/or row thresholds, rewrites grouped\n    files into new parquet files, and optionally changes compression. Supports a\n    dry-run mode that returns the compaction plan without modifying files.\n\n    The implementation uses the shared core planning algorithm for consistent\n    behavior across backends. It processes data in a group-based, streaming fashion:\n    it reads only the files in a given group into memory when processing that group\n    and never materializes the entire dataset as a single table.\n\n    Args:\n        path: Dataset root directory (local path or fsspec URL).\n        target_mb_per_file: Optional max output size per file; must be &gt; 0.\n        target_rows_per_file: Optional max rows per output file; must be &gt; 0.\n        partition_filter: Optional list of partition prefixes (e.g. ``[\"date=2025-11-15\"]``)\n            used to limit both stats collection and rewrites to matching paths.\n        compression: Optional parquet compression codec; defaults to ``\"snappy\"``.\n        dry_run: When ``True`` the function returns a plan + before/after stats\n            without reading or writing any parquet data.\n        filesystem: Optional ``fsspec.AbstractFileSystem`` to reuse existing FS clients.\n\n    Returns:\n        A stats dictionary describing before/after file counts, total bytes,\n        rewritten bytes, and optional ``planned_groups`` when ``dry_run`` is enabled.\n        The structure follows the canonical ``MaintenanceStats`` format from the shared core.\n\n    Raises:\n        ValueError: If thresholds are invalid or no files match partition filter.\n        FileNotFoundError: If the path does not exist.\n\n    Example:\n        ```python\n        result = compact_parquet_dataset_pyarrow(\n            \"/path/to/dataset\",\n            target_mb_per_file=64,\n            dry_run=True,\n        )\n        print(f\"Files before: {result['before_file_count']}\")\n        print(f\"Files after: {result['after_file_count']}\")\n        ```\n\n    Note:\n        This function delegates dataset discovery and compaction planning to the\n        shared ``fsspeckit.core.maintenance`` module, ensuring consistent behavior\n        across DuckDB and PyArrow backends.\n    \"\"\"\n    from fsspeckit.core.maintenance import plan_compaction_groups, MaintenanceStats\n\n    # Get dataset stats using shared logic\n    stats = collect_dataset_stats_pyarrow(\n        path=path, filesystem=filesystem, partition_filter=partition_filter\n    )\n    files = stats[\"files\"]\n\n    # Use shared compaction planning\n    plan_result = plan_compaction_groups(\n        file_infos=files,\n        target_mb_per_file=target_mb_per_file,\n        target_rows_per_file=target_rows_per_file,\n    )\n\n    groups = plan_result[\"groups\"]\n    planned_stats = plan_result[\"planned_stats\"]\n\n    # Update planned stats with compression info\n    planned_stats.compression_codec = compression\n    planned_stats.dry_run = dry_run\n\n    # If dry run, return the plan\n    if dry_run:\n        result = planned_stats.to_dict()\n        result[\"planned_groups\"] = groups\n        return result\n\n    # Execute compaction\n    if not groups:\n        return planned_stats.to_dict()\n\n    # Execute the compaction\n    for group in groups:\n        # Read all files in this group\n        tables = []\n        for file_info in group[\"files\"]:\n            file_path = file_info[\"path\"]\n            table = pq.read_table(\n                file_path,\n                filesystem=filesystem,\n            )\n            tables.append(table)\n\n        # Concatenate tables\n        if len(tables) &gt; 1:\n            combined = pa.concat_tables(tables, promote_options=\"permissive\")\n        else:\n            combined = tables[0]\n\n        # Write to output file\n        output_path = group[\"output_path\"]\n        pq.write_table(\n            combined,\n            output_path,\n            filesystem=filesystem,\n            compression=compression or \"snappy\",\n        )\n\n    # Remove original files\n    for group in groups:\n        for file_info in group[\"files\"]:\n            file_path = file_info[\"path\"]\n            filesystem.rm(file_path)\n\n    return planned_stats.to_dict()\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.pyarrow.convert_large_types_to_normal","title":"fsspeckit.datasets.pyarrow.convert_large_types_to_normal","text":"<pre><code>convert_large_types_to_normal(schema: Schema) -&gt; Schema\n</code></pre> <p>Convert large types (like large_string) to normal types.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>Schema</code> <p>PyArrow schema</p> required <p>Returns:</p> Type Description <code>Schema</code> <p>Schema with large types converted</p> Source code in <code>src/fsspeckit/datasets/pyarrow_schema.py</code> <pre><code>def convert_large_types_to_normal(schema: pa.Schema) -&gt; pa.Schema:\n    \"\"\"Convert large types (like large_string) to normal types.\n\n    Args:\n        schema: PyArrow schema\n\n    Returns:\n        Schema with large types converted\n    \"\"\"\n    fields = []\n    for field in schema:\n        if pa.types.is_large_string(field.type):\n            field = field.with_type(pa.string())\n        elif pa.types.is_large_utf8(field.type):\n            field = field.with_type(pa.utf8())\n        elif pa.types.is_large_list(field.type):\n            field = field.with_type(pa.list_(field.type.value_type))\n        fields.append(field)\n\n    return pa.schema(fields)\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.pyarrow.merge_parquet_dataset_pyarrow","title":"fsspeckit.datasets.pyarrow.merge_parquet_dataset_pyarrow","text":"<pre><code>merge_parquet_dataset_pyarrow(\n    sources: list[str],\n    output_path: str,\n    target: str | None = None,\n    strategy: str | MergeStrategy = \"deduplicate\",\n    key_columns: list[str] | str | None = None,\n    filesystem: AbstractFileSystem | None = None,\n    compression: str | None = None,\n    row_group_size: int | None = 500000,\n    max_rows_per_file: int | None = 5000000,\n    verbose: bool = False,\n    **kwargs: Any,\n) -&gt; MergeStats\n</code></pre> <p>Merge multiple parquet datasets using PyArrow with various strategies.</p> <p>This function provides dataset merging capabilities with support for: - Multiple merge strategies (upsert, insert, update, full_merge, deduplicate) - Key-based merging for relational operations - Batch processing for large datasets - Configurable output settings</p> <p>Parameters:</p> Name Type Description Default <code>sources</code> <code>list[str]</code> <p>List of source dataset paths</p> required <code>output_path</code> <code>str</code> <p>Path for merged output</p> required <code>target</code> <code>str | None</code> <p>Target dataset path (for upsert/update strategies)</p> <code>None</code> <code>strategy</code> <code>str | MergeStrategy</code> <p>Merge strategy to use</p> <code>'deduplicate'</code> <code>key_columns</code> <code>list[str] | str | None</code> <p>Key columns for merging (required for relational strategies)</p> <code>None</code> <code>filesystem</code> <code>AbstractFileSystem | None</code> <p>fsspec filesystem instance</p> <code>None</code> <code>compression</code> <code>str | None</code> <p>Output compression codec</p> <code>None</code> <code>row_group_size</code> <code>int | None</code> <p>Rows per parquet row group</p> <code>500000</code> <code>max_rows_per_file</code> <code>int | None</code> <p>Max rows per output file</p> <code>5000000</code> <code>verbose</code> <code>bool</code> <p>Print progress information</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>MergeStats</code> <p>MergeStats with merge statistics</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required parameters are missing</p> <code>FileNotFoundError</code> <p>If sources don't exist</p> Example <pre><code>stats = merge_parquet_dataset_pyarrow(\n    sources=[\"dataset1/\", \"dataset2/\"],\n    output_path=\"merged/\",\n    strategy=\"deduplicate\",\n    key_columns=[\"id\"],\n    verbose=True,\n)\nprint(f\"Merged {stats.total_rows} rows\")\n</code></pre> Source code in <code>src/fsspeckit/datasets/pyarrow_dataset.py</code> <pre><code>def merge_parquet_dataset_pyarrow(\n    sources: list[str],\n    output_path: str,\n    target: str | None = None,\n    strategy: str | CoreMergeStrategy = \"deduplicate\",\n    key_columns: list[str] | str | None = None,\n    filesystem: AbstractFileSystem | None = None,\n    compression: str | None = None,\n    row_group_size: int | None = 500_000,\n    max_rows_per_file: int | None = 5_000_000,\n    verbose: bool = False,\n    **kwargs: Any,\n) -&gt; MergeStats:\n    \"\"\"Merge multiple parquet datasets using PyArrow with various strategies.\n\n    This function provides dataset merging capabilities with support for:\n    - Multiple merge strategies (upsert, insert, update, full_merge, deduplicate)\n    - Key-based merging for relational operations\n    - Batch processing for large datasets\n    - Configurable output settings\n\n    Args:\n        sources: List of source dataset paths\n        output_path: Path for merged output\n        target: Target dataset path (for upsert/update strategies)\n        strategy: Merge strategy to use\n        key_columns: Key columns for merging (required for relational strategies)\n        filesystem: fsspec filesystem instance\n        compression: Output compression codec\n        row_group_size: Rows per parquet row group\n        max_rows_per_file: Max rows per output file\n        verbose: Print progress information\n        **kwargs: Additional arguments\n\n    Returns:\n        MergeStats with merge statistics\n\n    Raises:\n        ValueError: If required parameters are missing\n        FileNotFoundError: If sources don't exist\n\n    Example:\n        ```python\n        stats = merge_parquet_dataset_pyarrow(\n            sources=[\"dataset1/\", \"dataset2/\"],\n            output_path=\"merged/\",\n            strategy=\"deduplicate\",\n            key_columns=[\"id\"],\n            verbose=True,\n        )\n        print(f\"Merged {stats.total_rows} rows\")\n        ```\n    \"\"\"\n    # Validate inputs using shared core logic\n    validate_merge_inputs(\n        sources=sources,\n        strategy=strategy,\n        key_columns=key_columns,\n        target=target,\n    )\n\n    validate_strategy_compatibility(strategy, key_columns, target)\n\n    # Normalize parameters\n    if key_columns is not None:\n        key_columns = _normalize_key_columns(key_columns)\n\n    # Get filesystem\n    if filesystem is None:\n        filesystem = fsspec_filesystem(\"file\")\n\n    pa_filesystem = _ensure_pyarrow_filesystem(filesystem)\n\n    # Load target if provided\n    target_table = None\n    if target and strategy in [\"upsert\", \"update\"]:\n        target_table = _load_source_table_pyarrow(target, filesystem)\n\n        if key_columns:\n            _ensure_no_null_keys_table(target_table, key_columns)\n\n    # Process sources\n    merged_data = []\n    total_rows = 0\n\n    for source_path in sources:\n        if verbose:\n            logger.info(\"Processing source: %s\", source_path)\n\n        source_table = _load_source_table_pyarrow(source_path, filesystem)\n\n        if key_columns:\n            _ensure_no_null_keys_table(source_table, key_columns)\n\n        if strategy == \"full_merge\":\n            # Simply concatenate all data\n            merged_data.append(source_table)\n            total_rows += source_table.num_rows\n\n        elif strategy == \"deduplicate\":\n            # Remove duplicates based on key columns\n            if key_columns:\n                # Group by keys and keep first occurrence\n                table = source_table\n\n                # Use PyArrow's group_by for deduplication\n                # This is a simplified implementation\n                groups = table.group_by(key_columns).aggregate([])\n                keys = groups.select(key_columns)\n\n                # Get unique keys\n                unique_keys = []\n                for row in keys.to_pylist():\n                    unique_keys.append(tuple(row[col] for col in key_columns))\n\n                # Filter to keep only unique rows\n                filtered = []\n                for row in table.to_pylist():\n                    key = tuple(row[col] for col in key_columns)\n                    if key in unique_keys:\n                        filtered.append(row)\n                        unique_keys.remove(key)  # Remove to avoid duplicates\n\n                if filtered:\n                    deduped = pa.Table.from_pylist(filtered, schema=table.schema)\n                    merged_data.append(deduped)\n                    total_rows += deduped.num_rows\n            else:\n                # No key columns, remove exact duplicates\n                merged_data.append(source_table)\n                total_rows += source_table.num_rows\n\n        elif strategy in [\"upsert\", \"insert\", \"update\"] and target_table is not None:\n            # Key-based relational operations\n            if strategy == \"insert\":\n                # Only insert non-existing rows\n                target_keys = _extract_key_tuples(target_table, key_columns)\n                source_keys = _extract_key_tuples(source_table, key_columns)\n\n                # Find keys that don't exist in target\n                new_keys = set(source_keys) - set(target_keys)\n\n                # Filter source for new keys\n                new_rows = []\n                for row in source_table.to_pylist():\n                    key = tuple(row[col] for col in key_columns)\n                    if key in new_keys:\n                        new_rows.append(row)\n\n                if new_rows:\n                    new_table = pa.Table.from_pylist(new_rows, schema=source_table.schema)\n                    merged_data.append(new_table)\n                    total_rows += new_table.num_rows\n\n            elif strategy == \"update\":\n                # Update existing rows\n                target_keys = _extract_key_tuples(target_table, key_columns)\n                source_keys = _extract_key_tuples(source_table, key_columns)\n\n                # Find common keys\n                common_keys = set(source_keys) &amp; set(target_keys)\n\n                # Build updated target\n                updated_data = []\n\n                # Keep non-matching rows from target\n                for row in target_table.to_pylist():\n                    key = tuple(row[col] for col in key_columns)\n                    if key not in common_keys:\n                        updated_data.append(row)\n\n                # Add updated rows from source\n                for row in source_table.to_pylist():\n                    key = tuple(row[col] for col in key_columns)\n                    if key in common_keys:\n                        updated_data.append(row)\n\n                if updated_data:\n                    updated_table = pa.Table.from_pylist(updated_data, schema=target_table.schema)\n                    merged_data.append(updated_table)\n                    total_rows += updated_table.num_rows\n\n            elif strategy == \"upsert\":\n                # Insert or update\n                all_data = [target_table] if target_table else []\n                all_data.append(source_table)\n\n                if all_data:\n                    combined = pa.concat_tables(all_data, promote_options=\"permissive\")\n\n                    # Deduplicate based on keys\n                    if key_columns:\n                        # Group by keys and keep last occurrence\n                        # This is a simplified implementation\n                        groups = combined.group_by(key_columns).aggregate([])\n                        keys = groups.select(key_columns)\n\n                        unique_keys = []\n                        for row in keys.to_pylist():\n                            unique_keys.append(tuple(row[col] for col in key_columns))\n\n                        # Keep only last occurrence of each key\n                        filtered = []\n                        seen = set()\n                        for row in reversed(combined.to_pylist()):\n                            key = tuple(row[col] for col in key_columns)\n                            if key not in seen:\n                                filtered.append(row)\n                                seen.add(key)\n\n                        if filtered:\n                            deduped = pa.Table.from_pylist(\n                                list(reversed(filtered)), schema=combined.schema\n                            )\n                            merged_data.append(deduped)\n                            total_rows += deduped.num_rows\n                    else:\n                        merged_data.append(combined)\n                        total_rows += combined.num_rows\n\n    # Combine all data\n    if merged_data:\n        if len(merged_data) == 1:\n            final_table = merged_data[0]\n        else:\n            final_table = pa.concat_tables(\n                merged_data, promote_options=\"permissive\"\n            )\n    else:\n        # No data to merge\n        final_table = pa.table({})\n\n    # Write output\n    pq.write_table(\n        final_table,\n        output_path,\n        filesystem=pa_filesystem,\n        compression=compression,\n        row_group_size=row_group_size,\n        max_rows_per_file=max_rows_per_file,\n    )\n\n    # Calculate stats\n    stats = calculate_merge_stats(\n        sources=sources,\n        target=output_path,\n        strategy=strategy,\n        total_rows=total_rows,\n        output_rows=final_table.num_rows,\n    )\n\n    if verbose:\n        logger.info(\"\\nMerge complete: %s\", stats)\n\n    return stats\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.pyarrow.opt_dtype","title":"fsspeckit.datasets.pyarrow.opt_dtype","text":"<pre><code>opt_dtype(\n    table: Table,\n    strict: bool = False,\n    columns: list[str] | None = None,\n) -&gt; Table\n</code></pre> <p>Optimize dtypes in a PyArrow table based on data analysis.</p> <p>This function analyzes the data in each column and attempts to downcast to more appropriate types (e.g., int64 -&gt; int32, float64 -&gt; float32, string -&gt; int/bool where applicable).</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>Table</code> <p>PyArrow table to optimize</p> required <code>strict</code> <code>bool</code> <p>Whether to use strict type checking</p> <code>False</code> <code>columns</code> <code>list[str] | None</code> <p>List of columns to optimize (None for all)</p> <code>None</code> <p>Returns:</p> Type Description <code>Table</code> <p>Table with optimized dtypes</p> Example <pre><code>import pyarrow as pa\n\ntable = pa.table(\n    {\n        \"a\": pa.array([1, 2, 3], type=pa.int64()),\n        \"b\": pa.array([1.0, 2.0, 3.0], type=pa.float64()),\n    },\n)\noptimized = opt_dtype(table)\nprint(optimized.column(0).type)  # DataType(int32)\nprint(optimized.column(1).type)  # DataType(float32)\n</code></pre> Source code in <code>src/fsspeckit/datasets/pyarrow_schema.py</code> <pre><code>def opt_dtype(\n    table: pa.Table,\n    strict: bool = False,\n    columns: list[str] | None = None,\n) -&gt; pa.Table:\n    \"\"\"Optimize dtypes in a PyArrow table based on data analysis.\n\n    This function analyzes the data in each column and attempts to downcast\n    to more appropriate types (e.g., int64 -&gt; int32, float64 -&gt; float32,\n    string -&gt; int/bool where applicable).\n\n    Args:\n        table: PyArrow table to optimize\n        strict: Whether to use strict type checking\n        columns: List of columns to optimize (None for all)\n\n    Returns:\n        Table with optimized dtypes\n\n    Example:\n        ```python\n        import pyarrow as pa\n\n        table = pa.table(\n            {\n                \"a\": pa.array([1, 2, 3], type=pa.int64()),\n                \"b\": pa.array([1.0, 2.0, 3.0], type=pa.float64()),\n            },\n        )\n        optimized = opt_dtype(table)\n        print(optimized.column(0).type)  # DataType(int32)\n        print(optimized.column(1).type)  # DataType(float32)\n        ```\n    \"\"\"\n    from fsspeckit.common.misc import run_parallel\n\n    if columns is None:\n        columns = table.column_names\n\n    # Process columns in parallel\n    results = run_parallel(\n        _process_column_for_opt_dtype,\n        [(table, col, strict) for col in columns],\n        backend=\"threading\",\n        n_jobs=-1,\n    )\n\n    # Build new table with optimized columns\n    new_columns = {}\n    for col_name, optimized_array in results:\n        new_columns[col_name] = optimized_array\n\n    # Keep non-optimized columns as-is\n    for col_name in table.column_names:\n        if col_name not in new_columns:\n            new_columns[col_name] = table.column(col_name)\n\n    return pa.table(new_columns)\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.pyarrow.optimize_parquet_dataset_pyarrow","title":"fsspeckit.datasets.pyarrow.optimize_parquet_dataset_pyarrow","text":"<pre><code>optimize_parquet_dataset_pyarrow(\n    path: str,\n    target_mb_per_file: int | None = None,\n    target_rows_per_file: int | None = None,\n    partition_filter: list[str] | None = None,\n    compression: str | None = None,\n    filesystem: AbstractFileSystem | None = None,\n    verbose: bool = False,\n) -&gt; dict[str, Any]\n</code></pre> <p>Optimize a parquet dataset through compaction and optional statistics recalculation.</p> <p>This is a convenience function that combines compaction with optional statistics recalculation. It's particularly useful after many small write operations have created a large number of small files.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Dataset root directory</p> required <code>target_mb_per_file</code> <code>int | None</code> <p>Target size per file in MB</p> <code>None</code> <code>target_rows_per_file</code> <code>int | None</code> <p>Target rows per file</p> <code>None</code> <code>partition_filter</code> <code>list[str] | None</code> <p>Optional partition filters</p> <code>None</code> <code>compression</code> <code>str | None</code> <p>Compression codec to use</p> <code>None</code> <code>filesystem</code> <code>AbstractFileSystem | None</code> <p>Optional filesystem instance</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Print progress information</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Optimization statistics</p> Example <pre><code>stats = optimize_parquet_dataset_pyarrow(\n    \"dataset/\",\n    target_mb_per_file=64,\n    compression=\"zstd\",\n)\nprint(\n    f\"Reduced from {stats['before_file_count']} \"\n    f\"to {stats['after_file_count']} files\",\n)\n</code></pre> Source code in <code>src/fsspeckit/datasets/pyarrow_dataset.py</code> <pre><code>def optimize_parquet_dataset_pyarrow(\n    path: str,\n    target_mb_per_file: int | None = None,\n    target_rows_per_file: int | None = None,\n    partition_filter: list[str] | None = None,\n    compression: str | None = None,\n    filesystem: AbstractFileSystem | None = None,\n    verbose: bool = False,\n) -&gt; dict[str, Any]:\n    \"\"\"Optimize a parquet dataset through compaction and optional statistics recalculation.\n\n    This is a convenience function that combines compaction with optional statistics\n    recalculation. It's particularly useful after many small write operations have\n    created a large number of small files.\n\n    Args:\n        path: Dataset root directory\n        target_mb_per_file: Target size per file in MB\n        target_rows_per_file: Target rows per file\n        partition_filter: Optional partition filters\n        compression: Compression codec to use\n        filesystem: Optional filesystem instance\n        verbose: Print progress information\n\n    Returns:\n        Optimization statistics\n\n    Example:\n        ```python\n        stats = optimize_parquet_dataset_pyarrow(\n            \"dataset/\",\n            target_mb_per_file=64,\n            compression=\"zstd\",\n        )\n        print(\n            f\"Reduced from {stats['before_file_count']} \"\n            f\"to {stats['after_file_count']} files\",\n        )\n        ```\n    \"\"\"\n    # Use compaction\n    result = compact_parquet_dataset_pyarrow(\n        path=path,\n        target_mb_per_file=target_mb_per_file,\n        target_rows_per_file=target_rows_per_file,\n        partition_filter=partition_filter,\n        compression=compression,\n        dry_run=False,\n        filesystem=filesystem,\n    )\n\n    if verbose:\n        logger.info(\"Optimization complete: %s\", result)\n\n    return result\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.pyarrow.unify_schemas","title":"fsspeckit.datasets.pyarrow.unify_schemas","text":"<pre><code>unify_schemas(\n    schemas: list[Schema],\n    standardize_timezones: bool = True,\n    verbose: bool = False,\n) -&gt; Schema\n</code></pre> <p>Unify multiple PyArrow schemas into a single schema.</p> <p>This function handles type conflicts by: 1. Finding fields with conflicting types 2. Attempting to normalize compatible types 3. Using fallback strategies for incompatible types 4. Removing problematic fields if necessary</p> <p>Parameters:</p> Name Type Description Default <code>schemas</code> <code>list[Schema]</code> <p>List of schemas to unify</p> required <code>standardize_timezones</code> <code>bool</code> <p>Whether to standardize timezone info</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>Whether to print conflict information</p> <code>False</code> <p>Returns:</p> Type Description <code>Schema</code> <p>Unified PyArrow schema</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If schemas cannot be unified</p> Source code in <code>src/fsspeckit/datasets/pyarrow_schema.py</code> <pre><code>def unify_schemas(\n    schemas: list[pa.Schema],\n    standardize_timezones: bool = True,\n    verbose: bool = False,\n) -&gt; pa.Schema:\n    \"\"\"Unify multiple PyArrow schemas into a single schema.\n\n    This function handles type conflicts by:\n    1. Finding fields with conflicting types\n    2. Attempting to normalize compatible types\n    3. Using fallback strategies for incompatible types\n    4. Removing problematic fields if necessary\n\n    Args:\n        schemas: List of schemas to unify\n        standardize_timezones: Whether to standardize timezone info\n        verbose: Whether to print conflict information\n\n    Returns:\n        Unified PyArrow schema\n\n    Raises:\n        ValueError: If schemas cannot be unified\n    \"\"\"\n    if not schemas:\n        raise ValueError(\"Cannot unify empty list of schemas\")\n\n    if len(schemas) == 1:\n        return schemas[0]\n\n    # Remove duplicate schemas\n    schemas = _unique_schemas(schemas)\n\n    # Standardize timezones if requested\n    if standardize_timezones:\n        schemas = standardize_schema_timezones(schemas, standardize_timezones)\n\n    # Find conflicts\n    conflicts = _find_conflicting_fields(schemas)\n\n    if not conflicts:\n        # No conflicts, concatenate all fields\n        all_fields = []\n        for schema in schemas:\n            all_fields.extend(schema)\n        return pa.schema(all_fields)\n\n    if verbose:\n        _log_conflict_summary(conflicts, verbose)\n\n    # Try to normalize types\n    try:\n        normalized = _normalize_schema_types(schemas, conflicts)\n\n        # Check if normalization resolved conflicts\n        remaining_conflicts = _find_conflicting_fields(normalized)\n\n        if not remaining_conflicts:\n            # Normalization successful\n            all_fields = []\n            for schema in normalized:\n                all_fields.extend(schema)\n            return pa.schema(all_fields)\n\n        # Fall through to next strategy\n        conflicts = remaining_conflicts\n\n    except (pa.ArrowInvalid, pa.ArrowTypeError, pa.ArrowNotImplementedError) as e:\n        # Normalization failed, log and continue to fallback\n        logger.debug(\n            \"Schema type normalization failed: %s. Trying aggressive fallback.\",\n            str(e)\n        )\n\n    # Try aggressive fallback\n    try:\n        return _aggressive_fallback_unification(schemas)\n    except (pa.ArrowInvalid, pa.ArrowTypeError, pa.ArrowNotImplementedError) as e:\n        # Aggressive fallback failed, log and try last resort\n        logger.debug(\n            \"Aggressive fallback unification failed: %s. Trying last resort cleanup.\",\n            str(e)\n        )\n\n    # Last resort: remove problematic fields\n    cleaned = _remove_problematic_fields(schemas)\n    all_fields = []\n    for schema in cleaned:\n        all_fields.extend(schema)\n\n    if verbose and conflicts:\n        logger.debug(\"Removed %d conflicting fields during unification\", len(conflicts))\n\n    return pa.schema(all_fields)\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.pyarrow_dataset","title":"fsspeckit.datasets.pyarrow_dataset","text":"<p>PyArrow dataset operations including merge and maintenance helpers.</p> <p>This module contains functions for dataset-level operations including: - Dataset merging with various strategies - Dataset statistics collection - Dataset compaction and optimization - Maintenance operations</p>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.pyarrow_dataset-classes","title":"Classes","text":""},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.pyarrow_dataset-functions","title":"Functions","text":""},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.pyarrow_dataset.collect_dataset_stats_pyarrow","title":"fsspeckit.datasets.pyarrow_dataset.collect_dataset_stats_pyarrow","text":"<pre><code>collect_dataset_stats_pyarrow(\n    path: str,\n    filesystem: AbstractFileSystem | None = None,\n    partition_filter: list[str] | None = None,\n) -&gt; dict[str, Any]\n</code></pre> <p>Collect file-level statistics for a parquet dataset using shared core logic.</p> <p>This function delegates to the shared <code>fsspeckit.core.maintenance.collect_dataset_stats</code> function, ensuring consistent dataset discovery and statistics across both DuckDB and PyArrow backends.</p> <p>The helper walks the given dataset directory on the provided filesystem, discovers parquet files (recursively), and returns basic statistics:</p> <ul> <li>Per-file path, size in bytes, and number of rows</li> <li>Aggregated total bytes and total rows</li> </ul> <p>The function is intentionally streaming/metadata-driven and never materializes the full dataset as a single :class:<code>pyarrow.Table</code>.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Root directory of the parquet dataset.</p> required <code>filesystem</code> <code>AbstractFileSystem | None</code> <p>Optional fsspec filesystem. If omitted, a local \"file\" filesystem is used.</p> <code>None</code> <code>partition_filter</code> <code>list[str] | None</code> <p>Optional list of partition prefix filters (e.g. [\"date=2025-11-04\"]). Only files whose path relative to <code>path</code> starts with one of these prefixes are included.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict with keys:</p> <code>dict[str, Any]</code> <ul> <li><code>files</code>: list of <code>{\"path\", \"size_bytes\", \"num_rows\"}</code> dicts</li> </ul> <code>dict[str, Any]</code> <ul> <li><code>total_bytes</code>: sum of file sizes</li> </ul> <code>dict[str, Any]</code> <ul> <li><code>total_rows</code>: sum of row counts</li> </ul> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the path does not exist or no parquet files match the optional partition filter.</p> Note <p>This is a thin wrapper around the shared core function. See :func:<code>fsspeckit.core.maintenance.collect_dataset_stats</code> for the authoritative implementation.</p> Source code in <code>src/fsspeckit/datasets/pyarrow_dataset.py</code> <pre><code>def collect_dataset_stats_pyarrow(\n    path: str,\n    filesystem: AbstractFileSystem | None = None,\n    partition_filter: list[str] | None = None,\n) -&gt; dict[str, Any]:\n    \"\"\"Collect file-level statistics for a parquet dataset using shared core logic.\n\n    This function delegates to the shared ``fsspeckit.core.maintenance.collect_dataset_stats``\n    function, ensuring consistent dataset discovery and statistics across both DuckDB\n    and PyArrow backends.\n\n    The helper walks the given dataset directory on the provided filesystem,\n    discovers parquet files (recursively), and returns basic statistics:\n\n    - Per-file path, size in bytes, and number of rows\n    - Aggregated total bytes and total rows\n\n    The function is intentionally streaming/metadata-driven and never\n    materializes the full dataset as a single :class:`pyarrow.Table`.\n\n    Args:\n        path: Root directory of the parquet dataset.\n        filesystem: Optional fsspec filesystem. If omitted, a local \"file\"\n            filesystem is used.\n        partition_filter: Optional list of partition prefix filters\n            (e.g. [\"date=2025-11-04\"]). Only files whose path relative to\n            ``path`` starts with one of these prefixes are included.\n\n    Returns:\n        Dict with keys:\n\n        - ``files``: list of ``{\"path\", \"size_bytes\", \"num_rows\"}`` dicts\n        - ``total_bytes``: sum of file sizes\n        - ``total_rows``: sum of row counts\n\n    Raises:\n        FileNotFoundError: If the path does not exist or no parquet files\n            match the optional partition filter.\n\n    Note:\n        This is a thin wrapper around the shared core function. See\n        :func:`fsspeckit.core.maintenance.collect_dataset_stats` for the\n        authoritative implementation.\n    \"\"\"\n    from fsspeckit.core.maintenance import collect_dataset_stats\n\n    return collect_dataset_stats(\n        path=path,\n        filesystem=filesystem,\n        partition_filter=partition_filter,\n    )\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.pyarrow_dataset.compact_parquet_dataset_pyarrow","title":"fsspeckit.datasets.pyarrow_dataset.compact_parquet_dataset_pyarrow","text":"<pre><code>compact_parquet_dataset_pyarrow(\n    path: str,\n    target_mb_per_file: int | None = None,\n    target_rows_per_file: int | None = None,\n    partition_filter: list[str] | None = None,\n    compression: str | None = None,\n    dry_run: bool = False,\n    filesystem: AbstractFileSystem | None = None,\n) -&gt; dict[str, Any]\n</code></pre> <p>Compact a parquet dataset directory into fewer larger files using PyArrow and shared planning.</p> <p>Groups small files based on size (MB) and/or row thresholds, rewrites grouped files into new parquet files, and optionally changes compression. Supports a dry-run mode that returns the compaction plan without modifying files.</p> <p>The implementation uses the shared core planning algorithm for consistent behavior across backends. It processes data in a group-based, streaming fashion: it reads only the files in a given group into memory when processing that group and never materializes the entire dataset as a single table.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Dataset root directory (local path or fsspec URL).</p> required <code>target_mb_per_file</code> <code>int | None</code> <p>Optional max output size per file; must be &gt; 0.</p> <code>None</code> <code>target_rows_per_file</code> <code>int | None</code> <p>Optional max rows per output file; must be &gt; 0.</p> <code>None</code> <code>partition_filter</code> <code>list[str] | None</code> <p>Optional list of partition prefixes (e.g. <code>[\"date=2025-11-15\"]</code>) used to limit both stats collection and rewrites to matching paths.</p> <code>None</code> <code>compression</code> <code>str | None</code> <p>Optional parquet compression codec; defaults to <code>\"snappy\"</code>.</p> <code>None</code> <code>dry_run</code> <code>bool</code> <p>When <code>True</code> the function returns a plan + before/after stats without reading or writing any parquet data.</p> <code>False</code> <code>filesystem</code> <code>AbstractFileSystem | None</code> <p>Optional <code>fsspec.AbstractFileSystem</code> to reuse existing FS clients.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A stats dictionary describing before/after file counts, total bytes,</p> <code>dict[str, Any]</code> <p>rewritten bytes, and optional <code>planned_groups</code> when <code>dry_run</code> is enabled.</p> <code>dict[str, Any]</code> <p>The structure follows the canonical <code>MaintenanceStats</code> format from the shared core.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If thresholds are invalid or no files match partition filter.</p> <code>FileNotFoundError</code> <p>If the path does not exist.</p> Example <pre><code>result = compact_parquet_dataset_pyarrow(\n    \"/path/to/dataset\",\n    target_mb_per_file=64,\n    dry_run=True,\n)\nprint(f\"Files before: {result['before_file_count']}\")\nprint(f\"Files after: {result['after_file_count']}\")\n</code></pre> Note <p>This function delegates dataset discovery and compaction planning to the shared <code>fsspeckit.core.maintenance</code> module, ensuring consistent behavior across DuckDB and PyArrow backends.</p> Source code in <code>src/fsspeckit/datasets/pyarrow_dataset.py</code> <pre><code>def compact_parquet_dataset_pyarrow(\n    path: str,\n    target_mb_per_file: int | None = None,\n    target_rows_per_file: int | None = None,\n    partition_filter: list[str] | None = None,\n    compression: str | None = None,\n    dry_run: bool = False,\n    filesystem: AbstractFileSystem | None = None,\n) -&gt; dict[str, Any]:\n    \"\"\"Compact a parquet dataset directory into fewer larger files using PyArrow and shared planning.\n\n    Groups small files based on size (MB) and/or row thresholds, rewrites grouped\n    files into new parquet files, and optionally changes compression. Supports a\n    dry-run mode that returns the compaction plan without modifying files.\n\n    The implementation uses the shared core planning algorithm for consistent\n    behavior across backends. It processes data in a group-based, streaming fashion:\n    it reads only the files in a given group into memory when processing that group\n    and never materializes the entire dataset as a single table.\n\n    Args:\n        path: Dataset root directory (local path or fsspec URL).\n        target_mb_per_file: Optional max output size per file; must be &gt; 0.\n        target_rows_per_file: Optional max rows per output file; must be &gt; 0.\n        partition_filter: Optional list of partition prefixes (e.g. ``[\"date=2025-11-15\"]``)\n            used to limit both stats collection and rewrites to matching paths.\n        compression: Optional parquet compression codec; defaults to ``\"snappy\"``.\n        dry_run: When ``True`` the function returns a plan + before/after stats\n            without reading or writing any parquet data.\n        filesystem: Optional ``fsspec.AbstractFileSystem`` to reuse existing FS clients.\n\n    Returns:\n        A stats dictionary describing before/after file counts, total bytes,\n        rewritten bytes, and optional ``planned_groups`` when ``dry_run`` is enabled.\n        The structure follows the canonical ``MaintenanceStats`` format from the shared core.\n\n    Raises:\n        ValueError: If thresholds are invalid or no files match partition filter.\n        FileNotFoundError: If the path does not exist.\n\n    Example:\n        ```python\n        result = compact_parquet_dataset_pyarrow(\n            \"/path/to/dataset\",\n            target_mb_per_file=64,\n            dry_run=True,\n        )\n        print(f\"Files before: {result['before_file_count']}\")\n        print(f\"Files after: {result['after_file_count']}\")\n        ```\n\n    Note:\n        This function delegates dataset discovery and compaction planning to the\n        shared ``fsspeckit.core.maintenance`` module, ensuring consistent behavior\n        across DuckDB and PyArrow backends.\n    \"\"\"\n    from fsspeckit.core.maintenance import plan_compaction_groups, MaintenanceStats\n\n    # Get dataset stats using shared logic\n    stats = collect_dataset_stats_pyarrow(\n        path=path, filesystem=filesystem, partition_filter=partition_filter\n    )\n    files = stats[\"files\"]\n\n    # Use shared compaction planning\n    plan_result = plan_compaction_groups(\n        file_infos=files,\n        target_mb_per_file=target_mb_per_file,\n        target_rows_per_file=target_rows_per_file,\n    )\n\n    groups = plan_result[\"groups\"]\n    planned_stats = plan_result[\"planned_stats\"]\n\n    # Update planned stats with compression info\n    planned_stats.compression_codec = compression\n    planned_stats.dry_run = dry_run\n\n    # If dry run, return the plan\n    if dry_run:\n        result = planned_stats.to_dict()\n        result[\"planned_groups\"] = groups\n        return result\n\n    # Execute compaction\n    if not groups:\n        return planned_stats.to_dict()\n\n    # Execute the compaction\n    for group in groups:\n        # Read all files in this group\n        tables = []\n        for file_info in group[\"files\"]:\n            file_path = file_info[\"path\"]\n            table = pq.read_table(\n                file_path,\n                filesystem=filesystem,\n            )\n            tables.append(table)\n\n        # Concatenate tables\n        if len(tables) &gt; 1:\n            combined = pa.concat_tables(tables, promote_options=\"permissive\")\n        else:\n            combined = tables[0]\n\n        # Write to output file\n        output_path = group[\"output_path\"]\n        pq.write_table(\n            combined,\n            output_path,\n            filesystem=filesystem,\n            compression=compression or \"snappy\",\n        )\n\n    # Remove original files\n    for group in groups:\n        for file_info in group[\"files\"]:\n            file_path = file_info[\"path\"]\n            filesystem.rm(file_path)\n\n    return planned_stats.to_dict()\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.pyarrow_dataset.merge_parquet_dataset_pyarrow","title":"fsspeckit.datasets.pyarrow_dataset.merge_parquet_dataset_pyarrow","text":"<pre><code>merge_parquet_dataset_pyarrow(\n    sources: list[str],\n    output_path: str,\n    target: str | None = None,\n    strategy: str | MergeStrategy = \"deduplicate\",\n    key_columns: list[str] | str | None = None,\n    filesystem: AbstractFileSystem | None = None,\n    compression: str | None = None,\n    row_group_size: int | None = 500000,\n    max_rows_per_file: int | None = 5000000,\n    verbose: bool = False,\n    **kwargs: Any,\n) -&gt; MergeStats\n</code></pre> <p>Merge multiple parquet datasets using PyArrow with various strategies.</p> <p>This function provides dataset merging capabilities with support for: - Multiple merge strategies (upsert, insert, update, full_merge, deduplicate) - Key-based merging for relational operations - Batch processing for large datasets - Configurable output settings</p> <p>Parameters:</p> Name Type Description Default <code>sources</code> <code>list[str]</code> <p>List of source dataset paths</p> required <code>output_path</code> <code>str</code> <p>Path for merged output</p> required <code>target</code> <code>str | None</code> <p>Target dataset path (for upsert/update strategies)</p> <code>None</code> <code>strategy</code> <code>str | MergeStrategy</code> <p>Merge strategy to use</p> <code>'deduplicate'</code> <code>key_columns</code> <code>list[str] | str | None</code> <p>Key columns for merging (required for relational strategies)</p> <code>None</code> <code>filesystem</code> <code>AbstractFileSystem | None</code> <p>fsspec filesystem instance</p> <code>None</code> <code>compression</code> <code>str | None</code> <p>Output compression codec</p> <code>None</code> <code>row_group_size</code> <code>int | None</code> <p>Rows per parquet row group</p> <code>500000</code> <code>max_rows_per_file</code> <code>int | None</code> <p>Max rows per output file</p> <code>5000000</code> <code>verbose</code> <code>bool</code> <p>Print progress information</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>MergeStats</code> <p>MergeStats with merge statistics</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required parameters are missing</p> <code>FileNotFoundError</code> <p>If sources don't exist</p> Example <pre><code>stats = merge_parquet_dataset_pyarrow(\n    sources=[\"dataset1/\", \"dataset2/\"],\n    output_path=\"merged/\",\n    strategy=\"deduplicate\",\n    key_columns=[\"id\"],\n    verbose=True,\n)\nprint(f\"Merged {stats.total_rows} rows\")\n</code></pre> Source code in <code>src/fsspeckit/datasets/pyarrow_dataset.py</code> <pre><code>def merge_parquet_dataset_pyarrow(\n    sources: list[str],\n    output_path: str,\n    target: str | None = None,\n    strategy: str | CoreMergeStrategy = \"deduplicate\",\n    key_columns: list[str] | str | None = None,\n    filesystem: AbstractFileSystem | None = None,\n    compression: str | None = None,\n    row_group_size: int | None = 500_000,\n    max_rows_per_file: int | None = 5_000_000,\n    verbose: bool = False,\n    **kwargs: Any,\n) -&gt; MergeStats:\n    \"\"\"Merge multiple parquet datasets using PyArrow with various strategies.\n\n    This function provides dataset merging capabilities with support for:\n    - Multiple merge strategies (upsert, insert, update, full_merge, deduplicate)\n    - Key-based merging for relational operations\n    - Batch processing for large datasets\n    - Configurable output settings\n\n    Args:\n        sources: List of source dataset paths\n        output_path: Path for merged output\n        target: Target dataset path (for upsert/update strategies)\n        strategy: Merge strategy to use\n        key_columns: Key columns for merging (required for relational strategies)\n        filesystem: fsspec filesystem instance\n        compression: Output compression codec\n        row_group_size: Rows per parquet row group\n        max_rows_per_file: Max rows per output file\n        verbose: Print progress information\n        **kwargs: Additional arguments\n\n    Returns:\n        MergeStats with merge statistics\n\n    Raises:\n        ValueError: If required parameters are missing\n        FileNotFoundError: If sources don't exist\n\n    Example:\n        ```python\n        stats = merge_parquet_dataset_pyarrow(\n            sources=[\"dataset1/\", \"dataset2/\"],\n            output_path=\"merged/\",\n            strategy=\"deduplicate\",\n            key_columns=[\"id\"],\n            verbose=True,\n        )\n        print(f\"Merged {stats.total_rows} rows\")\n        ```\n    \"\"\"\n    # Validate inputs using shared core logic\n    validate_merge_inputs(\n        sources=sources,\n        strategy=strategy,\n        key_columns=key_columns,\n        target=target,\n    )\n\n    validate_strategy_compatibility(strategy, key_columns, target)\n\n    # Normalize parameters\n    if key_columns is not None:\n        key_columns = _normalize_key_columns(key_columns)\n\n    # Get filesystem\n    if filesystem is None:\n        filesystem = fsspec_filesystem(\"file\")\n\n    pa_filesystem = _ensure_pyarrow_filesystem(filesystem)\n\n    # Load target if provided\n    target_table = None\n    if target and strategy in [\"upsert\", \"update\"]:\n        target_table = _load_source_table_pyarrow(target, filesystem)\n\n        if key_columns:\n            _ensure_no_null_keys_table(target_table, key_columns)\n\n    # Process sources\n    merged_data = []\n    total_rows = 0\n\n    for source_path in sources:\n        if verbose:\n            logger.info(\"Processing source: %s\", source_path)\n\n        source_table = _load_source_table_pyarrow(source_path, filesystem)\n\n        if key_columns:\n            _ensure_no_null_keys_table(source_table, key_columns)\n\n        if strategy == \"full_merge\":\n            # Simply concatenate all data\n            merged_data.append(source_table)\n            total_rows += source_table.num_rows\n\n        elif strategy == \"deduplicate\":\n            # Remove duplicates based on key columns\n            if key_columns:\n                # Group by keys and keep first occurrence\n                table = source_table\n\n                # Use PyArrow's group_by for deduplication\n                # This is a simplified implementation\n                groups = table.group_by(key_columns).aggregate([])\n                keys = groups.select(key_columns)\n\n                # Get unique keys\n                unique_keys = []\n                for row in keys.to_pylist():\n                    unique_keys.append(tuple(row[col] for col in key_columns))\n\n                # Filter to keep only unique rows\n                filtered = []\n                for row in table.to_pylist():\n                    key = tuple(row[col] for col in key_columns)\n                    if key in unique_keys:\n                        filtered.append(row)\n                        unique_keys.remove(key)  # Remove to avoid duplicates\n\n                if filtered:\n                    deduped = pa.Table.from_pylist(filtered, schema=table.schema)\n                    merged_data.append(deduped)\n                    total_rows += deduped.num_rows\n            else:\n                # No key columns, remove exact duplicates\n                merged_data.append(source_table)\n                total_rows += source_table.num_rows\n\n        elif strategy in [\"upsert\", \"insert\", \"update\"] and target_table is not None:\n            # Key-based relational operations\n            if strategy == \"insert\":\n                # Only insert non-existing rows\n                target_keys = _extract_key_tuples(target_table, key_columns)\n                source_keys = _extract_key_tuples(source_table, key_columns)\n\n                # Find keys that don't exist in target\n                new_keys = set(source_keys) - set(target_keys)\n\n                # Filter source for new keys\n                new_rows = []\n                for row in source_table.to_pylist():\n                    key = tuple(row[col] for col in key_columns)\n                    if key in new_keys:\n                        new_rows.append(row)\n\n                if new_rows:\n                    new_table = pa.Table.from_pylist(new_rows, schema=source_table.schema)\n                    merged_data.append(new_table)\n                    total_rows += new_table.num_rows\n\n            elif strategy == \"update\":\n                # Update existing rows\n                target_keys = _extract_key_tuples(target_table, key_columns)\n                source_keys = _extract_key_tuples(source_table, key_columns)\n\n                # Find common keys\n                common_keys = set(source_keys) &amp; set(target_keys)\n\n                # Build updated target\n                updated_data = []\n\n                # Keep non-matching rows from target\n                for row in target_table.to_pylist():\n                    key = tuple(row[col] for col in key_columns)\n                    if key not in common_keys:\n                        updated_data.append(row)\n\n                # Add updated rows from source\n                for row in source_table.to_pylist():\n                    key = tuple(row[col] for col in key_columns)\n                    if key in common_keys:\n                        updated_data.append(row)\n\n                if updated_data:\n                    updated_table = pa.Table.from_pylist(updated_data, schema=target_table.schema)\n                    merged_data.append(updated_table)\n                    total_rows += updated_table.num_rows\n\n            elif strategy == \"upsert\":\n                # Insert or update\n                all_data = [target_table] if target_table else []\n                all_data.append(source_table)\n\n                if all_data:\n                    combined = pa.concat_tables(all_data, promote_options=\"permissive\")\n\n                    # Deduplicate based on keys\n                    if key_columns:\n                        # Group by keys and keep last occurrence\n                        # This is a simplified implementation\n                        groups = combined.group_by(key_columns).aggregate([])\n                        keys = groups.select(key_columns)\n\n                        unique_keys = []\n                        for row in keys.to_pylist():\n                            unique_keys.append(tuple(row[col] for col in key_columns))\n\n                        # Keep only last occurrence of each key\n                        filtered = []\n                        seen = set()\n                        for row in reversed(combined.to_pylist()):\n                            key = tuple(row[col] for col in key_columns)\n                            if key not in seen:\n                                filtered.append(row)\n                                seen.add(key)\n\n                        if filtered:\n                            deduped = pa.Table.from_pylist(\n                                list(reversed(filtered)), schema=combined.schema\n                            )\n                            merged_data.append(deduped)\n                            total_rows += deduped.num_rows\n                    else:\n                        merged_data.append(combined)\n                        total_rows += combined.num_rows\n\n    # Combine all data\n    if merged_data:\n        if len(merged_data) == 1:\n            final_table = merged_data[0]\n        else:\n            final_table = pa.concat_tables(\n                merged_data, promote_options=\"permissive\"\n            )\n    else:\n        # No data to merge\n        final_table = pa.table({})\n\n    # Write output\n    pq.write_table(\n        final_table,\n        output_path,\n        filesystem=pa_filesystem,\n        compression=compression,\n        row_group_size=row_group_size,\n        max_rows_per_file=max_rows_per_file,\n    )\n\n    # Calculate stats\n    stats = calculate_merge_stats(\n        sources=sources,\n        target=output_path,\n        strategy=strategy,\n        total_rows=total_rows,\n        output_rows=final_table.num_rows,\n    )\n\n    if verbose:\n        logger.info(\"\\nMerge complete: %s\", stats)\n\n    return stats\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.pyarrow_dataset.optimize_parquet_dataset_pyarrow","title":"fsspeckit.datasets.pyarrow_dataset.optimize_parquet_dataset_pyarrow","text":"<pre><code>optimize_parquet_dataset_pyarrow(\n    path: str,\n    target_mb_per_file: int | None = None,\n    target_rows_per_file: int | None = None,\n    partition_filter: list[str] | None = None,\n    compression: str | None = None,\n    filesystem: AbstractFileSystem | None = None,\n    verbose: bool = False,\n) -&gt; dict[str, Any]\n</code></pre> <p>Optimize a parquet dataset through compaction and optional statistics recalculation.</p> <p>This is a convenience function that combines compaction with optional statistics recalculation. It's particularly useful after many small write operations have created a large number of small files.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Dataset root directory</p> required <code>target_mb_per_file</code> <code>int | None</code> <p>Target size per file in MB</p> <code>None</code> <code>target_rows_per_file</code> <code>int | None</code> <p>Target rows per file</p> <code>None</code> <code>partition_filter</code> <code>list[str] | None</code> <p>Optional partition filters</p> <code>None</code> <code>compression</code> <code>str | None</code> <p>Compression codec to use</p> <code>None</code> <code>filesystem</code> <code>AbstractFileSystem | None</code> <p>Optional filesystem instance</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Print progress information</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Optimization statistics</p> Example <pre><code>stats = optimize_parquet_dataset_pyarrow(\n    \"dataset/\",\n    target_mb_per_file=64,\n    compression=\"zstd\",\n)\nprint(\n    f\"Reduced from {stats['before_file_count']} \"\n    f\"to {stats['after_file_count']} files\",\n)\n</code></pre> Source code in <code>src/fsspeckit/datasets/pyarrow_dataset.py</code> <pre><code>def optimize_parquet_dataset_pyarrow(\n    path: str,\n    target_mb_per_file: int | None = None,\n    target_rows_per_file: int | None = None,\n    partition_filter: list[str] | None = None,\n    compression: str | None = None,\n    filesystem: AbstractFileSystem | None = None,\n    verbose: bool = False,\n) -&gt; dict[str, Any]:\n    \"\"\"Optimize a parquet dataset through compaction and optional statistics recalculation.\n\n    This is a convenience function that combines compaction with optional statistics\n    recalculation. It's particularly useful after many small write operations have\n    created a large number of small files.\n\n    Args:\n        path: Dataset root directory\n        target_mb_per_file: Target size per file in MB\n        target_rows_per_file: Target rows per file\n        partition_filter: Optional partition filters\n        compression: Compression codec to use\n        filesystem: Optional filesystem instance\n        verbose: Print progress information\n\n    Returns:\n        Optimization statistics\n\n    Example:\n        ```python\n        stats = optimize_parquet_dataset_pyarrow(\n            \"dataset/\",\n            target_mb_per_file=64,\n            compression=\"zstd\",\n        )\n        print(\n            f\"Reduced from {stats['before_file_count']} \"\n            f\"to {stats['after_file_count']} files\",\n        )\n        ```\n    \"\"\"\n    # Use compaction\n    result = compact_parquet_dataset_pyarrow(\n        path=path,\n        target_mb_per_file=target_mb_per_file,\n        target_rows_per_file=target_rows_per_file,\n        partition_filter=partition_filter,\n        compression=compression,\n        dry_run=False,\n        filesystem=filesystem,\n    )\n\n    if verbose:\n        logger.info(\"Optimization complete: %s\", result)\n\n    return result\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.pyarrow_schema","title":"fsspeckit.datasets.pyarrow_schema","text":"<p>PyArrow schema utilities for type inference, unification, and optimization.</p> <p>This module contains functions for working with PyArrow schemas including: - Schema unification across multiple tables - Type inference and optimization - Timezone handling - Schema casting</p>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.pyarrow_schema-classes","title":"Classes","text":""},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.pyarrow_schema-functions","title":"Functions","text":""},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.pyarrow_schema.cast_schema","title":"fsspeckit.datasets.pyarrow_schema.cast_schema","text":"<pre><code>cast_schema(table: Table, schema: Schema) -&gt; Table\n</code></pre> <p>Cast a PyArrow table to a target schema.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>Table</code> <p>Source table</p> required <code>schema</code> <code>Schema</code> <p>Target schema</p> required <p>Returns:</p> Type Description <code>Table</code> <p>Table cast to target schema</p> Source code in <code>src/fsspeckit/datasets/pyarrow_schema.py</code> <pre><code>def cast_schema(table: pa.Table, schema: pa.Schema) -&gt; pa.Table:\n    \"\"\"Cast a PyArrow table to a target schema.\n\n    Args:\n        table: Source table\n        schema: Target schema\n\n    Returns:\n        Table cast to target schema\n    \"\"\"\n    # Filter schema to only include columns present in the table\n    table_schema = table.schema\n    valid_fields = []\n\n    for field in schema:\n        if field.name in table_schema.names:\n            valid_fields.append(field)\n\n    target_schema = pa.schema(valid_fields)\n\n    # Cast the table\n    return table.cast(target_schema)\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.pyarrow_schema.convert_large_types_to_normal","title":"fsspeckit.datasets.pyarrow_schema.convert_large_types_to_normal","text":"<pre><code>convert_large_types_to_normal(schema: Schema) -&gt; Schema\n</code></pre> <p>Convert large types (like large_string) to normal types.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>Schema</code> <p>PyArrow schema</p> required <p>Returns:</p> Type Description <code>Schema</code> <p>Schema with large types converted</p> Source code in <code>src/fsspeckit/datasets/pyarrow_schema.py</code> <pre><code>def convert_large_types_to_normal(schema: pa.Schema) -&gt; pa.Schema:\n    \"\"\"Convert large types (like large_string) to normal types.\n\n    Args:\n        schema: PyArrow schema\n\n    Returns:\n        Schema with large types converted\n    \"\"\"\n    fields = []\n    for field in schema:\n        if pa.types.is_large_string(field.type):\n            field = field.with_type(pa.string())\n        elif pa.types.is_large_utf8(field.type):\n            field = field.with_type(pa.utf8())\n        elif pa.types.is_large_list(field.type):\n            field = field.with_type(pa.list_(field.type.value_type))\n        fields.append(field)\n\n    return pa.schema(fields)\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.pyarrow_schema.dominant_timezone_per_column","title":"fsspeckit.datasets.pyarrow_schema.dominant_timezone_per_column","text":"<pre><code>dominant_timezone_per_column(\n    schemas: list[Schema],\n) -&gt; dict[str, str]\n</code></pre> <p>Determine the dominant timezone for each column across schemas.</p> <p>Parameters:</p> Name Type Description Default <code>schemas</code> <code>list[Schema]</code> <p>List of PyArrow schemas to analyze</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, str]</code> <p>Mapping of column names to dominant timezone strings</p> Source code in <code>src/fsspeckit/datasets/pyarrow_schema.py</code> <pre><code>def dominant_timezone_per_column(schemas: list[pa.Schema]) -&gt; dict[str, str]:\n    \"\"\"Determine the dominant timezone for each column across schemas.\n\n    Args:\n        schemas: List of PyArrow schemas to analyze\n\n    Returns:\n        dict: Mapping of column names to dominant timezone strings\n    \"\"\"\n    from collections import Counter\n\n    timezone_counts: dict[str, Counter] = {}\n\n    for schema in schemas:\n        for field in schema:\n            if pa.types.is_timestamp(field.type):\n                tz = field.type.tz\n                if tz:\n                    if field.name not in timezone_counts:\n                        timezone_counts[field.name] = Counter()\n                    timezone_counts[field.name][tz] += 1\n\n    # Select dominant timezone for each column\n    result = {}\n    for col_name, counter in timezone_counts.items():\n        result[col_name] = counter.most_common(1)[0][0]\n\n    return result\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.pyarrow_schema.opt_dtype","title":"fsspeckit.datasets.pyarrow_schema.opt_dtype","text":"<pre><code>opt_dtype(\n    table: Table,\n    strict: bool = False,\n    columns: list[str] | None = None,\n) -&gt; Table\n</code></pre> <p>Optimize dtypes in a PyArrow table based on data analysis.</p> <p>This function analyzes the data in each column and attempts to downcast to more appropriate types (e.g., int64 -&gt; int32, float64 -&gt; float32, string -&gt; int/bool where applicable).</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>Table</code> <p>PyArrow table to optimize</p> required <code>strict</code> <code>bool</code> <p>Whether to use strict type checking</p> <code>False</code> <code>columns</code> <code>list[str] | None</code> <p>List of columns to optimize (None for all)</p> <code>None</code> <p>Returns:</p> Type Description <code>Table</code> <p>Table with optimized dtypes</p> Example <pre><code>import pyarrow as pa\n\ntable = pa.table(\n    {\n        \"a\": pa.array([1, 2, 3], type=pa.int64()),\n        \"b\": pa.array([1.0, 2.0, 3.0], type=pa.float64()),\n    },\n)\noptimized = opt_dtype(table)\nprint(optimized.column(0).type)  # DataType(int32)\nprint(optimized.column(1).type)  # DataType(float32)\n</code></pre> Source code in <code>src/fsspeckit/datasets/pyarrow_schema.py</code> <pre><code>def opt_dtype(\n    table: pa.Table,\n    strict: bool = False,\n    columns: list[str] | None = None,\n) -&gt; pa.Table:\n    \"\"\"Optimize dtypes in a PyArrow table based on data analysis.\n\n    This function analyzes the data in each column and attempts to downcast\n    to more appropriate types (e.g., int64 -&gt; int32, float64 -&gt; float32,\n    string -&gt; int/bool where applicable).\n\n    Args:\n        table: PyArrow table to optimize\n        strict: Whether to use strict type checking\n        columns: List of columns to optimize (None for all)\n\n    Returns:\n        Table with optimized dtypes\n\n    Example:\n        ```python\n        import pyarrow as pa\n\n        table = pa.table(\n            {\n                \"a\": pa.array([1, 2, 3], type=pa.int64()),\n                \"b\": pa.array([1.0, 2.0, 3.0], type=pa.float64()),\n            },\n        )\n        optimized = opt_dtype(table)\n        print(optimized.column(0).type)  # DataType(int32)\n        print(optimized.column(1).type)  # DataType(float32)\n        ```\n    \"\"\"\n    from fsspeckit.common.misc import run_parallel\n\n    if columns is None:\n        columns = table.column_names\n\n    # Process columns in parallel\n    results = run_parallel(\n        _process_column_for_opt_dtype,\n        [(table, col, strict) for col in columns],\n        backend=\"threading\",\n        n_jobs=-1,\n    )\n\n    # Build new table with optimized columns\n    new_columns = {}\n    for col_name, optimized_array in results:\n        new_columns[col_name] = optimized_array\n\n    # Keep non-optimized columns as-is\n    for col_name in table.column_names:\n        if col_name not in new_columns:\n            new_columns[col_name] = table.column(col_name)\n\n    return pa.table(new_columns)\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.pyarrow_schema.remove_empty_columns","title":"fsspeckit.datasets.pyarrow_schema.remove_empty_columns","text":"<pre><code>remove_empty_columns(table: Table) -&gt; Table\n</code></pre> <p>Remove empty columns from a PyArrow table.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>Table</code> <p>PyArrow table</p> required <p>Returns:</p> Type Description <code>Table</code> <p>Table with empty columns removed</p> Source code in <code>src/fsspeckit/datasets/pyarrow_schema.py</code> <pre><code>def remove_empty_columns(table: pa.Table) -&gt; pa.Table:\n    \"\"\"Remove empty columns from a PyArrow table.\n\n    Args:\n        table: PyArrow table\n\n    Returns:\n        Table with empty columns removed\n    \"\"\"\n    empty_cols = _identify_empty_columns(table)\n    if not empty_cols:\n        return table\n\n    return table.drop(empty_cols)\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.pyarrow_schema.standardize_schema_timezones","title":"fsspeckit.datasets.pyarrow_schema.standardize_schema_timezones","text":"<pre><code>standardize_schema_timezones(\n    schemas: list[Schema],\n    standardize_timezones: bool = True,\n) -&gt; list[Schema]\n</code></pre> <p>Standardize timezone information across schemas.</p> <p>Parameters:</p> Name Type Description Default <code>schemas</code> <code>list[Schema]</code> <p>List of schemas to standardize</p> required <code>standardize_timezones</code> <code>bool</code> <p>Whether to standardize timezones</p> <code>True</code> <p>Returns:</p> Type Description <code>list[Schema]</code> <p>List of schemas with standardized timezones</p> Source code in <code>src/fsspeckit/datasets/pyarrow_schema.py</code> <pre><code>def standardize_schema_timezones(\n    schemas: list[pa.Schema],\n    standardize_timezones: bool = True,\n) -&gt; list[pa.Schema]:\n    \"\"\"Standardize timezone information across schemas.\n\n    Args:\n        schemas: List of schemas to standardize\n        standardize_timezones: Whether to standardize timezones\n\n    Returns:\n        List of schemas with standardized timezones\n    \"\"\"\n    if not standardize_timezones:\n        return schemas\n\n    return standardize_schema_timezones_by_majority(schemas)\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.pyarrow_schema.standardize_schema_timezones_by_majority","title":"fsspeckit.datasets.pyarrow_schema.standardize_schema_timezones_by_majority","text":"<pre><code>standardize_schema_timezones_by_majority(\n    schemas: list[Schema],\n) -&gt; list[Schema]\n</code></pre> <p>Standardize timezone information across schemas based on majority.</p> <p>Parameters:</p> Name Type Description Default <code>schemas</code> <code>list[Schema]</code> <p>List of schemas to standardize</p> required <p>Returns:</p> Type Description <code>list[Schema]</code> <p>List of schemas with standardized timezones</p> Source code in <code>src/fsspeckit/datasets/pyarrow_schema.py</code> <pre><code>def standardize_schema_timezones_by_majority(\n    schemas: list[pa.Schema],\n) -&gt; list[pa.Schema]:\n    \"\"\"Standardize timezone information across schemas based on majority.\n\n    Args:\n        schemas: List of schemas to standardize\n\n    Returns:\n        List of schemas with standardized timezones\n    \"\"\"\n    # Get dominant timezones\n    dominant_tz = dominant_timezone_per_column(schemas)\n\n    # Apply dominant timezone to all schemas\n    standardized = []\n    for schema in schemas:\n        fields = []\n        for field in schema:\n            if pa.types.is_timestamp(field.type) and field.name in dominant_tz:\n                # Update timezone to dominant one\n                tz = dominant_tz[field.name]\n                if field.type.tz != tz:\n                    field = field.with_type(\n                        pa.timestamp(\"us\", tz=tz) if tz else pa.timestamp(\"us\")\n                    )\n            fields.append(field)\n        standardized.append(pa.schema(fields))\n\n    return standardized\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.pyarrow_schema.unify_schemas","title":"fsspeckit.datasets.pyarrow_schema.unify_schemas","text":"<pre><code>unify_schemas(\n    schemas: list[Schema],\n    standardize_timezones: bool = True,\n    verbose: bool = False,\n) -&gt; Schema\n</code></pre> <p>Unify multiple PyArrow schemas into a single schema.</p> <p>This function handles type conflicts by: 1. Finding fields with conflicting types 2. Attempting to normalize compatible types 3. Using fallback strategies for incompatible types 4. Removing problematic fields if necessary</p> <p>Parameters:</p> Name Type Description Default <code>schemas</code> <code>list[Schema]</code> <p>List of schemas to unify</p> required <code>standardize_timezones</code> <code>bool</code> <p>Whether to standardize timezone info</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>Whether to print conflict information</p> <code>False</code> <p>Returns:</p> Type Description <code>Schema</code> <p>Unified PyArrow schema</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If schemas cannot be unified</p> Source code in <code>src/fsspeckit/datasets/pyarrow_schema.py</code> <pre><code>def unify_schemas(\n    schemas: list[pa.Schema],\n    standardize_timezones: bool = True,\n    verbose: bool = False,\n) -&gt; pa.Schema:\n    \"\"\"Unify multiple PyArrow schemas into a single schema.\n\n    This function handles type conflicts by:\n    1. Finding fields with conflicting types\n    2. Attempting to normalize compatible types\n    3. Using fallback strategies for incompatible types\n    4. Removing problematic fields if necessary\n\n    Args:\n        schemas: List of schemas to unify\n        standardize_timezones: Whether to standardize timezone info\n        verbose: Whether to print conflict information\n\n    Returns:\n        Unified PyArrow schema\n\n    Raises:\n        ValueError: If schemas cannot be unified\n    \"\"\"\n    if not schemas:\n        raise ValueError(\"Cannot unify empty list of schemas\")\n\n    if len(schemas) == 1:\n        return schemas[0]\n\n    # Remove duplicate schemas\n    schemas = _unique_schemas(schemas)\n\n    # Standardize timezones if requested\n    if standardize_timezones:\n        schemas = standardize_schema_timezones(schemas, standardize_timezones)\n\n    # Find conflicts\n    conflicts = _find_conflicting_fields(schemas)\n\n    if not conflicts:\n        # No conflicts, concatenate all fields\n        all_fields = []\n        for schema in schemas:\n            all_fields.extend(schema)\n        return pa.schema(all_fields)\n\n    if verbose:\n        _log_conflict_summary(conflicts, verbose)\n\n    # Try to normalize types\n    try:\n        normalized = _normalize_schema_types(schemas, conflicts)\n\n        # Check if normalization resolved conflicts\n        remaining_conflicts = _find_conflicting_fields(normalized)\n\n        if not remaining_conflicts:\n            # Normalization successful\n            all_fields = []\n            for schema in normalized:\n                all_fields.extend(schema)\n            return pa.schema(all_fields)\n\n        # Fall through to next strategy\n        conflicts = remaining_conflicts\n\n    except (pa.ArrowInvalid, pa.ArrowTypeError, pa.ArrowNotImplementedError) as e:\n        # Normalization failed, log and continue to fallback\n        logger.debug(\n            \"Schema type normalization failed: %s. Trying aggressive fallback.\",\n            str(e)\n        )\n\n    # Try aggressive fallback\n    try:\n        return _aggressive_fallback_unification(schemas)\n    except (pa.ArrowInvalid, pa.ArrowTypeError, pa.ArrowNotImplementedError) as e:\n        # Aggressive fallback failed, log and try last resort\n        logger.debug(\n            \"Aggressive fallback unification failed: %s. Trying last resort cleanup.\",\n            str(e)\n        )\n\n    # Last resort: remove problematic fields\n    cleaned = _remove_problematic_fields(schemas)\n    all_fields = []\n    for schema in cleaned:\n        all_fields.extend(schema)\n\n    if verbose and conflicts:\n        logger.debug(\"Removed %d conflicting fields during unification\", len(conflicts))\n\n    return pa.schema(all_fields)\n</code></pre>"},{"location":"api/fsspeckit.sql.filters/","title":"<code>fsspeckit.sql.filters</code> API Reference","text":""},{"location":"api/fsspeckit.sql.filters/#fsspeckit.sql.filters","title":"filters","text":""},{"location":"api/fsspeckit.sql.filters/#fsspeckit.sql.filters-functions","title":"Functions","text":""},{"location":"api/fsspeckit.sql.filters/#fsspeckit.sql.filters.sql2polars_filter","title":"fsspeckit.sql.filters.sql2polars_filter","text":"<pre><code>sql2polars_filter(string: str, schema: Schema) -&gt; Expr\n</code></pre> <p>Generates a filter expression for Polars based on a given string and schema.</p> <p>Parameters:</p> Name Type Description Default <code>string</code> <code>str</code> <p>The string containing the filter expression.</p> required <code>schema</code> <code>Schema</code> <p>The Polars schema used to validate the filter expression.</p> required <p>Returns:</p> Type Description <code>Expr</code> <p>pl.Expr: The generated filter expression.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input string is invalid or contains unsupported operations.</p> Source code in <code>src/fsspeckit/sql/filters/__init__.py</code> <pre><code>def sql2polars_filter(string: str, schema: pl.Schema) -&gt; pl.Expr:\n    \"\"\"\n    Generates a filter expression for Polars based on a given string and schema.\n\n    Parameters:\n        string (str): The string containing the filter expression.\n        schema (pl.Schema): The Polars schema used to validate the filter expression.\n\n    Returns:\n        pl.Expr: The generated filter expression.\n\n    Raises:\n        ValueError: If the input string is invalid or contains unsupported operations.\n    \"\"\"\n    from fsspeckit.common.optional import _import_polars, _import_sqlglot\n\n    pl = _import_polars()\n    sqlglot = _import_sqlglot()\n    from sqlglot import exp, parse_one\n\n    def parse_value(val: str, dtype: pl.DataType) -&gt; Any:\n        \"\"\"Parse and convert value based on the field type.\"\"\"\n        if isinstance(val, (tuple, list)):\n            return type(val)(parse_value(v, dtype) for v in val)\n\n        # Remove quotes from the value if present\n        val = val.strip().strip(\"'\\\"\")\n\n        if dtype == pl.Datetime:\n            return timestamp_from_string(val, tz=dtype.time_zone)\n        elif dtype == pl.Date:\n            return timestamp_from_string(val).date()\n        elif dtype == pl.Time:\n            return timestamp_from_string(val).time()\n        elif dtype in (pl.Int8, pl.Int16, pl.Int32, pl.Int64):\n            return int(float(val.replace(\",\", \".\")))\n        elif dtype in (pl.Float32, pl.Float64):\n            return float(val.replace(\",\", \".\"))\n        elif dtype == pl.Boolean:\n            return val.lower() in (\"true\", \"1\", \"yes\")\n        else:\n            return val\n\n    def _get_field_type_from_context(expr):\n        \"\"\"Try to determine the field type from a comparison expression.\"\"\"\n        if isinstance(expr.this, exp.Column):\n            field_name = expr.this.name\n            if field_name in schema.names():\n                return schema[field_name]\n        elif isinstance(expr.expression, exp.Column):\n            field_name = expr.expression.name\n            if field_name in schema.names():\n                return schema[field_name]\n        return None\n\n    def _convert_expression(expr, field_type=None) -&gt; pl.Expr:\n        \"\"\"Convert a sqlglot expression to a Polars expression.\"\"\"\n        if isinstance(expr, exp.Column):\n            field_name = expr.name\n            if field_name not in schema.names():\n                raise ValueError(f\"Unknown field: {field_name}\")\n            return pl.col(field_name)\n\n        elif isinstance(expr, exp.Literal):\n            # Convert literal value based on field type if available\n            if field_type:\n                val = str(expr.this)\n                # Remove quotes if present\n                val = val.strip().strip(\"'\\\"\")\n                return parse_value(val, field_type)\n            return expr.this\n\n        elif isinstance(expr, exp.Null):\n            return None\n\n        elif isinstance(expr, (exp.EQ, exp.NEQ, exp.GT, exp.GTE, exp.LT, exp.LTE)):\n            # Binary comparison operations\n            # Try to determine field type from context\n            context_type = _get_field_type_from_context(expr)\n\n            left = _convert_expression(expr.this, context_type)\n            right = _convert_expression(expr.expression, context_type)\n\n            if isinstance(expr, exp.EQ):\n                return left == right\n            elif isinstance(expr, exp.NEQ):\n                return left != right\n            elif isinstance(expr, exp.GT):\n                return left &gt; right\n            elif isinstance(expr, exp.GTE):\n                return left &gt;= right\n            elif isinstance(expr, exp.LT):\n                return left &lt; right\n            elif isinstance(expr, exp.LTE):\n                return left &lt;= right\n\n        elif isinstance(expr, exp.In):\n            # IN operation\n            context_type = _get_field_type_from_context(expr)\n            left = _convert_expression(expr.this, context_type)\n            # Convert the IN list\n            if hasattr(expr, \"expression\") and hasattr(expr.expression, \"expressions\"):\n                right = [\n                    _convert_expression(e, context_type)\n                    for e in expr.expression.expressions\n                ]\n            else:\n                right = _convert_expression(expr.expression, context_type)\n            return left.is_in(right)\n\n        elif isinstance(expr, exp.Not):\n            # NOT operation - check if it's NOT IN or IS NOT NULL\n            inner = expr.this\n            if isinstance(inner, exp.In):\n                # NOT IN case\n                context_type = _get_field_type_from_context(inner)\n                left = _convert_expression(inner.this, context_type)\n                if hasattr(inner, \"expression\") and hasattr(\n                    inner.expression, \"expressions\"\n                ):\n                    right = [\n                        _convert_expression(e, context_type)\n                        for e in inner.expression.expressions\n                    ]\n                else:\n                    right = _convert_expression(inner.expression, context_type)\n                return ~left.is_in(right)\n            elif isinstance(inner, exp.Is):\n                # IS NOT NULL case\n                left = _convert_expression(inner.this)\n                return left.is_not_null()\n            else:\n                # Generic NOT\n                return ~_convert_expression(inner)\n\n        elif isinstance(expr, exp.Is):\n            left = _convert_expression(expr.this)\n            return left.is_null()\n\n        elif isinstance(expr, exp.And):\n            return _convert_expression(expr.this) &amp; _convert_expression(expr.expression)\n\n        elif isinstance(expr, exp.Or):\n            return _convert_expression(expr.this) | _convert_expression(expr.expression)\n\n        elif isinstance(expr, exp.Not):\n            return ~_convert_expression(expr.this)\n\n        elif isinstance(expr, exp.Paren):\n            return _convert_expression(expr.this)\n\n        else:\n            raise ValueError(f\"Unsupported expression type: {type(expr)}\")\n\n    try:\n        # Parse the SQL expression using sqlglot\n        parsed = parse_one(string)\n\n        # Convert to Polars expression\n        return _convert_expression(parsed)\n\n    except Exception as e:\n        raise ValueError(f\"Failed to parse SQL expression: {e}\")\n</code></pre>"},{"location":"api/fsspeckit.sql.filters/#fsspeckit.sql.filters.sql2pyarrow_filter","title":"fsspeckit.sql.filters.sql2pyarrow_filter","text":"<pre><code>sql2pyarrow_filter(\n    string: str, schema: Schema\n) -&gt; Expression\n</code></pre> <p>Generates a filter expression for PyArrow based on a given string and schema.</p> <p>Parameters:</p> Name Type Description Default <code>string</code> <code>str</code> <p>The string containing the filter expression.</p> required <code>schema</code> <code>Schema</code> <p>The PyArrow schema used to validate the filter expression.</p> required <p>Returns:</p> Type Description <code>Expression</code> <p>pc.Expression: The generated filter expression.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input string is invalid or contains unsupported operations.</p> Source code in <code>src/fsspeckit/sql/filters/__init__.py</code> <pre><code>def sql2pyarrow_filter(string: str, schema: pa.Schema) -&gt; pc.Expression:\n    \"\"\"\n    Generates a filter expression for PyArrow based on a given string and schema.\n\n    Parameters:\n        string (str): The string containing the filter expression.\n        schema (pa.Schema): The PyArrow schema used to validate the filter expression.\n\n    Returns:\n        pc.Expression: The generated filter expression.\n\n    Raises:\n        ValueError: If the input string is invalid or contains unsupported operations.\n    \"\"\"\n    from fsspeckit.common.optional import _import_pyarrow, _import_sqlglot\n\n    pa = _import_pyarrow()\n    import pyarrow.compute as pc\n\n    sqlglot = _import_sqlglot()\n    from sqlglot import exp, parse_one\n\n    def parse_value(val: str, type_: pa.DataType) -&gt; Any:\n        \"\"\"Parse and convert value based on the field type.\"\"\"\n        if isinstance(val, (tuple, list)):\n            return type(val)(parse_value(v, type_) for v in val)\n\n        # Remove quotes from the value if present\n        val = val.strip().strip(\"'\\\"\")\n\n        if pa.types.is_timestamp(type_):\n            return timestamp_from_string(val, tz=type_.tz)\n        elif pa.types.is_date(type_):\n            parsed = timestamp_from_string(val)\n            return parsed.date() if hasattr(parsed, \"date\") else parsed\n        elif pa.types.is_time(type_):\n            parsed = timestamp_from_string(val)\n            return parsed.time() if hasattr(parsed, \"time\") else parsed\n\n        elif pa.types.is_integer(type_):\n            return int(float(val.replace(\",\", \".\")))\n        elif pa.types.is_floating(type_):\n            return float(val.replace(\",\", \".\"))\n        elif pa.types.is_boolean(type_):\n            return val.lower() in (\"true\", \"1\", \"yes\")\n        else:\n            return val\n\n    def _get_field_type_from_context(expr):\n        \"\"\"Try to determine the field type from a comparison expression.\"\"\"\n        if isinstance(expr.this, exp.Column):\n            field_name = expr.this.name\n            if field_name in schema.names:\n                return schema.field(field_name).type\n        elif isinstance(expr.expression, exp.Column):\n            field_name = expr.expression.name\n            if field_name in schema.names:\n                return schema.field(field_name).type\n        return None\n\n    def _convert_expression(expr, field_type=None) -&gt; pc.Expression:\n        \"\"\"Convert a sqlglot expression to a PyArrow compute expression.\"\"\"\n        if isinstance(expr, exp.Column):\n            field_name = expr.name\n            if field_name not in schema.names:\n                raise ValueError(f\"Unknown field: {field_name}\")\n            return pc.field(field_name)\n\n        elif isinstance(expr, exp.Literal):\n            # Convert literal value based on field type if available\n            if field_type:\n                val = str(expr.this)\n                # Remove quotes if present\n                val = val.strip().strip(\"'\\\"\")\n                return parse_value(val, field_type)\n            return expr.this\n\n        elif isinstance(expr, exp.Boolean):\n            return bool(expr.this)\n\n        elif isinstance(expr, exp.Null):\n            return None\n\n        elif isinstance(expr, (exp.EQ, exp.NEQ, exp.GT, exp.GTE, exp.LT, exp.LTE)):\n            # Binary comparison operations\n            # Try to determine field type from context\n            context_type = _get_field_type_from_context(expr)\n\n            left = _convert_expression(expr.this, context_type)\n            right = _convert_expression(expr.expression, context_type)\n\n            if isinstance(expr, exp.EQ):\n                return left == right\n            elif isinstance(expr, exp.NEQ):\n                return left != right\n            elif isinstance(expr, exp.GT):\n                return left &gt; right\n            elif isinstance(expr, exp.GTE):\n                return left &gt;= right\n            elif isinstance(expr, exp.LT):\n                return left &lt; right\n            elif isinstance(expr, exp.LTE):\n                return left &lt;= right\n\n        elif isinstance(expr, exp.In):\n            # IN operation\n            context_type = _get_field_type_from_context(expr)\n            left = _convert_expression(expr.this, context_type)\n            expressions = expr.args.get(\"expressions\")\n            if expressions is None and getattr(expr, \"expression\", None) is not None:\n                expressions = getattr(expr.expression, \"expressions\", None)\n\n            if expressions is None:\n                right = _convert_expression(expr.expression, context_type)\n            else:\n                right = [\n                    _convert_expression(e, context_type)\n                    for e in expressions  # type: ignore[arg-type]\n                ]\n            return left.isin(right)\n\n        elif isinstance(expr, exp.Not):\n            # NOT operation - check if it's NOT IN or IS NOT NULL\n            inner = expr.this\n            if isinstance(inner, exp.In):\n                # NOT IN case\n                context_type = _get_field_type_from_context(inner)\n                left = _convert_expression(inner.this, context_type)\n                expressions = inner.args.get(\"expressions\")\n                if (\n                    expressions is None\n                    and getattr(inner, \"expression\", None) is not None\n                ):\n                    expressions = getattr(inner.expression, \"expressions\", None)\n\n                if expressions is None:\n                    right = _convert_expression(inner.expression, context_type)\n                else:\n                    right = [\n                        _convert_expression(e, context_type)\n                        for e in expressions  # type: ignore[arg-type]\n                    ]\n                return ~left.isin(right)\n            elif isinstance(inner, exp.Is):\n                # IS NOT NULL case\n                left = _convert_expression(inner.this)\n                return ~left.is_null(nan_is_null=True)\n            else:\n                # Generic NOT\n                return ~_convert_expression(inner)\n\n        elif isinstance(expr, exp.Is):\n            left = _convert_expression(expr.this)\n            return left.is_null(nan_is_null=True)\n\n        elif isinstance(expr, exp.And):\n            return _convert_expression(expr.this) &amp; _convert_expression(expr.expression)\n\n        elif isinstance(expr, exp.Or):\n            return _convert_expression(expr.this) | _convert_expression(expr.expression)\n\n        elif isinstance(expr, exp.Not):\n            return ~_convert_expression(expr.this)\n\n        elif isinstance(expr, exp.Paren):\n            return _convert_expression(expr.this)\n\n        else:\n            raise ValueError(f\"Unsupported expression type: {type(expr)}\")\n\n    try:\n        # Parse the SQL expression using sqlglot\n        parsed = parse_one(string)\n\n        # Convert to PyArrow expression\n        return _convert_expression(parsed)\n\n    except Exception as e:\n        raise ValueError(f\"Failed to parse SQL expression: {e}\")\n</code></pre>"},{"location":"api/fsspeckit.storage_options.base/","title":"<code>fsspeckit.storage_options.base</code> API Documentation","text":"<p>This module defines the base class for filesystem storage configuration options.</p>"},{"location":"api/fsspeckit.storage_options.base/#basestorageoptions","title":"<code>BaseStorageOptions</code>","text":"<p>Base class for filesystem storage configuration options.</p> <p>Provides common functionality for all storage option classes including:</p> <ul> <li>YAML serialization/deserialization</li> <li>Dictionary conversion</li> <li>Filesystem instance creation</li> <li>Configuration updates</li> </ul> <p>Attributes:</p> <ul> <li><code>protocol</code> (<code>str</code>): Storage protocol identifier (e.g., \"s3\", \"gs\", \"file\")</li> </ul> <p>Example:</p> <pre><code>from fsspeckit.storage_options.base import BaseStorageOptions\n\n# Create and save options\noptions = BaseStorageOptions(protocol=\"s3\")\noptions.to_yaml(\"config.yml\")\n\n# Load from YAML\nloaded = BaseStorageOptions.from_yaml(\"config.yml\")\nprint(loaded.protocol)\n# 's3'\n</code></pre>"},{"location":"api/fsspeckit.storage_options.base/#to_dict","title":"<code>to_dict()</code>","text":"<p>Convert storage options to dictionary.</p> Parameter Type Description <code>with_protocol</code> <code>bool</code> Whether to include protocol in output dictionary Returns Type Description <code>dict</code> <code>dict</code> Dictionary of storage options with non-None values <p>Example:</p> <pre><code>from fsspeckit.storage_options.base import BaseStorageOptions\n\noptions = BaseStorageOptions(protocol=\"s3\")\nprint(options.to_dict())\n# {}\nprint(options.to_dict(with_protocol=True))\n# {'protocol': 's3'}\n</code></pre>"},{"location":"api/fsspeckit.storage_options.base/#from_yaml","title":"<code>from_yaml()</code>","text":"<p>Load storage options from YAML file.</p> Parameter Type Description <code>path</code> <code>str</code> Path to YAML configuration file <code>fs</code> <code>AbstractFileSystem</code> Filesystem to use for reading file Returns Type Description <code>BaseStorageOptions</code> <code>BaseStorageOptions</code> Loaded storage options instance <p>Example:</p> <pre><code># Load from local file\nfrom fsspeckit.storage_options.base import BaseStorageOptions\nfrom fsspec.implementations.local import LocalFileSystem\n\n# Assuming 'config.yml' exists and contains valid YAML for BaseStorageOptions\n# For example, a file named config.yml with content:\n# protocol: s3\n#\n# To make this example runnable, we'll create a dummy config.yml\nfs_local = LocalFileSystem()\nfs_local.write_text(\"config.yml\", \"protocol: s3\")\n\noptions = BaseStorageOptions.from_yaml(\"config.yml\")\nprint(options.protocol)\n# 's3'\n\n# Clean up the dummy file\nfs_local.rm(\"config.yml\")\n</code></pre>"},{"location":"api/fsspeckit.storage_options.base/#to_yaml","title":"<code>to_yaml()</code>","text":"<p>Save storage options to YAML file.</p> Parameter Type Description <code>path</code> <code>str</code> Path where to save configuration <code>fs</code> <code>AbstractFileSystem</code> Filesystem to use for writing <p>Example:</p> <pre><code>from fsspeckit.storage_options.base import BaseStorageOptions\nfrom fsspec.implementations.local import LocalFileSystem\n\noptions = BaseStorageOptions(protocol=\"s3\")\nfs_local = LocalFileSystem()\noptions.to_yaml(\"config.yml\", fs=fs_local) # Specify filesystem for writing\n</code></pre>"},{"location":"api/fsspeckit.storage_options.base/#to_filesystem","title":"<code>to_filesystem()</code>","text":"<p>Create fsspec filesystem instance from options.</p> Returns Type Description <code>AbstractFileSystem</code> <code>AbstractFileSystem</code> Configured filesystem instance <p>Example:</p> <pre><code>from fsspeckit.storage_options.base import BaseStorageOptions\n\noptions = BaseStorageOptions(protocol=\"file\")\nfs = options.to_filesystem()\n# Example usage: list files in a dummy directory\nimport os\nimport tempfile\n\nwith tempfile.TemporaryDirectory() as tmpdir:\n    dummy_file_path = os.path.join(tmpdir, \"test.txt\")\n    with open(dummy_file_path, \"w\") as f:\n        f.write(\"dummy content\")\n    fs_temp = options.to_filesystem()\n    files = fs_temp.ls(tmpdir)\n    print(files)\n</code></pre>"},{"location":"api/fsspeckit.storage_options.base/#update","title":"<code>update()</code>","text":"<p>Update storage options with new values.</p> Parameter Type Description <code>**kwargs</code> <code>Any</code> New option values to set Returns Type Description <code>BaseStorageOptions</code> <code>BaseStorageOptions</code> Updated instance <p>Example:</p> <pre><code>from fsspeckit.storage_options.base import BaseStorageOptions\n\noptions = BaseStorageOptions(protocol=\"s3\")\noptions = options.update(region=\"us-east-1\")\nprint(options.region)\n# 'us-east-1'\n</code></pre>"},{"location":"api/fsspeckit.storage_options.cloud/","title":"<code>fsspeckit.storage_options.cloud</code> API Documentation","text":"<p>This module defines storage option classes for various cloud providers, including Azure, Google Cloud Storage (GCS), and Amazon Web Services (AWS) S3. These classes provide structured ways to configure access to cloud storage, supporting different authentication methods and specific cloud service parameters.</p>"},{"location":"api/fsspeckit.storage_options.cloud/#azurestorageoptions","title":"<code>AzureStorageOptions</code>","text":"<p>Azure Storage configuration options.</p> <p>Provides configuration for Azure storage services:</p> <ul> <li>Azure Blob Storage (<code>az://</code>)</li> <li>Azure Data Lake Storage Gen2 (<code>abfs://</code>)</li> <li>Azure Data Lake Storage Gen1 (<code>adl://</code>)</li> </ul> <p>Supports multiple authentication methods:</p> <ul> <li>Connection string</li> <li>Account key</li> <li>Service principal</li> <li>Managed identity</li> <li>SAS token</li> </ul> <p>Attributes:</p> <ul> <li><code>protocol</code> (<code>str</code>): Storage protocol (\"az\", \"abfs\", or \"adl\")</li> <li><code>account_name</code> (<code>str</code>): Storage account name</li> <li><code>account_key</code> (<code>str</code>): Storage account access key</li> <li><code>connection_string</code> (<code>str</code>): Full connection string</li> <li><code>tenant_id</code> (<code>str</code>): Azure AD tenant ID</li> <li><code>client_id</code> (<code>str</code>): Service principal client ID</li> <li><code>client_secret</code> (<code>str</code>): Service principal client secret</li> <li><code>sas_token</code> (<code>str</code>): SAS token for limited access</li> </ul> <p>Example:</p> <pre><code>from fsspeckit.storage_options.cloud import AzureStorageOptions\n\n# Blob Storage with account key\noptions = AzureStorageOptions(\n    protocol=\"az\",\n    account_name=\"mystorageacct\",\n    account_key=\"key123...\"\n)\n\n# Data Lake with service principal\noptions = AzureStorageOptions(\n    protocol=\"abfs\",\n    account_name=\"mydatalake\",\n    tenant_id=\"tenant123\",\n    client_id=\"client123\",\n    client_secret=\"secret123\"\n)\n\n# Simple connection string auth\noptions = AzureStorageOptions(\n    protocol=\"az\",\n    connection_string=\"DefaultEndpoints...\"\n)\n</code></pre>"},{"location":"api/fsspeckit.storage_options.cloud/#from_env","title":"<code>from_env()</code>","text":"<p>Create storage options from environment variables.</p> <p>Reads standard Azure environment variables:</p> <ul> <li><code>AZURE_STORAGE_PROTOCOL</code></li> <li><code>AZURE_STORAGE_ACCOUNT_NAME</code></li> <li><code>AZURE_STORAGE_ACCOUNT_KEY</code></li> <li><code>AZURE_STORAGE_CONNECTION_STRING</code></li> <li><code>AZURE_TENANT_ID</code></li> <li><code>AZURE_CLIENT_ID</code></li> <li><code>AZURE_CLIENT_SECRET</code></li> <li><code>AZURE_STORAGE_SAS_TOKEN</code></li> </ul> Returns Type Description <code>AzureStorageOptions</code> <code>AzureStorageOptions</code> Configured storage options <p>Example:</p> <pre><code># With environment variables set:\nfrom fsspeckit.storage_options.cloud import AzureStorageOptions\nimport os\n\n# Set environment variables for testing (replace with actual values if needed)\nos.environ[\"AZURE_STORAGE_ACCOUNT_NAME\"] = \"mystorageacct\"\nos.environ[\"AZURE_STORAGE_ACCOUNT_KEY\"] = \"dummy_key\" # Dummy key for example\n\noptions = AzureStorageOptions.from_env()\nprint(options.account_name)  # From AZURE_STORAGE_ACCOUNT_NAME\n# 'mystorageacct'\n\n# Clean up environment variables\ndel os.environ[\"AZURE_STORAGE_ACCOUNT_NAME\"]\ndel os.environ[\"AZURE_STORAGE_ACCOUNT_KEY\"]\n</code></pre>"},{"location":"api/fsspeckit.storage_options.cloud/#to_env","title":"<code>to_env()</code>","text":"<p>Export options to environment variables.</p> <p>Sets standard Azure environment variables.</p> <p>Example:</p> <pre><code>from fsspeckit.storage_options.cloud import AzureStorageOptions\nimport os\n\noptions = AzureStorageOptions(\n    protocol=\"az\",\n    account_name=\"mystorageacct\",\n    account_key=\"key123\"\n)\noptions.to_env()\nprint(os.getenv(\"AZURE_STORAGE_ACCOUNT_NAME\"))\n# 'mystorageacct'\n\n# Clean up environment variables\ndel os.environ[\"AZURE_STORAGE_ACCOUNT_NAME\"]\ndel os.environ[\"AZURE_STORAGE_ACCOUNT_KEY\"]\n</code></pre>"},{"location":"api/fsspeckit.storage_options.cloud/#gcsstorageoptions","title":"<code>GcsStorageOptions</code>","text":"<p>Google Cloud Storage configuration options.</p> <p>Provides configuration for GCS access with support for:</p> <ul> <li>Service account authentication</li> <li>Default application credentials</li> <li>Token-based authentication</li> <li>Project configuration</li> <li>Custom endpoints</li> </ul> <p>Attributes:</p> <ul> <li><code>protocol</code> (<code>str</code>): Storage protocol (\"gs\" or \"gcs\")</li> <li><code>token</code> (<code>str</code>): Path to service account JSON file</li> <li><code>project</code> (<code>str</code>): Google Cloud project ID</li> <li><code>access_token</code> (<code>str</code>): OAuth2 access token</li> <li><code>endpoint_url</code> (<code>str</code>): Custom storage endpoint</li> <li><code>timeout</code> (<code>int</code>): Request timeout in seconds</li> </ul> <p>Example:</p> <pre><code>from fsspeckit.storage_options.cloud import GcsStorageOptions\n\n# Service account auth\noptions = GcsStorageOptions(\n    protocol=\"gs\",\n    token=\"path/to/service-account.json\",\n    project=\"my-project-123\"\n)\n\n# Application default credentials\noptions = GcsStorageOptions(\n    protocol=\"gcs\",\n    project=\"my-project-123\"\n)\n\n# Custom endpoint (e.g., test server)\noptions = GcsStorageOptions(\n    protocol=\"gs\",\n    endpoint_url=\"http://localhost:4443\",\n    token=\"test-token.json\"\n)\n</code></pre>"},{"location":"api/fsspeckit.storage_options.cloud/#from_env_1","title":"<code>from_env()</code>","text":"<p>Create storage options from environment variables.</p> <p>Reads standard GCP environment variables:</p> <ul> <li><code>GOOGLE_CLOUD_PROJECT</code>: Project</li> <li><code>GOOGLE_APPLICATION_CREDENTIALS</code>: Service account file path</li> <li><code>STORAGE_EMULATOR_HOST</code>: Custom endpoint (for testing)</li> <li><code>GCS_OAUTH_TOKEN</code>: OAuth2 access token</li> </ul> Returns Type Description <code>GcsStorageOptions</code> <code>GcsStorageOptions</code> Configured storage options <p>Example:</p> <pre><code># With environment variables set:\nfrom fsspeckit.storage_options.cloud import GcsStorageOptions\nimport os\n\n# Set environment variables for testing (replace with actual values if needed)\nos.environ[\"GOOGLE_CLOUD_PROJECT\"] = \"my-project-123\"\n\noptions = GcsStorageOptions.from_env()\nprint(options.project)  # From GOOGLE_CLOUD_PROJECT\n# 'my-project-123'\n\n# Clean up environment variables\ndel os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n</code></pre>"},{"location":"api/fsspeckit.storage_options.cloud/#to_env_1","title":"<code>to_env()</code>","text":"<p>Export options to environment variables.</p> <p>Sets standard GCP environment variables.</p> <p>Example:</p> <pre><code>from fsspeckit.storage_options.cloud import GcsStorageOptions\nimport os\n\noptions = GcsStorageOptions(\n    protocol=\"gs\",\n    project=\"my-project\",\n    token=\"service-account.json\"\n)\noptions.to_env()\nprint(os.getenv(\"GOOGLE_CLOUD_PROJECT\"))\n# 'my-project'\n\n# Clean up environment variables\ndel os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n</code></pre>"},{"location":"api/fsspeckit.storage_options.cloud/#to_fsspec_kwargs","title":"<code>to_fsspec_kwargs()</code>","text":"<p>Convert options to fsspec filesystem arguments.</p> Returns Type Description <code>dict</code> <code>dict</code> Arguments suitable for GCSFileSystem <p>Example:</p> <pre><code>from fsspeckit.storage_options.cloud import GcsStorageOptions\nfrom fsspeckit.core.base import filesystem\n\noptions = GcsStorageOptions(\n    protocol=\"gs\",\n    token=\"service-account.json\",\n    project=\"my-project\"\n)\nkwargs = options.to_fsspec_kwargs()\nfs = filesystem(\"gcs\", **kwargs)\n</code></pre>"},{"location":"api/fsspeckit.storage_options.cloud/#awsstorageoptions","title":"<code>AwsStorageOptions</code>","text":"<p>AWS S3 storage configuration options.</p> <p>Provides comprehensive configuration for S3 access with support for:</p> <ul> <li>Multiple authentication methods (keys, profiles, environment)</li> <li>Custom endpoints for S3-compatible services</li> <li>Region configuration</li> <li>SSL/TLS settings</li> <li>Anonymous access for public buckets</li> </ul> <p>Attributes:</p> <ul> <li><code>protocol</code> (<code>str</code>): Always \"s3\" for S3 storage</li> <li><code>access_key_id</code> (<code>str</code>): AWS access key ID</li> <li><code>secret_access_key</code> (<code>str</code>): AWS secret access key</li> <li><code>session_token</code> (<code>str</code>): AWS session token</li> <li><code>endpoint_url</code> (<code>str</code>): Custom S3 endpoint URL</li> <li><code>region</code> (<code>str</code>): AWS region name</li> <li><code>allow_invalid_certificates</code> (<code>bool</code>): Skip SSL certificate validation</li> <li><code>allow_http</code> (<code>bool</code>): Allow unencrypted HTTP connections</li> <li><code>anonymous</code> (<code>bool</code>): Use anonymous (unsigned) S3 access</li> </ul> <p>Example:</p> <pre><code># Basic credentials\noptions = AwsStorageOptions(\n    access_key_id=\"AKIAXXXXXXXX\",\n    secret_access_key=\"SECRETKEY\",\n    region=\"us-east-1\"\n)\n\n# Profile-based auth\noptions = AwsStorageOptions.create(profile=\"dev\")\n\n# S3-compatible service (MinIO)\noptions = AwsStorageOptions(\n    endpoint_url=\"http://localhost:9000\",\n    access_key_id=\"minioadmin\",\n    secret_access_key=\"minioadmin\",\n    allow_http=True\n)\n\n# Anonymous access for public buckets\noptions = AwsStorageOptions(anonymous=True)\n</code></pre>"},{"location":"api/fsspeckit.storage_options.cloud/#create","title":"<code>create()</code>","text":"<p>Creates an <code>AwsStorageOptions</code> instance, handling aliases and profile loading.</p> Parameter Type Description <code>protocol</code> <code>str</code> Storage protocol, defaults to \"s3\". <code>access_key_id</code> <code>str | None</code> AWS access key ID. <code>secret_access_key</code> <code>str | None</code> AWS secret access key. <code>session_token</code> <code>str | None</code> AWS session token. <code>endpoint_url</code> <code>str | None</code> Custom S3 endpoint URL. <code>region</code> <code>str | None</code> AWS region name. <code>allow_invalid_certificates</code> <code>bool | None</code> Skip SSL certificate validation. <code>allow_http</code> <code>bool | None</code> Allow unencrypted HTTP connections. <code>anonymous</code> <code>bool | None</code> Use anonymous (unsigned) S3 access. <code>key</code> <code>str | None</code> Alias for <code>access_key_id</code>. <code>secret</code> <code>str | None</code> Alias for <code>secret_access_key</code>. <code>token</code> <code>str | None</code> Alias for <code>session_token</code>. <code>profile</code> <code>str | None</code> AWS credentials profile name to load credentials from. Returns Type Description <code>AwsStorageOptions</code> <code>AwsStorageOptions</code> An initialized <code>AwsStorageOptions</code> instance."},{"location":"api/fsspeckit.storage_options.cloud/#from_aws_credentials","title":"<code>from_aws_credentials()</code>","text":"<p>Create storage options from AWS credentials file.</p> <p>Loads credentials from <code>~/.aws/credentials</code> and <code>~/.aws/config</code> files.</p> Parameter Type Description <code>profile</code> <code>str</code> AWS credentials profile name <code>allow_invalid_certificates</code> <code>bool</code> Skip SSL certificate validation <code>allow_http</code> <code>bool</code> Allow unencrypted HTTP connections <code>anonymous</code> <code>bool</code> Use anonymous (unsigned) S3 access Returns Type Description <code>AwsStorageOptions</code> <code>AwsStorageOptions</code> Configured storage options Raises Type Description <code>ValueError</code> <code>ValueError</code> If profile not found <code>FileNotFoundError</code> <code>FileNotFoundError</code> If credentials files missing <p>Example:</p> <pre><code># Load developer profile\noptions = AwsStorageOptions.from_aws_credentials(\n    profile=\"dev\",\n    allow_http=True  # For local testing\n)\n</code></pre>"},{"location":"api/fsspeckit.storage_options.cloud/#from_env_2","title":"<code>from_env()</code>","text":"<p>Create storage options from environment variables.</p> <p>Reads standard AWS environment variables:</p> <ul> <li><code>AWS_ACCESS_KEY_ID</code></li> <li><code>AWS_SECRET_ACCESS_KEY</code></li> <li><code>AWS_SESSION_TOKEN</code></li> <li><code>AWS_ENDPOINT_URL</code></li> <li><code>AWS_DEFAULT_REGION</code></li> <li><code>ALLOW_INVALID_CERTIFICATE</code></li> <li><code>AWS_ALLOW_HTTP</code></li> <li><code>AWS_S3_ANONYMOUS</code></li> </ul> Returns Type Description <code>AwsStorageOptions</code> <code>AwsStorageOptions</code> Configured storage options <p>Example:</p> <pre><code># Load from environment\nfrom fsspeckit.storage_options.cloud import AwsStorageOptions\nimport os\n\n# Set environment variables for testing (replace with actual values if needed)\nos.environ[\"AWS_DEFAULT_REGION\"] = \"us-east-1\"\n\noptions = AwsStorageOptions.from_env()\nprint(options.region)\n# 'us-east-1'  # From AWS_DEFAULT_REGION\n\n# Clean up environment variables\ndel os.environ[\"AWS_DEFAULT_REGION\"]\n</code></pre>"},{"location":"api/fsspeckit.storage_options.cloud/#to_fsspec_kwargs_1","title":"<code>to_fsspec_kwargs()</code>","text":"<p>Convert options to fsspec filesystem arguments.</p> Returns Type Description <code>dict</code> <code>dict</code> Arguments suitable for fsspec S3FileSystem <p>Example:</p> <pre><code>options = AwsStorageOptions(\n    access_key_id=\"KEY\",\n    secret_access_key=\"SECRET\",\n    region=\"us-west-2\"\n)\nkwargs = options.to_fsspec_kwargs()\nfs = filesystem(\"s3\", **kwargs)\n</code></pre>"},{"location":"api/fsspeckit.storage_options.cloud/#to_object_store_kwargs","title":"<code>to_object_store_kwargs()</code>","text":"<p>Convert options to object store arguments.</p> Parameter Type Description <code>with_conditional_put</code> <code>bool</code> Add etag-based conditional put support Returns Type Description <code>dict</code> <code>dict</code> Arguments suitable for object store clients <p>Example:</p> <pre><code>from fsspeckit.storage_options.cloud import AwsStorageOptions\n# Assuming ObjectStore is a hypothetical client for demonstration\n# from some_object_store_library import ObjectStore\n\noptions = AwsStorageOptions(\n    access_key_id=\"KEY\",\n    secret_access_key=\"SECRET\"\n)\nkwargs = options.to_object_store_kwargs()\n# client = ObjectStore(**kwargs)\n</code></pre>"},{"location":"api/fsspeckit.storage_options.cloud/#to_env_2","title":"<code>to_env()</code>","text":"<p>Export options to environment variables.</p> <p>Sets standard AWS environment variables.</p> <p>Example:</p> <pre><code>from fsspeckit.storage_options.cloud import AwsStorageOptions\nimport os\n\noptions = AwsStorageOptions(\n    access_key_id=\"KEY\",\n    secret_access_key=\"SECRET\",\n    region=\"us-east-1\"\n)\noptions.to_env()\nprint(os.getenv(\"AWS_ACCESS_KEY_ID\"))\n# 'KEY'\n\n# Clean up environment variables\ndel os.environ[\"AWS_ACCESS_KEY_ID\"]\ndel os.environ[\"AWS_SECRET_ACCESS_KEY\"]\nif \"AWS_DEFAULT_REGION\" in os.environ: # Only delete if it was set\n    del os.environ[\"AWS_DEFAULT_REGION\"]\n</code></pre>"},{"location":"api/fsspeckit.storage_options.cloud/#to_filesystem","title":"<code>to_filesystem()</code>","text":"<p>Create fsspec filesystem instance from options.</p> Returns Type Description <code>AbstractFileSystem</code> <code>AbstractFileSystem</code> Configured filesystem instance"},{"location":"api/fsspeckit.storage_options.core/","title":"<code>fsspeckit.storage_options.core</code> API Reference","text":""},{"location":"api/fsspeckit.storage_options.core/#localstorageoptions","title":"<code>LocalStorageOptions</code>","text":"<p>Local filesystem configuration options.</p> <p>Provides basic configuration for local file access. While this class is simple, it maintains consistency with other storage options and enables transparent switching between local and remote storage.</p> <p>Attributes:</p> <ul> <li><code>protocol</code> (<code>str</code>): Always \"file\" for local filesystem</li> <li><code>auto_mkdir</code> (<code>bool</code>): Create directories automatically</li> <li><code>mode</code> (<code>int</code>): Default file creation mode (unix-style)</li> </ul> <p>Example: <pre><code># Basic local access\noptions = LocalStorageOptions()\nfs = options.to_filesystem()\nfiles = fs.ls(\"/path/to/data\")\n\n# With auto directory creation\noptions = LocalStorageOptions(auto_mkdir=True)\nfs = options.to_filesystem()\nwith fs.open(\"/new/path/file.txt\", \"w\") as f:\n    f.write(\"test\")  # Creates /new/path/ automatically\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.core/#to_fsspec_kwargs","title":"<code>to_fsspec_kwargs()</code>","text":"<p>Convert options to fsspec filesystem arguments.</p> <p>Returns:</p> <ul> <li><code>dict</code>: Arguments suitable for LocalFileSystem</li> </ul> <p>Example: <pre><code>options = LocalStorageOptions(auto_mkdir=True)\nkwargs = options.to_fsspec_kwargs()\nfs = filesystem(\"file\", **kwargs)\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.core/#from_dict","title":"<code>from_dict()</code>","text":"<p>Create appropriate storage options instance from dictionary.</p> <p>Factory function that creates the correct storage options class based on protocol.</p> <p>Parameters:</p> Name Type Description <code>protocol</code> <code>str</code> Storage protocol identifier (e.g., \"s3\", \"gs\", \"file\") <code>storage_options</code> <code>dict</code> Dictionary of configuration options <p>Returns:</p> <ul> <li><code>BaseStorageOptions</code>: Appropriate storage options instance</li> </ul> <p>Raises:</p> <ul> <li><code>ValueError</code>: If protocol is not supported</li> </ul> <p>Example: <pre><code># Create S3 options\noptions = from_dict(\"s3\", {\n    \"access_key_id\": \"KEY\",\n    \"secret_access_key\": \"SECRET\"\n})\nprint(type(options).__name__)\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.core/#from_env","title":"<code>from_env()</code>","text":"<p>Create storage options from environment variables.</p> <p>Factory function that creates and configures storage options from protocol-specific environment variables.</p> <p>Parameters:</p> Name Type Description <code>protocol</code> <code>str</code> Storage protocol identifier (e.g., \"s3\", \"github\") <p>Returns:</p> <ul> <li><code>BaseStorageOptions</code>: Configured storage options instance</li> </ul> <p>Raises:</p> <ul> <li><code>ValueError</code>: If protocol is not supported</li> </ul> <p>Example: <pre><code># With AWS credentials in environment\noptions = from_env(\"s3\")\nprint(options.access_key_id)\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.core/#infer_protocol_from_uri","title":"<code>infer_protocol_from_uri()</code>","text":"<p>Infer the storage protocol from a URI string.</p> <p>Analyzes the URI to determine the appropriate storage protocol based on the scheme or path format.</p> <p>Parameters:</p> Name Type Description <code>uri</code> <code>str</code> URI or path string to analyze. <p>Typical examples include:</p> <ul> <li><code>\\\"s3://bucket/path\\\"</code></li> <li><code>\\\"gs://bucket/path\\\"</code></li> <li><code>\\\"github://org/repo\\\"</code></li> <li><code>\\\"/local/path\\\"</code></li> </ul> <p>Returns:</p> <ul> <li><code>str</code>: Inferred protocol identifier</li> </ul> <p>Example: <pre><code># S3 protocol\ninfer_protocol_from_uri(\"s3://my-bucket/data\")\n\n# Local file\ninfer_protocol_from_uri(\"/home/user/data\")\n\n# GitHub repository\ninfer_protocol_from_uri(\"github://microsoft/vscode\")\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.core/#storage_options_from_uri","title":"<code>storage_options_from_uri()</code>","text":"<p>Create storage options instance from a URI string.</p> <p>Infers the protocol and extracts relevant configuration from the URI to create appropriate storage options.</p> <p>Parameters:</p> Name Type Description <code>uri</code> <code>str</code> URI string containing protocol and optional configuration. <p>Typical examples include:</p> <ul> <li><code>\\\"s3://bucket/path\\\"</code></li> <li><code>\\\"gs://project/bucket/path\\\"</code></li> <li><code>\\\"github://org/repo\\\"</code></li> </ul> <p>Returns:</p> <ul> <li><code>BaseStorageOptions</code>: Configured storage options instance</li> </ul> <p>Example: <pre><code># S3 options\nopts = storage_options_from_uri(\"s3://my-bucket/data\")\nprint(opts.protocol)\n\n# GitHub options\nopts = storage_options_from_uri(\"github://microsoft/vscode\")\nprint(opts.org)\nprint(opts.repo)\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.core/#merge_storage_options","title":"<code>merge_storage_options()</code>","text":"<p>Merge multiple storage options into a single configuration.</p> <p>Combines options from multiple sources with control over precedence.</p> <p>Parameters:</p> Name Type Description <code>*options</code> <code>BaseStorageOptions</code> or <code>dict</code> Storage options to merge. <code>overwrite</code> <code>bool</code> Whether later options override earlier ones <p>Each entry in <code>*options</code> may be:</p> <ul> <li>A <code>BaseStorageOptions</code> instance</li> <li>A dictionary of options</li> <li><code>None</code> (ignored)</li> </ul> <p>Returns:</p> <ul> <li><code>BaseStorageOptions</code>: Combined storage options</li> </ul> <p>Example: <pre><code># Merge with overwrite\nbase = AwsStorageOptions(\n    region=\"us-east-1\",\n    access_key_id=\"OLD_KEY\"\n)\noverride = {\"access_key_id\": \"NEW_KEY\"}\nmerged = merge_storage_options(base, override)\nprint(merged.access_key_id)\n\n# Preserve existing values\nmerged = merge_storage_options(\n    base,\n    override,\n    overwrite=False\n)\nprint(merged.access_key_id)\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.core/#storageoptions","title":"<code>StorageOptions</code>","text":"<p>High-level storage options container and factory.</p> <p>Provides a unified interface for creating and managing storage options for different protocols.</p> <p>Attributes:</p> <ul> <li><code>storage_options</code> (<code>BaseStorageOptions</code>): Underlying storage options instance</li> </ul> <p>Example: <pre><code># Create from protocol\noptions = StorageOptions.create(\n    protocol=\"s3\",\n    access_key_id=\"KEY\",\n    secret_access_key=\"SECRET\"\n)\n\n# Create from existing options\ns3_opts = AwsStorageOptions(access_key_id=\"KEY\")\noptions = StorageOptions(storage_options=s3_opts)\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.core/#create","title":"<code>create()</code>","text":"<p>Create storage options from arguments.</p> <p>Parameters:</p> Name Type Description <code>**data</code> <code>dict</code> Keyword arguments describing either protocol/configuration or a pre-configured instance. <p>Accepted patterns:</p> <ul> <li><code>protocol=...</code> plus configuration fields</li> <li><code>storage_options=&lt;BaseStorageOptions instance&gt;</code></li> </ul> <p>Returns:</p> <ul> <li><code>StorageOptions</code>: Configured storage options instance</li> </ul> <p>Raises:</p> <ul> <li><code>ValueError</code>: If protocol missing or invalid</li> </ul> <p>Example: <pre><code># Direct protocol config\noptions = StorageOptions.create(\n    protocol=\"s3\",\n    region=\"us-east-1\"\n)\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.core/#from_yaml","title":"<code>from_yaml()</code>","text":"<p>Create storage options from YAML configuration.</p> <p>Parameters:</p> Name Type Description <code>path</code> <code>str</code> Path to YAML configuration file <code>fs</code> <code>fsspec.AbstractFileSystem</code>, optional Filesystem for reading configuration <p>Returns:</p> <ul> <li><code>StorageOptions</code>: Configured storage options</li> </ul> <p>Example: <pre><code># Load from config file\noptions = StorageOptions.from_yaml(\"storage.yml\")\nprint(options.storage_options.protocol)\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.core/#from_env_1","title":"<code>from_env()</code>","text":"<p>Create storage options from environment variables.</p> <p>Parameters:</p> Name Type Description <code>protocol</code> <code>str</code> Storage protocol to configure <p>Returns:</p> <ul> <li><code>StorageOptions</code>: Environment-configured options</li> </ul> <p>Example: <pre><code># Load AWS config from environment\noptions = StorageOptions.from_env(\"s3\")\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.core/#to_filesystem","title":"<code>to_filesystem()</code>","text":"<p>Create fsspec filesystem instance.</p> <p>Returns:</p> <ul> <li><code>AbstractFileSystem</code>: Configured filesystem instance</li> </ul> <p>Example: <pre><code>options = StorageOptions.create(protocol=\"file\")\nfs = options.to_filesystem()\nfiles = fs.ls(\"/data\")\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.core/#to_dict","title":"<code>to_dict()</code>","text":"<p>Convert storage options to dictionary.</p> <p>Parameters:</p> Name Type Description <code>with_protocol</code> <code>bool</code> Whether to include protocol in output <p>Returns:</p> <ul> <li><code>dict</code>: Storage options as dictionary</li> </ul> <p>Example: <pre><code>options = StorageOptions.create(\n    protocol=\"s3\",\n    region=\"us-east-1\"\n)\nprint(options.to_dict())\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.core/#to_object_store_kwargs","title":"<code>to_object_store_kwargs()</code>","text":"<p>Get options formatted for object store clients.</p> <p>Parameters:</p> Name Type Description <code>with_conditional_put</code> <code>bool</code> Add etag-based conditional put support <p>Returns:</p> <ul> <li><code>dict</code>: Object store configuration dictionary</li> </ul> <p>Example: <pre><code>options = StorageOptions.create(protocol=\"s3\")\nkwargs = options.to_object_store_kwargs()\n# store = ObjectStore(**kwargs)\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.core/#basestorageoptions","title":"<code>BaseStorageOptions</code>","text":"<p>Base class for filesystem storage configuration options.</p> <p>Provides common functionality for all storage option classes including: - YAML serialization/deserialization - Dictionary conversion - Filesystem instance creation - Configuration updates</p> <p>Attributes:</p> <ul> <li><code>protocol</code> (<code>str</code>): Storage protocol identifier (e.g., \"s3\", \"gs\", \"file\")</li> </ul> <p>Example: <pre><code># Create and save options\noptions = BaseStorageOptions(protocol=\"s3\")\noptions.to_yaml(\"config.yml\")\n\n# Load from YAML\nloaded = BaseStorageOptions.from_yaml(\"config.yml\")\nprint(loaded.protocol)\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.core/#to_dict_1","title":"<code>to_dict()</code>","text":"<p>Convert storage options to dictionary.</p> <p>Parameters:</p> Name Type Description <code>with_protocol</code> <code>bool</code> Whether to include protocol in output dictionary <p>Returns:</p> <ul> <li><code>dict</code>: Dictionary of storage options with non-None values</li> </ul> <p>Example: <pre><code>options = BaseStorageOptions(protocol=\"s3\")\nprint(options.to_dict())\nprint(options.to_dict(with_protocol=True))\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.core/#from_yaml_1","title":"<code>from_yaml()</code>","text":"<p>Load storage options from YAML file.</p> <p>Parameters:</p> Name Type Description <code>path</code> <code>str</code> Path to YAML configuration file <code>fs</code> <code>fsspec.AbstractFileSystem</code>, optional Filesystem to use for reading file <p>Returns:</p> <ul> <li><code>BaseStorageOptions</code>: Loaded storage options instance</li> </ul> <p>Example: <pre><code># Load from local file\noptions = BaseStorageOptions.from_yaml(\"config.yml\")\nprint(options.protocol)\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.core/#to_yaml","title":"<code>to_yaml()</code>","text":"<p>Save storage options to YAML file.</p> <p>Parameters:</p> Name Type Description <code>path</code> <code>str</code> Path where to save configuration <code>fs</code> <code>fsspec.AbstractFileSystem</code>, optional Filesystem to use for writing <p>Example: <pre><code>options = BaseStorageOptions(protocol=\"s3\")\noptions.to_yaml(\"config.yml\")\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.core/#to_filesystem_1","title":"<code>to_filesystem()</code>","text":"<p>Create fsspec filesystem instance from options.</p> <p>Returns:</p> <ul> <li><code>AbstractFileSystem</code>: Configured filesystem instance</li> </ul> <p>Example: <pre><code>options = BaseStorageOptions(protocol=\"file\")\nfs = options.to_filesystem()\nfiles = fs.ls(\"/path/to/data\")\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.core/#update","title":"<code>update()</code>","text":"<p>Update storage options with new values.</p> <p>Parameters:</p> Name Type Description <code>**kwargs</code> <code>dict</code> New option values to set <p>Returns:</p> <ul> <li><code>BaseStorageOptions</code>: Updated instance</li> </ul> <p>Example: <pre><code>options = BaseStorageOptions(protocol=\"s3\")\noptions = options.update(region=\"us-east-1\")\nprint(options.region)\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.git/","title":"<code>fsspeckit.storage_options.git</code> API Reference","text":""},{"location":"api/fsspeckit.storage_options.git/#githubstorageoptions","title":"<code>GitHubStorageOptions</code>","text":"<p>GitHub repository storage configuration options.</p> <p>Provides access to files in GitHub repositories with support for: - Public and private repositories - Branch/tag/commit selection - Token-based authentication - Custom GitHub Enterprise instances</p> <p>Attributes:</p> <ul> <li><code>protocol</code> (<code>str</code>): Always \"github\" for GitHub storage</li> <li><code>org</code> (<code>str</code>): Organization or user name</li> <li><code>repo</code> (<code>str</code>): Repository name</li> <li><code>ref</code> (<code>str</code>): Git reference (branch, tag, or commit SHA</li> <li><code>token</code> (<code>str</code>): GitHub personal access token</li> <li><code>api_url</code> (<code>str</code>): Custom GitHub API URL for enterprise instances</li> </ul> <p>Example: <pre><code># Public repository\noptions = GitHubStorageOptions(\n    org=\"microsoft\",\n    repo=\"vscode\",\n    ref=\"main\"\n)\n\n# Private repository\noptions = GitHubStorageOptions(\n    org=\"myorg\",\n    repo=\"private-repo\",\n    token=\"ghp_xxxx\",\n    ref=\"develop\"\n)\n\n# Enterprise instance\noptions = GitHubStorageOptions(\n    org=\"company\",\n    repo=\"internal\",\n    api_url=\"https://github.company.com/api/v3\",\n    token=\"ghp_xxxx\"\n)\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.git/#from_env","title":"<code>from_env()</code>","text":"<p>Create storage options from environment variables.</p> <p>Reads standard GitHub environment variables: - GITHUB_ORG: Organization or user name - GITHUB_REPO: Repository name - GITHUB_REF: Git reference - GITHUB_TOKEN: Personal access token - GITHUB_API_URL: Custom API URL</p> <p>Returns:</p> <ul> <li><code>GitHubStorageOptions</code>: Configured storage options</li> </ul> <p>Example: <pre><code># With environment variables set:\noptions = GitHubStorageOptions.from_env()\nprint(options.org)  # From GITHUB_ORG 'microsoft'\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.git/#to_env","title":"<code>to_env()</code>","text":"<p>Export options to environment variables.</p> <p>Sets standard GitHub environment variables.</p> <p>Example: <pre><code>options = GitHubStorageOptions(\n    org=\"microsoft\",\n    repo=\"vscode\",\n    token=\"ghp_xxxx\"\n)\noptions.to_env()\nprint(os.getenv(\"GITHUB_ORG\"))  # 'microsoft'\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.git/#to_fsspec_kwargs","title":"<code>to_fsspec_kwargs()</code>","text":"<p>Convert options to fsspec filesystem arguments.</p> <p>Returns:</p> <ul> <li><code>dict</code>: Arguments suitable for GitHubFileSystem</li> </ul> <p>Example: <pre><code>options = GitHubStorageOptions(\n    org=\"microsoft\",\n    repo=\"vscode\",\n    token=\"ghp_xxxx\"\n)\nkwargs = options.to_fsspec_kwargs()\nfs = filesystem(\"github\", **kwargs)\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.git/#gitlabstorageoptions","title":"<code>GitLabStorageOptions</code>","text":"<p>GitLab repository storage configuration options.</p> <p>Provides access to files in GitLab repositories with support for: - Public and private repositories - Self-hosted GitLab instances - Project ID or name-based access - Branch/tag/commit selection - Token-based authentication</p> <p>Attributes:</p> <ul> <li><code>protocol</code> (<code>str</code>): Always \"gitlab\" for GitLab storage</li> <li><code>base_url</code> (<code>str</code>): GitLab instance URL, defaults to gitlab.com</li> <li><code>project_id</code> (<code>str</code> | <code>int</code>): Project ID number</li> <li><code>project_name</code> (<code>str</code>): Project name/path</li> <li><code>ref</code> (<code>str</code>): Git reference (branch, tag, or commit SHA)</li> <li><code>token</code> (<code>str</code>): GitLab personal access token</li> <li><code>api_version</code> (<code>str</code>): API version to use</li> </ul> <p>Example: <pre><code># Public project on gitlab.com\noptions = GitLabStorageOptions(\n    project_name=\"group/project\",\n    ref=\"main\"\n)\n\n# Private project with token\noptions = GitLabStorageOptions(\n    project_id=12345,\n    token=\"glpat_xxxx\",\n    ref=\"develop\"\n)\n\n# Self-hosted instance\noptions = GitLabStorageOptions(\n    base_url=\"https://gitlab.company.com\",\n    project_name=\"internal/project\",\n    token=\"glpat_xxxx\"\n)\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.git/#from_env_1","title":"<code>from_env()</code>","text":"<p>Create storage options from environment variables.</p> <p>Reads standard GitLab environment variables: - GITLAB_URL: Instance URL - GITLAB_PROJECT_ID: Project ID - GITLAB_PROJECT_NAME: Project name/path - GITLAB_REF: Git reference - GITLAB_TOKEN: Personal access token - GITLAB_API_VERSION: API version</p> <p>Returns:</p> <ul> <li><code>GitLabStorageOptions</code>: Configured storage options</li> </ul> <p>Example: <pre><code># With environment variables set:\noptions = GitLabStorageOptions.from_env()\nprint(options.project_id)  # From GITLAB_PROJECT_ID '12345'\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.git/#to_env_1","title":"<code>to_env()</code>","text":"<p>Export options to environment variables.</p> <p>Sets standard GitLab environment variables.</p> <p>Example: <pre><code>options = GitLabStorageOptions(\n    project_id=12345,\n    token=\"glpat_xxxx\"\n)\noptions.to_env()\nprint(os.getenv(\"GITLAB_PROJECT_ID\"))  # '12345'\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.git/#to_fsspec_kwargs_1","title":"<code>to_fsspec_kwargs()</code>","text":"<p>Convert options to fsspec filesystem arguments.</p> <p>Returns:</p> <ul> <li><code>dict</code>: Arguments suitable for GitLabFileSystem</li> </ul> <p>Example: <pre><code>options = GitLabStorageOptions(\n    project_id=12345,\n    token=\"glpat_xxxx\"\n)\nkwargs = options.to_fsspec_kwargs()\nfs = filesystem(\"gitlab\", **kwargs)\n</code></pre></p>"},{"location":"api/fsspeckit.utils.datetime/","title":"<code>fsspeckit.utils.datetime</code> (legacy documentation)","text":"<p>There is no longer a dedicated <code>fsspeckit.utils.datetime</code> module in the codebase. Datetime helpers live in the canonical domain package <code>fsspeckit.common.datetime</code> and are not re-exported via the <code>fsspeckit.utils</code> fa\u00e7ade.</p> <p>This page exists only to help migrate older code that previously relied on <code>fsspec-utils</code>/<code>utils</code> datetime helpers.</p>"},{"location":"api/fsspeckit.utils.datetime/#canonical-datetime-helpers","title":"Canonical datetime helpers","text":"<p>The current, supported helpers are:</p> <ul> <li><code>get_timestamp_column</code></li> <li><code>get_timedelta_str</code></li> <li><code>timestamp_from_string</code></li> </ul> <p>All of them live in <code>fsspeckit.common.datetime</code>.</p>"},{"location":"api/fsspeckit.utils.datetime/#recommended-imports","title":"Recommended imports","text":"<p>Preferred (domain package):</p> <pre><code>from fsspeckit.common.datetime import (\n    get_timestamp_column,\n    get_timedelta_str,\n    timestamp_from_string,\n)\n</code></pre> <p>If you have legacy code importing from a non-existent <code>fsspeckit.utils.datetime</code> module, you should update it to use the canonical imports above.</p>"},{"location":"api/fsspeckit.utils.logging/","title":"<code>fsspeckit.utils.logging</code>","text":"<p>Note: The <code>fsspeckit.utils.logging</code> module is maintained for backwards compatibility. New code should import logging utilities directly from <code>fsspeckit.common.logging</code> for better discoverability.</p> <p>This module is equivalent to <code>fsspeckit.common.logging</code> and exports the same functions for backwards compatibility only.</p>"},{"location":"api/fsspeckit.utils.logging/#fsspeckit.common.logging","title":"logging","text":"<p>Logging configuration utilities for fsspeckit.</p>"},{"location":"api/fsspeckit.utils.logging/#fsspeckit.common.logging-functions","title":"Functions","text":""},{"location":"api/fsspeckit.utils.logging/#fsspeckit.common.logging.get_logger","title":"fsspeckit.common.logging.get_logger","text":"<pre><code>get_logger(name: str = 'fsspeckit') -&gt; logger\n</code></pre> <p>Get a logger instance for the given name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Logger name, typically the module name.</p> <code>'fsspeckit'</code> <p>Returns:</p> Type Description <code>logger</code> <p>Configured logger instance.</p> Example <pre><code>logger = get_logger(__name__)\nlogger.info(\"This is a log message\")\n</code></pre> Source code in <code>src/fsspeckit/common/logging.py</code> <pre><code>def get_logger(name: str = \"fsspeckit\") -&gt; \"logger\":\n    \"\"\"Get a logger instance for the given name.\n\n    Args:\n        name: Logger name, typically the module name.\n\n    Returns:\n        Configured logger instance.\n\n    Example:\n        ```python\n        logger = get_logger(__name__)\n        logger.info(\"This is a log message\")\n        ```\n    \"\"\"\n    return logger.bind(name=name)\n</code></pre>"},{"location":"api/fsspeckit.utils.logging/#fsspeckit.common.logging.setup_logging","title":"fsspeckit.common.logging.setup_logging","text":"<pre><code>setup_logging(\n    level: Optional[str] = None,\n    disable: bool = False,\n    format_string: Optional[str] = None,\n) -&gt; None\n</code></pre> <p>Configure the Loguru logger for fsspeckit.</p> <p>Removes the default handler and adds a new one targeting stderr with customizable level and format.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>Optional[str]</code> <p>Log level (DEBUG, INFO, WARNING, ERROR, CRITICAL).    If None, uses fsspeckit_LOG_LEVEL environment variable    or defaults to \"INFO\".</p> <code>None</code> <code>disable</code> <code>bool</code> <p>Whether to disable logging for fsspeckit package.</p> <code>False</code> <code>format_string</code> <code>Optional[str]</code> <p>Custom format string for log messages.           If None, uses a default comprehensive format.</p> <code>None</code> Example <pre><code># Basic setup\nsetup_logging()\n\n# Custom level and format\nsetup_logging(level=\"DEBUG\", format_string=\"{time} | {level} | {message}\")\n\n# Disable logging\nsetup_logging(disable=True)\n</code></pre> Source code in <code>src/fsspeckit/common/logging.py</code> <pre><code>def setup_logging(\n    level: Optional[str] = None,\n    disable: bool = False,\n    format_string: Optional[str] = None,\n) -&gt; None:\n    \"\"\"Configure the Loguru logger for fsspeckit.\n\n    Removes the default handler and adds a new one targeting stderr\n    with customizable level and format.\n\n    Args:\n        level: Log level (DEBUG, INFO, WARNING, ERROR, CRITICAL).\n               If None, uses fsspeckit_LOG_LEVEL environment variable\n               or defaults to \"INFO\".\n        disable: Whether to disable logging for fsspeckit package.\n        format_string: Custom format string for log messages.\n                      If None, uses a default comprehensive format.\n\n    Example:\n        ```python\n        # Basic setup\n        setup_logging()\n\n        # Custom level and format\n        setup_logging(level=\"DEBUG\", format_string=\"{time} | {level} | {message}\")\n\n        # Disable logging\n        setup_logging(disable=True)\n        ```\n    \"\"\"\n    # Determine log level\n    if level is None:\n        level = os.getenv(\"fsspeckit_LOG_LEVEL\", \"INFO\")\n\n    # Default format if none provided\n    if format_string is None:\n        format_string = (\n            \"&lt;green&gt;{time:YYYY-MM-DD HH:mm:ss.SSS}&lt;/green&gt; | \"\n            \"&lt;level&gt;{level: &lt;8}&lt;/level&gt; | \"\n            \"&lt;cyan&gt;{name}&lt;/cyan&gt;:&lt;cyan&gt;{function}&lt;/cyan&gt;:&lt;cyan&gt;{line}&lt;/cyan&gt; - \"\n            \"&lt;level&gt;{message}&lt;/level&gt;\"\n        )\n\n    # Remove the default handler added by Loguru\n    logger.remove()\n\n    # Add new handler with custom configuration\n    logger.add(\n        sys.stderr,\n        level=level.upper(),\n        format=format_string,\n    )\n\n    # Optionally disable logging for this package\n    if disable:\n        logger.disable(\"fsspeckit\")\n</code></pre>"},{"location":"api/fsspeckit.utils.misc/","title":"<code>fsspeckit.utils.misc</code> (compatibility fa\u00e7ade)","text":"<p>The <code>fsspeckit.utils.misc</code> module is a backwards-compatibility fa\u00e7ade. It re-exports a small set of helpers from <code>fsspeckit.common.misc</code> and <code>rich.progress</code> so that existing code continues to work after the domain refactor.</p> <p>New code SHOULD import from the canonical modules instead of this fa\u00e7ade.</p>"},{"location":"api/fsspeckit.utils.misc/#exposed-helpers","title":"Exposed helpers","text":"Compat name Canonical helper Canonical module <code>get_partitions_from_path</code> <code>get_partitions_from_path</code> <code>fsspeckit.common.misc</code> <code>run_parallel</code> <code>run_parallel</code> <code>fsspeckit.common.misc</code> <code>sync_dir</code> <code>sync_dir</code> <code>fsspeckit.common.misc</code> <code>sync_files</code> <code>sync_files</code> <code>fsspeckit.common.misc</code> <code>Progress</code> <code>Progress</code> <code>rich.progress.Progress</code> <p>The full, up-to-date API documentation for these helpers lives under <code>fsspeckit.common</code> in the domain package reference.</p>"},{"location":"api/fsspeckit.utils.misc/#recommended-imports","title":"Recommended imports","text":"<p>Preferred (domain packages):</p> <pre><code>from fsspeckit.common.misc import run_parallel, sync_dir\nfrom rich.progress import Progress\n</code></pre> <p>Backwards-compatible (fa\u00e7ade):</p> <pre><code>from fsspeckit.utils import run_parallel, sync_dir\nfrom fsspeckit.utils.misc import Progress\n</code></pre> <p>Use the fa\u00e7ade only when maintaining legacy code; for new development, always prefer the canonical imports from <code>fsspeckit.common.misc</code>.</p>"},{"location":"api/fsspeckit.utils.polars/","title":"<code>fsspeckit.utils.polars</code> (compatibility fa\u00e7ade)","text":"<p>The <code>fsspeckit.utils.polars</code> module is a backwards-compatibility fa\u00e7ade. It re-exports a small set of helpers from <code>fsspeckit.common.polars</code> so that older code can continue to work after the domain refactor.</p> <p>New code SHOULD import directly from <code>fsspeckit.common.polars</code>.</p>"},{"location":"api/fsspeckit.utils.polars/#exposed-helpers","title":"Exposed helpers","text":"Compat name Canonical helper Canonical module <code>opt_dtype_pl</code> <code>opt_dtype</code> <code>fsspeckit.common.polars</code> <code>pl</code> <code>pl</code> <code>fsspeckit.common.polars</code> <code>explode_all</code> <code>explode_all</code> <code>fsspeckit.common.polars</code> <code>drop_null_columns</code> <code>drop_null_columns</code> <code>fsspeckit.common.polars</code> <p>Full API documentation for these helpers is available under the <code>fsspeckit.common</code> domain package reference. This page focuses on the compatibility layer only.</p>"},{"location":"api/fsspeckit.utils.polars/#recommended-imports","title":"Recommended imports","text":"<p>Preferred (domain package):</p> <pre><code>from fsspeckit.common.polars import opt_dtype, explode_all, drop_null_columns\nimport polars as pl\n</code></pre> <p>Backwards-compatible (fa\u00e7ade):</p> <pre><code>from fsspeckit.utils.polars import opt_dtype_pl, explode_all, drop_null_columns\n</code></pre> <p>Use the fa\u00e7ade only when maintaining legacy code. For new development, prefer the canonical imports from <code>fsspeckit.common.polars</code>.</p>"},{"location":"api/fsspeckit.utils.pyarrow/","title":"<code>fsspeckit.utils.pyarrow</code> (compatibility fa\u00e7ade)","text":"<p>The <code>fsspeckit.utils.pyarrow</code> module is a backwards-compatibility fa\u00e7ade. It re-exports a subset of PyArrow-related helpers from the canonical <code>fsspeckit.datasets.pyarrow</code> module so that older code can continue to work.</p> <p>New code SHOULD import directly from <code>fsspeckit.datasets.pyarrow</code>.</p>"},{"location":"api/fsspeckit.utils.pyarrow/#exposed-helpers","title":"Exposed helpers","text":"Compat name Canonical helper Canonical module <code>cast_schema</code> <code>cast_schema</code> <code>fsspeckit.datasets.pyarrow</code> <code>convert_large_types_to_normal</code> <code>convert_large_types_to_normal</code> <code>fsspeckit.datasets.pyarrow</code> <code>dominant_timezone_per_column</code> <code>dominant_timezone_per_column</code> <code>fsspeckit.datasets.pyarrow</code> <code>merge_parquet_dataset_pyarrow</code> <code>merge_parquet_dataset_pyarrow</code> <code>fsspeckit.datasets.pyarrow</code> <code>opt_dtype_pa</code> <code>opt_dtype</code> <code>fsspeckit.datasets.pyarrow</code> <code>standardize_schema_timezones</code> <code>standardize_schema_timezones</code> <code>fsspeckit.datasets.pyarrow</code> <code>standardize_schema_timezones_by_majority</code> <code>standardize_schema_timezones_by_majority</code> <code>fsspeckit.datasets.pyarrow</code> <p>Detailed parameter and return-type documentation for these helpers is available in the <code>fsspeckit.datasets</code> API reference. This page documents only the legacy compatibility surface.</p>"},{"location":"api/fsspeckit.utils.pyarrow/#recommended-imports","title":"Recommended imports","text":"<p>Preferred (domain packages):</p> <pre><code>from fsspeckit.datasets.pyarrow import merge_parquet_dataset_pyarrow\n</code></pre> <p>Backwards-compatible (fa\u00e7ade):</p> <pre><code>from fsspeckit.utils.pyarrow import merge_parquet_dataset_pyarrow\n</code></pre> <p>Use the fa\u00e7ade only to keep older code working while you migrate imports to <code>fsspeckit.datasets.pyarrow</code>.</p>"},{"location":"api/fsspeckit.utils.sql/","title":"<code>fsspeckit.utils.sql</code> (compatibility fa\u00e7ade)","text":"<p>The <code>fsspeckit.utils.sql</code> module is a backwards-compatibility fa\u00e7ade. It exposes a single helper from the canonical <code>fsspeckit.sql</code> package.</p> <p>New code SHOULD import from <code>fsspeckit.sql</code> directly.</p>"},{"location":"api/fsspeckit.utils.sql/#exposed-helper","title":"Exposed helper","text":"Compat name Canonical helper Canonical module <code>get_table_names</code> <code>get_table_names</code> <code>fsspeckit.sql</code>"},{"location":"api/fsspeckit.utils.sql/#example","title":"Example","text":"<p>Preferred (domain package):</p> <pre><code>from fsspeckit.sql import get_table_names\n\nquery = \"SELECT a FROM my_table WHERE b &gt; 10\"\ntables = get_table_names(query)\nprint(tables)  # ['my_table']\n</code></pre> <p>Backwards-compatible (fa\u00e7ade):</p> <pre><code>from fsspeckit.utils.sql import get_table_names\n</code></pre> <p>Parameter and return-type details for <code>get_table_names</code> are documented in the <code>fsspeckit.sql</code> API reference. This page focuses on the compatibility layer only.</p>"},{"location":"api/fsspeckit.utils.types/","title":"<code>fsspeckit.utils.types</code> (legacy documentation)","text":"<p>There is no standalone <code>fsspeckit.utils.types</code> module in the current codebase. Type-conversion helpers live in the canonical <code>fsspeckit.common.types</code> module and are also exposed via the top-level <code>fsspeckit.utils</code> fa\u00e7ade.</p> <p>This page exists to document the compatibility mapping and to help migrate older code.</p>"},{"location":"api/fsspeckit.utils.types/#canonical-helpers","title":"Canonical helpers","text":"<p>The primary helpers are:</p> <ul> <li><code>dict_to_dataframe</code></li> <li><code>to_pyarrow_table</code></li> </ul> <p>They live in <code>fsspeckit.common.types</code>.</p>"},{"location":"api/fsspeckit.utils.types/#compatibility-mapping","title":"Compatibility mapping","text":"Usage style Import path Canonical (recommended) <code>from fsspeckit.common.types import dict_to_dataframe, to_pyarrow_table</code> Fa\u00e7ade (backwards compatible) <code>from fsspeckit.utils import dict_to_dataframe, to_pyarrow_table</code> <p>For full parameter/return documentation and examples, see the <code>fsspeckit.common</code> API reference. This page is intentionally minimal to avoid duplicating and drifting from the canonical docs.</p>"},{"location":"explanation/","title":"Architecture &amp; Concepts","text":"<p>Use these pages to understand how fsspeckit is designed and why its APIs look the way they do.</p> <ul> <li>Architecture Overview \u2013 System design, domain boundaries, and integration patterns</li> <li>Key Concepts \u2013 Path safety, datasets vs files, SQL filters, optional dependencies</li> </ul>"},{"location":"explanation/architecture/","title":"Architecture Overview","text":"<p><code>fsspeckit</code> extends <code>fsspec</code> with enhanced filesystem utilities and storage option configurations for working with various data formats and storage backends. This document provides a technical reference for understanding the system's design and implementation patterns.</p>"},{"location":"explanation/architecture/#executive-overview","title":"Executive Overview","text":""},{"location":"explanation/architecture/#purpose-and-value-proposition","title":"Purpose and Value Proposition","text":"<p><code>fsspeckit</code> provides enhanced data processing capabilities through a modular, domain-driven architecture that focuses on filesystem operations, storage configuration, and cross-framework SQL filter translation. The system enables users to work with multiple storage backends and data processing frameworks through unified APIs.</p>"},{"location":"explanation/architecture/#core-architectural-principles","title":"Core Architectural Principles","text":"<ol> <li>Domain-Driven Design: Clear separation of concerns through domain-specific packages</li> <li>Backend Neutrality: Consistent interfaces across different storage providers</li> <li>Practical Utilities: Focus on implemented features rather than theoretical capabilities</li> <li>Backwards Compatibility: Migration path for existing users</li> <li>Type Safety: Strong typing and validation throughout the codebase</li> </ol>"},{"location":"explanation/architecture/#target-use-cases","title":"Target Use Cases","text":"<ul> <li>Multi-Cloud Data Access: Unified access to AWS S3, Azure Blob, Google Cloud Storage</li> <li>Dataset Operations: High-performance dataset operations with DuckDB and PyArrow</li> <li>Git Integration: Filesystem access to GitHub and GitLab repositories</li> <li>SQL Filter Translation: Cross-framework SQL expression conversion</li> <li>Storage Configuration: Environment-based storage option management</li> </ul>"},{"location":"explanation/architecture/#backwards-compatibility","title":"Backwards Compatibility","text":"<ul> <li>Utils Fa\u00e7ade: The <code>fsspeckit.utils</code> package serves as a backwards-compatible fa\u00e7ade that re-exports from domain packages (<code>datasets</code>, <code>sql</code>, <code>common</code>). </li> </ul>"},{"location":"explanation/architecture/#supported-imports","title":"Supported Imports","text":"<p>The following imports are supported for backwards compatibility: - <code>setup_logging</code> - from <code>fsspeckit.common.logging</code> - <code>run_parallel</code> - from <code>fsspeckit.common.misc</code>  - <code>get_partitions_from_path</code> - from <code>fsspeckit.common.misc</code> - <code>to_pyarrow_table</code> - from <code>fsspeckit.common.types</code> - <code>dict_to_dataframe</code> - from <code>fsspeckit.common.types</code> - <code>opt_dtype_pl</code> - from <code>fsspeckit.common.polars</code> - <code>opt_dtype_pa</code> - from <code>fsspeckit.common.types</code> - <code>cast_schema</code> - from <code>fsspeckit.common.types</code> - <code>convert_large_types_to_normal</code> - from <code>fsspeckit.common.types</code> - <code>pl</code> - from <code>fsspeckit.common.polars</code> - <code>sync_dir</code> - from <code>fsspeckit.common.misc</code> - <code>sync_files</code> - from <code>fsspeckit.common.misc</code> - <code>DuckDBParquetHandler</code> - from <code>fsspeckit.datasets</code> - <code>Progress</code> - from <code>fsspeckit.utils.misc</code> (shim for <code>rich.progress.Progress</code>)</p>"},{"location":"explanation/architecture/#migration-path","title":"Migration Path","text":"<ul> <li>Existing Code: All existing <code>fsspeckit.utils</code> imports continue to work unchanged</li> <li>New Development: New code should import directly from domain packages for better discoverability</li> <li>Deprecated Paths: Deeper import paths like <code>fsspeckit.utils.misc.Progress</code> are deprecated but functional for at least one major version</li> </ul>"},{"location":"explanation/architecture/#deprecation-notices","title":"Deprecation Notices","text":"<ul> <li><code>fsspeckit.utils</code> module is deprecated and exists only for backwards compatibility</li> <li>New implementation code should not live in <code>fsspeckit.utils</code></li> <li>Use domain-specific imports: <code>fsspeckit.datasets</code>, <code>fsspeckit.sql</code>, <code>fsspeckit.common</code> for new development</li> </ul>"},{"location":"explanation/architecture/#architectural-decision-records-adrs","title":"Architectural Decision Records (ADRs)","text":""},{"location":"explanation/architecture/#adr-001-domain-package-architecture","title":"ADR-001: Domain Package Architecture","text":"<p>Decision: Organize fsspeckit into domain-specific packages (core, storage_options, datasets, sql, common) rather than a monolithic structure.</p> <p>Rationale: - Separation of Concerns: Each domain has distinct responsibilities and user patterns - Discoverability: Users can easily find relevant functionality without searching large modules - Testing: Isolated testing for each domain with clear boundaries - Maintenance: Changes to one domain don't impact others</p> <p>Migration Path: Existing imports through <code>fsspeckit.utils</code> continue working while new code uses domain-specific imports.</p>"},{"location":"explanation/architecture/#adr-002-backend-neutral-planning-layer","title":"ADR-002: Backend-Neutral Planning Layer","text":"<p>Decision: Centralize merge and maintenance planning logic in the core package with backend-specific delegates.</p> <p>Rationale: - Consistency: All backends use identical merge semantics and validation - Maintainability: Single source of truth for business logic - Performance: Shared optimization strategies across implementations - Testing: Consistent behavior validation across all backends</p> <p>Implementation: Both DuckDB and PyArrow backends delegate to <code>core.merge</code> and <code>core.maintenance</code> for planning, validation, and statistics calculation.</p>"},{"location":"explanation/architecture/#adr-003-storage-options-factory-pattern","title":"ADR-003: Storage Options Factory Pattern","text":"<p>Decision: Implement factory pattern for storage configuration with environment-based setup.</p> <p>Rationale: - Portability: Code works across different cloud providers without changes - Configuration: Environment-based configuration for production deployments - Flexibility: Users can override defaults for specific requirements</p> <p>Implementation Pattern: <pre><code># Protocol-agnostic approach\nfrom fsspeckit.storage_options import storage_options_from_env\noptions = storage_options_from_env(\"s3\")\n\n# URI-based inference\nfrom fsspeckit.core.filesystem import filesystem\nfs = filesystem(\"s3://bucket/path\")  # Auto-detects protocol\n</code></pre></p>"},{"location":"explanation/architecture/#core-architecture-deep-dive","title":"Core Architecture Deep Dive","text":""},{"location":"explanation/architecture/#domain-package-breakdown","title":"Domain Package Breakdown","text":""},{"location":"explanation/architecture/#fsspeckitcore-foundation-layer","title":"<code>fsspeckit.core</code> - Foundation Layer","text":"<p>The core package provides fundamental filesystem APIs and path safety utilities:</p> <p>Key Components:</p> <ul> <li> <p><code>AbstractFileSystem</code> (<code>core/ext.py</code>): Extended base class with enhanced functionality   <pre><code>class AbstractFileSystem(fsspec.AbstractFileSystem):\n    \"\"\"Enhanced filesystem with smart path handling and protocol inference.\"\"\"\n</code></pre></p> </li> <li> <p><code>DirFileSystem</code>: Path-safe filesystem wrapper   <pre><code>class DirFileSystem(AbstractFileSystem):\n    \"\"\"Filesystem wrapper that restricts operations within specified directories.\"\"\"\n</code></pre></p> </li> <li> <p><code>filesystem()</code> function: Enhanced filesystem creation with URI inference   <pre><code>def filesystem(protocol: str, **storage_options) -&gt; AbstractFileSystem:\n    \"\"\"Create filesystem with protocol inference and validation.\"\"\"\n</code></pre></p> </li> </ul> <p>Integration Patterns: - Protocol detection and inference from URIs - Smart path normalization and validation - Directory confinement for security</p>"},{"location":"explanation/architecture/#fsspeckitstorage_options-configuration-layer","title":"<code>fsspeckit.storage_options</code> - Configuration Layer","text":"<p>Manages storage configurations for cloud and Git providers:</p> <p>Factory Pattern Implementation: <pre><code>def from_dict(protocol: str, storage_options: dict) -&gt; BaseStorageOptions\ndef from_env(protocol: str) -&gt; BaseStorageOptions\ndef storage_options_from_uri(uri: str) -&gt; BaseStorageOptions\n</code></pre></p> <p>Provider Implementations: - <code>AwsStorageOptions</code>: AWS S3 configuration with region, credentials, and endpoint settings - <code>GcsStorageOptions</code>: Google Cloud Storage setup - <code>AzureStorageOptions</code>: Azure Blob Storage configuration - <code>GitHubStorageOptions</code>: GitHub repository access with token authentication - <code>GitLabStorageOptions</code>: GitLab repository configuration</p> <p>Key Features: - YAML serialization for persistent configuration - Environment variable auto-configuration - Protocol inference from URIs - Unified interface across all providers</p>"},{"location":"explanation/architecture/#fsspeckitdatasets-data-processing-layer","title":"<code>fsspeckit.datasets</code> - Data Processing Layer","text":"<p>High-performance dataset operations for large-scale data processing:</p> <p>DuckDB Implementation: <pre><code>class DuckDBParquetHandler:\n    \"\"\"High-performance parquet operations with atomic guarantees.\"\"\"\n\n    def __init__(self, storage_options=None):\n        self.storage_options = storage_options\n\n    def write_parquet_dataset(self, table, path, **kwargs):\n        \"\"\"Atomic dataset writing with backup-and-restore.\"\"\"\n\n    def execute_sql(self, query, **kwargs):\n        \"\"\"Parameterized SQL execution with fsspec integration.\"\"\"\n</code></pre></p> <p>PyArrow Implementation: <pre><code># Optimization and compaction functions\ndef optimize_parquet_dataset_pyarrow(dataset_path, **kwargs):\n    \"\"\"Z-ordering and file size optimization.\"\"\"\n\ndef compact_parquet_dataset_pyarrow(dataset_path, **kwargs):\n    \"\"\"Dataset compaction with atomic operations.\"\"\"\n</code></pre></p> <p>Backend Integration: - Shared merge logic from <code>core.merge</code> - Common maintenance operations from <code>core.maintenance</code> - Consistent statistics and validation across backends</p>"},{"location":"explanation/architecture/#fsspeckitsql-query-translation-layer","title":"<code>fsspeckit.sql</code> - Query Translation Layer","text":"<p>SQL-to-filter translation for cross-framework compatibility:</p> <p>Core Functions: <pre><code>def sql2pyarrow_filter(string: str, schema: pa.Schema) -&gt; pc.Expression:\n    \"\"\"Convert SQL WHERE clause to PyArrow filter expression.\"\"\"\n\ndef sql2polars_filter(string: str, schema: pl.Schema) -&gt; pl.Expr:\n    \"\"\"Convert SQL WHERE clause to Polars filter expression.\"\"\"\n</code></pre></p> <p>Integration Points: - Cross-framework SQL expression translation - Schema-aware filter generation - Unified SQL parsing using sqlglot - Table name extraction for validation</p>"},{"location":"explanation/architecture/#fsspeckitcommon-shared-utilities-layer","title":"<code>fsspeckit.common</code> - Shared Utilities Layer","text":"<p>Cross-cutting utilities used across all domains:</p> <p>Parallel Processing: <pre><code>def run_parallel(\n    func: Callable,\n    data: Sequence[Any],\n    max_workers: Optional[int] = None,\n    progress: bool = True\n) -&gt; List[Any]:\n    \"\"\"Parallel execution with progress tracking and error handling.\"\"\"\n</code></pre></p> <p>Type Conversion: <pre><code>def convert_large_types_to_normal(table: pa.Table) -&gt; pa.Table:\n    \"\"\"Convert large string types to normal string types for compatibility.\"\"\"\n\ndef dict_to_dataframe(data: Dict[str, Any], library: str = \"polars\"):\n    \"\"\"Convert dictionaries to Polars/Pandas DataFrames.\"\"\"\n</code></pre></p> <p>File Operations: <pre><code>def sync_dir(src_fs, dst_fs, src_path: str, dst_path: str, **kwargs):\n    \"\"\"Synchronize directories between filesystems.\"\"\"\n\ndef extract_partitions(path: str, **kwargs) -&gt; Dict[str, str]:\n    \"\"\"Extract partition information from file paths.\"\"\"\n</code></pre></p>"},{"location":"explanation/architecture/#fsspeckitutils-backwards-compatibility-facade","title":"<code>fsspeckit.utils</code> - Backwards Compatibility Fa\u00e7ade","text":"<p>Re-exports selected helpers from domain packages for backwards compatibility:</p> <pre><code># Re-exports for backwards compatibility\nfrom ..common.misc import run_parallel\nfrom ..common.datetime import timestamp_from_string\nfrom ..common.types import dict_to_dataframe, to_pyarrow_table\n</code></pre> <p>Migration Strategy: - Immediate compatibility with existing code - Gradual migration to domain-specific imports - Deprecation warnings for discouraged patterns</p>"},{"location":"explanation/architecture/#integration-patterns","title":"Integration Patterns","text":""},{"location":"explanation/architecture/#cross-domain-communication","title":"Cross-Domain Communication","text":"<p>Import Patterns: <pre><code># Core \u2192 Storage Options\nfrom fsspeckit.storage_options.base import BaseStorageOptions\n\n# Datasets \u2192 Core Merge Logic\nfrom fsspeckit.core.merge import (\n    MergeStrategy, validate_merge_inputs,\n    calculate_merge_stats, check_null_keys\n)\n\n# Storage Options \u2192 Core Filesystem\nfrom fsspeckit.core.filesystem import filesystem\n</code></pre></p> <p>Configuration Flow: <pre><code># Environment-based configuration\nfrom fsspeckit.storage_options import storage_options_from_env\noptions = storage_options_from_env(\"s3\")\nfs = filesystem(\"s3\", storage_options=options.to_dict())\n\n# URI-based configuration\nfs = filesystem(\"s3://bucket/path\")  # Auto-detects and configures\n</code></pre></p>"},{"location":"explanation/architecture/#error-handling-architecture","title":"Error Handling Architecture","text":"<p>Consistent Exception Types: - <code>ValueError</code> for configuration and validation errors - <code>FileNotFoundError</code> for missing resources - <code>PermissionError</code> for access control issues - Custom exceptions for domain-specific errors</p>"},{"location":"explanation/architecture/#security-architecture","title":"Security Architecture","text":"<p><code>fsspeckit</code> implements security best practices through the <code>fsspeckit.common.security</code> module, providing utilities to prevent common vulnerabilities in data processing workflows.</p> <p>Core Security Helpers:</p> <ol> <li>Path Validation: Prevent path traversal attacks and ensure operations stay within allowed directories</li> <li><code>validate_path()</code>: Validates filesystem paths and enforces base directory confinement</li> <li> <p>Integration with <code>DirFileSystem</code> for path-safe operations</p> </li> <li> <p>Credential Scrubbing: Protect sensitive information in logs and error messages</p> </li> <li><code>scrub_credentials()</code>: Removes credential-like values from strings</li> <li><code>scrub_exception()</code>: Safely formats exceptions without exposing secrets</li> <li> <p><code>safe_format_error()</code>: Creates secure error messages for production logging</p> </li> <li> <p>Compression Safety: Prevent codec injection attacks</p> </li> <li> <p><code>validate_compression_codec()</code>: Ensures only safe codecs (snappy, gzip, lz4, zstd, brotli) are used</p> </li> <li> <p>Column Validation: Prevent column injection in SQL-like operations</p> </li> <li><code>validate_columns()</code>: Validates requested columns exist in schema</li> </ol> <p>Production Security Patterns:</p> <p>The security helpers are integrated throughout fsspeckit's architecture:</p> <pre><code># Filesystem operations use path validation\nsafe_fs = DirFileSystem(fs=base_fs, path=\"/data/allowed\")\n\n# Dataset operations validate inputs\nhandler.compact_parquet_dataset(\n    path=validate_path(dataset_path, base_dir=\"/data/allowed\"),\n    compression=validate_compression_codec(user_codec)\n)\n\n# Error messages are scrubbed before logging\nlogger.error(safe_format_error(\"read file\", path=path, error=e))\n</code></pre> <p>Security in Production:</p> <p>For production deployments, the architecture emphasizes: - Credential scrubbing in all error paths - Path validation for all filesystem operations - Safe error formatting for observability - Integration with centralized logging systems - Multi-tenant isolation through <code>DirFileSystem</code></p> <p>These security measures are particularly important for: - Multi-cloud deployments with sensitive credentials - Multi-tenant environments requiring strict isolation - Compliance requirements (SOC2, PCI-DSS, etc.) - Centralized logging and monitoring systems</p> <p>Data Flow Patterns</p> <p>Typical Data Processing Pipeline: <pre><code># 1. Configuration Setup\nfrom fsspeckit.storage_options import storage_options_from_env\nfrom fsspeckit.core.filesystem import filesystem\nfrom fsspeckit.datasets import DuckDBParquetHandler\n\nstorage_options = storage_options_from_env(\"s3\")\nfs = filesystem(\"s3\", storage_options=storage_options.to_dict())\n\n# 2. Data Processing\nhandler = DuckDBParquetHandler(storage_options=storage_options.to_dict())\n\n# Data ingestion and processing\nhandler.write_parquet_dataset(data, \"s3://bucket/raw/\")\nresult = handler.execute_sql(\"\"\"\n    SELECT region, SUM(amount) as total\n    FROM parquet_scan('s3://bucket/raw/')\n    GROUP BY region\n\"\"\")\n\n# Output\nhandler.write_parquet_dataset(result, \"s3://bucket/processed/\")\n</code></pre></p> <p>Cross-Storage Operations: <pre><code># Sync between cloud providers\nfrom fsspeckit.core.filesystem import filesystem\nfrom fsspeckit.common import sync_dir\n\nsrc_fs = filesystem(\"s3\", storage_options=s3_options)\ndst_fs = filesystem(\"az\", storage_options=azure_options)\n\nsync_dir(\n    src_fs, dst_fs,\n    \"s3://bucket/data/\",\n    \"az://container/data/\",\n    progress=True\n)\n</code></pre></p>"},{"location":"explanation/architecture/#performance-and-scalability-architecture","title":"Performance and Scalability Architecture","text":""},{"location":"explanation/architecture/#caching-strategy","title":"Caching Strategy","text":"<p>Filesystem Level Caching: - Support for fsspec's built-in caching mechanisms - Optional directory structure preservation - Configurable cache size and location</p>"},{"location":"explanation/architecture/#parallel-processing-architecture","title":"Parallel Processing Architecture","text":"<p>Worker Pool Management: <pre><code>from fsspeckit.common import run_parallel\n\ndef process_file(file_path):\n    # Process individual file\n    return processed_data\n\n# Parallel execution with automatic resource management\nresults = run_parallel(process_file, file_list, max_workers=8)\n</code></pre></p> <p>Resource Optimization: - Automatic worker count detection based on CPU cores - Memory-aware chunking for large datasets - Progress tracking and error handling</p>"},{"location":"explanation/architecture/#memory-management","title":"Memory Management","text":"<p>Efficient Data Processing: - Streaming operations for large files - Chunked processing with configurable batch sizes - Type conversion for PyArrow compatibility</p>"},{"location":"explanation/architecture/#extension-points-and-customization","title":"Extension Points and Customization","text":""},{"location":"explanation/architecture/#adding-new-storage-providers","title":"Adding New Storage Providers","text":"<p>Custom Storage Options: <pre><code>from fsspeckit.storage_options.base import BaseStorageOptions\n\nclass CustomStorageOptions(BaseStorageOptions):\n    \"\"\"Custom storage provider configuration.\"\"\"\n\n    provider: str = \"custom\"\n    custom_endpoint: Optional[str] = None\n\n    def to_filesystem(self) -&gt; AbstractFileSystem:\n        \"\"\"Create filesystem instance.\"\"\"\n        return CustomFileSystem(\n            endpoint=self.custom_endpoint,\n            **self.get_storage_options()\n        )\n</code></pre></p>"},{"location":"explanation/architecture/#custom-processing-backends","title":"Custom Processing Backends","text":"<p>Extending Dataset Operations: <pre><code>class CustomDatasetHandler:\n    \"\"\"Custom dataset processing backend.\"\"\"\n\n    def __init__(self, storage_options=None):\n        self.storage_options = storage_options\n\n    def write_dataset(self, data, path, **kwargs):\n        \"\"\"Custom dataset writing logic.\"\"\"\n        pass\n\n    def read_dataset(self, path, **kwargs):\n        \"\"\"Custom dataset reading logic.\"\"\"\n        pass\n</code></pre></p>"},{"location":"explanation/architecture/#migration-guide","title":"Migration Guide","text":"<p>For details on historical changes between versions, consult the project changelog and release notes.</p>"},{"location":"explanation/architecture/#quick-reference","title":"Quick Reference","text":"<p>Step 1: Update Imports <pre><code># Old imports (still work via utils fa\u00e7ade)\nfrom fsspec_utils import run_parallel\n\n# New recommended imports\nfrom fsspeckit.common import run_parallel\n</code></pre></p> <p>Step 2: Update Configuration <pre><code># Old configuration style\nstorage_options = {\"key\": \"value\", \"secret\": \"secret\"}\n\n# New configuration style\nfrom fsspeckit.storage_options import AwsStorageOptions\nstorage_options = AwsStorageOptions(\n    access_key=\"key\",\n    secret_key=\"secret\"\n)\n</code></pre></p> <p>Step 3: Update Filesystem Creation <pre><code># Old method\nfs = fsspec.filesystem(\"s3\", **storage_options)\n\n# New method\nfrom fsspeckit.core.filesystem import filesystem\nfs = filesystem(\"s3\", storage_options=storage_options.to_dict())\n</code></pre></p>"},{"location":"explanation/architecture/#future-features-not-yet-implemented","title":"Future Features (Not Yet Implemented)","text":"<p>The following features are planned but not yet implemented:</p> <ul> <li>Performance Tracking: Built-in performance monitoring and metrics collection</li> <li>Plugin Registry: Dynamic plugin discovery and registration system</li> <li>Circuit Breaker Patterns: Advanced resilience patterns for distributed systems</li> <li>Delta Lake Integration: Delta Lake write helpers and compatibility</li> <li>Advanced Monitoring: Comprehensive observability and health checking</li> </ul>"},{"location":"explanation/architecture/#conclusion","title":"Conclusion","text":"<p>The fsspeckit architecture provides a practical foundation for data processing across multiple storage backends and processing frameworks. The domain-driven design ensures clear separation of concerns while maintaining consistent interfaces and behavior across all components.</p> <p>The modular architecture enables easy extension and customization while maintaining backwards compatibility for existing users. Built-in performance optimizations and cross-framework compatibility make fsspeckit suitable for data processing workflows.</p> <p>For specific implementation details and code examples, refer to the individual domain package documentation.</p>"},{"location":"explanation/concepts/","title":"Concepts","text":"<p>This page explains key concepts that help you understand how and why fsspeckit's APIs are designed the way they are.</p>"},{"location":"explanation/concepts/#path-safety-and-dirfilesystem","title":"Path Safety and DirFileSystem","text":""},{"location":"explanation/concepts/#why-path-safety-matters","title":"Why Path Safety Matters","text":"<p>Traditional filesystems allow access to any path the user has permissions for, which can create security vulnerabilities:</p> <pre><code># Dangerous: Unrestricted filesystem access\nimport os\nos.open(\"../../../etc/passwd\", os.O_RDONLY)  # Can access system files\n\n# Safe: Path-constrained access\nfrom fsspeckit.core.filesystem import DirFileSystem, filesystem\n\n# Automatic path safety (default)\nsafe_fs = filesystem(\"/data/allowed\", dirfs=True)\nsafe_fs.open(\"../../../etc/passwd\", \"r\")  # Raises ValueError/PermissionError\n\n# Manual path confinement\nbase_fs = filesystem(\"file\")\nconfined_fs = DirFileSystem(fs=base_fs, path=\"/data/allowed\")\n</code></pre>"},{"location":"explanation/concepts/#dirfilesystem-behavior","title":"DirFileSystem Behavior","text":"<p><code>DirFileSystem</code> wraps any filesystem and confines all operations to a specified base directory:</p> <ul> <li>Path Resolution: All paths are resolved relative to the base directory</li> <li>Escape Prevention: Attempts to access parent directories are blocked</li> <li>Security: Prevents path traversal attacks in multi-tenant environments</li> <li>Isolation: Each user/process gets its own safe filesystem space</li> </ul>"},{"location":"explanation/concepts/#use-cases","title":"Use Cases","text":"<pre><code># Multi-tenant data processing\ntenant_fs = filesystem(f\"/data/tenants/{tenant_id}\", dirfs=True)\n\n# Test isolation\ntest_fs = filesystem(\"/tmp/test_data\", dirfs=True)\n\n# Production data isolation\nprod_fs = filesystem(\"/data/production\", dirfs=True)\n</code></pre>"},{"location":"explanation/concepts/#dataset-vs-file-level-operations","title":"Dataset vs File-Level Operations","text":""},{"location":"explanation/concepts/#why-datasets-matter","title":"Why Datasets Matter","text":"<p>Individual file operations are simple but don't scale to modern data workloads:</p> <pre><code># File-level: Manual and error-prone\nimport glob\nimport pandas as pd\n\ndataframes = []\nfor file_path in glob.glob(\"data/*.parquet\"):\n    df = pd.read_parquet(file_path)\n    dataframes.append(df)\n\ncombined_df = pd.concat(dataframes, ignore_index=True)\n\n# Dataset-level: Automatic and optimized\nfrom fsspeckit.datasets.pyarrow import process_dataset_in_batches\n\nfor batch in process_dataset_in_batches(\n    dataset_path=\"data/\",\n    batch_size=\"100MB\",\n    process_func=lambda batch: process_batch(batch)\n):\n    # Process optimized batches\n    pass\n</code></pre>"},{"location":"explanation/concepts/#dataset-advantages","title":"Dataset Advantages","text":"<p>Schema Evolution: Datasets handle schema changes across files <pre><code># Datasets automatically handle schema differences\ntable = fs.read_parquet(\"data/*.parquet\", concat=True)  # Schema unification\n</code></pre></p> <p>Partitioning: Datasets understand partitioned directory structures <pre><code># Automatic partition discovery\ndataset = fs.pyarrow_dataset(\"partitioned_data/\")\n# Reads: data/year=2023/month=01/*.parquet, data/year=2023/month=02/*.parquet\n</code></pre></p> <p>Pushdown Operations: Datasets can push filters and projections to storage <pre><code># Efficient filtering at storage level\ndataset = fs.pyarrow_dataset(\"large_dataset/\")\nfiltered = dataset.to_table(\n    filter=pyarrow.compute.greater(dataset.column(\"value\"), 100),\n    columns=[\"id\", \"name\", \"value\"]  # Column projection\n)\n</code></pre></p>"},{"location":"explanation/concepts/#when-to-use-each","title":"When to Use Each","text":"<p>File-level operations: - Small numbers of files - Simple read/write operations - When you need explicit file paths - Legacy file processing workflows</p> <p>Dataset operations: - Large numbers of files - Complex data processing pipelines - When schema evolution is needed - Partitioned data structures - Analytics workloads</p>"},{"location":"explanation/concepts/#sql-filter-abstraction","title":"SQL Filter Abstraction","text":""},{"location":"explanation/concepts/#why-sql-filters","title":"Why SQL Filters","text":"<p>Different data frameworks have different filter syntaxes, creating maintenance overhead:</p> <pre><code># Framework-specific filtering (complex to maintain)\n# PyArrow\nimport pyarrow.compute as pc\npyarrow_filter = pc.and_(\n    pc.greater(pc.field(\"value\"), 100),\n    pc.equal(pc.field(\"category\"), \"important\")\n)\n\n# Polars\nimport polars as pl\npolars_filter = (pl.col(\"value\") &gt; 100) &amp; (pl.col(\"category\") == \"important\")\n\n# Pandas\npandas_filter = (df[\"value\"] &gt; 100) &amp; (df[\"category\"] == \"important\")\n</code></pre>"},{"location":"explanation/concepts/#sql-filter-benefits","title":"SQL Filter Benefits","text":"<p>Unified Syntax: Write filters once using familiar SQL <pre><code>from fsspeckit.sql.filters import sql2pyarrow_filter, sql2polars_filter\n\nsql_filter = \"value &gt; 100 AND category = 'important'\"\n\n# Convert to any framework\npyarrow_filter = sql2pyarrow_filter(sql_filter, schema)\npolars_filter = sql2polars_filter(sql_filter, schema)\n</code></pre></p> <p>Cross-Framework Compatibility: Same filter works everywhere <pre><code># Use same filter across different frameworks\nfilters = [\n    \"amount &gt; 1000\",\n    \"category IN ('A', 'B', 'C')\",\n    \"timestamp &gt;= '2023-01-01'\",\n    \"status = 'active'\"\n]\n\nfor filter_sql in filters:\n    # Works with PyArrow\n    arrow_expr = sql2pyarrow_filter(filter_sql, arrow_schema)\n\n    # Works with Polars\n    polars_expr = sql2polars_filter(filter_sql, polars_schema)\n\n    # Same logic, different frameworks\n</code></pre></p> <p>Schema Awareness: SQL filters are converted based on actual data types <pre><code>import pyarrow as pa\n\nschema = pa.schema([\n    (\"id\", pa.int64()),\n    (\"name\", pa.string()),\n    (\"value\", pa.float64()),\n    (\"timestamp\", pa.timestamp(\"us\"))\n])\n\n# Type-aware conversion\nfilter_expr = sql2pyarrow_filter(\"value &gt; 100\", schema)  # Uses float comparison\nfilter_expr = sql2pyarrow_filter(\"name = 'test'\", schema)  # Uses string comparison\n</code></pre></p>"},{"location":"explanation/concepts/#supported-sql-features","title":"Supported SQL Features","text":"<p>Comparison Operators: <code>=</code>, <code>!=</code>, <code>&gt;</code>, <code>&gt;=</code>, <code>&lt;</code>, <code>&lt;=</code>, <code>BETWEEN</code> Logical Operators: <code>AND</code>, <code>OR</code>, <code>NOT</code> String Operations: <code>LIKE</code>, <code>IN</code>, <code>NOT IN</code> Null Handling: <code>IS NULL</code>, <code>IS NOT NULL</code> Date Functions: <code>YEAR()</code>, <code>MONTH()</code>, <code>DAY()</code>, <code>DATE()</code></p>"},{"location":"explanation/concepts/#lazy-optional-dependencies","title":"Lazy Optional Dependencies","text":""},{"location":"explanation/concepts/#why-lazy-dependencies","title":"Why Lazy Dependencies","text":"<p>fsspeckit supports many optional features, but not all users need all dependencies:</p> <pre><code># Problem: Heavy dependencies for all users\n# Traditional approach forces all dependencies:\npip install fsspeckit duckdb pyarrow polars sqlglot orjson pandas\n\n# Even if you only need basic filesystem operations\n</code></pre>"},{"location":"explanation/concepts/#lazy-loading-benefits","title":"Lazy Loading Benefits","text":"<p>Lightweight Installation: Core functionality works without heavy dependencies <pre><code># This works with minimal installation\nfrom fsspeckit.core.filesystem import filesystem\nfrom fsspeckit.storage_options import AwsStorageOptions\n\nfs = filesystem(\"s3\", storage_options=options.to_dict())\n</code></pre></p> <p>On-Demand Loading: Dependencies loaded only when features are used <pre><code>from fsspeckit.datasets import DuckDBParquetHandler\n\n# Import succeeds even without duckdb\nhandler = DuckDBParquetHandler()\n\n# duckdb is loaded only when actually used\ntry:\n    handler.write_parquet_dataset(data, \"path/\")\nexcept ImportError as e:\n    print(f\"Install with: pip install duckdb\")\n</code></pre></p> <p>Clear Error Messages: Users get helpful messages about missing dependencies <pre><code># Clear guidance on what to install\nImportError: The 'duckdb' package is required for DuckDBParquetHandler.\nInstall it with: pip install duckdb\n</code></pre></p>"},{"location":"explanation/concepts/#dependency-management","title":"Dependency Management","text":"<p>Core Dependencies (always required): - <code>fsspec</code> - Filesystem abstraction - <code>pydantic</code> - Configuration validation</p> <p>Optional Dependencies (loaded on demand): - <code>duckdb</code> - Dataset operations and SQL analytics - <code>pyarrow</code> - Dataset operations and type conversion - <code>polars</code> - DataFrame operations and optimization - <code>sqlglot</code> - SQL filter translation - <code>orjson</code> - Fast JSON processing</p>"},{"location":"explanation/concepts/#storage-options-factory-pattern","title":"Storage Options Factory Pattern","text":""},{"location":"explanation/concepts/#why-factory-pattern","title":"Why Factory Pattern","text":"<p>Different cloud providers have different configuration patterns, creating complexity:</p> <pre><code># Provider-specific configuration (complex to maintain)\n# AWS\naws_config = {\n    \"key\": \"AKIA...\",\n    \"secret\": \"secret...\",\n    \"region\": \"us-east-1\",\n    \"client_kwargs\": {\"region_name\": \"us-east-1\"}\n}\n\n# GCS\ngcs_config = {\n    \"token\": \"path/to/service-account.json\",\n    \"project\": \"my-project\"\n}\n\n# Azure\nazure_config = {\n    \"account_name\": \"storageaccount\",\n    \"account_key\": \"secret...\"\n}\n</code></pre>"},{"location":"explanation/concepts/#factory-pattern-benefits","title":"Factory Pattern Benefits","text":"<p>Unified Interface: Same pattern across all providers <pre><code>from fsspeckit.storage_options import (\n    AwsStorageOptions,\n    GcsStorageOptions,\n    AzureStorageOptions,\n    storage_options_from_env\n)\n\n# Same pattern for all providers\naws_opts = AwsStorageOptions(region=\"us-east-1\", ...)\ngcs_opts = GcsStorageOptions(project=\"my-project\", ...)\nazure_opts = AzureStorageOptions(account_name=\"account\", ...)\n\n# All have same methods\naws_fs = aws_opts.to_filesystem()\ngcs_fs = gcs_opts.to_filesystem()\nazure_fs = azure_opts.to_filesystem()\n</code></pre></p> <p>Environment Integration: Automatic configuration from environment variables <pre><code># Load from environment (production pattern)\naws_options = storage_options_from_env(\"s3\")  # Reads AWS_* env vars\ngcs_options = storage_options_from_env(\"gs\")   # Reads GOOGLE_* env vars\nazure_options = storage_options_from_env(\"az\")  # Reads AZURE_* env vars\n</code></pre></p> <p>Type Safety: Structured configuration with validation <pre><code># Type-safe configuration\naws_options = AwsStorageOptions(\n    region=\"us-east-1\",  # Validated\n    access_key_id=\"key\",   # Validated\n    secret_access_key=\"secret\"  # Validated\n)\n\n# vs. raw dictionaries (error-prone)\naws_config = {\n    \"region\": \"us-east-1\",\n    \"key\": \"AKIA...\",  # Wrong key name\n    \"secret\": \"secret\"\n}\n</code></pre></p>"},{"location":"explanation/concepts/#configuration-methods","title":"Configuration Methods","text":"<p>Manual Configuration: For development and specific requirements Environment Configuration: For production deployments URI-Based Configuration: For dynamic configuration from strings YAML Serialization: For persistent configuration files</p>"},{"location":"explanation/concepts/#domain-package-architecture","title":"Domain Package Architecture","text":""},{"location":"explanation/concepts/#why-domain-packages","title":"Why Domain Packages","text":"<p>Monolithic utility modules create discoverability and maintenance issues:</p> <pre><code># Problem: Large, flat module structure\nfrom fsspeckit.utils import (\n    run_parallel,           # Parallel processing\n    DuckDBParquetHandler,   # Dataset operations\n    sql2pyarrow_filter,     # SQL filtering\n    AwsStorageOptions,       # Storage configuration\n    dict_to_dataframe,      # Type conversion\n    # ... 50+ more functions\n)\n</code></pre>"},{"location":"explanation/concepts/#domain-benefits","title":"Domain Benefits","text":"<p>Discoverability: Clear organization by functionality <pre><code># Clear what you're importing\nfrom fsspeckit.datasets import DuckDBParquetHandler    # Obvious: datasets\nfrom fsspeckit.sql.filters import sql2pyarrow_filter  # Obvious: SQL\nfrom fsspeckit.common.misc import run_parallel         # Obvious: utilities\n</code></pre></p> <p>Maintainability: Changes to one domain don't affect others Type Hints: Better IDE support and autocomplete Testing: Isolated testing for each domain Documentation: Focused docs for specific use cases</p>"},{"location":"explanation/concepts/#domain-boundaries","title":"Domain Boundaries","text":"<p>Core (<code>fsspeckit.core</code>): Filesystem creation and extended I/O Storage Options (<code>fsspeckit.storage_options</code>): Cloud provider configuration Datasets (<code>fsspeckit.datasets</code>): Large-scale data processing SQL (<code>fsspeckit.sql</code>): Query translation and filtering Common (<code>fsspeckit.common</code>): Shared utilities and helpers Utils (<code>fsspeckit.utils</code>): Backwards compatibility fa\u00e7ade</p>"},{"location":"explanation/concepts/#integration-patterns","title":"Integration Patterns","text":""},{"location":"explanation/concepts/#cross-domain-communication","title":"Cross-Domain Communication","text":"<p>Domains communicate through well-defined interfaces:</p> <pre><code># Storage Options \u2192 Core Filesystem\nfrom fsspeckit.storage_options import AwsStorageOptions\nfrom fsspeckit.core.filesystem import filesystem\n\noptions = AwsStorageOptions(region=\"us-east-1\")\nfs = filesystem(\"s3\", storage_options=options.to_dict())\n\n# Datasets \u2192 Core Merge Logic\nfrom fsspeckit.datasets.pyarrow import merge_parquet_dataset_pyarrow\nfrom fsspeckit.core.merge import MergeStrategy\n\n# Both use shared merge planning\nmerge_parquet_dataset_pyarrow(\n    dataset_paths=[\"part1/\", \"part2/\"],\n    merge_strategy=MergeStrategy.SCHEMA_EVOLUTION\n)\n</code></pre>"},{"location":"explanation/concepts/#configuration-flow","title":"Configuration Flow","text":"<pre><code># Environment \u2192 Storage Options \u2192 Filesystem \u2192 Operations\nfrom fsspeckit.storage_options import storage_options_from_env\nfrom fsspeckit.core.filesystem import filesystem\nfrom fsspeckit.datasets import DuckDBParquetHandler\n\n# 1. Load configuration\noptions = storage_options_from_env(\"s3\")\n\n# 2. Create filesystem\nfs = filesystem(\"s3\", storage_options=options.to_dict())\n\n# 3. Use in operations\nhandler = DuckDBParquetHandler(storage_options=options.to_dict())\n</code></pre>"},{"location":"explanation/concepts/#performance-architecture","title":"Performance Architecture","text":""},{"location":"explanation/concepts/#caching-strategy","title":"Caching Strategy","text":"<p>Filesystem-Level Caching: Reduce remote storage access <pre><code># Transparent caching for remote filesystems\nfs = filesystem(\"s3://bucket/\", cached=True)\n\n# First access: downloads and caches\ndata1 = fs.cat(\"large_file.parquet\")\n\n# Second access: reads from cache\ndata2 = fs.cat(\"large_file.parquet\")  # Much faster\n</code></pre></p> <p>Memory Management: Efficient data processing <pre><code># Batch processing for memory efficiency\nfor batch in fs.read_parquet(\"large_dataset/*.parquet\", batch_size=\"100MB\"):\n    process_batch(batch)  # Only one batch in memory at a time\n</code></pre></p>"},{"location":"explanation/concepts/#parallel-processing","title":"Parallel Processing","text":"<p>I/O Parallelism: Multiple files processed concurrently <pre><code># Parallel file operations\ndf = fs.read_csv(\"data/*.csv\", use_threads=True, num_threads=4)\n\n# Parallel custom processing\nfrom fsspeckit.common.misc import run_parallel\n\nresults = run_parallel(\n    func=process_file,\n    data=file_list,\n    max_workers=8\n)\n</code></pre></p>"},{"location":"explanation/concepts/#security-architecture","title":"Security Architecture","text":""},{"location":"explanation/concepts/#multi-layer-security","title":"Multi-Layer Security","text":"<p>Path Safety: Prevent directory traversal <pre><code># Confined filesystem operations\nsafe_fs = filesystem(\"/data/allowed\", dirfs=True)\n</code></pre></p> <p>Credential Protection: Prevent secret leakage <pre><code>from fsspeckit.common.security import scrub_credentials\n\nerror_msg = f\"Failed: access_key=AKIAIOSFODNN7EXAMPLE\"\nsafe_msg = scrub_credentials(error_msg)\n# Output: \"Failed: access_key=[REDACTED]\"\n</code></pre></p> <p>Input Validation: Prevent injection attacks <pre><code>from fsspeckit.common.security import (\n    validate_path,\n    validate_columns,\n    validate_compression_codec\n)\n\nsafe_path = validate_path(user_input, base_dir=\"/data/allowed\")\nsafe_columns = validate_columns(user_columns, valid_columns=schema_columns)\nsafe_codec = validate_compression_codec(user_codec)\n</code></pre></p>"},{"location":"explanation/concepts/#production-security","title":"Production Security","text":"<p>Environment-Based Configuration: No hardcoded credentials Audit Logging: Safe logging with credential scrubbing Multi-Tenant Isolation: Path confinement per tenant Compliance: Built-in controls for regulatory requirements</p>"},{"location":"explanation/concepts/#related-documentation","title":"Related Documentation","text":"<ul> <li>Architecture - Detailed system design</li> <li>API Guide - Capability overview</li> <li>How-to Guides - Practical implementation</li> </ul>"},{"location":"how-to/","title":"How-to Guides","text":"<p>Use these guides when you know what you want to achieve and need a concrete recipe.</p> <ul> <li>Configure Cloud Storage \u2013 AWS, GCP, Azure setup</li> <li>Work with Filesystems \u2013 Local and remote operations</li> <li>Read and Write Datasets \u2013 JSON, CSV, Parquet operations</li> <li>Use SQL Filters \u2013 Cross-framework filtering</li> <li>Sync and Manage Files \u2013 File synchronization</li> <li>Optimize Performance \u2013 Caching, parallelism, and memory usage</li> </ul>"},{"location":"how-to/configure-cloud-storage/","title":"Configure Cloud Storage","text":"<p>This guide shows you how to configure cloud storage providers for use with fsspeckit. You'll learn how to use environment variables, structured configuration classes, and URI-based setup.</p>"},{"location":"how-to/configure-cloud-storage/#environment-based-configuration","title":"Environment-Based Configuration","text":"<p>The recommended approach for production deployments is to load configuration from environment variables.</p>"},{"location":"how-to/configure-cloud-storage/#aws-s3","title":"AWS S3","text":"<pre><code>from fsspeckit.storage_options import storage_options_from_env\nfrom fsspeckit.core.filesystem import filesystem\n\n# Set environment variables\nimport os\nos.environ[\"AWS_ACCESS_KEY_ID\"] = \"your_access_key\"\nos.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"your_secret_key\"\nos.environ[\"AWS_DEFAULT_REGION\"] = \"us-east-1\"\n\n# Load AWS options from environment\naws_options = storage_options_from_env(\"s3\")\nfs = filesystem(\"s3\", storage_options=aws_options.to_dict())\n\nprint(f\"Created S3 filesystem in region: {aws_options.region}\")\n</code></pre>"},{"location":"how-to/configure-cloud-storage/#google-cloud-storage","title":"Google Cloud Storage","text":"<pre><code># Set environment variables\nos.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/path/to/service-account.json\"\nos.environ[\"GOOGLE_CLOUD_PROJECT\"] = \"your-gcp-project\"\n\n# Load GCS options from environment\ngcs_options = storage_options_from_env(\"gs\")\nfs = filesystem(\"gs\", storage_options=gcs_options.to_dict())\n</code></pre>"},{"location":"how-to/configure-cloud-storage/#azure-blob-storage","title":"Azure Blob Storage","text":"<pre><code># Set environment variables\nos.environ[\"AZURE_STORAGE_ACCOUNT\"] = \"your_storage_account\"\nos.environ[\"AZURE_STORAGE_KEY\"] = \"your_storage_key\"\n\n# Load Azure options from environment\nazure_options = storage_options_from_env(\"az\")\nfs = filesystem(\"az\", storage_options=azure_options.to_dict())\n</code></pre>"},{"location":"how-to/configure-cloud-storage/#manual-configuration-with-storage-options-classes","title":"Manual Configuration with Storage Options Classes","text":"<p>For more control, use the structured storage options classes.</p>"},{"location":"how-to/configure-cloud-storage/#aws-s3-configuration","title":"AWS S3 Configuration","text":"<pre><code>from fsspeckit.storage_options import AwsStorageOptions\n\n# Configure AWS S3\naws_options = AwsStorageOptions(\n    region=\"us-east-1\",\n    access_key_id=\"YOUR_ACCESS_KEY\",\n    secret_access_key=\"YOUR_SECRET_KEY\",\n    endpoint_url=None,  # Use default endpoint\n    allow_http=False,  # Enforce HTTPS\n    assume_role_arn=None  # Optional role assumption\n)\n\n# Create filesystem\nfs = aws_options.to_filesystem()\n</code></pre>"},{"location":"how-to/configure-cloud-storage/#google-cloud-storage-configuration","title":"Google Cloud Storage Configuration","text":"<pre><code>from fsspeckit.storage_options import GcsStorageOptions\n\ngcs_options = GcsStorageOptions(\n    project=\"your-gcp-project\",\n    token=\"path/to/service-account.json\",  # or None for default credentials\n    endpoint_override=None  # Use default endpoint\n)\n\nfs = gcs_options.to_filesystem()\n</code></pre>"},{"location":"how-to/configure-cloud-storage/#azure-blob-storage-configuration","title":"Azure Blob Storage Configuration","text":"<pre><code>from fsspeckit.storage_options import AzureStorageOptions\n\nazure_options = AzureStorageOptions(\n    account_name=\"yourstorageaccount\",\n    account_key=\"YOUR_ACCOUNT_KEY\",\n    connection_string=None,  # Alternative to account_name/account_key\n    sas_token=None  # Optional SAS token\n)\n\nfs = azure_options.to_filesystem()\n</code></pre>"},{"location":"how-to/configure-cloud-storage/#github-repository-configuration","title":"GitHub Repository Configuration","text":"<pre><code>from fsspeckit.storage_options import GitHubStorageOptions\n\ngithub_options = GitHubStorageOptions(\n    token=\"github_pat_YOUR_TOKEN\",\n    default_branch=\"main\"\n)\n\nfs = github_options.to_filesystem()\n</code></pre>"},{"location":"how-to/configure-cloud-storage/#gitlab-repository-configuration","title":"GitLab Repository Configuration","text":"<pre><code>from fsspeckit.storage_options import GitLabStorageOptions\n\ngitlab_options = GitLabStorageOptions(\n    project_id=12345,\n    token=\"glpat_xxx\"\n)\n\nfs = gitlab_options.to_filesystem()\n</code></pre>"},{"location":"how-to/configure-cloud-storage/#uri-based-configuration","title":"URI-Based Configuration","text":"<p>You can extract storage options directly from URIs, which is useful for configuration files or command-line arguments.</p> <pre><code>from fsspeckit.storage_options import storage_options_from_uri\nfrom fsspeckit.core.filesystem import filesystem\n\n# Extract storage options from URIs\nuris = [\n    \"s3://bucket/path?region=us-east-1&amp;endpoint_url=https://s3.amazonaws.com\",\n    \"gs://bucket/path?project=my-gcp-project\",\n    \"az://container/path?account_name=mystorageaccount\"\n]\n\nfor uri in uris:\n    options = storage_options_from_uri(uri)\n    fs = filesystem(options.protocol, storage_options=options.to_dict())\n    print(f\"URI: {uri}\")\n    print(f\"Protocol: {options.protocol}\")\n    print(f\"Filesystem: {type(fs).__name__}\")\n    print()\n</code></pre>"},{"location":"how-to/configure-cloud-storage/#multi-cloud-configuration","title":"Multi-Cloud Configuration","text":"<p>You can configure multiple cloud providers simultaneously:</p> <pre><code>from fsspeckit.storage_options import (\n    AwsStorageOptions,\n    GcsStorageOptions,\n    AzureStorageOptions\n)\n\n# AWS configuration\naws_config = AwsStorageOptions(\n    region=\"us-west-2\",\n    access_key_id=\"aws_key\",\n    secret_access_key=\"aws_secret\"\n)\n\n# Google Cloud configuration\ngcs_config = GcsStorageOptions(\n    project=\"gcp-project\",\n    token=\"path/to/service-account.json\"\n)\n\n# Azure configuration\nazure_config = AzureStorageOptions(\n    account_name=\"storageaccount\",\n    account_key=\"azure_key\"\n)\n\n# Create filesystems for each provider\naws_fs = aws_config.to_filesystem()\ngcs_fs = gcs_config.to_filesystem()\nazure_fs = azure_config.to_filesystem()\n\n# Use them interchangeably\nfor provider, fs in [(\"AWS\", aws_fs), (\"GCS\", gcs_fs), (\"Azure\", azure_fs)]:\n    print(f\"{provider} filesystem: {type(fs).__name__}\")\n</code></pre>"},{"location":"how-to/configure-cloud-storage/#configuration-methods","title":"Configuration Methods","text":"<p>All storage option classes provide useful conversion methods:</p> <pre><code>opts = AwsStorageOptions(...)\n\n# Convert to fsspec kwargs\nkwargs = opts.to_fsspec_kwargs()\n\n# Convert to filesystem\nfs = opts.to_filesystem()\n\n# Convert to object store kwargs (for deltalake, etc.)\nobj_store_kwargs = opts.to_object_store_kwargs()\n\n# Convert to YAML\nyaml_str = opts.to_yaml()\n\n# Load from YAML\nopts = AwsStorageOptions.from_yaml(yaml_str)\n\n# Convert to environment variables\nenv = opts.to_env()\n\n# Load from environment\nopts = AwsStorageOptions.from_env()\n</code></pre>"},{"location":"how-to/configure-cloud-storage/#best-practices","title":"Best Practices","text":""},{"location":"how-to/configure-cloud-storage/#production-deployments","title":"Production Deployments","text":"<ol> <li>Use Environment Variables: Store credentials in environment variables, not in code</li> <li>IAM Roles: Use IAM roles instead of access keys when possible</li> <li>Endpoint Configuration: Use appropriate endpoints for your region</li> <li>HTTPS Only: Ensure <code>allow_http=False</code> for security</li> </ol>"},{"location":"how-to/configure-cloud-storage/#security","title":"Security","text":"<pre><code># Good: Use environment variables\naws_options = storage_options_from_env(\"s3\")\n\n# Good: Use structured classes\naws_options = AwsStorageOptions(\n    region=\"us-east-1\",\n    access_key_id=get_secret(\"aws_access_key\"),\n    secret_access_key=get_secret(\"aws_secret_key\")\n)\n\n# Avoid: Hardcoded credentials\naws_options = AwsStorageOptions(\n    region=\"us-east-1\",\n    access_key_id=\"AKIAIOSFODNN7EXAMPLE\",  # Don't do this!\n    secret_access_key=\"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\"  # Don't do this!\n)\n</code></pre>"},{"location":"how-to/configure-cloud-storage/#testing","title":"Testing","text":"<p>For testing, use local filesystem or mock services:</p> <pre><code>from fsspeckit.storage_options import LocalStorageOptions\n\n# Use local filesystem for testing\nlocal_options = LocalStorageOptions(auto_mkdir=True)\ntest_fs = local_options.to_filesystem()\n</code></pre>"},{"location":"how-to/configure-cloud-storage/#troubleshooting","title":"Troubleshooting","text":""},{"location":"how-to/configure-cloud-storage/#common-issues","title":"Common Issues","text":"<ol> <li>Authentication Errors: Verify credentials and permissions</li> <li>Region Mismatch: Ensure correct region configuration</li> <li>Network Issues: Check connectivity to cloud endpoints</li> <li>IAM Permissions: Verify IAM roles and policies</li> </ol>"},{"location":"how-to/configure-cloud-storage/#debug-configuration","title":"Debug Configuration","text":"<pre><code># Debug storage options\naws_options = AwsStorageOptions(...)\nprint(\"FSSpec kwargs:\", aws_options.to_fsspec_kwargs())\nprint(\"Object store kwargs:\", aws_options.to_object_store_kwargs())\n\n# Test filesystem creation\ntry:\n    fs = aws_options.to_filesystem()\n    files = fs.ls(\"/\")\n    print(f\"Successfully connected, found {len(files)} files\")\nexcept Exception as e:\n    print(f\"Connection failed: {e}\")\n</code></pre> <p>For more information on filesystem operations, see Work with Filesystems.</p>"},{"location":"how-to/optimize-performance/","title":"Optimize Performance","text":"<p>This guide covers performance optimization techniques for fsspeckit, including caching, parallel processing, and dataset optimization strategies.</p>"},{"location":"how-to/optimize-performance/#caching-strategies","title":"Caching Strategies","text":""},{"location":"how-to/optimize-performance/#filesystem-caching","title":"Filesystem Caching","text":"<pre><code>from fsspeckit.core.filesystem import filesystem\n\n# Enable caching for remote filesystems\nfs = filesystem(\"s3://bucket/\", cached=True)\n\n# Configure cache directory\nfs = filesystem(\"s3://bucket/\", cached=True, cache_storage=\"/fast/ssd/cache\")\n\n# Enable verbose cache logging\nfs = filesystem(\"s3://bucket/\", cached=True, verbose=True)\n\n# Cache with specific size limits\nfs = filesystem(\n    \"s3://bucket/\", \n    cached=True, \n    cache_storage=\"/tmp/cache\",\n    use_listings_cache=True,\n    skip_instance_cache=False\n)\n</code></pre>"},{"location":"how-to/optimize-performance/#cache-management","title":"Cache Management","text":"<pre><code># Monitor cache usage\nif hasattr(fs, 'cache_size'):\n    print(f\"Current cache size: {fs.cache_size}\")\n\n# Clear cache when needed\nfs.clear_cache()\n\n# Sync cache to ensure data is written\nfs.sync_cache()\n\n# Force cache refresh\nfs.invalidate_cache()\n</code></pre>"},{"location":"how-to/optimize-performance/#cache-best-practices","title":"Cache Best Practices","text":"<pre><code># Good: Use caching for remote filesystems\nremote_fs = filesystem(\"s3://data/\", cached=True)\n\n# Good: Use fast storage for cache\nremote_fs = filesystem(\n    \"s3://data/\", \n    cached=True, \n    cache_storage=\"/nvme/cache\"  # Fast NVMe storage\n)\n\n# Not necessary: Local filesystems don't benefit from caching\nlocal_fs = filesystem(\"file\")  # cached=False by default\n\n# Configure for different workloads\n# For read-heavy workloads\nread_heavy_fs = filesystem(\"s3://data/\", cached=True, cache_storage=\"/ssd/cache\")\n\n# For write-heavy workloads\nwrite_heavy_fs = filesystem(\"s3://output/\", cached=False)  # Avoid cache for writes\n</code></pre>"},{"location":"how-to/optimize-performance/#parallel-processing","title":"Parallel Processing","text":""},{"location":"how-to/optimize-performance/#parallel-file-operations","title":"Parallel File Operations","text":"<pre><code>from fsspeckit.common.misc import run_parallel\n\ndef process_file(file_path):\n    \"\"\"Process individual file\"\"\"\n    # Your processing logic here\n    return len(file_path)\n\n# List of files to process\nfile_list = [f\"file_{i}.parquet\" for i in range(100)]\n\n# Process files in parallel\nresults = run_parallel(\n    func=process_file,\n    data=file_list,\n    max_workers=8,  # Use 8 parallel workers\n    progress=True   # Show progress bar\n)\n\nprint(f\"Processed {len(results)} files\")\n</code></pre>"},{"location":"how-to/optimize-performance/#parallel-io-operations","title":"Parallel I/O Operations","text":"<pre><code># Parallel CSV reading\ndf = fs.read_csv(\"data/*.csv\", use_threads=True, num_threads=4)\n\n# Parallel JSON reading\ndf = fs.read_json(\"data/*.json\", use_threads=True, num_threads=4)\n\n# Parallel Parquet reading\ntable = fs.read_parquet(\"data/*.parquet\", use_threads=True)\n</code></pre>"},{"location":"how-to/optimize-performance/#parallel-dataset-processing","title":"Parallel Dataset Processing","text":"<pre><code>def process_batch(batch_table):\n    \"\"\"Process individual batch\"\"\"\n    # Example: calculate statistics\n    total_rows = len(batch_table)\n    if \"amount\" in batch_table.column_names:\n        total_amount = batch_table.column(\"amount\").to_pandas().sum()\n        return {\"rows\": total_rows, \"total_amount\": total_amount}\n    return {\"rows\": total_rows}\n\n# Process dataset in parallel batches\nfrom fsspeckit.datasets.pyarrow import process_dataset_in_batches\n\nbatch_results = []\nfor i, result in enumerate(process_dataset_in_batches(\n    dataset_path=\"s3://bucket/large-dataset/\",\n    batch_size=\"100MB\",\n    process_func=process_batch,\n    max_workers=4\n)):\n    batch_results.append(result)\n    print(f\"Batch {i+1}: {result}\")\n\n# Aggregate results\ntotal_rows = sum(r[\"rows\"] for r in batch_results)\ntotal_amount = sum(r.get(\"total_amount\", 0) for r in batch_results)\nprint(f\"Total rows: {total_rows}, Total amount: {total_amount}\")\n</code></pre>"},{"location":"how-to/optimize-performance/#dataset-optimization","title":"Dataset Optimization","text":""},{"location":"how-to/optimize-performance/#parquet-dataset-optimization","title":"Parquet Dataset Optimization","text":"<pre><code>from fsspeckit.datasets.pyarrow import (\n    optimize_parquet_dataset_pyarrow,\n    compact_parquet_dataset_pyarrow\n)\n\n# Optimize dataset with Z-ordering\noptimize_parquet_dataset_pyarrow(\n    dataset_path=\"s3://bucket/large-dataset/\",\n    z_order_columns=[\"category\", \"timestamp\"],\n    target_file_size=\"256MB\",\n    compression=\"zstd\"\n)\n\n# Compact small files\ncompact_parquet_dataset_pyarrow(\n    dataset_path=\"s3://bucket/fragmented-dataset/\",\n    target_file_size=\"128MB\",\n    compression=\"snappy\"\n)\n</code></pre>"},{"location":"how-to/optimize-performance/#advanced-optimization","title":"Advanced Optimization","text":"<pre><code># Optimize with multiple strategies\noptimize_parquet_dataset_pyarrow(\n    dataset_path=\"s3://bucket/dataset/\",\n    z_order_columns=[\"user_id\", \"event_date\", \"category\"],\n    target_file_size=\"512MB\",\n    compression=\"zstd\",\n    max_rows_per_group=1000000,\n    max_rows_per_file=5000000\n)\n\n# Compact with filtering\ncompact_parquet_dataset_pyarrow(\n    dataset_path=\"s3://bucket/dataset/\",\n    target_file_size=\"256MB\",\n    filters=[(\"status\", \"=\", \"active\")],  # Only compact active files\n    delete_intermediate=True\n)\n</code></pre>"},{"location":"how-to/optimize-performance/#schema-optimization","title":"Schema Optimization","text":"<pre><code>from fsspeckit.common.types import convert_large_types_to_normal\nfrom fsspeckit.datasets import opt_dtype_pa\nimport pyarrow as pa\n\n# Convert large string types to normal strings\nlarge_string_table = pa.Table.from_pydict({\n    \"text\": pa.array([\"value1\", \"value2\"], type=pa.large_string())\n})\n\noptimized_table = convert_large_types_to_normal(large_string_table)\nprint(f\"Original schema: {large_string_table.schema}\")\nprint(f\"Optimized schema: {optimized_table.schema}\")\n\n# Optimize data types\noptimized_table = opt_dtype_pa(large_string_table)\nprint(f\"Further optimized schema: {optimized_table.schema}\")\n</code></pre>"},{"location":"how-to/optimize-performance/#memory-optimization","title":"Memory Optimization","text":""},{"location":"how-to/optimize-performance/#batch-processing","title":"Batch Processing","text":"<pre><code>def process_large_dataset_efficiently(dataset_path):\n    \"\"\"Process large dataset with memory efficiency\"\"\"\n\n    from fsspeckit.datasets.pyarrow import process_dataset_in_batches\n    import pyarrow as pa\n\n    total_rows = 0\n    batch_count = 0\n\n    # Process in small batches to manage memory\n    for batch in process_dataset_in_batches(\n        dataset_path=dataset_path,\n        batch_size=\"50MB\",  # Small batch size\n        process_func=lambda batch: len(batch),\n        max_workers=2  # Fewer workers to reduce memory pressure\n    ):\n        total_rows += batch\n        batch_count += 1\n\n        if batch_count % 10 == 0:\n            print(f\"Processed {batch_count} batches, {total_rows} total rows\")\n\n    return total_rows\n\n# Usage\nrow_count = process_large_dataset_efficiently(\"s3://bucket/huge-dataset/\")\n</code></pre>"},{"location":"how-to/optimize-performance/#column-projection","title":"Column Projection","text":"<pre><code># Read only needed columns to reduce memory usage\nessential_columns = [\"id\", \"timestamp\", \"user_id\", \"event_type\"]\n\n# For PyArrow\nimport pyarrow.parquet as pq\ndataset = pq.ParquetDataset(\"large_dataset.parquet\")\nfiltered_table = dataset.read(columns=essential_columns)\n\n# For Polars\nimport polars as pl\ndf = pl.read_parquet(\"large_dataset.parquet\", columns=essential_columns)\n\n# For fsspeckit extended I/O\ntable = fs.read_parquet_file(\"large_dataset.parquet\", columns=essential_columns)\n</code></pre>"},{"location":"how-to/optimize-performance/#memory-efficient-data-types","title":"Memory-Efficient Data Types","text":"<pre><code>import polars as pl\nfrom fsspeckit.common.polars import opt_dtype_pl\n\n# Create DataFrame with suboptimal types\ndf = pl.DataFrame({\n    \"id\": [1, 2, 3, 4, 5],  # Could be int32\n    \"category\": [\"A\", \"B\", \"A\", \"B\", \"A\"],  # Could be categorical\n    \"value\": [10.5, 20.3, 15.7, 25.1, 12.8],  # Could be float32\n    \"flag\": [True, False, True, False, True]  # Could be boolean\n})\n\n# Optimize data types\noptimized_df = opt_dtype_pl(df, shrink_numerics=True)\nprint(f\"Original memory usage: {df.estimated_size('mb'):.2f} MB\")\nprint(f\"Optimized memory usage: {optimized_df.estimated_size('mb'):.2f} MB\")\n</code></pre>"},{"location":"how-to/optimize-performance/#io-optimization","title":"I/O Optimization","text":""},{"location":"how-to/optimize-performance/#efficient-file-reading","title":"Efficient File Reading","text":"<pre><code># Use appropriate batch sizes for different file types\ncsv_batch_size = \"50MB\"    # Smaller for text files\nparquet_batch_size = \"200MB\"  # Larger for columnar files\njson_batch_size = \"25MB\"     # Smallest for JSON\n\n# Read with optimal settings\ndf_csv = fs.read_csv(\"data/*.csv\", batch_size=csv_batch_size, use_threads=True)\ntable_parquet = fs.read_parquet(\"data/*.parquet\", batch_size=parquet_batch_size)\ndf_json = fs.read_json(\"data/*.json\", batch_size=json_batch_size, use_threads=True)\n</code></pre>"},{"location":"how-to/optimize-performance/#write-optimization","title":"Write Optimization","text":"<pre><code>import pyarrow as pa\n\n# Write with optimal file size and compression\ntable = pa.table({...})\n\nfs.write_pyarrow_dataset(\n    data=table,\n    path=\"optimized_dataset\",\n    format=\"parquet\",\n    compression=\"zstd\",  # Good compression ratio\n    max_rows_per_file=1000000,  # Target ~100MB files\n    existing_data_behavior=\"overwrite_or_ignore\"\n)\n\n# Write with partitioning for query performance\nfs.write_pyarrow_dataset(\n    data=table,\n    path=\"partitioned_dataset\",\n    partition_by=[\"year\", \"month\", \"day\"],  # Partition by date\n    format=\"parquet\",\n    compression=\"snappy\",  # Faster compression for hot data\n    max_rows_per_file=500000\n)\n</code></pre>"},{"location":"how-to/optimize-performance/#connection-pooling","title":"Connection Pooling","text":"<pre><code># Reuse filesystem instances for connection pooling\nclass FilesystemPool:\n    def __init__(self):\n        self._pool = {}\n\n    def get_filesystem(self, protocol, storage_options=None):\n        key = (protocol, str(storage_options))\n\n        if key not in self._pool:\n            self._pool[key] = filesystem(protocol, storage_options=storage_options)\n\n        return self._pool[key]\n\n# Usage\npool = FilesystemPool()\n\n# Reuse filesystem instances\ns3_fs = pool.get_filesystem(\"s3\", s3_options)\ngcs_fs = pool.get_filesystem(\"gs\", gcs_options)\n\n# This reuses existing connections\ns3_fs_again = pool.get_filesystem(\"s3\", s3_options)\n</code></pre>"},{"location":"how-to/optimize-performance/#performance-monitoring","title":"Performance Monitoring","text":""},{"location":"how-to/optimize-performance/#benchmarking-operations","title":"Benchmarking Operations","text":"<pre><code>import time\nfrom contextlib import contextmanager\n\n@contextmanager\ndef benchmark(operation_name):\n    \"\"\"Simple benchmarking context manager\"\"\"\n    start_time = time.time()\n    try:\n        yield\n    finally:\n        end_time = time.time()\n        duration = end_time - start_time\n        print(f\"{operation_name}: {duration:.2f} seconds\")\n\n# Benchmark different operations\nwith benchmark(\"CSV Read\"):\n    df_csv = fs.read_csv(\"data.csv\")\n\nwith benchmark(\"Parquet Read\"):\n    table_parquet = fs.read_parquet_file(\"data.parquet\")\n\nwith benchmark(\"JSON Read\"):\n    df_json = fs.read_json_file(\"data.json\")\n\nwith benchmark(\"Parallel CSV Read\"):\n    df_parallel = fs.read_csv(\"data/*.csv\", use_threads=True, num_threads=4)\n</code></pre>"},{"location":"how-to/optimize-performance/#cache-performance-analysis","title":"Cache Performance Analysis","text":"<pre><code>def analyze_cache_performance(fs, test_files):\n    \"\"\"Analyze cache hit/miss performance\"\"\"\n\n    cache_stats = {\"hits\": 0, \"misses\": 0, \"total_time\": 0}\n\n    for file_path in test_files:\n        start_time = time.time()\n\n        # First access (cache miss)\n        content1 = fs.cat(file_path)\n        first_time = time.time() - start_time\n\n        # Second access (cache hit)\n        start_time = time.time()\n        content2 = fs.cat(file_path)\n        second_time = time.time() - start_time\n\n        # Verify content is same\n        assert content1 == content2, \"Cache returned different content\"\n\n        cache_stats[\"misses\"] += 1\n        cache_stats[\"hits\"] += 1\n        cache_stats[\"total_time\"] += first_time + second_time\n\n        print(f\"File: {file_path}\")\n        print(f\"  First access: {first_time:.3f}s (miss)\")\n        print(f\"  Second access: {second_time:.3f}s (hit)\")\n        print(f\"  Speedup: {first_time/second_time:.1f}x\")\n\n    return cache_stats\n\n# Usage\ntest_files = fs.ls(\"/test/data/\")[:5]  # Test first 5 files\nstats = analyze_cache_performance(fs, test_files)\nprint(f\"Cache analysis: {stats}\")\n</code></pre>"},{"location":"how-to/optimize-performance/#configuration-tuning","title":"Configuration Tuning","text":""},{"location":"how-to/optimize-performance/#worker-count-optimization","title":"Worker Count Optimization","text":"<pre><code>import os\nimport psutil\n\ndef get_optimal_workers():\n    \"\"\"Calculate optimal worker count based on system resources\"\"\"\n\n    # Get CPU count\n    cpu_count = os.cpu_count()\n\n    # Get available memory\n    available_memory_gb = psutil.virtual_memory().available / (1024**3)\n\n    # Estimate memory per worker (conservative estimate)\n    memory_per_worker_gb = 1  # 1GB per worker\n\n    # Calculate workers based on memory\n    memory_limited_workers = int(available_memory_gb / memory_per_worker_gb)\n\n    # Use the minimum of CPU and memory limits\n    optimal_workers = min(cpu_count, memory_limited_workers)\n\n    # Ensure at least 1 worker\n    optimal_workers = max(1, optimal_workers)\n\n    return optimal_workers\n\n# Usage\noptimal_workers = get_optimal_workers()\nprint(f\"Using {optimal_workers} workers for parallel operations\")\n\nresults = run_parallel(\n    func=process_file,\n    data=file_list,\n    max_workers=optimal_workers,\n    progress=True\n)\n</code></pre>"},{"location":"how-to/optimize-performance/#batch-size-optimization","title":"Batch Size Optimization","text":"<pre><code>def find_optimal_batch_size(dataset_path, test_sizes):\n    \"\"\"Find optimal batch size through experimentation\"\"\"\n\n    import pyarrow as pa\n\n    results = []\n\n    for batch_size in test_sizes:\n        start_time = time.time()\n        batch_count = 0\n        total_rows = 0\n\n        try:\n            for batch in process_dataset_in_batches(\n                dataset_path=dataset_path,\n                batch_size=batch_size,\n                process_func=lambda batch: len(batch),\n                max_workers=2\n            ):\n                batch_count += 1\n                total_rows += batch\n\n            duration = time.time() - start_time\n            throughput = total_rows / duration\n\n            results.append({\n                \"batch_size\": batch_size,\n                \"duration\": duration,\n                \"throughput\": throughput,\n                \"batch_count\": batch_count\n            })\n\n            print(f\"Batch size {batch_size}: {throughput:.0f} rows/sec\")\n\n        except Exception as e:\n            print(f\"Batch size {batch_size} failed: {e}\")\n\n    # Find best throughput\n    best_result = max(results, key=lambda x: x[\"throughput\"])\n    print(f\"\\nOptimal batch size: {best_result['batch_size']}\")\n    print(f\"Throughput: {best_result['throughput']:.0f} rows/sec\")\n\n    return best_result\n\n# Usage\ntest_sizes = [\"25MB\", \"50MB\", \"100MB\", \"200MB\", \"500MB\"]\noptimal = find_optimal_batch_size(\"s3://bucket/dataset/\", test_sizes)\n</code></pre>"},{"location":"how-to/optimize-performance/#best-practices","title":"Best Practices","text":""},{"location":"how-to/optimize-performance/#general-performance-tips","title":"General Performance Tips","text":"<pre><code># 1. Enable caching for remote filesystems\nremote_fs = filesystem(\"s3://data/\", cached=True)\n\n# 2. Use appropriate batch sizes\nlarge_batch_fs = fs.read_parquet(\"large_files/*.parquet\", batch_size=\"200MB\")\nsmall_batch_fs = fs.read_csv(\"text_files/*.csv\", batch_size=\"25MB\")\n\n# 3. Leverage parallel processing\nparallel_results = run_parallel(process_func, data_list, max_workers=8)\n\n# 4. Optimize data types\noptimized_df = opt_dtype_pl(df, shrink_numerics=True)\n\n# 5. Use column projection\nessential_data = fs.read_parquet_file(\"data.parquet\", columns=[\"id\", \"value\"])\n</code></pre>"},{"location":"how-to/optimize-performance/#environment-specific-optimization","title":"Environment-Specific Optimization","text":"<pre><code># Development environment - prioritize speed\ndev_fs = filesystem(\n    \"s3://dev-data/\",\n    cached=True,\n    cache_storage=\"/tmp/dev_cache\",\n    use_listings_cache=False  # Skip caching for frequently changing data\n)\n\n# Production environment - prioritize stability\nprod_fs = filesystem(\n    \"s3://prod-data/\",\n    cached=True,\n    cache_storage=\"/ssd/prod_cache\",\n    use_listings_cache=True,\n    skip_instance_cache=False\n)\n\n# Analytics environment - prioritize throughput\nanalytics_fs = filesystem(\n    \"s3://analytics-data/\",\n    cached=True,\n    cache_storage=\"/nvme/analytics_cache\"\n)\n</code></pre>"},{"location":"how-to/optimize-performance/#monitoring-and-alerting","title":"Monitoring and Alerting","text":"<pre><code>def setup_performance_monitoring():\n    \"\"\"Setup performance monitoring for production\"\"\"\n\n    import logging\n\n    # Configure logging\n    logging.basicConfig(level=logging.INFO)\n    logger = logging.getLogger(\"fsspeckit_performance\")\n\n    def monitor_operation(operation_func, operation_name):\n        \"\"\"Monitor operation performance\"\"\"\n        start_time = time.time()\n\n        try:\n            result = operation_func()\n            duration = time.time() - start_time\n\n            logger.info(f\"{operation_name} completed in {duration:.2f}s\")\n\n            # Alert on slow operations\n            if duration &gt; 300:  # 5 minutes\n                logger.warning(f\"Slow operation: {operation_name} took {duration:.2f}s\")\n\n            return result\n\n        except Exception as e:\n            duration = time.time() - start_time\n            logger.error(f\"{operation_name} failed after {duration:.2f}s: {e}\")\n            raise\n\n    return monitor_operation\n\n# Usage\nmonitor = setup_performance_monitoring()\n\ndef monitored_sync():\n    return sync_dir(src_fs, dst_fs, \"/src/\", \"/dst/\")\n\n# Monitor the sync operation\nmonitor(monitored_sync, \"directory_sync\")\n</code></pre>"},{"location":"how-to/optimize-performance/#troubleshooting-performance-issues","title":"Troubleshooting Performance Issues","text":""},{"location":"how-to/optimize-performance/#common-performance-problems","title":"Common Performance Problems","text":"<pre><code># 1. Slow first access, fast subsequent access\n# Solution: Enable caching\nfs = filesystem(\"s3://data/\", cached=True)\n\n# 2. High memory usage\n# Solution: Use smaller batch sizes and column projection\ndf = fs.read_parquet(\"data.parquet\", columns=[\"id\", \"name\"], batch_size=\"50MB\")\n\n# 3. Slow processing of many small files\n# Solution: Use parallel processing and compact files\nresults = run_parallel(process_file, file_list, max_workers=8)\ncompact_parquet_dataset_pyarrow(\"path/\", target_file_size=\"256MB\")\n\n# 4. Intermittent slowdowns\n# Solution: Use connection pooling and retry logic\npool = FilesystemPool()\nfs = pool.get_filesystem(\"s3\", options)\n</code></pre>"},{"location":"how-to/optimize-performance/#performance-debugging","title":"Performance Debugging","text":"<pre><code>def debug_performance(fs, operation):\n    \"\"\"Debug performance issues\"\"\"\n\n    import psutil\n    import tracemalloc\n\n    # Start memory tracing\n    tracemalloc.start()\n    start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB\n\n    start_time = time.time()\n\n    try:\n        result = operation()\n\n        end_time = time.time()\n        end_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB\n        current, peak = tracemalloc.get_traced_memory()\n\n        print(f\"Operation completed in {end_time - start_time:.2f}s\")\n        print(f\"Memory usage: {end_memory - start_memory:.1f}MB increase\")\n        print(f\"Peak memory: {peak / 1024 / 1024:.1f}MB\")\n\n        return result\n\n    finally:\n        tracemalloc.stop()\n\n# Usage\ndef test_operation():\n    return fs.read_parquet(\"large_dataset.parquet\")\n\ndebug_performance(fs, test_operation)\n</code></pre> <p>For more information on dataset operations, see Read and Write Datasets.</p>"},{"location":"how-to/read-and-write-datasets/","title":"Read and Write Datasets","text":"<p>This guide covers how to read and write datasets in various formats using fsspeckit's extended I/O helpers and dataset operations.</p>"},{"location":"how-to/read-and-write-datasets/#reading-json-data","title":"Reading JSON Data","text":""},{"location":"how-to/read-and-write-datasets/#single-json-files","title":"Single JSON Files","text":"<pre><code>from fsspeckit.core.filesystem import filesystem\n\nfs = filesystem(\".\")\n\n# Read JSON file as dictionary\ndata = fs.read_json_file(\"data.json\")\nprint(f\"Keys: {list(data.keys())}\")\n\n# Read JSON file as DataFrame\ndf = fs.read_json_file(\"data.json\", as_dataframe=True)\nprint(df.head())\n</code></pre>"},{"location":"how-to/read-and-write-datasets/#multiple-json-files","title":"Multiple JSON Files","text":"<pre><code># Read multiple JSON files with batching\nfor batch in fs.read_json(\"data/*.json\", batch_size=5):\n    print(f\"Processing batch with {len(batch)} files\")\n    # Process batch\n    process_batch(batch)\n\n# Read all JSON files and concatenate\ndf = fs.read_json(\"data/*.json\", concat=True)\nprint(f\"Total rows: {len(df)}\")\n\n# Read with threading for performance\ndf = fs.read_json(\"data/*.json\", use_threads=True, num_threads=4)\n\n# Include source file path\ndf = fs.read_json(\"data/*.json\", include_file_path=True)\nprint(df.columns)  # Includes '_source_file' column\n</code></pre>"},{"location":"how-to/read-and-write-datasets/#json-lines-format","title":"JSON Lines Format","text":"<pre><code># Read JSON Lines (newline-delimited JSON)\ndf = fs.read_json(\"data/lines.jsonl\", as_dataframe=True)\nprint(f\"Records: {len(df)}\")\n\n# Read JSON Lines in batches\nfor batch in fs.read_json(\"data/lines.jsonl\", batch_size=1000):\n    process_batch(batch)\n</code></pre>"},{"location":"how-to/read-and-write-datasets/#reading-csv-data","title":"Reading CSV Data","text":""},{"location":"how-to/read-and-write-datasets/#single-csv-files","title":"Single CSV Files","text":"<pre><code># Read CSV file\ndf = fs.read_csv_file(\"data.csv\")\nprint(df.head())\n\n# Read with specific columns\ndf = fs.read_csv_file(\"data.csv\", columns=[\"id\", \"name\", \"value\"])\n\n# Read with data type optimization\ndf = fs.read_csv_file(\"data.csv\", opt_dtypes=True)\n</code></pre>"},{"location":"how-to/read-and-write-datasets/#multiple-csv-files","title":"Multiple CSV Files","text":"<pre><code># Read multiple CSV files\ndf = fs.read_csv(\"data/*.csv\", concat=True)\nprint(f\"Combined rows: {len(df)}\")\n\n# Batch processing\nfor batch in fs.read_csv(\"data/*.csv\", batch_size=10):\n    print(f\"Batch size: {len(batch)}\")\n    process_batch(batch)\n\n# With parallel processing\ndf = fs.read_csv(\"data/*.csv\", use_threads=True, num_threads=4)\n\n# Optimize data types automatically\ndf = fs.read_csv(\"data/*.csv\", opt_dtypes=True)\n</code></pre>"},{"location":"how-to/read-and-write-datasets/#advanced-csv-options","title":"Advanced CSV Options","text":"<pre><code># Read with custom delimiter\ndf = fs.read_csv_file(\"data.tsv\", delimiter=\"\\t\")\n\n# Read with specific encoding\ndf = fs.read_csv_file(\"data.csv\", encoding=\"utf-8\")\n\n# Read with header handling\ndf = fs.read_csv_file(\"data_no_header.csv\", header=None, names=[\"col1\", \"col2\"])\n</code></pre>"},{"location":"how-to/read-and-write-datasets/#reading-parquet-data","title":"Reading Parquet Data","text":""},{"location":"how-to/read-and-write-datasets/#single-parquet-files","title":"Single Parquet Files","text":"<pre><code># Read single Parquet file\ntable = fs.read_parquet_file(\"data.parquet\")\nprint(f\"Schema: {table.schema}\")\nprint(f\"Rows: {len(table)}\")\n\n# Read as DataFrame\ndf = fs.read_parquet_file(\"data.parquet\", as_dataframe=True)\nprint(df.head())\n</code></pre>"},{"location":"how-to/read-and-write-datasets/#multiple-parquet-files","title":"Multiple Parquet Files","text":"<pre><code># Read multiple Parquet files with schema unification\ntable = fs.read_parquet(\"data/*.parquet\", concat=True)\nprint(f\"Combined rows: {len(table)}\")\n\n# Batch reading for large datasets\nfor batch in fs.read_parquet(\"data/*.parquet\", batch_size=20):\n    print(f\"Batch rows: {len(batch)}\")\n    process_batch(batch)\n\n# Read partitioned data\ntable = fs.read_parquet(\"partitioned_data/**/*.parquet\", concat=True)\nprint(f\"Partitioned rows: {len(table)}\")\n\n# Include source file path\ntable = fs.read_parquet(\"data/*.parquet\", include_file_path=True)\nprint(f\"Columns: {table.column_names}\")\n</code></pre>"},{"location":"how-to/read-and-write-datasets/#column-selection-and-filtering","title":"Column Selection and Filtering","text":"<pre><code># Read specific columns\ntable = fs.read_parquet_file(\"data.parquet\", columns=[\"id\", \"name\", \"value\"])\n\n# Read with row filtering (PyArrow dataset)\ndataset = fs.pyarrow_dataset(\"data/\")\nfiltered_table = dataset.to_table(\n    filter=pyarrow.compute.greater(dataset.column(\"value\"), 100)\n)\n</code></pre>"},{"location":"how-to/read-and-write-datasets/#universal-file-reading","title":"Universal File Reading","text":""},{"location":"how-to/read-and-write-datasets/#auto-detect-format","title":"Auto-Detect Format","text":"<pre><code># Auto-detect format from file extension\ndf = fs.read_files(\"data/mixed/*\", format=\"auto\")\n\n# Process mixed file types\nfor file_path in [\"data.json\", \"data.csv\", \"data.parquet\"]:\n    df = fs.read_files(file_path, format=\"auto\")\n    print(f\"File: {file_path}, Rows: {len(df)}\")\n</code></pre>"},{"location":"how-to/read-and-write-datasets/#explicit-format-specification","title":"Explicit Format Specification","text":"<pre><code># Force specific format\ndf = fs.read_files(\"data.txt\", format=\"csv\")\n\n# Control result type\ndf_polars = fs.read_files(\"data/*.parquet\", as_dataframe=True)\ntable_arrow = fs.read_files(\"data/*.parquet\", as_dataframe=False)\n</code></pre>"},{"location":"how-to/read-and-write-datasets/#writing-data","title":"Writing Data","text":""},{"location":"how-to/read-and-write-datasets/#writing-dataframes","title":"Writing DataFrames","text":"<pre><code>import polars as pl\n\n# Create sample DataFrame\ndf = pl.DataFrame({\n    \"id\": [1, 2, 3, 4, 5],\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"Diana\", \"Eve\"],\n    \"value\": [10.5, 20.3, 15.7, 25.1, 12.8],\n    \"category\": [\"A\", \"B\", \"A\", \"B\", \"A\"]\n})\n\n# Write to Parquet\nfs.write_parquet(df, \"output.parquet\")\n\n# Write to CSV\nfs.write_csv(df, \"output.csv\")\n\n# Write to JSON\nfs.write_json(df, \"output.json\")\n\n# Write with compression\nfs.write_parquet(df, \"output.parquet\", compression=\"zstd\")\n</code></pre>"},{"location":"how-to/read-and-write-datasets/#writing-pyarrow-tables","title":"Writing PyArrow Tables","text":"<pre><code>import pyarrow as pa\n\n# Create PyArrow table\ntable = pa.table({\n    \"id\": [1, 2, 3, 4, 5],\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"Diana\", \"Eve\"],\n    \"value\": [10.5, 20.3, 15.7, 25.1, 12.8]\n})\n\n# Write table\nfs.write_parquet(table, \"output.parquet\")\n</code></pre>"},{"location":"how-to/read-and-write-datasets/#append-mode","title":"Append Mode","text":"<pre><code># Append to existing files\nfs.write_csv(new_df, \"output.csv\", mode=\"append\")\nfs.write_parquet(new_df, \"output.parquet\", mode=\"append\")\n</code></pre>"},{"location":"how-to/read-and-write-datasets/#writing-datasets","title":"Writing Datasets","text":""},{"location":"how-to/read-and-write-datasets/#partitioned-datasets","title":"Partitioned Datasets","text":"<pre><code>import pyarrow as pa\n\n# Create table with partition columns\ntable = pa.table({\n    \"year\": [2023, 2023, 2024, 2024],\n    \"month\": [1, 2, 1, 2],\n    \"value\": [10, 20, 30, 40],\n    \"category\": [\"A\", \"B\", \"A\", \"B\"]\n})\n\n# Write partitioned dataset\nfs.write_pyarrow_dataset(\n    data=table,\n    path=\"partitioned_data\",\n    partition_by=[\"year\", \"month\"],\n    format=\"parquet\",\n    compression=\"zstd\"\n)\n\n# Result structure:\n# partitioned_data/year=2023/month=1/...parquet\n# partitioned_data/year=2023/month=2/...parquet\n# partitioned_data/year=2024/month=1/...parquet\n# partitioned_data/year=2024/month=2/...parquet\n</code></pre>"},{"location":"how-to/read-and-write-datasets/#dataset-with-custom-options","title":"Dataset with Custom Options","text":"<pre><code># Write with specific options\nfs.write_pyarrow_dataset(\n    data=table,\n    path=\"dataset\",\n    format=\"parquet\",\n    compression=\"snappy\",\n    max_rows_per_file=1000000,\n    existing_data_behavior=\"overwrite_or_ignore\"\n)\n</code></pre>"},{"location":"how-to/read-and-write-datasets/#duckdb-dataset-operations","title":"DuckDB Dataset Operations","text":""},{"location":"how-to/read-and-write-datasets/#basic-dataset-operations","title":"Basic Dataset Operations","text":"<pre><code>from fsspeckit.datasets import DuckDBParquetHandler\nimport polars as pl\n\n# Initialize handler\nhandler = DuckDBParquetHandler(storage_options=storage_options)\n\n# Create sample data\ndata = pl.DataFrame({\n    \"id\": [1, 2, 3, 4, 5],\n    \"category\": [\"A\", \"B\", \"A\", \"B\", \"A\"],\n    \"value\": [10.5, 20.3, 15.7, 25.1, 12.8],\n    \"timestamp\": [\"2023-01-01\", \"2023-01-02\", \"2023-01-03\", \"2023-01-04\", \"2023-01-05\"]\n})\n\n# Write dataset\nhandler.write_parquet_dataset(data, \"s3://bucket/my-dataset/\")\n\n# Read dataset back\nresult = handler.read_parquet_dataset(\"s3://bucket/my-dataset/\")\nprint(result)\n</code></pre>"},{"location":"how-to/read-and-write-datasets/#sql-analytics","title":"SQL Analytics","text":"<pre><code># Execute SQL queries on datasets\nresult = handler.execute_sql(\"\"\"\n    SELECT \n        category,\n        COUNT(*) as count,\n        AVG(value) as avg_value,\n        SUM(value) as total_value,\n        MIN(timestamp) as first_date,\n        MAX(timestamp) as last_date\n    FROM parquet_scan('s3://bucket/my-dataset/')\n    GROUP BY category\n    ORDER BY category\n\"\"\")\n\nprint(result)\n\n# Complex analytics\nanalytics = handler.execute_sql(\"\"\"\n    WITH daily_stats AS (\n        SELECT \n            DATE(timestamp) as date,\n            category,\n            COUNT(*) as daily_count,\n            AVG(value) as daily_avg\n        FROM parquet_scan('s3://bucket/my-dataset/')\n        GROUP BY DATE(timestamp), category\n    )\n    SELECT \n        date,\n        category,\n        daily_count,\n        daily_avg,\n        LAG(daily_avg) OVER (PARTITION BY category ORDER BY date) as prev_day_avg\n    FROM daily_stats\n    ORDER BY date, category\n\"\"\")\n\nprint(analytics)\n</code></pre>"},{"location":"how-to/read-and-write-datasets/#performance-optimization","title":"Performance Optimization","text":""},{"location":"how-to/read-and-write-datasets/#batch-processing","title":"Batch Processing","text":"<pre><code># Process large datasets in batches\ndef process_batch(batch_table):\n    \"\"\"Process individual batch\"\"\"\n    # Your processing logic here\n    return len(batch_table)\n\n# Process in batches\ntotal_rows = 0\nfor batch in fs.read_parquet(\"large_dataset/*.parquet\", batch_size=\"100MB\"):\n    batch_rows = process_batch(batch)\n    total_rows += batch_rows\n    print(f\"Processed {total_rows} total rows\")\n</code></pre>"},{"location":"how-to/read-and-write-datasets/#parallel-processing","title":"Parallel Processing","text":"<pre><code># Use threading for multiple files\ndf = fs.read_csv(\"data/*.csv\", use_threads=True, num_threads=4)\n\n# Use parallel processing for custom operations\nfrom fsspeckit.common.misc import run_parallel\n\ndef process_file(file_path):\n    df = fs.read_csv_file(file_path)\n    return len(df)\n\nfile_list = [\"file1.csv\", \"file2.csv\", \"file3.csv\"]\nresults = run_parallel(\n    func=process_file,\n    data=file_list,\n    max_workers=4,\n    progress=True\n)\n\nprint(f\"File row counts: {results}\")\n</code></pre>"},{"location":"how-to/read-and-write-datasets/#memory-optimization","title":"Memory Optimization","text":"<pre><code># Optimize data types\ndf = fs.read_csv(\"data.csv\", opt_dtypes=True)\n\n# Read specific columns only\ndf = fs.read_parquet_file(\"large_data.parquet\", columns=[\"id\", \"name\"])\n\n# Use column projection for datasets\ndataset = fs.pyarrow_dataset(\"data/\")\nfiltered_data = dataset.to_table(\n    columns=[\"id\", \"name\", \"value\"],\n    filter=pyarrow.compute.greater(dataset.column(\"value\"), 100)\n)\n</code></pre>"},{"location":"how-to/read-and-write-datasets/#error-handling","title":"Error Handling","text":""},{"location":"how-to/read-and-write-datasets/#robust-file-operations","title":"Robust File Operations","text":"<pre><code>def safe_read_dataset(path, max_retries=3):\n    \"\"\"Read dataset with retry logic\"\"\"\n    for attempt in range(max_retries):\n        try:\n            return fs.read_parquet_file(path)\n        except Exception as e:\n            if attempt == max_retries - 1:\n                raise\n            print(f\"Attempt {attempt + 1} failed: {e}\")\n            time.sleep(2 ** attempt)  # Exponential backoff\n\n# Usage\ntry:\n    data = safe_read_dataset(\"s3://bucket/dataset.parquet\")\nexcept Exception as e:\n    print(f\"Failed to read dataset: {e}\")\n    # Fallback to local copy\n    data = fs.read_parquet_file(\"local_backup.parquet\")\n</code></pre>"},{"location":"how-to/read-and-write-datasets/#validation","title":"Validation","text":"<pre><code># Validate data before writing\ndef validate_dataframe(df):\n    \"\"\"Basic DataFrame validation\"\"\"\n    if len(df) == 0:\n        raise ValueError(\"DataFrame is empty\")\n\n    if df.is_null().any().any():\n        print(\"Warning: DataFrame contains null values\")\n\n    return True\n\n# Write with validation\ndata = pl.DataFrame({\"id\": [1, 2, 3], \"value\": [10, 20, 30]})\nvalidate_dataframe(data)\nfs.write_parquet(data, \"validated_data.parquet\")\n</code></pre>"},{"location":"how-to/read-and-write-datasets/#best-practices","title":"Best Practices","text":"<ol> <li>Use Appropriate Format: Choose Parquet for analytics, CSV for simplicity, JSON for flexibility</li> <li>Batch Processing: Process large datasets in batches to manage memory</li> <li>Compression: Use compression (snappy, zstd) for storage efficiency</li> <li>Partitioning: Partition datasets by query patterns for better performance</li> <li>Column Projection: Read only needed columns to reduce I/O</li> <li>Error Handling: Implement retry logic and fallback strategies</li> <li>Validation: Validate data before writing to ensure quality</li> </ol> <p>For more information on SQL filtering, see Use SQL Filters.</p>"},{"location":"how-to/sync-and-manage-files/","title":"Sync and Manage Files","text":"<p>This guide covers how to synchronize files and directories between different storage backends and manage file operations efficiently.</p>"},{"location":"how-to/sync-and-manage-files/#file-synchronization","title":"File Synchronization","text":""},{"location":"how-to/sync-and-manage-files/#basic-file-sync","title":"Basic File Sync","text":"<pre><code>from fsspeckit.common.misc import sync_files\nfrom fsspeckit.core.filesystem import filesystem\n\n# Create source and destination filesystems\nsrc_fs = filesystem(\"file\")\ndst_fs = filesystem(\"s3://bucket/\")\n\n# Sync individual files\nsync_files(\n    add_files=[\"file1.txt\", \"file2.txt\", \"file3.txt\"],\n    delete_files=[],\n    src_fs=src_fs,\n    dst_fs=dst_fs,\n    src_path=\"/source/\",\n    dst_path=\"/target/\",\n    verbose=True\n)\n</code></pre>"},{"location":"how-to/sync-and-manage-files/#sync-with-deletions","title":"Sync with Deletions","text":"<pre><code># Sync files including deletions\nsync_files(\n    add_files=[\"new_file.txt\", \"updated_file.txt\"],\n    delete_files=[\"old_file.txt\", \"obsolete_file.txt\"],\n    src_fs=src_fs,\n    dst_fs=dst_fs,\n    src_path=\"/source/\",\n    dst_path=\"/target/\",\n    verbose=True\n)\n</code></pre>"},{"location":"how-to/sync-and-manage-files/#directory-synchronization","title":"Directory Synchronization","text":"<pre><code>from fsspeckit.common.misc import sync_dir\n\n# Synchronize entire directories\nsync_dir(\n    src_fs=src_fs,\n    dst_fs=dst_fs,\n    src_path=\"/local/data/\",\n    dst_path=\"/s3/remote-data/\",\n    parallel=True,\n    n_jobs=4,\n    verbose=True\n)\n</code></pre>"},{"location":"how-to/sync-and-manage-files/#cross-cloud-synchronization","title":"Cross-Cloud Synchronization","text":"<pre><code># Sync between different cloud providers\ns3_fs = filesystem(\"s3\", storage_options=s3_options)\ngcs_fs = filesystem(\"gs\", storage_options=gcs_options)\n\nsync_dir(\n    src_fs=s3_fs,\n    dst_fs=gcs_fs,\n    src_path=\"s3://source-bucket/data/\",\n    dst_path=\"gs://dest-bucket/data/\",\n    progress=True\n)\n</code></pre>"},{"location":"how-to/sync-and-manage-files/#advanced-synchronization","title":"Advanced Synchronization","text":""},{"location":"how-to/sync-and-manage-files/#selective-sync","title":"Selective Sync","text":"<pre><code>import os\nfrom pathlib import Path\n\ndef get_recent_files(directory, days=7):\n    \"\"\"Get files modified in last N days\"\"\"\n    recent_files = []\n    cutoff_time = time.time() - (days * 24 * 3600)\n\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            file_path = os.path.join(root, file)\n            if os.path.getmtime(file_path) &gt; cutoff_time:\n                rel_path = os.path.relpath(file_path, directory)\n                recent_files.append(rel_path)\n\n    return recent_files\n\n# Sync only recent files\nrecent_files = get_recent_files(\"/data/source\", days=7)\nsync_files(\n    add_files=recent_files,\n    delete_files=[],\n    src_fs=src_fs,\n    dst_fs=dst_fs,\n    src_path=\"/data/source/\",\n    dst_path=\"/backup/recent/\",\n    verbose=True\n)\n</code></pre>"},{"location":"how-to/sync-and-manage-files/#batch-synchronization","title":"Batch Synchronization","text":"<pre><code>def sync_in_batches(src_fs, dst_fs, file_list, batch_size=100):\n    \"\"\"Sync files in batches to avoid overwhelming the system\"\"\"\n\n    for i in range(0, len(file_list), batch_size):\n        batch = file_list[i:i + batch_size]\n        print(f\"Syncing batch {i//batch_size + 1}: {len(batch)} files\")\n\n        sync_files(\n            add_files=batch,\n            delete_files=[],\n            src_fs=src_fs,\n            dst_fs=dst_fs,\n            src_path=\"/source/\",\n            dst_path=\"/target/\",\n            verbose=False\n        )\n\n        # Small delay between batches\n        time.sleep(1)\n\n# Usage\nall_files = src_fs.ls(\"/source/\", detail=True)\nfile_names = [f['name'] for f in all_files if f['type'] == 'file']\nsync_in_batches(src_fs, dst_fs, file_names, batch_size=50)\n</code></pre>"},{"location":"how-to/sync-and-manage-files/#conflict-resolution","title":"Conflict Resolution","text":"<pre><code>def sync_with_conflict_resolution(src_fs, dst_fs, src_path, dst_path, strategy=\"newer\"):\n    \"\"\"Sync with conflict resolution\"\"\"\n\n    # Get file lists from both sides\n    src_files = src_fs.ls(src_path, detail=True)\n    dst_files = dst_fs.ls(dst_path, detail=True)\n\n    src_file_dict = {f['name']: f for f in src_files}\n    dst_file_dict = {f['name']: f for f in dst_files}\n\n    files_to_add = []\n    files_to_delete = []\n\n    # Check for files to add/update\n    for name, src_info in src_file_dict.items():\n        if name not in dst_file_dict:\n            # File exists only in source\n            files_to_add.append(name)\n        else:\n            dst_info = dst_file_dict[name]\n\n            # Conflict resolution based on strategy\n            if strategy == \"newer\":\n                if src_info['modified'] &gt; dst_info['modified']:\n                    files_to_add.append(name)\n            elif strategy == \"larger\":\n                if src_info['size'] &gt; dst_info['size']:\n                    files_to_add.append(name)\n            elif strategy == \"source_wins\":\n                files_to_add.append(name)\n\n    # Check for files to delete\n    for name in dst_file_dict:\n        if name not in src_file_dict:\n            files_to_delete.append(name)\n\n    # Perform sync\n    if files_to_add or files_to_delete:\n        sync_files(\n            add_files=files_to_add,\n            delete_files=files_to_delete,\n            src_fs=src_fs,\n            dst_fs=dst_fs,\n            src_path=src_path,\n            dst_path=dst_path,\n            verbose=True\n        )\n\n    return len(files_to_add), len(files_to_delete)\n\n# Usage\nadded, deleted = sync_with_conflict_resolution(\n    src_fs, dst_fs, \n    \"/source/\", \"/target/\", \n    strategy=\"newer\"\n)\nprint(f\"Added/updated: {added}, Deleted: {deleted}\")\n</code></pre>"},{"location":"how-to/sync-and-manage-files/#partition-management","title":"Partition Management","text":""},{"location":"how-to/sync-and-manage-files/#extract-partition-information","title":"Extract Partition Information","text":"<pre><code>from fsspeckit.common.misc import get_partitions_from_path\n\n# Extract partitions from file paths\npaths = [\n    \"data/year=2023/month=01/day=15/file.parquet\",\n    \"data/year=2023/month=01/day=16/file.parquet\",\n    \"data/year=2023/month=02/day=01/file.parquet\",\n    \"data/category=sales/region=us-east/data.parquet\"\n]\n\nfor path in paths:\n    partitions = get_partitions_from_path(path)\n    print(f\"Path: {path}\")\n    print(f\"Partitions: {partitions}\")\n    print()\n\n# Output:\n# Path: data/year=2023/month=01/day=15/file.parquet\n# Partitions: {'year': '2023', 'month': '01', 'day': '15'}\n</code></pre>"},{"location":"how-to/sync-and-manage-files/#partition-based-sync","title":"Partition-Based Sync","text":"<pre><code>def sync_partitions(src_fs, dst_fs, base_path, partition_filter=None):\n    \"\"\"Sync specific partitions\"\"\"\n\n    # Get all directories (partitions)\n    all_dirs = src_fs.find(base_path, directories=True, files=False)\n\n    # Filter partitions if specified\n    if partition_filter:\n        filtered_dirs = []\n        for dir_path in all_dirs:\n            partitions = get_partitions_from_path(dir_path)\n            if partition_filter(partitions):\n                filtered_dirs.append(dir_path)\n        dirs_to_sync = filtered_dirs\n    else:\n        dirs_to_sync = all_dirs\n\n    # Sync each partition\n    for partition_dir in dirs_to_sync:\n        rel_path = os.path.relpath(partition_dir, base_path)\n\n        sync_dir(\n            src_fs=src_fs,\n            dst_fs=dst_fs,\n            src_path=partition_dir,\n            dst_path=f\"{base_path}/{rel_path}\",\n            verbose=True\n        )\n\n# Usage: Sync only 2023 data\ndef filter_2023(partitions):\n    return partitions.get('year') == '2023'\n\nsync_partitions(src_fs, dst_fs, \"/data/\", filter_2023)\n</code></pre>"},{"location":"how-to/sync-and-manage-files/#partition-maintenance","title":"Partition Maintenance","text":"<pre><code>def cleanup_old_partitions(fs, base_path, partition_key, keep_count):\n    \"\"\"Keep only N most recent partitions\"\"\"\n\n    # Get all partition directories\n    partition_dirs = []\n    for item in fs.ls(base_path, detail=True):\n        if item['type'] == 'directory':\n            partitions = get_partitions_from_path(item['name'])\n            if partition_key in partitions:\n                partition_dirs.append((partitions[partition_key], item['name']))\n\n    # Sort by partition value (assuming it's sortable)\n    partition_dirs.sort(reverse=True)\n\n    # Delete old partitions\n    for i, (value, dir_path) in enumerate(partition_dirs):\n        if i &gt;= keep_count:\n            print(f\"Deleting old partition: {dir_path} (value: {value})\")\n            fs.rm(dir_path, recursive=True)\n\n# Usage: Keep last 12 months of data\ncleanup_old_partitions(fs, \"/data/\", \"month\", 12)\n</code></pre>"},{"location":"how-to/sync-and-manage-files/#file-operations","title":"File Operations","text":""},{"location":"how-to/sync-and-manage-files/#batch-file-operations","title":"Batch File Operations","text":"<pre><code>def batch_copy_files(src_fs, dst_fs, file_pairs, batch_size=10):\n    \"\"\"Copy multiple files in batches\"\"\"\n\n    for i in range(0, len(file_pairs), batch_size):\n        batch = file_pairs[i:i + batch_size]\n        print(f\"Processing batch {i//batch_size + 1}: {len(batch)} files\")\n\n        for src_path, dst_path in batch:\n            try:\n                # Copy file\n                with src_fs.open(src_path, 'rb') as src_file:\n                    with dst_fs.open(dst_path, 'wb') as dst_file:\n                        dst_file.write(src_file.read())\n\n                print(f\"Copied: {src_path} -&gt; {dst_path}\")\n\n            except Exception as e:\n                print(f\"Failed to copy {src_path}: {e}\")\n\n# Usage\nfile_pairs = [\n    (\"/source/file1.txt\", \"/dest/file1.txt\"),\n    (\"/source/file2.txt\", \"/dest/file2.txt\"),\n    # ... more pairs\n]\n\nbatch_copy_files(src_fs, dst_fs, file_pairs, batch_size=5)\n</code></pre>"},{"location":"how-to/sync-and-manage-files/#file-validation","title":"File Validation","text":"<pre><code>def validate_file_transfer(src_fs, dst_fs, src_path, dst_path):\n    \"\"\"Validate that file was transferred correctly\"\"\"\n\n    # Check file sizes\n    src_info = src_fs.info(src_path)\n    dst_info = dst_fs.info(dst_path)\n\n    if src_info['size'] != dst_info['size']:\n        return False, f\"Size mismatch: {src_info['size']} vs {dst_info['size']}\"\n\n    # Optional: Check checksums for critical files\n    if src_info['size'] &lt; 100 * 1024 * 1024:  # Files &lt; 100MB\n        import hashlib\n\n        # Calculate source checksum\n        with src_fs.open(src_path, 'rb') as f:\n            src_hash = hashlib.md5(f.read()).hexdigest()\n\n        # Calculate destination checksum\n        with dst_fs.open(dst_path, 'rb') as f:\n            dst_hash = hashlib.md5(f.read()).hexdigest()\n\n        if src_hash != dst_hash:\n            return False, f\"Checksum mismatch: {src_hash} vs {dst_hash}\"\n\n    return True, \"Validation successful\"\n\ndef safe_sync_with_validation(src_fs, dst_fs, file_list):\n    \"\"\"Sync files with validation\"\"\"\n\n    failed_files = []\n\n    for file_path in file_list:\n        # Copy file\n        dst_path = file_path\n        try:\n            with src_fs.open(file_path, 'rb') as src_file:\n                with dst_fs.open(dst_path, 'wb') as dst_file:\n                    dst_file.write(src_file.read())\n\n            # Validate\n            is_valid, message = validate_file_transfer(src_fs, dst_fs, file_path, dst_path)\n\n            if is_valid:\n                print(f\"\u2713 {file_path}: {message}\")\n            else:\n                print(f\"\u2717 {file_path}: {message}\")\n                failed_files.append(file_path)\n\n        except Exception as e:\n            print(f\"\u2717 {file_path}: Transfer failed - {e}\")\n            failed_files.append(file_path)\n\n    return failed_files\n\n# Usage\nfailed = safe_sync_with_validation(src_fs, dst_fs, file_list)\nif failed:\n    print(f\"Failed transfers: {len(failed)}\")\n</code></pre>"},{"location":"how-to/sync-and-manage-files/#performance-optimization","title":"Performance Optimization","text":""},{"location":"how-to/sync-and-manage-files/#parallel-operations","title":"Parallel Operations","text":"<pre><code>from fsspeckit.common.misc import run_parallel\n\ndef copy_single_file(args):\n    \"\"\"Copy single file - for parallel execution\"\"\"\n    src_fs, dst_fs, src_path, dst_path = args\n\n    try:\n        with src_fs.open(src_path, 'rb') as src_file:\n            with dst_fs.open(dst_path, 'wb') as dst_file:\n                dst_file.write(src_file.read())\n        return True, src_path, None\n    except Exception as e:\n        return False, src_path, str(e)\n\ndef parallel_copy(src_fs, dst_fs, file_pairs, max_workers=8):\n    \"\"\"Copy files in parallel\"\"\"\n\n    # Prepare arguments\n    args_list = [(src_fs, dst_fs, src, dst) for src, dst in file_pairs]\n\n    # Run in parallel\n    results = run_parallel(\n        func=copy_single_file,\n        data=args_list,\n        max_workers=max_workers,\n        progress=True\n    )\n\n    # Process results\n    successful = []\n    failed = []\n\n    for success, file_path, error in results:\n        if success:\n            successful.append(file_path)\n        else:\n            failed.append((file_path, error))\n\n    return successful, failed\n\n# Usage\nfile_pairs = [(\"src/file1.txt\", \"dst/file1.txt\") for _ in range(100)]\nsuccessful, failed = parallel_copy(src_fs, dst_fs, file_pairs, max_workers=4)\n\nprint(f\"Successfully copied: {len(successful)}\")\nprint(f\"Failed: {len(failed)}\")\nfor file_path, error in failed:\n    print(f\"  {file_path}: {error}\")\n</code></pre>"},{"location":"how-to/sync-and-manage-files/#memory-efficient-operations","title":"Memory-Efficient Operations","text":"<pre><code>def copy_large_file(src_fs, dst_fs, src_path, dst_path, chunk_size=8*1024*1024):\n    \"\"\"Copy large files in chunks to manage memory\"\"\"\n\n    with src_fs.open(src_path, 'rb') as src_file:\n        with dst_fs.open(dst_path, 'wb') as dst_file:\n            while True:\n                chunk = src_file.read(chunk_size)\n                if not chunk:\n                    break\n                dst_file.write(chunk)\n\n    return True\n\ndef sync_large_files(src_fs, dst_fs, file_list, size_threshold=100*1024*1024):\n    \"\"\"Sync files with special handling for large files\"\"\"\n\n    for file_path in file_list:\n        file_info = src_fs.info(file_path)\n\n        if file_info['size'] &gt; size_threshold:\n            print(f\"Copying large file ({file_info['size']/1024/1024:.1f}MB): {file_path}\")\n            copy_large_file(src_fs, dst_fs, file_path, file_path)\n        else:\n            # Use regular copy for smaller files\n            with src_fs.open(file_path, 'rb') as src_file:\n                with dst_fs.open(file_path, 'wb') as dst_file:\n                    dst_file.write(src_file.read())\n</code></pre>"},{"location":"how-to/sync-and-manage-files/#error-handling-and-recovery","title":"Error Handling and Recovery","text":""},{"location":"how-to/sync-and-manage-files/#retry-logic","title":"Retry Logic","text":"<pre><code>import time\nimport random\n\ndef sync_with_retry(src_fs, dst_fs, src_path, dst_path, max_retries=3, backoff_factor=2):\n    \"\"\"Sync with exponential backoff retry\"\"\"\n\n    for attempt in range(max_retries):\n        try:\n            sync_dir(\n                src_fs=src_fs,\n                dst_fs=dst_fs,\n                src_path=src_path,\n                dst_path=dst_path,\n                verbose=False\n            )\n            return True, None\n\n        except Exception as e:\n            if attempt == max_retries - 1:\n                return False, str(e)\n\n            # Calculate delay with jitter\n            base_delay = backoff_factor ** attempt\n            jitter = random.uniform(0, 0.1 * base_delay)\n            delay = base_delay + jitter\n\n            print(f\"Attempt {attempt + 1} failed: {e}\")\n            print(f\"Retrying in {delay:.1f} seconds...\")\n            time.sleep(delay)\n\n    return False, \"Max retries exceeded\"\n\n# Usage\nsuccess, error = sync_with_retry(src_fs, dst_fs, \"/src/\", \"/dst/\")\nif not success:\n    print(f\"Sync failed after retries: {error}\")\n</code></pre>"},{"location":"how-to/sync-and-manage-files/#incremental-sync","title":"Incremental Sync","text":"<pre><code>def incremental_sync(src_fs, dst_fs, src_path, dst_path, state_file=\"sync_state.json\"):\n    \"\"\"Incremental sync using state file\"\"\"\n\n    import json\n    import os\n\n    # Load previous state\n    state = {}\n    if os.path.exists(state_file):\n        with open(state_file, 'r') as f:\n            state = json.load(f)\n\n    # Get current files\n    src_files = src_fs.ls(src_path, detail=True)\n\n    files_to_sync = []\n    updated_state = {}\n\n    for file_info in src_files:\n        file_path = file_info['name']\n        file_key = os.path.relpath(file_path, src_path)\n\n        # Check if file needs sync\n        last_modified = file_info['modified']\n        file_size = file_info['size']\n\n        if (file_key not in state or \n            state[file_key]['modified'] != last_modified or\n            state[file_key]['size'] != file_size):\n\n            files_to_sync.append(file_path)\n\n        # Update state\n        updated_state[file_key] = {\n            'modified': last_modified,\n            'size': file_size\n        }\n\n    # Sync changed files\n    if files_to_sync:\n        print(f\"Syncing {len(files_to_sync)} changed files...\")\n\n        for file_path in files_to_sync:\n            rel_path = os.path.relpath(file_path, src_path)\n            dst_path_full = os.path.join(dst_path, rel_path)\n\n            # Ensure destination directory exists\n            dst_fs.makedirs(os.path.dirname(dst_path_full), exist_ok=True)\n\n            # Copy file\n            with src_fs.open(file_path, 'rb') as src_file:\n                with dst_fs.open(dst_path_full, 'wb') as dst_file:\n                    dst_file.write(src_file.read())\n\n            print(f\"Synced: {rel_path}\")\n\n    # Save updated state\n    with open(state_file, 'w') as f:\n        json.dump(updated_state, f, indent=2, default=str)\n\n    return len(files_to_sync)\n\n# Usage\nsynced_count = incremental_sync(src_fs, dst_fs, \"/source/\", \"/dest/\")\nprint(f\"Synced {synced_count} files\")\n</code></pre>"},{"location":"how-to/sync-and-manage-files/#best-practices","title":"Best Practices","text":"<ol> <li>Use Parallel Operations: Enable parallel processing for large file sets</li> <li>Batch Processing: Process files in batches to manage resources</li> <li>Validation: Verify file transfers, especially for critical data</li> <li>Error Handling: Implement retry logic and proper error handling</li> <li>Incremental Sync: Use state tracking for efficient incremental updates</li> <li>Memory Management: Use chunked copying for large files</li> <li>Monitoring: Use verbose logging and progress tracking</li> </ol> <p>For more information on filesystem operations, see Work with Filesystems.</p>"},{"location":"how-to/use-sql-filters/","title":"Use SQL Filters","text":"<p>This guide shows you how to use fsspeckit's SQL filter translation to convert SQL WHERE clauses into framework-specific filter expressions for PyArrow and Polars.</p>"},{"location":"how-to/use-sql-filters/#overview","title":"Overview","text":"<p>SQL filter translation allows you to: - Write filters once using familiar SQL syntax - Apply the same logic across different data frameworks - Maintain consistent filtering logic across your codebase</p>"},{"location":"how-to/use-sql-filters/#pyarrow-filter-translation","title":"PyArrow Filter Translation","text":""},{"location":"how-to/use-sql-filters/#basic-usage","title":"Basic Usage","text":"<pre><code>import pyarrow as pa\nfrom fsspeckit.sql.filters import sql2pyarrow_filter\n\n# Define schema\nschema = pa.schema([\n    (\"id\", pa.int64()),\n    (\"name\", pa.string()),\n    (\"category\", pa.string()),\n    (\"value\", pa.float64()),\n    (\"timestamp\", pa.timestamp(\"us\"))\n])\n\n# Convert SQL to PyArrow filter\nsql_filter = \"id &gt; 100 AND category IN ('A', 'B', 'C')\"\npyarrow_filter = sql2pyarrow_filter(sql_filter, schema)\nprint(f\"PyArrow filter: {pyarrow_filter}\")\n\n# Apply filter to dataset\nimport pyarrow.parquet as pq\ndataset = pq.ParquetDataset(\"data.parquet\")\nfiltered_table = dataset.to_table(filter=pyarrow_filter)\nprint(f\"Filtered rows: {len(filtered_table)}\")\n</code></pre>"},{"location":"how-to/use-sql-filters/#complex-sql-filters","title":"Complex SQL Filters","text":"<pre><code># Complex conditions\nsql_filters = [\n    \"id &gt; 100 AND category IN ('A', 'B', 'C')\",\n    \"value LIKE 'prefix%' AND amount &gt; 1000.0\",\n    \"timestamp &gt;= '2023-01-01' AND timestamp &lt;= '2023-12-31'\",\n    \"category = 'IMPORTANT' AND (amount BETWEEN 100 AND 1000)\",\n    \"(name LIKE 'test%' OR name LIKE 'demo%') AND value &gt; 50\"\n]\n\nfor sql_filter in sql_filters:\n    pyarrow_filter = sql2pyarrow_filter(sql_filter, schema)\n    print(f\"SQL: {sql_filter}\")\n    print(f\"PyArrow: {pyarrow_filter}\")\n    print()\n</code></pre>"},{"location":"how-to/use-sql-filters/#real-world-dataset-filtering","title":"Real-world Dataset Filtering","text":"<pre><code>import pandas as pd\nimport pyarrow.parquet as pq\nfrom fsspeckit.sql.filters import sql2pyarrow_filter\n\n# Load a dataset\ndataset = pq.ParquetDataset(\"s3://bucket/large-dataset/\")\ntable = dataset.to_table()\n\n# Define your schema\nschema = table.schema\n\n# Use SQL to create filters\nsql_conditions = [\n    \"category = 'HIGH_PRIORITY'\",\n    \"amount &gt; 50000\",\n    \"timestamp &gt;= '2023-06-01'\",\n    \"status IN ('ACTIVE', 'PENDING')\"\n]\n\n# Apply filters incrementally\nfiltered_data = table\nfor condition in sql_conditions:\n    filter_expr = sql2pyarrow_filter(condition, schema)\n    filtered_data = filtered_data.filter(filter_expr)\n\nprint(f\"Original rows: {len(table)}\")\nprint(f\"Filtered rows: {len(filtered_data)}\")\n</code></pre>"},{"location":"how-to/use-sql-filters/#polars-filter-translation","title":"Polars Filter Translation","text":""},{"location":"how-to/use-sql-filters/#basic-usage_1","title":"Basic Usage","text":"<pre><code>import polars as pl\nfrom fsspeckit.sql.filters import sql2polars_filter\n\n# Define schema\nschema = pl.Schema({\n    \"id\": pl.Int64,\n    \"name\": pl.String,\n    \"category\": pl.String,\n    \"value\": pl.Float64,\n    \"timestamp\": pl.Datetime\n})\n\n# Convert SQL to Polars filter\nsql_filter = \"value LIKE 'prefix%' AND timestamp &gt;= '2023-01-01'\"\npolars_filter = sql2polars_filter(sql_filter, schema)\nprint(f\"Polars filter: {polars_filter}\")\n\n# Apply filter to DataFrame\ndf = pl.read_parquet(\"data.parquet\")\nfiltered_df = df.filter(polars_filter)\nprint(f\"Filtered rows: {len(filtered_df)}\")\n</code></pre>"},{"location":"how-to/use-sql-filters/#complex-polars-filters","title":"Complex Polars Filters","text":"<pre><code># Multiple conditions\nsql_filters = [\n    \"id &gt; 100 AND name = 'test'\",\n    \"value BETWEEN 10 AND 100 OR category = 'SPECIAL'\",\n    \"timestamp &gt;= '2023-01-01' AND (status IN ('ACTIVE', 'PENDING'))\",\n    \"name LIKE '%test%' AND value &gt; 0\",\n    \"category IN ('A', 'B', 'C') AND timestamp &lt;= '2023-12-31'\"\n]\n\npolars_schema = pl.Schema({\n    \"id\": pl.Int64,\n    \"name\": pl.String,\n    \"category\": pl.String,\n    \"value\": pl.Float64,\n    \"timestamp\": pl.Datetime,\n    \"status\": pl.String\n})\n\nfor sql_filter in sql_filters:\n    polars_filter = sql2polars_filter(sql_filter, polars_schema)\n    print(f\"SQL: {sql_filter}\")\n    print(f\"Polars: {polars_filter}\")\n    print()\n</code></pre>"},{"location":"how-to/use-sql-filters/#dataframe-filtering","title":"DataFrame Filtering","text":"<pre><code># Load DataFrame\ndf = pl.read_parquet(\"large_dataset.parquet\")\n\n# Define schema from DataFrame\nschema = df.schema\n\n# Apply multiple filters\nfilters = [\n    \"amount &gt; 1000\",\n    \"category = 'PREMIUM'\",\n    \"timestamp &gt;= '2023-01-01'\"\n]\n\nfiltered_df = df\nfor filter_sql in filters:\n    filter_expr = sql2polars_filter(filter_sql, schema)\n    filtered_df = filtered_df.filter(filter_expr)\n\nprint(f\"Original rows: {len(df)}\")\nprint(f\"Filtered rows: {len(filtered_df)}\")\n</code></pre>"},{"location":"how-to/use-sql-filters/#cross-framework-compatibility","title":"Cross-Framework Compatibility","text":""},{"location":"how-to/use-sql-filters/#same-sql-different-frameworks","title":"Same SQL, Different Frameworks","text":"<pre><code>import pyarrow as pa\nimport polars as pl\nfrom fsspeckit.sql.filters import sql2pyarrow_filter, sql2polars_filter\n\n# Define schemas\npyarrow_schema = pa.schema([\n    (\"id\", pa.int64()),\n    (\"category\", pa.string()),\n    (\"value\", pa.float64()),\n    (\"timestamp\", pa.timestamp(\"us\"))\n])\n\npolars_schema = pl.Schema({\n    \"id\": pl.Int64,\n    \"category\": pl.String,\n    \"value\": pl.Float64,\n    \"timestamp\": pl.Datetime\n})\n\n# Same SQL filter for both frameworks\nsql_filter = \"category IN ('A', 'B') AND value &gt; 100.0 AND timestamp &gt;= '2023-01-01'\"\n\n# Convert to both frameworks\npyarrow_filter = sql2pyarrow_filter(sql_filter, pyarrow_schema)\npolars_filter = sql2polars_filter(sql_filter, polars_schema)\n\nprint(f\"SQL: {sql_filter}\")\nprint(f\"PyArrow: {pyarrow_filter}\")\nprint(f\"Polars: {polars_filter}\")\n\n# Apply to data\n# PyArrow\nimport pyarrow.parquet as pq\ndataset = pq.ParquetDataset(\"data.parquet\")\narrow_result = dataset.to_table(filter=pyarrow_filter)\n\n# Polars\ndf = pl.read_parquet(\"data.parquet\")\npolars_result = df.filter(polars_filter)\n\nprint(f\"PyArrow result: {len(arrow_result)} rows\")\nprint(f\"Polars result: {len(polars_result)} rows\")\n</code></pre>"},{"location":"how-to/use-sql-filters/#advanced-sql-features","title":"Advanced SQL Features","text":""},{"location":"how-to/use-sql-filters/#supported-sql-operators","title":"Supported SQL Operators","text":"<pre><code># Comparison operators\nsql_filters = [\n    \"id = 100\",                    # Equals\n    \"id != 100\",                   # Not equals\n    \"id &gt; 100\",                    # Greater than\n    \"id &gt;= 100\",                   # Greater than or equal\n    \"id &lt; 100\",                    # Less than\n    \"id &lt;= 100\",                   # Less than or equal\n    \"value BETWEEN 10 AND 100\"      # Between\n]\n\n# Logical operators\nsql_filters.extend([\n    \"id &gt; 100 AND name = 'test'\",           # AND\n    \"id &gt; 100 OR name = 'test'\",            # OR\n    \"NOT (id &gt; 100)\",                      # NOT\n    \"(id &gt; 100 AND name = 'test') OR value = 50\"  # Complex logic\n])\n\n# String operations\nsql_filters.extend([\n    \"name LIKE 'prefix%'\",                   # LIKE with wildcard\n    \"name LIKE '%suffix'\",                   # LIKE with wildcard\n    \"name LIKE '%middle%'\",                  # LIKE with wildcard\n    \"name IN ('value1', 'value2', 'value3')\",  # IN clause\n    \"name NOT IN ('bad1', 'bad2')\"         # NOT IN clause\n])\n\n# Null operations\nsql_filters.extend([\n    \"name IS NULL\",                          # IS NULL\n    \"name IS NOT NULL\",                      # IS NOT NULL\n])\n</code></pre>"},{"location":"how-to/use-sql-filters/#date-and-time-operations","title":"Date and Time Operations","text":"<pre><code># Date/time filters\ndatetime_filters = [\n    \"timestamp &gt;= '2023-01-01'\",\n    \"timestamp &gt; '2023-01-01 10:30:00'\",\n    \"timestamp BETWEEN '2023-01-01' AND '2023-12-31'\",\n    \"DATE(timestamp) = '2023-01-15'\",\n    \"YEAR(timestamp) = 2023\",\n    \"MONTH(timestamp) = 6\",\n    \"DAY(timestamp) = 15\"\n]\n\n# Apply to dataset\nfor filter_sql in datetime_filters:\n    filter_expr = sql2pyarrow_filter(filter_sql, schema)\n    filtered_data = dataset.to_table(filter=filter_expr)\n    print(f\"{filter_sql}: {len(filtered_data)} rows\")\n</code></pre>"},{"location":"how-to/use-sql-filters/#practical-examples","title":"Practical Examples","text":""},{"location":"how-to/use-sql-filters/#e-commerce-analytics","title":"E-commerce Analytics","text":"<pre><code># E-commerce dataset filtering\necommerce_filters = [\n    # High-value customers\n    \"total_spent &gt; 1000 AND customer_type = 'PREMIUM'\",\n\n    # Recent active users\n    \"last_purchase &gt;= '2023-06-01' AND login_count &gt; 10\",\n\n    # Specific product categories\n    \"category IN ('Electronics', 'Books', 'Home') AND price &gt; 50\",\n\n    # Geographic filtering\n    \"country IN ('US', 'CA', 'UK') AND order_total &gt; 100\",\n\n    # Time-based campaigns\n    \"order_date &gt;= '2023-11-01' AND order_date &lt;= '2023-11-30' AND campaign_id IS NOT NULL\"\n]\n\nfor filter_sql in ecommerce_filters:\n    filter_expr = sql2pyarrow_filter(filter_sql, ecommerce_schema)\n    results = dataset.to_table(filter=filter_expr)\n    print(f\"{filter_sql}: {len(results)} orders\")\n</code></pre>"},{"location":"how-to/use-sql-filters/#iot-data-processing","title":"IoT Data Processing","text":"<pre><code># IoT sensor data filtering\niot_filters = [\n    # Sensor health checks\n    \"sensor_type = 'temperature' AND value BETWEEN -40 AND 125\",\n\n    # Anomaly detection\n    \"value &gt; (avg_value + 3 * std_dev) OR value &lt; (avg_value - 3 * std_dev)\",\n\n    # Device status\n    \"device_status = 'ACTIVE' AND battery_level &gt; 20\",\n\n    # Time windows\n    \"timestamp &gt;= '2023-01-01' AND timestamp &lt; '2023-01-02' AND location = 'warehouse_a'\",\n\n    # Data quality\n    \"signal_strength &gt; 0.8 AND error_rate &lt; 0.01\"\n]\n\nfor filter_sql in iot_filters:\n    filter_expr = sql2polars_filter(filter_sql, iot_schema)\n    filtered_df = iot_df.filter(filter_expr)\n    print(f\"{filter_sql}: {len(filtered_df)} readings\")\n</code></pre>"},{"location":"how-to/use-sql-filters/#financial-data-analysis","title":"Financial Data Analysis","text":"<pre><code># Financial transaction filtering\nfinancial_filters = [\n    # Large transactions\n    \"amount &gt; 10000 AND transaction_type = 'WIRE'\",\n\n    # Suspicious patterns\n    \"amount &gt; 5000 AND time_of_day BETWEEN 2 AND 5 AND country != 'US'\",\n\n    # Regulatory reporting\n    \"transaction_type IN ('WIRE', 'ACH', 'INTERNATIONAL') AND amount &gt; 1000\",\n\n    # Risk assessment\n    \"risk_score &gt; 80 AND customer_age &lt; 25 AND amount &gt; 5000\",\n\n    # Compliance checks\n    \"sanctioned_country = FALSE AND amount &gt; 100000 AND verified_customer = TRUE\"\n]\n\nfor filter_sql in financial_filters:\n    filter_expr = sql2pyarrow_filter(filter_sql, financial_schema)\n    compliance_data = dataset.to_table(filter=filter_expr)\n    print(f\"{filter_sql}: {len(compliance_data)} transactions\")\n</code></pre>"},{"location":"how-to/use-sql-filters/#performance-considerations","title":"Performance Considerations","text":""},{"location":"how-to/use-sql-filters/#filter-optimization","title":"Filter Optimization","text":"<pre><code># Good: Specific filters that can be pushed down\ngood_filters = [\n    \"id = 100\",                    # Equality on indexed column\n    \"category = 'IMPORTANT'\",        # Low cardinality column\n    \"timestamp &gt;= '2023-01-01'\"     # Range filter\n]\n\n# Less optimal: Complex expressions\ncomplex_filters = [\n    \"UPPER(name) LIKE 'TEST%'\",      # Function on column\n    \"value * 1.1 &gt; 100\",           # Calculation on column\n    \"SUBSTRING(description, 1, 5) = 'ERROR'\"  # String function\n]\n\n# Use specific filters when possible\nfor filter_sql in good_filters:\n    filter_expr = sql2pyarrow_filter(filter_sql, schema)\n    filtered_data = dataset.to_table(filter=filter_expr)\n</code></pre>"},{"location":"how-to/use-sql-filters/#batch-filtering","title":"Batch Filtering","text":"<pre><code># Apply multiple filters efficiently\ndef apply_multiple_filters(dataset, filters, schema):\n    \"\"\"Apply multiple filters with early exit\"\"\"\n    result = dataset.to_table()\n\n    for i, filter_sql in enumerate(filters):\n        filter_expr = sql2pyarrow_filter(filter_sql, schema)\n        result = result.filter(filter_expr)\n\n        print(f\"Filter {i+1} ({filter_sql}): {len(result)} rows\")\n\n        # Early exit if no data\n        if len(result) == 0:\n            break\n\n    return result\n\n# Usage\nfilters = [\n    \"category = 'ACTIVE'\",\n    \"timestamp &gt;= '2023-01-01'\",\n    \"value &gt; 100\"\n]\n\nresult = apply_multiple_filters(dataset, filters, schema)\n</code></pre>"},{"location":"how-to/use-sql-filters/#error-handling","title":"Error Handling","text":""},{"location":"how-to/use-sql-filters/#schema-validation","title":"Schema Validation","text":"<pre><code>def safe_filter_conversion(sql_filter, schema):\n    \"\"\"Convert SQL filter with error handling\"\"\"\n    try:\n        filter_expr = sql2pyarrow_filter(sql_filter, schema)\n        return filter_expr, None\n    except Exception as e:\n        return None, str(e)\n\n# Test with invalid filters\ntest_filters = [\n    \"id &gt; 100\",                    # Valid\n    \"invalid_column = 100\",          # Invalid column\n    \"id &gt; 'not_a_number'\",          # Type mismatch\n    \"id &gt; 100 AND\"                 # Invalid SQL syntax\n]\n\nfor filter_sql in test_filters:\n    filter_expr, error = safe_filter_conversion(filter_sql, schema)\n    if error:\n        print(f\"Error in '{filter_sql}': {error}\")\n    else:\n        print(f\"Valid filter: {filter_sql}\")\n</code></pre>"},{"location":"how-to/use-sql-filters/#fallback-strategies","title":"Fallback Strategies","text":"<pre><code>def apply_filter_with_fallback(dataset, sql_filter, schema):\n    \"\"\"Apply filter with fallback options\"\"\"\n    try:\n        # Try PyArrow filter\n        filter_expr = sql2pyarrow_filter(sql_filter, schema)\n        return dataset.to_table(filter=filter_expr)\n    except Exception as e:\n        print(f\"PyArrow filter failed: {e}\")\n\n        try:\n            # Fallback to Polars\n            import polars as pl\n            df = pl.from_arrow(dataset.to_table())\n            polars_filter = sql2polars_filter(sql_filter, df.schema)\n            return df.filter(polars_filter).to_arrow()\n        except Exception as e2:\n            print(f\"Polars filter also failed: {e2}\")\n\n            # Final fallback: load all data and filter manually\n            print(\"Loading all data and filtering manually\")\n            return dataset.to_table()\n\n# Usage\nresult = apply_filter_with_fallback(dataset, \"id &gt; 100\", schema)\n</code></pre>"},{"location":"how-to/use-sql-filters/#best-practices","title":"Best Practices","text":"<ol> <li>Use Specific Columns: Filter on indexed or low-cardinality columns when possible</li> <li>Avoid Functions: Don't use functions on columns in WHERE clauses (e.g., <code>UPPER(name)</code>)</li> <li>Type Consistency: Ensure filter values match column data types</li> <li>Schema Validation: Always provide accurate schema for conversion</li> <li>Error Handling: Implement proper error handling for invalid SQL</li> <li>Performance Testing: Test filter performance on large datasets</li> <li>Incremental Filtering: Apply multiple simple filters instead of one complex filter</li> </ol> <p>For more information on dataset operations, see Read and Write Datasets.</p>"},{"location":"how-to/work-with-filesystems/","title":"Work with Filesystems","text":"<p>This guide covers how to create and use filesystems with fsspeckit, including path safety, caching, and advanced filesystem operations.</p>"},{"location":"how-to/work-with-filesystems/#creating-filesystems","title":"Creating Filesystems","text":""},{"location":"how-to/work-with-filesystems/#basic-local-filesystem","title":"Basic Local Filesystem","text":"<pre><code>from fsspeckit.core.filesystem import filesystem\n\n# Create local filesystem\nfs = filesystem(\"file\")\n\n# Create with auto-mkdir\nfs = filesystem(\"file\", auto_mkdir=True)\n</code></pre>"},{"location":"how-to/work-with-filesystems/#cloud-filesystems","title":"Cloud Filesystems","text":"<pre><code># Auto-detect protocol from URI\ns3_fs = filesystem(\"s3://bucket/path\")      # S3\ngcs_fs = filesystem(\"gs://bucket/path\")      # Google Cloud Storage\naz_fs = filesystem(\"az://container/path\")    # Azure Blob Storage\ngithub_fs = filesystem(\"github://owner/repo\") # GitHub\n\n# Manual protocol specification\ns3_fs = filesystem(\"s3\", storage_options={\"region\": \"us-east-1\"})\n</code></pre>"},{"location":"how-to/work-with-filesystems/#with-storage-options","title":"With Storage Options","text":"<pre><code>from fsspeckit.storage_options import AwsStorageOptions\n\naws_options = AwsStorageOptions(\n    region=\"us-east-1\",\n    access_key_id=\"YOUR_ACCESS_KEY\",\n    secret_access_key=\"YOUR_SECRET_KEY\"\n)\n\nfs = filesystem(\"s3\", storage_options=aws_options.to_dict())\n</code></pre>"},{"location":"how-to/work-with-filesystems/#path-safety-with-dirfilesystem","title":"Path Safety with DirFileSystem","text":"<p>fsspeckit wraps filesystems in <code>DirFileSystem</code> by default for enhanced security.</p>"},{"location":"how-to/work-with-filesystems/#basic-path-safety","title":"Basic Path Safety","text":"<pre><code>from fsspeckit.core.filesystem import filesystem, DirFileSystem\n\n# Default behavior: wrapped in DirFileSystem\nfs = filesystem(\"/data\", dirfs=True)\n\n# All operations confined to /data directory\nfs.ls(\"/subdir\")  # Works\nfs.open(\"/data/file.txt\", \"r\")  # Works\n\n# Attempting to escape fails\ntry:\n    fs.open(\"../../../etc/passwd\", \"r\")\nexcept (ValueError, PermissionError) as e:\n    print(f\"Security check worked: {e}\")\n</code></pre>"},{"location":"how-to/work-with-filesystems/#manual-dirfilesystem-creation","title":"Manual DirFileSystem Creation","text":"<pre><code># Create base filesystem\nbase_fs = filesystem(\"file\")\n\n# Create safe filesystem confined to specific directory\nsafe_fs = DirFileSystem(fs=base_fs, path=\"/allowed/directory\")\n\ntry:\n    # This works - within allowed directory\n    with safe_fs.open(\"/allowed/directory/file.txt\", \"w\") as f:\n        f.write(\"Safe content\")\n\n    # This fails - outside allowed directory\n    safe_fs.open(\"/etc/passwd\", \"r\")  # Raises ValueError/PermissionError\n\nexcept (ValueError, PermissionError) as e:\n    print(f\"Security check worked: {e}\")\n</code></pre>"},{"location":"how-to/work-with-filesystems/#hierarchical-filesystems","title":"Hierarchical Filesystems","text":"<pre><code># Create parent filesystem\nparent_fs = filesystem(\"/datasets\", dirfs=True)\n\n# Create child filesystem with parent as base\nchild_fs = filesystem(\"/datasets/project1\", dirfs=True, base_fs=parent_fs)\n\n# Files are accessible only within the base directory\n# Relative paths are resolved relative to parent filesystem's base directory\n</code></pre>"},{"location":"how-to/work-with-filesystems/#caching","title":"Caching","text":"<p>Caching improves performance for remote filesystems by storing frequently accessed data locally.</p>"},{"location":"how-to/work-with-filesystems/#enable-caching","title":"Enable Caching","text":"<pre><code># Enable caching with default settings\nfs = filesystem(\"s3://bucket/\", cached=True)\n\n# Enable with custom cache directory\nfs = filesystem(\"s3://bucket/\", cached=True, cache_storage=\"/tmp/cache\")\n\n# Enable with verbose logging\nfs = filesystem(\"s3://bucket/\", cached=True, verbose=True)\n</code></pre>"},{"location":"how-to/work-with-filesystems/#cache-management","title":"Cache Management","text":"<pre><code># Clear all caches\nfs.clear_cache()\n\n# Sync cache (ensure data is written)\nfs.sync_cache()\n\n# Check cache size (for MonitoredSimpleCacheFileSystem)\nif hasattr(fs, 'cache_size'):\n    size = fs.cache_size\n    print(f\"Cache size: {size}\")\n</code></pre>"},{"location":"how-to/work-with-filesystems/#cache-best-practices","title":"Cache Best Practices","text":"<pre><code># Good: Enable caching for remote filesystems\nremote_fs = filesystem(\"s3://data/\", cached=True)\n\n# Not necessary: Local filesystems don't need caching\nlocal_fs = filesystem(\"file\")  # cached=False by default\n\n# Use with large files\nfs = filesystem(\"s3://large-datasets/\", cached=True, cache_storage=\"/ssd/cache\")\n</code></pre>"},{"location":"how-to/work-with-filesystems/#basic-file-operations","title":"Basic File Operations","text":""},{"location":"how-to/work-with-filesystems/#reading-files","title":"Reading Files","text":"<pre><code># Read text file\nwith fs.open(\"data.txt\", \"r\") as f:\n    content = f.read()\n\n# Read binary file\nwith fs.open(\"data.bin\", \"rb\") as f:\n    data = f.read()\n\n# Read first few bytes\nwith fs.open(\"large_file.txt\", \"r\") as f:\n    header = f.read(1024)\n</code></pre>"},{"location":"how-to/work-with-filesystems/#writing-files","title":"Writing Files","text":"<pre><code># Write text file\nwith fs.open(\"output.txt\", \"w\") as f:\n    f.write(\"Hello, World!\")\n\n# Write binary file\nwith fs.open(\"output.bin\", \"wb\") as f:\n    f.write(binary_data)\n\n# Append to file\nwith fs.open(\"log.txt\", \"a\") as f:\n    f.write(\"New log entry\\n\")\n</code></pre>"},{"location":"how-to/work-with-filesystems/#directory-operations","title":"Directory Operations","text":"<pre><code># List files\nfiles = fs.ls(\"/path/to/directory\")\nprint(f\"Files: {files}\")\n\n# List with details\nfiles = fs.ls(\"/path/\", detail=True)\nfor file_info in files:\n    print(f\"{file_info['name']}: {file_info['size']} bytes\")\n\n# Create directory\nfs.makedirs(\"/new/directory\", exist_ok=True)\n\n# Check if path exists\nexists = fs.exists(\"/path/to/file\")\nis_dir = fs.isdir(\"/path/to/directory\")\nis_file = fs.isfile(\"/path/to/file\")\n\n# Get file info\ninfo = fs.info(\"/path/to/file\")\nprint(f\"Size: {info['size']}, Modified: {info['modified']}\")\n</code></pre>"},{"location":"how-to/work-with-filesystems/#extended-io-operations","title":"Extended I/O Operations","text":"<p>fsspeckit adds rich I/O methods to all fsspec filesystems.</p>"},{"location":"how-to/work-with-filesystems/#json-operations","title":"JSON Operations","text":"<pre><code># Read single JSON file\ndata = fs.read_json_file(\"data.json\")  # Returns dict\ndf = fs.read_json_file(\"data.json\", as_dataframe=True)  # Returns Polars DF\n\n# Read multiple JSON files with batching\nfor batch in fs.read_json(\"data/*.json\", batch_size=5):\n    # Process batch\n    pass\n\n# Read JSON Lines format\ndf = fs.read_json(\"data/lines.jsonl\", as_dataframe=True)\n\n# With threading\ndf = fs.read_json(\"data/*.json\", use_threads=True, num_threads=4)\n\n# Include source file path\ndf = fs.read_json(\"data/*.json\", include_file_path=True)\n</code></pre>"},{"location":"how-to/work-with-filesystems/#csv-operations","title":"CSV Operations","text":"<pre><code># Read single CSV\ndf = fs.read_csv_file(\"data.csv\")\n\n# Read multiple CSV files\ndf = fs.read_csv(\"data/*.csv\", concat=True)\n\n# Batch reading\nfor batch in fs.read_csv(\"data/*.csv\", batch_size=10):\n    pass\n\n# Optimize data types\ndf = fs.read_csv(\"data/*.csv\", opt_dtypes=True)\n\n# With parallelism\ndf = fs.read_csv(\"data/*.csv\", use_threads=True)\n</code></pre>"},{"location":"how-to/work-with-filesystems/#parquet-operations","title":"Parquet Operations","text":"<pre><code># Read single Parquet file\ntable = fs.read_parquet_file(\"data.parquet\")\n\n# Read multiple with schema unification\ntable = fs.read_parquet(\"data/*.parquet\", concat=True)\n\n# Batch reading\nfor batch in fs.read_parquet(\"data/*.parquet\", batch_size=20):\n    pass\n\n# With partitioning support\ntable = fs.read_parquet(\"partitioned_data/**/*.parquet\", concat=True)\n\n# Include file path column\ntable = fs.read_parquet(\"data/*.parquet\", include_file_path=True)\n</code></pre>"},{"location":"how-to/work-with-filesystems/#universal-reader","title":"Universal Reader","text":"<pre><code># Auto-detect format from file extension\ndf = fs.read_files(\"data/mixed/*\", format=\"auto\")\n\n# Explicit format\ndf = fs.read_files(\"data/*.csv\", format=\"csv\")\n\n# Control result type\ndf_polars = fs.read_files(\"data/*.parquet\", as_dataframe=True)\ntable_arrow = fs.read_files(\"data/*.parquet\", as_dataframe=False)\n</code></pre>"},{"location":"how-to/work-with-filesystems/#writing-operations","title":"Writing Operations","text":""},{"location":"how-to/work-with-filesystems/#dataframe-writing","title":"DataFrame Writing","text":"<pre><code>import polars as pl\n\n# Create DataFrame\ndf = pl.DataFrame({\"col1\": [1, 2, 3], \"col2\": [4, 5, 6]})\n\n# Write Parquet\nfs.write_parquet(df, \"output.parquet\")\n\n# Write CSV\nfs.write_csv(df, \"output.csv\")\n\n# Write JSON\nfs.write_json(df, \"output.json\")\n</code></pre>"},{"location":"how-to/work-with-filesystems/#dataset-writing","title":"Dataset Writing","text":"<pre><code>import pyarrow as pa\n\n# Write partitioned dataset\ntable = pa.table({\"year\": [2023, 2023, 2024], \"value\": [10, 20, 30]})\nfs.write_pyarrow_dataset(\n    data=table,\n    path=\"partitioned_data\",\n    partition_by=[\"year\"],\n    format=\"parquet\",\n    compression=\"zstd\"\n)\n</code></pre>"},{"location":"how-to/work-with-filesystems/#error-handling","title":"Error Handling","text":"<p>Always wrap filesystem operations in try-except blocks:</p> <pre><code>from fsspeckit.storage_options import AwsStorageOptions\n\ntry:\n    # Try to create filesystem\n    storage_options = AwsStorageOptions(\n        region=\"us-east-1\",\n        access_key_id=\"invalid_key\",\n        secret_access_key=\"invalid_secret\"\n    )\n    fs = storage_options.to_filesystem()\n\n    # Try to use it\n    files = fs.ls(\"s3://bucket/\")\n\nexcept Exception as e:\n    print(f\"Operation failed: {e}\")\n\n    # Fall back to local filesystem\n    fs = filesystem(\"file\")\n    print(\"Fell back to local filesystem\")\n</code></pre>"},{"location":"how-to/work-with-filesystems/#performance-tips","title":"Performance Tips","text":""},{"location":"how-to/work-with-filesystems/#use-caching-for-remote-storage","title":"Use Caching for Remote Storage","text":"<pre><code># Good: Enable caching for remote filesystems\nremote_fs = filesystem(\"s3://data/\", cached=True)\n\n# Configure cache size for large datasets\nfs = filesystem(\"s3://large-data/\", cached=True, cache_storage=\"/fast/ssd/cache\")\n</code></pre>"},{"location":"how-to/work-with-filesystems/#batch-operations","title":"Batch Operations","text":"<pre><code># Good: Batch file operations\nfor batch in fs.read_json(\"data/*.json\", batch_size=100):\n    process_batch(batch)\n\n# Good: Use threading for multiple files\ndf = fs.read_csv(\"data/*.csv\", use_threads=True, num_threads=4)\n</code></pre>"},{"location":"how-to/work-with-filesystems/#column-projection","title":"Column Projection","text":"<pre><code># Good: Read only needed columns\ndf = fs.read_parquet(\"large_dataset.parquet\", columns=[\"id\", \"name\"])\n</code></pre>"},{"location":"how-to/work-with-filesystems/#best-practices","title":"Best Practices","text":"<ol> <li>Use DirFileSystem: Always use path-safe filesystems for security</li> <li>Enable Caching: Use caching for remote filesystems</li> <li>Error Handling: Wrap operations in try-except blocks</li> <li>Batch Processing: Use batch operations for large datasets</li> <li>Environment Configuration: Load credentials from environment variables</li> </ol> <p>For more information on dataset operations, see Read and Write Datasets.</p>"},{"location":"reference/api-guide/","title":"API Guide","text":"<p>This guide provides a capability-oriented overview of fsspeckit's public API. For detailed method signatures and parameters, see the generated API documentation.</p>"},{"location":"reference/api-guide/#core-capabilities","title":"Core Capabilities","text":""},{"location":"reference/api-guide/#filesystem-factory","title":"Filesystem Factory","text":"<p>Create configured filesystems with protocol inference and path safety.</p> <p>Capability: Configure storage Functions: <code>filesystem()</code> API Reference: fsspeckit.core.filesystem How-to Guides: Work with Filesystems, Configure Cloud Storage</p>"},{"location":"reference/api-guide/#storage-options","title":"Storage Options","text":"<p>Structured configuration for cloud and Git providers.</p> <p>Capability: Configure storage Classes: <code>AwsStorageOptions</code>, <code>GcsStorageOptions</code>, <code>AzureStorageOptions</code>, <code>GitHubStorageOptions</code>, <code>GitLabStorageOptions</code> API Reference: fsspeckit.storage_options How-to Guides: Configure Cloud Storage</p>"},{"location":"reference/api-guide/#data-processing-capabilities","title":"Data Processing Capabilities","text":""},{"location":"reference/api-guide/#dataset-operations","title":"Dataset Operations","text":"<p>High-performance dataset operations with DuckDB and PyArrow.</p> <p>Capability: Process datasets Classes: <code>DuckDBParquetHandler</code> Functions: <code>optimize_parquet_dataset_pyarrow</code>, <code>compact_parquet_dataset_pyarrow</code> API Reference: fsspeckit.datasets How-to Guides: Read and Write Datasets</p>"},{"location":"reference/api-guide/#extended-io","title":"Extended I/O","text":"<p>Enhanced file reading and writing capabilities.</p> <p>Capability: Read/write files Methods: <code>read_json()</code>, <code>read_csv()</code>, <code>read_parquet()</code>, <code>write_json()</code>, <code>write_csv()</code>, <code>write_parquet()</code> API Reference: fsspeckit.core.ext How-to Guides: Read and Write Datasets</p>"},{"location":"reference/api-guide/#sql-filter-translation","title":"SQL Filter Translation","text":"<p>Convert SQL WHERE clauses to framework-specific expressions.</p> <p>Capability: Filter data with SQL Functions: <code>sql2pyarrow_filter()</code>, <code>sql2polars_filter()</code> API Reference: fsspeckit.sql.filters How-to Guides: Use SQL Filters</p>"},{"location":"reference/api-guide/#utility-capabilities","title":"Utility Capabilities","text":""},{"location":"reference/api-guide/#parallel-processing","title":"Parallel Processing","text":"<p>Execute functions across multiple inputs with progress tracking.</p> <p>Capability: Process data in parallel Function: <code>run_parallel()</code> API Reference: fsspeckit.common.misc How-to Guides: Optimize Performance</p>"},{"location":"reference/api-guide/#file-synchronization","title":"File Synchronization","text":"<p>Synchronize files and directories between storage backends.</p> <p>Capability: Sync files Functions: <code>sync_files()</code>, <code>sync_dir()</code> API Reference: fsspeckit.common.misc How-to Guides: Sync and Manage Files</p>"},{"location":"reference/api-guide/#type-conversion","title":"Type Conversion","text":"<p>Convert between different data formats and optimize data types.</p> <p>Capability: Convert and optimize data Functions: <code>dict_to_dataframe()</code>, <code>to_pyarrow_table()</code>, <code>convert_large_types_to_normal()</code> API Reference: fsspeckit.common.types How-to Guides: Read and Write Datasets</p>"},{"location":"reference/api-guide/#domain-package-organization","title":"Domain Package Organization","text":"<p>fsspeckit is organized into domain-specific packages for better discoverability:</p>"},{"location":"reference/api-guide/#core-package-fsspeckitcore","title":"Core Package (<code>fsspeckit.core</code>)","text":"<p>Foundation layer providing filesystem APIs and path safety.</p> <ul> <li>Filesystem Creation: Enhanced <code>filesystem()</code> function with protocol inference</li> <li>Path Safety: <code>DirFileSystem</code> wrapper for secure directory confinement</li> <li>Extended I/O: Rich file reading/writing methods</li> <li>Base Classes: Enhanced filesystem base classes</li> </ul>"},{"location":"reference/api-guide/#storage-options-fsspeckitstorage_options","title":"Storage Options (<code>fsspeckit.storage_options</code>)","text":"<p>Configuration layer for cloud and Git providers.</p> <ul> <li>Provider Classes: Structured configuration for AWS, GCP, Azure, GitHub, GitLab</li> <li>Factory Functions: Environment-based and URI-based configuration</li> <li>Conversion Methods: Serialize to/from YAML, environment variables</li> </ul>"},{"location":"reference/api-guide/#datasets-fsspeckitdatasets","title":"Datasets (<code>fsspeckit.datasets</code>)","text":"<p>Data processing layer for large-scale operations.</p> <ul> <li>DuckDB Handler: High-performance parquet operations with SQL integration</li> <li>PyArrow Helpers: Dataset optimization, compaction, and merging</li> <li>Schema Management: Type conversion and schema evolution</li> </ul>"},{"location":"reference/api-guide/#sql-fsspeckitsql","title":"SQL (<code>fsspeckit.sql</code>)","text":"<p>Query translation layer for cross-framework compatibility.</p> <ul> <li>Filter Translation: SQL to PyArrow and Polars expressions</li> <li>Schema Awareness: Type-aware filter generation</li> <li>Cross-Framework: Consistent querying across data backends</li> </ul>"},{"location":"reference/api-guide/#common-fsspeckitcommon","title":"Common (<code>fsspeckit.common</code>)","text":"<p>Shared utilities layer used across all domains.</p> <ul> <li>Parallel Processing: Concurrent execution with progress tracking</li> <li>Type Conversion: Format conversion and optimization</li> <li>File Operations: Synchronization and path utilities</li> <li>Security: Path validation and credential scrubbing</li> </ul>"},{"location":"reference/api-guide/#usage-patterns","title":"Usage Patterns","text":""},{"location":"reference/api-guide/#basic-workflow","title":"Basic Workflow","text":"<pre><code># 1. Configure storage\nfrom fsspeckit.storage_options import storage_options_from_env\nfrom fsspeckit.core.filesystem import filesystem\n\noptions = storage_options_from_env(\"s3\")\nfs = filesystem(\"s3\", storage_options=options.to_dict())\n\n# 2. Process data\nfrom fsspeckit.datasets import DuckDBParquetHandler\n\nhandler = DuckDBParquetHandler(storage_options=options.to_dict())\nresult = handler.execute_sql(\"SELECT * FROM parquet_scan('data/') WHERE category = 'A'\")\n\n# 3. Optimize performance\nfrom fsspeckit.common.misc import run_parallel\n\nresults = run_parallel(process_file, file_list, max_workers=4)\n</code></pre>"},{"location":"reference/api-guide/#advanced-workflow","title":"Advanced Workflow","text":"<pre><code># 1. Multi-cloud configuration\nfrom fsspeckit.storage_options import AwsStorageOptions, GcsStorageOptions\n\naws_fs = AwsStorageOptions(region=\"us-east-1\").to_filesystem()\ngcs_fs = GcsStorageOptions(project=\"my-project\").to_filesystem()\n\n# 2. Cross-framework filtering\nfrom fsspeckit.sql.filters import sql2pyarrow_filter, sql2polars_filter\n\npyarrow_filter = sql2pyarrow_filter(\"value &gt; 100\", schema)\npolars_filter = sql2polars_filter(\"value &gt; 100\", schema)\n\n# 3. Dataset optimization\nfrom fsspeckit.datasets.pyarrow import optimize_parquet_dataset_pyarrow\n\noptimize_parquet_dataset_pyarrow(\n    dataset_path=\"s3://bucket/data/\",\n    z_order_columns=[\"category\", \"timestamp\"],\n    target_file_size=\"256MB\"\n)\n</code></pre>"},{"location":"reference/api-guide/#migration-from-utils","title":"Migration from Utils","text":"<p>The <code>fsspeckit.utils</code> module provides backwards compatibility. For new code, use domain packages:</p> Legacy Import Domain Package Recommended Import <code>from fsspeckit.utils import run_parallel</code> Common <code>from fsspeckit.common.misc import run_parallel</code> <code>from fsspeckit.utils import DuckDBParquetHandler</code> Datasets <code>from fsspeckit.datasets import DuckDBParquetHandler</code> <code>from fsspeckit.utils import sql2pyarrow_filter</code> SQL <code>from fsspeckit.sql.filters import sql2pyarrow_filter</code> <code>from fsspeckit.utils import AwsStorageOptions</code> Storage Options <code>from fsspeckit.storage_options import AwsStorageOptions</code> <code>from fsspeckit.utils import dict_to_dataframe</code> Common <code>from fsspeckit.common.types import dict_to_dataframe</code> <p>For information on migration from older versions, refer to the project release notes.</p>"},{"location":"reference/api-guide/#error-handling","title":"Error Handling","text":"<p>fsspeckit uses consistent exception types:</p> <ul> <li><code>ValueError</code>: Configuration and validation errors</li> <li><code>FileNotFoundError</code>: Missing resources</li> <li><code>PermissionError</code>: Access control issues</li> <li><code>ImportError</code>: Missing optional dependencies</li> </ul>"},{"location":"reference/api-guide/#error-handling-pattern","title":"Error Handling Pattern","text":"<pre><code>from fsspeckit.storage_options import AwsStorageOptions\nfrom fsspeckit.core.filesystem import filesystem\n\ntry:\n    # Configure storage\n    options = AwsStorageOptions(region=\"us-east-1\", ...)\n    fs = options.to_filesystem()\n\n    # Use filesystem\n    files = fs.ls(\"s3://bucket/\")\n\nexcept ValueError as e:\n    # Configuration/validation errors\n    print(f\"Configuration error: {e}\")\nexcept FileNotFoundError as e:\n    # Missing resources\n    print(f\"Resource not found: {e}\")\nexcept PermissionError as e:\n    # Access control issues\n    print(f\"Access denied: {e}\")\nexcept ImportError as e:\n    # Missing optional dependencies\n    print(f\"Missing dependency: {e}\")\n</code></pre>"},{"location":"reference/api-guide/#optional-dependencies","title":"Optional Dependencies","text":"<p>fsspeckit uses lazy imports for optional dependencies:</p> Feature Required Package Install Command Dataset operations <code>duckdb&gt;=0.9.0</code> <code>pip install duckdb</code> PyArrow operations <code>pyarrow&gt;=10.0.0</code> <code>pip install pyarrow</code> Polars support <code>polars&gt;=0.19.0</code> <code>pip install polars</code> SQL filtering <code>sqlglot&gt;=20.0.0</code> <code>pip install sqlglot</code> Fast JSON <code>orjson&gt;=3.8.0</code> <code>pip install orjson</code>"},{"location":"reference/api-guide/#dependency-management","title":"Dependency Management","text":"<pre><code># Imports work even without dependencies\nfrom fsspeckit.datasets import DuckDBParquetHandler\nfrom fsspeckit.sql.filters import sql2pyarrow_filter\n\n# Dependencies are required when actually using features\ntry:\n    handler = DuckDBParquetHandler()\n    handler.write_parquet_dataset(data, \"path/\")\nexcept ImportError as e:\n    print(f\"Install with: pip install duckdb\")\n</code></pre>"},{"location":"reference/api-guide/#performance-considerations","title":"Performance Considerations","text":""},{"location":"reference/api-guide/#caching","title":"Caching","text":"<p>Enable caching for remote filesystems:</p> <pre><code>fs = filesystem(\"s3://bucket/\", cached=True, cache_storage=\"/fast/cache\")\n</code></pre>"},{"location":"reference/api-guide/#parallel-processing_1","title":"Parallel Processing","text":"<p>Use parallel execution for I/O bound operations:</p> <pre><code>from fsspeckit.common.misc import run_parallel\n\nresults = run_parallel(process_func, data_list, max_workers=8)\n</code></pre>"},{"location":"reference/api-guide/#batch-operations","title":"Batch Operations","text":"<p>Process large datasets in batches:</p> <pre><code>for batch in fs.read_parquet(\"data/*.parquet\", batch_size=\"100MB\"):\n    process_batch(batch)\n</code></pre>"},{"location":"reference/api-guide/#security-features","title":"Security Features","text":""},{"location":"reference/api-guide/#path-safety","title":"Path Safety","text":"<p>Filesystems are wrapped in <code>DirFileSystem</code> by default for security:</p> <pre><code># All operations confined to specified directory\nfs = filesystem(\"/data/\", dirfs=True)\n</code></pre>"},{"location":"reference/api-guide/#credential-scrubbing","title":"Credential Scrubbing","text":"<p>Prevent credential leakage in logs:</p> <pre><code>from fsspeckit.common.security import scrub_credentials\n\nerror_msg = f\"Failed: access_key=AKIAIOSFODNN7EXAMPLE\"\nsafe_msg = scrub_credentials(error_msg)\n# Output: \"Failed: access_key=[REDACTED]\"\n</code></pre>"},{"location":"reference/api-guide/#input-validation","title":"Input Validation","text":"<p>Validate user inputs for security:</p> <pre><code>from fsspeckit.common.security import validate_path, validate_columns\n\nsafe_path = validate_path(user_path, base_dir=\"/data/allowed\")\nsafe_columns = validate_columns(user_columns, valid_columns=schema_columns)\n</code></pre>"},{"location":"reference/api-guide/#api-reference-links","title":"API Reference Links","text":"<p>For detailed method signatures and parameters:</p> <ul> <li>Core APIs</li> <li>Storage Options</li> <li>Dataset Operations</li> <li>SQL Filtering</li> <li>Common Utilities</li> </ul>"},{"location":"reference/api-guide/#related-documentation","title":"Related Documentation","text":"<ul> <li>How-to Guides - Task-oriented recipes</li> <li>Tutorials - Step-by-step learning</li> <li>Explanation - Conceptual understanding</li> </ul>"},{"location":"reference/utils/","title":"Utils Reference","text":"<p>This page explains the <code>fsspeckit.utils</code> module as a backwards-compatible fa\u00e7ade and provides guidance for migrating to domain packages.</p>"},{"location":"reference/utils/#overview","title":"Overview","text":"<p><code>fsspeckit.utils</code> serves as a backwards-compatible fa\u00e7ade that re-exports selected helpers from domain packages. While existing code continues to work, new development should import directly from domain packages for better discoverability and type hints.</p>"},{"location":"reference/utils/#migration-mapping","title":"Migration Mapping","text":""},{"location":"reference/utils/#common-utilities","title":"Common Utilities","text":"Legacy Import Domain Package Recommended Import <code>from fsspeckit.utils import setup_logging</code> Common <code>from fsspeckit.common.logging import setup_logging</code> <code>from fsspeckit.utils import run_parallel</code> Common <code>from fsspeckit.common.misc import run_parallel</code> <code>from fsspeckit.utils import get_partitions_from_path</code> Common <code>from fsspeckit.common.misc import get_partitions_from_path</code> <code>from fsspeckit.utils import sync_files</code> Common <code>from fsspeckit.common.misc import sync_files</code> <code>from fsspeckit.utils import sync_dir</code> Common <code>from fsspeckit.common.misc import sync_dir</code> <code>from fsspeckit.utils import dict_to_dataframe</code> Common <code>from fsspeckit.common.types import dict_to_dataframe</code> <code>from fsspeckit.utils import to_pyarrow_table</code> Common <code>from fsspeckit.common.types import to_pyarrow_table</code> <code>from fsspeckit.utils import convert_large_types_to_normal</code> Common <code>from fsspeckit.common.types import convert_large_types_to_normal</code> <code>from fsspeckit.utils import opt_dtype_pl</code> Common <code>from fsspeckit.common.polars import opt_dtype_pl</code> <code>from fsspeckit.utils import opt_dtype_pa</code> Common <code>from fsspeckit.common.types import opt_dtype_pa</code> <code>from fsspeckit.utils import cast_schema</code> Common <code>from fsspeckit.common.types import cast_schema</code>"},{"location":"reference/utils/#dataset-operations","title":"Dataset Operations","text":"Legacy Import Domain Package Recommended Import <code>from fsspeckit.utils import DuckDBParquetHandler</code> Datasets <code>from fsspeckit.datasets import DuckDBParquetHandler</code>"},{"location":"reference/utils/#sql-filtering","title":"SQL Filtering","text":"Legacy Import Domain Package Recommended Import <code>from fsspeckit.utils import sql2pyarrow_filter</code> SQL <code>from fsspeckit.sql.filters import sql2pyarrow_filter</code> <code>from fsspeckit.utils import sql2polars_filter</code> SQL <code>from fsspeckit.sql.filters import sql2polars_filter</code>"},{"location":"reference/utils/#storage-options","title":"Storage Options","text":"Legacy Import Domain Package Recommended Import <code>from fsspeckit.utils import AwsStorageOptions</code> Storage Options <code>from fsspeckit.storage_options import AwsStorageOptions</code> <code>from fsspeckit.utils import GcsStorageOptions</code> Storage Options <code>from fsspeckit.storage_options import GcsStorageOptions</code> <code>from fsspeckit.utils import AzureStorageOptions</code> Storage Options <code>from fsspeckit.storage_options import AzureStorageOptions</code> <code>from fsspeckit.utils import storage_options_from_env</code> Storage Options <code>from fsspeckit.storage_options import storage_options_from_env</code>"},{"location":"reference/utils/#available-utils-functions","title":"Available Utils Functions","text":""},{"location":"reference/utils/#logging","title":"Logging","text":"<pre><code># Legacy (still works)\nfrom fsspeckit.utils import setup_logging\n\n# Recommended (new code)\nfrom fsspeckit.common.logging import setup_logging\n\n# Usage\nsetup_logging(level=\"INFO\", format_string=\"{time} | {level} | {message}\")\n</code></pre>"},{"location":"reference/utils/#parallel-processing","title":"Parallel Processing","text":"<pre><code># Legacy (still works)\nfrom fsspeckit.utils import run_parallel\n\n# Recommended (new code)\nfrom fsspeckit.common.misc import run_parallel\n\n# Usage\nresults = run_parallel(\n    func=process_file,\n    data=file_list,\n    max_workers=4,\n    progress=True\n)\n</code></pre>"},{"location":"reference/utils/#type-conversion","title":"Type Conversion","text":"<pre><code># Legacy (still works)\nfrom fsspeckit.utils import (\n    dict_to_dataframe,\n    to_pyarrow_table,\n    convert_large_types_to_normal\n)\n\n# Recommended (new code)\nfrom fsspeckit.common.types import (\n    dict_to_dataframe,\n    to_pyarrow_table,\n    convert_large_types_to_normal\n)\n\n# Usage\ndf = dict_to_dataframe({\"col1\": [1, 2, 3]}, library=\"polars\")\ntable = to_pyarrow_table(df)\nnormal_table = convert_large_types_to_normal(large_string_table)\n</code></pre>"},{"location":"reference/utils/#data-type-optimization","title":"Data Type Optimization","text":"<pre><code># Legacy (still works)\nfrom fsspeckit.utils import opt_dtype_pl, opt_dtype_pa\n\n# Recommended (new code)\nfrom fsspeckit.common.polars import opt_dtype_pl\nfrom fsspeckit.common.types import opt_dtype_pa\n\n# Usage\noptimized_df = opt_dtype_pl(df, shrink_numerics=True)\noptimized_table = opt_dtype_pa(table)\n</code></pre>"},{"location":"reference/utils/#file-operations","title":"File Operations","text":"<pre><code># Legacy (still works)\nfrom fsspeckit.utils import (\n    sync_files,\n    sync_dir,\n    get_partitions_from_path\n)\n\n# Recommended (new code)\nfrom fsspeckit.common.misc import (\n    sync_files,\n    sync_dir,\n    get_partitions_from_path\n)\n\n# Usage\nsync_files(\n    add_files=[\"file1.txt\", \"file2.txt\"],\n    delete_files=[],\n    src_fs=src_fs,\n    dst_fs=dst_fs,\n    src_path=\"/source/\",\n    dst_path=\"/target/\"\n)\n\nsync_dir(\n    src_fs=src_fs,\n    dst_fs=dst_fs,\n    src_path=\"/source/\",\n    dst_path=\"/target/\",\n    parallel=True\n)\n\npartitions = get_partitions_from_path(\"/data/year=2023/month=01/file.parquet\")\n# Returns: {'year': '2023', 'month': '01'}\n</code></pre>"},{"location":"reference/utils/#dataset-operations_1","title":"Dataset Operations","text":"<pre><code># Legacy (still works)\nfrom fsspeckit.utils import DuckDBParquetHandler\n\n# Recommended (new code)\nfrom fsspeckit.datasets import DuckDBParquetHandler\n\n# Usage\nhandler = DuckDBParquetHandler(storage_options=storage_options)\nhandler.write_parquet_dataset(data, \"s3://bucket/dataset/\")\nresult = handler.execute_sql(\"SELECT * FROM parquet_scan('s3://bucket/dataset/')\")\n</code></pre>"},{"location":"reference/utils/#sql-filtering_1","title":"SQL Filtering","text":"<pre><code># Legacy (still works)\nfrom fsspeckit.utils import sql2pyarrow_filter, sql2polars_filter\n\n# Recommended (new code)\nfrom fsspeckit.sql.filters import sql2pyarrow_filter, sql2polars_filter\n\n# Usage\npyarrow_filter = sql2pyarrow_filter(\"id &gt; 100\", schema)\npolars_filter = sql2polars_filter(\"id &gt; 100\", schema)\n</code></pre>"},{"location":"reference/utils/#storage-options_1","title":"Storage Options","text":"<pre><code># Legacy (still works)\nfrom fsspeckit.utils import (\n    AwsStorageOptions,\n    GcsStorageOptions,\n    AzureStorageOptions,\n    storage_options_from_env\n)\n\n# Recommended (new code)\nfrom fsspeckit.storage_options import (\n    AwsStorageOptions,\n    GcsStorageOptions,\n    AzureStorageOptions,\n    storage_options_from_env\n)\n\n# Usage\naws_options = storage_options_from_env(\"s3\")\naws_fs = AwsStorageOptions(\n    region=\"us-east-1\",\n    access_key_id=\"key\",\n    secret_access_key=\"secret\"\n).to_filesystem()\n</code></pre>"},{"location":"reference/utils/#migration-strategy","title":"Migration Strategy","text":""},{"location":"reference/utils/#phase-1-immediate-compatibility","title":"Phase 1: Immediate Compatibility","text":"<p>Existing code continues to work without changes:</p> <pre><code># This continues to work unchanged\nfrom fsspeckit.utils import run_parallel, DuckDBParquetHandler\n\nresults = run_parallel(process_func, data_list)\nhandler = DuckDBParquetHandler()\n</code></pre>"},{"location":"reference/utils/#phase-2-gradual-migration","title":"Phase 2: Gradual Migration","text":"<p>Gradually update imports to domain packages:</p> <pre><code># Mix of legacy and new imports during transition\nfrom fsspeckit.utils import run_parallel  # Legacy\nfrom fsspeckit.datasets import DuckDBParquetHandler  # New\n\n# Both work together\nresults = run_parallel(process_func, data_list)\nhandler = DuckDBParquetHandler()\n</code></pre>"},{"location":"reference/utils/#phase-3-complete-migration","title":"Phase 3: Complete Migration","text":"<p>Fully migrate to domain packages:</p> <pre><code># All imports from domain packages\nfrom fsspeckit.common.misc import run_parallel\nfrom fsspeckit.datasets import DuckDBParquetHandler\nfrom fsspeckit.storage_options import AwsStorageOptions\nfrom fsspeckit.sql.filters import sql2pyarrow_filter\n\n# Clean, discoverable imports\nresults = run_parallel(process_func, data_list)\nhandler = DuckDBParquetHandler()\naws_options = AwsStorageOptions(...)\nfilter_expr = sql2pyarrow_filter(\"id &gt; 100\", schema)\n</code></pre>"},{"location":"reference/utils/#benefits-of-domain-packages","title":"Benefits of Domain Packages","text":""},{"location":"reference/utils/#better-discoverability","title":"Better Discoverability","text":"<pre><code># Clear what you're importing from\nfrom fsspeckit.datasets import DuckDBParquetHandler  # Obvious: dataset operations\nfrom fsspeckit.sql.filters import sql2pyarrow_filter  # Obvious: SQL filtering\nfrom fsspeckit.common.misc import run_parallel  # Obvious: general utilities\n</code></pre>"},{"location":"reference/utils/#improved-type-hints","title":"Improved Type Hints","text":"<pre><code># Domain packages provide better type hints\nfrom fsspeckit.datasets import DuckDBParquetHandler\n\n# IDE shows proper type information\nhandler: DuckDBParquetHandler = DuckDBParquetHandler()\n</code></pre>"},{"location":"reference/utils/#reduced-import-conflicts","title":"Reduced Import Conflicts","text":"<pre><code># Domain-specific imports reduce conflicts\nfrom fsspeckit.common.types import dict_to_dataframe\nfrom fsspeckit.datasets import DuckDBParquetHandler\n\n# vs. unclear utils imports\nfrom fsspeckit.utils import dict_to_dataframe, DuckDBParquetHandler\n</code></pre>"},{"location":"reference/utils/#backwards-compatibility-guarantees","title":"Backwards Compatibility Guarantees","text":""},{"location":"reference/utils/#whats-guaranteed","title":"What's Guaranteed","text":"<ul> <li>All existing <code>fsspeckit.utils</code> imports continue to work</li> <li>Function signatures remain unchanged</li> <li>Behavior is identical to domain package equivalents</li> <li>No breaking changes in minor/patch versions</li> </ul>"},{"location":"reference/utils/#whats-not-guaranteed","title":"What's Not Guaranteed","text":"<ul> <li>New features will be added to domain packages first</li> <li>Some advanced features may only be available in domain packages</li> <li>Deep import paths (e.g., <code>fsspeckit.utils.misc.function</code>) are deprecated</li> </ul>"},{"location":"reference/utils/#deprecation-timeline","title":"Deprecation Timeline","text":"<ul> <li>Current: All utils imports work with deprecation warnings</li> <li>Next Major Version: Utils imports may show stronger deprecation warnings</li> <li>Future: Utils module may be removed (with advance notice)</li> </ul>"},{"location":"reference/utils/#best-practices","title":"Best Practices","text":""},{"location":"reference/utils/#for-new-code","title":"For New Code","text":"<pre><code># Recommended: Import from domain packages\nfrom fsspeckit.datasets import DuckDBParquetHandler\nfrom fsspeckit.common.misc import run_parallel\nfrom fsspeckit.storage_options import AwsStorageOptions\nfrom fsspeckit.sql.filters import sql2pyarrow_filter\n</code></pre>"},{"location":"reference/utils/#for-existing-code","title":"For Existing Code","text":"<pre><code># Option 1: Keep working (no immediate action required)\nfrom fsspeckit.utils import DuckDBParquetHandler, run_parallel\n\n# Option 2: Gradual migration (recommended)\nfrom fsspeckit.datasets import DuckDBParquetHandler  # Migrate this\nfrom fsspeckit.utils import run_parallel  # Keep this for now\n\n# Option 3: Full migration (best for active development)\nfrom fsspeckit.datasets import DuckDBParquetHandler\nfrom fsspeckit.common.misc import run_parallel\n</code></pre>"},{"location":"reference/utils/#for-libraries-and-frameworks","title":"For Libraries and Frameworks","text":"<pre><code># Recommended: Use domain packages in libraries\nclass MyDataProcessor:\n    def __init__(self):\n        # Clear dependencies\n        from fsspeckit.datasets import DuckDBParquetHandler\n        from fsspeckit.common.misc import run_parallel\n\n        self.handler = DuckDBParquetHandler()\n        self.run_parallel = run_parallel\n</code></pre>"},{"location":"reference/utils/#troubleshooting","title":"Troubleshooting","text":""},{"location":"reference/utils/#import-errors","title":"Import Errors","text":"<pre><code># If you see ImportError for domain packages\n# Make sure you're using a recent version of fsspeckit\npip install --upgrade fsspeckit\n\n# Domain packages were introduced in fsspeckit 0.5.x\n</code></pre>"},{"location":"reference/utils/#mixed-import-conflicts","title":"Mixed Import Conflicts","text":"<pre><code># Avoid mixing legacy and new imports for same functionality\n# Bad: This can cause confusion\nfrom fsspeckit.utils import run_parallel\nfrom fsspeckit.common.misc import run_parallel  # Conflict!\n\n# Good: Choose one approach\nfrom fsspeckit.common.misc import run_parallel  # Recommended\n</code></pre>"},{"location":"reference/utils/#finding-right-domain-package","title":"Finding Right Domain Package","text":"<pre><code># Not sure where to find a function?\n# Check the mapping table above or look at domain packages:\n\n# fsspeckit.common.* - General utilities, parallel processing, type conversion\n# fsspeckit.datasets.* - Dataset operations, DuckDB handler\n# fsspeckit.sql.* - SQL filter translation\n# fsspeckit.storage_options.* - Cloud storage configuration\n# fsspeckit.core.* - Filesystem creation, extended I/O\n</code></pre>"},{"location":"reference/utils/#api-reference","title":"API Reference","text":"<p>For detailed documentation of specific functions and classes:</p> <ul> <li>Common Utilities</li> <li>Dataset Operations</li> <li>SQL Filtering</li> <li>Storage Options</li> <li>Core Filesystem</li> </ul>"},{"location":"reference/utils/#related-documentation","title":"Related Documentation","text":"<ul> <li>API Guide - Capability-oriented API overview</li> <li>How-to Guides - Task-oriented recipes</li> <li>Architecture - Design principles</li> </ul>"},{"location":"tutorials/","title":"Tutorials","text":"<p>Start here if you are new to fsspeckit and want a guided introduction.</p> <ul> <li>Getting Started \u2013 Install fsspeckit, configure storage, and run your first dataset workflow</li> </ul>"},{"location":"tutorials/getting-started/","title":"Getting Started","text":"<p>This tutorial will walk you through your first steps with <code>fsspeckit</code>. You'll learn how to install the library, work with local and cloud storage, and perform basic dataset operations.</p>"},{"location":"tutorials/getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.11 or higher</li> <li>Basic familiarity with Python and data concepts</li> </ul>"},{"location":"tutorials/getting-started/#installation","title":"Installation","text":"<p>First, install <code>fsspeckit</code> with the dependencies you need:</p> <pre><code># Basic installation\npip install fsspeckit\n\n# With cloud storage support\npip install \"fsspeckit[aws,gcp,azure]\"\n\n# With all optional dependencies for data processing\npip install \"fsspeckit[aws,gcp,azure]\" duckdb pyarrow polars sqlglot\n</code></pre> <p>For detailed installation instructions, see the Installation Guide.</p>"},{"location":"tutorials/getting-started/#your-first-local-filesystem","title":"Your First Local Filesystem","text":"<p>Let's start by creating a local filesystem and performing basic operations:</p> <pre><code>from fsspeckit.core.filesystem import filesystem\nimport os\n\n# Create a local filesystem\n# Note: filesystem() wraps the filesystem in DirFileSystem by default (dirfs=True)\n# for path safety, confining all operations to the specified directory\nfs = filesystem(\"file\")\n\n# Define a directory path\nlocal_dir = \"./my_data/\"\nos.makedirs(local_dir, exist_ok=True)\n\n# Create and write a file\nwith fs.open(f\"{local_dir}example.txt\", \"w\") as f:\n    f.write(\"Hello, fsspeckit!\")\n\n# Read the file\nwith fs.open(f\"{local_dir}example.txt\", \"r\") as f:\n    content = f.read()\nprint(f\"Content: {content}\")\n\n# List files in directory\nfiles = fs.ls(local_dir)\nprint(f\"Files: {files}\")\n</code></pre> <p>Path Safety: The <code>filesystem()</code> function wraps filesystems in <code>DirFileSystem</code> by default (<code>dirfs=True</code>), which confines all operations to the specified directory path. This prevents accidental access to paths outside the intended directory.</p>"},{"location":"tutorials/getting-started/#working-with-cloud-storage","title":"Working with Cloud Storage","text":"<p>Now let's configure cloud storage. We'll use environment variables for credentials:</p> <pre><code>from fsspeckit.storage_options import storage_options_from_env\nfrom fsspeckit.core.filesystem import filesystem\n\n# Set environment variables (or set them in your environment)\nimport os\nos.environ[\"AWS_ACCESS_KEY_ID\"] = \"your_access_key\"\nos.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"your_secret_key\"\nos.environ[\"AWS_DEFAULT_REGION\"] = \"us-east-1\"\n\n# Load AWS options from environment\naws_options = storage_options_from_env(\"s3\")\nfs = filesystem(\"s3\", storage_options=aws_options.to_dict())\n\nprint(f\"Created S3 filesystem in region: {aws_options.region}\")\n</code></pre> <p>You can also configure storage manually:</p> <pre><code>from fsspeckit.storage_options import AwsStorageOptions\n\n# Configure AWS S3\naws_options = AwsStorageOptions(\n    region=\"us-east-1\",\n    access_key_id=\"YOUR_ACCESS_KEY\",\n    secret_access_key=\"YOUR_SECRET_KEY\"\n)\n\n# Create filesystem\naws_fs = aws_options.to_filesystem()\n</code></pre>"},{"location":"tutorials/getting-started/#protocol-inference","title":"Protocol Inference","text":"<p>The <code>filesystem()</code> function can automatically detect protocols from URIs:</p> <pre><code># Auto-detect protocols\ns3_fs = filesystem(\"s3://bucket/path\")      # S3\ngcs_fs = filesystem(\"gs://bucket/path\")      # Google Cloud Storage\naz_fs = filesystem(\"az://container/path\")    # Azure Blob Storage\ngithub_fs = filesystem(\"github://owner/repo\") # GitHub\n\n# All work with the same interface\nfor name, fs in [(\"S3\", s3_fs), (\"GCS\", gcs_fs)]:\n    try:\n        files = fs.ls(\"/\")\n        print(f\"{name} files: {len(files)}\")\n    except Exception as e:\n        print(f\"{name} error: {e}\")\n</code></pre>"},{"location":"tutorials/getting-started/#your-first-dataset-operation","title":"Your First Dataset Operation","text":"<p>Let's perform a basic dataset operation using the DuckDB Parquet Handler:</p> <pre><code>from fsspeckit.datasets import DuckDBParquetHandler\nimport polars as pl\n\n# Initialize handler with storage options\nstorage_options = {\"key\": \"value\", \"secret\": \"secret\"}\nhandler = DuckDBParquetHandler(storage_options=storage_options)\n\n# Create sample data\ndata = pl.DataFrame({\n    \"id\": [1, 2, 3, 4],\n    \"category\": [\"A\", \"B\", \"A\", \"B\"],\n    \"value\": [10.5, 20.3, 15.7, 25.1]\n})\n\n# Write dataset\nhandler.write_parquet_dataset(data, \"s3://bucket/my-dataset/\")\n\n# Execute SQL queries\nresult = handler.execute_sql(\"\"\"\n    SELECT category, COUNT(*) as count, AVG(value) as avg_value\n    FROM parquet_scan('s3://bucket/my-dataset/')\n    GROUP BY category\n\"\"\")\n\nprint(result)\n</code></pre>"},{"location":"tutorials/getting-started/#domain-package-structure","title":"Domain Package Structure","text":"<p><code>fsspeckit</code> is organized into domain-specific packages. Import from the appropriate package for your use case:</p> <pre><code># Filesystem creation and core functionality\nfrom fsspeckit.core.filesystem import filesystem\n\n# Storage configuration\nfrom fsspeckit.storage_options import AwsStorageOptions, storage_options_from_env\n\n# Dataset operations\nfrom fsspeckit.datasets import DuckDBParquetHandler\n\n# SQL filter translation\nfrom fsspeckit.sql.filters import sql2pyarrow_filter, sql2polars_filter\n\n# Common utilities\nfrom fsspeckit.common.misc import run_parallel\nfrom fsspeckit.common.types import dict_to_dataframe\n\n# Backwards compatibility (legacy)\nfrom fsspeckit.utils import DuckDBParquetHandler  # Still works\n</code></pre>"},{"location":"tutorials/getting-started/#next-steps","title":"Next Steps","text":"<p>Congratulations! You've completed the basic fsspeckit tutorial. Here are some recommended next steps:</p>"},{"location":"tutorials/getting-started/#explore-more-features","title":"Explore More Features","text":"<ul> <li>How-to Guides: Dive into specific tasks with our How-to Guides</li> <li>API Reference: Browse the API Reference for detailed documentation</li> <li>Architecture &amp; Concepts: Understand the design principles in Architecture &amp; Concepts</li> </ul>"},{"location":"tutorials/getting-started/#common-use-cases","title":"Common Use Cases","text":"<ol> <li>Cloud Data Processing: Use <code>storage_options_from_env()</code> for production deployments</li> <li>Dataset Operations: Use <code>DuckDBParquetHandler</code> for large-scale parquet operations</li> <li>SQL Filtering: Use <code>sql2pyarrow_filter()</code> and <code>sql2polars_filter()</code> for cross-framework compatibility</li> <li>Safe Operations: Use <code>DirFileSystem</code> for security-critical applications</li> <li>Performance: Use <code>run_parallel()</code> for concurrent file processing</li> </ol>"},{"location":"tutorials/getting-started/#production-tips","title":"Production Tips","text":"<ol> <li>Use Domain Packages: Import from <code>fsspeckit.datasets</code>, <code>fsspeckit.storage_options</code>, etc. instead of utils</li> <li>Environment Configuration: Load credentials from environment variables in production</li> <li>Error Handling: Always wrap remote filesystem operations in try-except blocks</li> <li>Type Safety: Use structured <code>StorageOptions</code> classes instead of raw dictionaries</li> <li>Testing: Use <code>LocalStorageOptions</code> and <code>DirFileSystem</code> for isolated test environments</li> </ol> <p>For more detailed information, explore the other sections of the documentation.</p>"}]}