{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to fsspeckit!","text":"<p><code>fsspeckit</code> is a powerful library designed to enhance <code>fsspec</code> (Filesystem Spec) with advanced utilities and extensions, making multi-format I/O, cloud storage configuration, caching, monitoring, and batch processing more streamlined and efficient.</p>"},{"location":"#purpose","title":"Purpose","text":"<p>This library aims to simplify complex data operations across various file systems, providing a unified and extended interface for handling diverse data formats and storage solutions. Whether you're working with local files, cloud storage like AWS S3, Azure Blob Storage, or Google Cloud Storage, <code>fsspeckit</code> provides the tools to manage your data effectively.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Multi-format Data I/O: Seamlessly read and write data in various formats, including JSON, CSV, and Parquet.</li> <li>Cloud Storage Configuration: Simplified utilities for configuring and interacting with different cloud storage providers.</li> <li>Enhanced Caching and Monitoring: Improve performance and gain insights into your data operations with built-in caching mechanisms and monitoring capabilities.</li> <li>Batch Processing and Parallel Operations: Efficiently handle large datasets and execute operations in parallel for improved throughput.</li> <li>Directory-like Filesystem: Interact with nested data structures as if they were traditional directories, even on object stores.</li> <li>Domain-Specific Packages: Organized into logical packages (<code>core</code>, <code>datasets</code>, <code>sql</code>, <code>common</code>) for better discoverability and cleaner APIs.</li> </ul>"},{"location":"#get-started","title":"Get Started","text":"<p>Ready to dive in? Check out our Quickstart Guide to begin using <code>fsspeckit</code> in your projects.</p>"},{"location":"#badges","title":"Badges","text":""},{"location":"advanced/","title":"Advanced Usage","text":"<p><code>fsspeckit</code> extends the capabilities of <code>fsspec</code> to provide enhanced filesystem utilities, storage option configurations, and cross-framework SQL filter translation. This section covers advanced features and configurations for getting the most out of the library.</p>"},{"location":"advanced/#domain-package-organization","title":"Domain Package Organization","text":"<p>The refactored <code>fsspeckit</code> architecture organizes functionality into domain-specific packages, making it easier to discover and use the right tools for each task.</p>"},{"location":"advanced/#choosing-the-right-package","title":"Choosing the Right Package","text":"<p>For optimal experience, import from the appropriate domain package:</p> <ul> <li>Dataset Operations: Use <code>fsspeckit.datasets</code> for DuckDB and PyArrow dataset operations</li> <li>SQL Filtering: Use <code>fsspeckit.sql</code> for SQL-to-filter translation</li> <li>Storage Configuration: Use <code>fsspeckit.storage_options</code> for cloud and Git provider setup</li> <li>General Utilities: Use <code>fsspeckit.common</code> for logging, parallel processing, type conversion</li> <li>Backwards Compatibility: <code>fsspeckit.utils</code> continues to work for existing code</li> </ul>"},{"location":"advanced/#import-examples-by-use-case","title":"Import Examples by Use Case","text":"<pre><code># Dataset operations (recommended)\nfrom fsspeckit.datasets import DuckDBParquetHandler\nfrom fsspeckit.datasets.pyarrow import merge_parquet_dataset_pyarrow\n\n# SQL filtering\nfrom fsspeckit.sql.filters import sql2pyarrow_filter, sql2polars_filter\n\n# Storage configuration\nfrom fsspeckit.storage_options import AwsStorageOptions, storage_options_from_env\n\n# Common utilities\nfrom fsspeckit.common.misc import run_parallel\nfrom fsspeckit.common.types import convert_large_types_to_normal\n\n# Filesystem creation\nfrom fsspeckit.core.filesystem import filesystem\n\n# Backwards compatible (legacy)\nfrom fsspeckit.utils import DuckDBParquetHandler  # Still works\n</code></pre>"},{"location":"advanced/#enhanced-filesystem-creation","title":"Enhanced Filesystem Creation","text":"<p>The <code>fsspeckit.core.filesystem.filesystem</code> function provides a centralized way to create fsspec filesystem objects with protocol inference and validation.</p>"},{"location":"advanced/#protocol-inference","title":"Protocol Inference","text":"<pre><code>from fsspeckit.core.filesystem import filesystem\n\n# Auto-detect protocol from URI\nfs = filesystem(\"s3://bucket/path\")  # Automatically detects S3\nfs = filesystem(\"gs://bucket/path\")  # Automatically detects GCS\nfs = filesystem(\"az://container/path\")  # Automatically detects Azure\nfs = filesystem(\"github://owner/repo\")  # Automatically detects GitHub\n</code></pre>"},{"location":"advanced/#storage-options-integration","title":"Storage Options Integration","text":"<pre><code>from fsspeckit.storage_options import AwsStorageOptions\nfrom fsspeckit.core.filesystem import filesystem\n\n# Configure S3 using structured options\naws_opts = AwsStorageOptions(\n    region=\"us-east-1\",\n    access_key_id=\"YOUR_ACCESS_KEY\",\n    secret_access_key=\"YOUR_SECRET_KEY\"\n)\n\n# Create filesystem with structured options\nfs = filesystem(\"s3\", storage_options=aws_opts.to_dict())\n</code></pre>"},{"location":"advanced/#advanced-dataset-operations","title":"Advanced Dataset Operations","text":""},{"location":"advanced/#duckdb-parquet-handler","title":"DuckDB Parquet Handler","text":"<p>The <code>DuckDBParquetHandler</code> provides high-performance dataset operations with atomic guarantees and fsspec integration.</p> <pre><code>from fsspeckit.datasets import DuckDBParquetHandler\nimport polars as pl\n\n# Initialize handler with storage options\nstorage_options = {\"key\": \"value\", \"secret\": \"secret\"}\nhandler = DuckDBParquetHandler(storage_options=storage_options)\n\n# Sample data\ndata = pl.DataFrame({\n    \"id\": [1, 2, 3, 4],\n    \"value\": [\"A\", \"B\", \"C\", \"D\"],\n    \"category\": [\"X\", \"Y\", \"X\", \"Y\"]\n})\n\n# Write dataset with atomic guarantees\nhandler.write_parquet_dataset(data, \"s3://bucket/dataset/\")\n\n# Execute SQL with fsspec integration\nresult = handler.execute_sql(\"\"\"\n    SELECT category, COUNT(*) as count, SUM(id) as total_id\n    FROM parquet_scan('s3://bucket/dataset/')\n    GROUP BY category\n\"\"\")\n\nprint(result)\n</code></pre>"},{"location":"advanced/#pyarrow-dataset-operations","title":"PyArrow Dataset Operations","text":"<pre><code>import pyarrow as pa\nimport pyarrow.parquet as pq\nfrom fsspeckit.datasets.pyarrow import (\n    merge_parquet_dataset_pyarrow,\n    optimize_parquet_dataset_pyarrow,\n    compact_parquet_dataset_pyarrow\n)\n\n# Merge multiple parquet datasets\nmerge_parquet_dataset_pyarrow(\n    dataset_path=\"s3://bucket/dataset/\",\n    output_path=\"s3://bucket/merged/\",\n    merge_strategy=\"schema_evolution\"\n)\n\n# Optimize dataset with Z-ordering\noptimize_parquet_dataset_pyarrow(\n    dataset_path=\"s3://bucket/dataset/\",\n    z_order_columns=[\"category\", \"timestamp\"],\n    target_file_size=\"256MB\"\n)\n\n# Compact small files\ncompact_parquet_dataset_pyarrow(\n    dataset_path=\"s3://bucket/dataset/\",\n    target_file_size=\"128MB\"\n)\n</code></pre>"},{"location":"advanced/#sql-filter-translation","title":"SQL Filter Translation","text":"<p>The <code>fsspeckit.sql</code> package provides cross-framework SQL-to-filter translation, enabling consistent querying across PyArrow and Polars.</p>"},{"location":"advanced/#pyarrow-filter-translation","title":"PyArrow Filter Translation","text":"<pre><code>import pyarrow as pa\nimport pyarrow.compute as pc\nfrom fsspeckit.sql.filters import sql2pyarrow_filter\n\n# Define schema\nschema = pa.schema([\n    (\"id\", pa.int64()),\n    (\"value\", pa.string()),\n    (\"timestamp\", pa.timestamp(\"us\"))\n])\n\n# Convert SQL to PyArrow filter expression\nsql_filter = \"id &gt; 100 AND value IN ('A', 'B', 'C')\"\npyarrow_filter = sql2pyarrow_filter(sql_filter, schema)\n\n# Apply filter to dataset\ndataset = pq.ParquetDataset(\"data.parquet\")\nfiltered_table = dataset.to_table(filter=pyarrow_filter)\n</code></pre>"},{"location":"advanced/#polars-filter-translation","title":"Polars Filter Translation","text":"<pre><code>import polars as pl\nfrom fsspeckit.sql.filters import sql2polars_filter\n\n# Define schema\nschema = pl.Schema({\n    \"id\": pl.Int64,\n    \"value\": pl.String,\n    \"timestamp\": pl.Datetime\n})\n\n# Convert SQL to Polars filter expression\nsql_filter = \"value LIKE 'prefix%' AND timestamp &gt;= '2023-01-01'\"\npolars_filter = sql2polars_filter(sql_filter, schema)\n\n# Apply filter to DataFrame\ndf = pl.read_parquet(\"data.parquet\")\nfiltered_df = df.filter(polars_filter)\n</code></pre>"},{"location":"advanced/#storage-options-management","title":"Storage Options Management","text":""},{"location":"advanced/#environment-based-configuration","title":"Environment-Based Configuration","text":"<p>Load storage configurations directly from environment variables for production deployments.</p> <pre><code>from fsspeckit.storage_options import storage_options_from_env\n\n# Load AWS options from environment variables\n# AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_DEFAULT_REGION\naws_options = storage_options_from_env(\"s3\")\n\n# Load GCS options from environment variables\n# GOOGLE_APPLICATION_CREDENTIALS, GOOGLE_CLOUD_PROJECT\ngcs_options = storage_options_from_env(\"gs\")\n\n# Create filesystems\naws_fs = filesystem(\"s3\", storage_options=aws_options.to_dict())\ngcs_fs = filesystem(\"gs\", storage_options=gcs_options.to_dict())\n</code></pre>"},{"location":"advanced/#uri-based-configuration","title":"URI-Based Configuration","text":"<pre><code>from fsspeckit.storage_options import storage_options_from_uri\n\n# Extract storage options from URI\ns3_options = storage_options_from_uri(\"s3://bucket/path?region=us-east-1\")\ngs_options = storage_options_from_uri(\"gs://bucket/path\")\n\n# Use extracted options\nfs = filesystem(s3_options.protocol, storage_options=s3_options.to_dict())\n</code></pre>"},{"location":"advanced/#provider-specific-configuration","title":"Provider-Specific Configuration","text":""},{"location":"advanced/#aws-s3-configuration","title":"AWS S3 Configuration","text":"<pre><code>from fsspeckit.storage_options import AwsStorageOptions\n\naws_options = AwsStorageOptions(\n    region=\"us-east-1\",\n    access_key_id=\"YOUR_ACCESS_KEY\",\n    secret_access_key=\"YOUR_SECRET_KEY\",\n    endpoint_url=None,  # Use default endpoint\n    allow_http=False,  # Enforce HTTPS\n    assume_role_arn=None  # Optional role assumption\n)\n\nfs = aws_options.to_filesystem()\n</code></pre>"},{"location":"advanced/#google-cloud-storage-configuration","title":"Google Cloud Storage Configuration","text":"<pre><code>from fsspeckit.storage_options import GcsStorageOptions\n\ngcs_options = GcsStorageOptions(\n    project=\"your-gcp-project\",\n    token=\"path/to/service-account.json\",  # or None for default credentials\n    endpoint_override=None  # Use default endpoint\n)\n\nfs = gcs_options.to_filesystem()\n</code></pre>"},{"location":"advanced/#azure-blob-storage-configuration","title":"Azure Blob Storage Configuration","text":"<pre><code>from fsspeckit.storage_options import AzureStorageOptions\n\nazure_options = AzureStorageOptions(\n    account_name=\"yourstorageaccount\",\n    account_key=\"YOUR_ACCOUNT_KEY\",\n    connection_string=None,  # Alternative to account_name/account_key\n    sas_token=None  # Optional SAS token\n)\n\nfs = azure_options.to_filesystem()\n</code></pre>"},{"location":"advanced/#github-configuration","title":"GitHub Configuration","text":"<pre><code>from fsspeckit.storage_options import GitHubStorageOptions\n\ngithub_options = GitHubStorageOptions(\n    token=\"github_pat_YOUR_TOKEN\",\n    default_branch=\"main\"\n)\n\nfs = github_options.to_filesystem()\n</code></pre>"},{"location":"advanced/#common-utilities","title":"Common Utilities","text":""},{"location":"advanced/#parallel-processing","title":"Parallel Processing","text":"<pre><code>from fsspeckit.common.misc import run_parallel\n\ndef process_file(file_path):\n    # Process individual file\n    return processed_data\n\nfile_list = [\"file1.parquet\", \"file2.parquet\", \"file3.parquet\"]\n\n# Process files in parallel\nresults = run_parallel(\n    func=process_file,\n    data=file_list,\n    max_workers=4,  # Optional: specify worker count\n    progress=True   # Show progress bar\n)\n</code></pre>"},{"location":"advanced/#type-conversion","title":"Type Conversion","text":"<pre><code>import pyarrow as pa\nfrom fsspeckit.common.types import convert_large_types_to_normal, dict_to_dataframe\n\n# Convert large string types to normal strings\ntable = pa.Table.from_pydict({\"large_strings\": [\"value1\", \"value2\"]})\nnormalized_table = convert_large_types_to_normal(table)\n\n# Convert dictionaries to DataFrames\ndata = {\"id\": [1, 2, 3], \"value\": [\"A\", \"B\", \"C\"]}\ndf = dict_to_dataframe(data, library=\"polars\")\n</code></pre>"},{"location":"advanced/#directory-synchronization","title":"Directory Synchronization","text":"<pre><code>from fsspeckit.core.filesystem import filesystem\nfrom fsspeckit.common.misc import sync_dir\n\n# Create filesystems for source and destination\nsrc_fs = filesystem(\"s3\", storage_options=src_options)\ndst_fs = filesystem(\"az\", storage_options=dst_options)\n\n# Synchronize directories\nsync_dir(\n    src_fs=src_fs,\n    dst_fs=dst_fs,\n    src_path=\"s3://source-bucket/data/\",\n    dst_path=\"az://dest-container/data/\",\n    progress=True\n)\n</code></pre>"},{"location":"advanced/#performance-optimization","title":"Performance Optimization","text":""},{"location":"advanced/#efficient-dataset-reading","title":"Efficient Dataset Reading","text":"<pre><code>import pyarrow as pa\nfrom fsspeckit.core.filesystem import filesystem\n\n# Create filesystem with optimal settings\nfs = filesystem(\"s3\", storage_options=storage_options)\n\n# Read dataset with filtering and projection\ndataset = pq.ParquetDataset(\n    \"s3://bucket/large-dataset/\",\n    filesystem=fs,\n    filters=[(\"category\", \"=\", \"important\")],  # Pushdown filtering\n    read_columns=[\"id\", \"value\", \"timestamp\"]  # Column projection\n)\n\n# Load only needed data\ntable = dataset.read()\n</code></pre>"},{"location":"advanced/#batch-processing-for-large-datasets","title":"Batch Processing for Large Datasets","text":"<pre><code>from fsspeckit.datasets.pyarrow import process_dataset_in_batches\n\ndef process_batch(batch_table):\n    \"\"\"Process individual batch\"\"\"\n    # Your processing logic here\n    return processed_batch\n\n# Process large dataset in batches\nprocess_dataset_in_batches(\n    dataset_path=\"s3://bucket/huge-dataset/\",\n    batch_size=\"100MB\",\n    process_func=process_batch,\n    max_workers=4\n)\n</code></pre>"},{"location":"advanced/#error-handling-and-validation","title":"Error Handling and Validation","text":""},{"location":"advanced/#input-validation","title":"Input Validation","text":"<pre><code>from fsspeckit.core.merge import validate_merge_inputs\n\n# Validate inputs before merge operations\nerrors = validate_merge_inputs(\n    input_paths=[\"s3://bucket/part1/\", \"s3://bucket/part2/\"],\n    schema=expected_schema\n)\n\nif errors:\n    print(f\"Validation errors: {errors}\")\n    # Handle validation errors\nelse:\n    # Proceed with merge operation\n    pass\n</code></pre>"},{"location":"advanced/#robust-file-operations","title":"Robust File Operations","text":"<pre><code>from fsspeckit.core.filesystem import DirFileSystem\nimport tempfile\n\n# Create safe filesystem that restricts operations to specific directory\nsafe_fs = DirFileSystem(\n    fs=base_fs,\n    path=\"/allowed/directory\"  # Operations confined to this directory\n)\n\n# Use safe filesystem for operations\ntry:\n    with safe_fs.open(\"/allowed/directory/file.txt\", \"r\") as f:\n        content = f.read()\nexcept FileNotFoundError:\n    print(\"File not found in allowed directory\")\nexcept PermissionError:\n    print(\"Operation outside allowed directory\")\n</code></pre>"},{"location":"advanced/#migration-guide","title":"Migration Guide","text":""},{"location":"advanced/#from-fsspec-utils-to-fsspeckit","title":"From fsspec-utils to fsspeckit","text":"<p>Step 1: Update Imports <pre><code># Old imports\nfrom fsspec_utils import run_parallel, storage_options_from_env\n\n# New imports\nfrom fsspeckit.common import run_parallel\nfrom fsspeckit.storage_options import storage_options_from_env\n</code></pre></p> <p>Step 2: Update Filesystem Creation <pre><code># Old method\nimport fsspec\nfs = fsspec.filesystem(\"s3\", **storage_options)\n\n# New method\nfrom fsspeckit.core.filesystem import filesystem\nfs = filesystem(\"s3\", storage_options=storage_options)\n</code></pre></p> <p>Step 3: Update Dataset Operations <pre><code># Old method\nfrom fsspec_utils import write_parquet_dataset\n\n# New method\nfrom fsspeckit.datasets import DuckDBParquetHandler\nhandler = DuckDBParquetHandler(storage_options=storage_options)\nhandler.write_parquet_dataset(data, path)\n</code></pre></p>"},{"location":"advanced/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Use Domain-Specific Imports: Import from <code>fsspeckit.datasets</code>, <code>fsspeckit.storage_options</code>, etc. instead of the utils fa\u00e7ade for better type hints and clearer code.</p> </li> <li> <p>Environment-Based Configuration: Use <code>storage_options_from_env()</code> in production to load credentials from environment variables.</p> </li> <li> <p>Protocol Inference: Let the <code>filesystem()</code> function auto-detect protocols from URIs when possible.</p> </li> <li> <p>Type Safety: Use the structured <code>StorageOptions</code> classes instead of raw dictionaries for better validation and IDE support.</p> </li> <li> <p>Error Handling: Always wrap filesystem operations in try-except blocks to handle network errors and permission issues gracefully.</p> </li> <li> <p>Performance: Use column projection and row filtering when working with large datasets to minimize data transfer.</p> </li> <li> <p>Testing: Use <code>LocalStorageOptions</code> and <code>DirFileSystem</code> for creating safe, isolated test environments.</p> </li> </ol>"},{"location":"api-guide/","title":"API Guide","text":"<p>This guide provides a comprehensive overview of the fsspeckit public API and how to use its main components.</p>"},{"location":"api-guide/#core-filesystem-factory","title":"Core Filesystem Factory","text":""},{"location":"api-guide/#filesystem","title":"<code>filesystem()</code>","text":"<p>The main entry point for creating configured filesystems:</p> <pre><code>from fsspeckit import filesystem\n\n# Basic local filesystem\nfs = filesystem(\"file\")\n\n# S3 with caching\nfs = filesystem(\"s3://my-bucket/\", cached=True)\n\n# With storage options\nfs = filesystem(\"s3\", storage_options={\"region\": \"us-west-2\"})\n\n# With base filesystem (DirFileSystem hierarchy)\nfs = filesystem(\"/data\", dirfs=True, base_fs=parent_fs)\n</code></pre> <p>Parameters: - <code>protocol_or_path</code> - Protocol (e.g., 's3', 'gs') or path - <code>storage_options</code> - Dict or StorageOptions object - <code>cached</code> - Enable caching (default: False) - <code>cache_storage</code> - Cache directory location - <code>verbose</code> - Log cache operations (default: False) - <code>dirfs</code> - Wrap in DirFileSystem (default: True) - <code>base_fs</code> - Parent DirFileSystem for hierarchy - <code>use_listings_cache</code> - Use listings cache (default: True) - <code>skip_instance_cache</code> - Skip instance cache (default: False) - <code>**kwargs</code> - Protocol-specific options</p>"},{"location":"api-guide/#storage-options-classes","title":"Storage Options Classes","text":"<p>Storage options provide structured configuration for different providers:</p> <pre><code>from fsspeckit.storage_options import (\n    AwsStorageOptions,\n    GcsStorageOptions,\n    AzureStorageOptions,\n    GitHubStorageOptions,\n    GitLabStorageOptions,\n    LocalStorageOptions\n)\n\n# AWS S3\naws = AwsStorageOptions(\n    region=\"us-east-1\",\n    access_key_id=\"key\",\n    secret_access_key=\"secret\"\n)\nfs = aws.to_filesystem()\n\n# Google Cloud Storage\ngcs = GcsStorageOptions(\n    project=\"my-project\",\n    token=\"path/to/service-account.json\"\n)\nfs = gcs.to_filesystem()\n\n# Azure\nazure = AzureStorageOptions(\n    account_name=\"myaccount\",\n    account_key=\"key...\"\n)\nfs = azure.to_filesystem()\n\n# GitHub Repository\ngithub = GitHubStorageOptions(\n    org=\"microsoft\",\n    repo=\"vscode\",\n    token=\"ghp_xxx\"\n)\nfs = github.to_filesystem()\n\n# GitLab Repository\ngitlab = GitLabStorageOptions(\n    project_id=12345,\n    token=\"glpat_xxx\"\n)\nfs = gitlab.to_filesystem()\n\n# Local\nlocal = LocalStorageOptions(auto_mkdir=True)\nfs = local.to_filesystem()\n</code></pre>"},{"location":"api-guide/#storageoptions-factories","title":"StorageOptions Factories","text":"<p>Create storage options from various sources:</p> <pre><code>from fsspeckit.storage_options import (\n    from_dict,\n    from_env,\n    merge_storage_options,\n    infer_protocol_from_uri,\n    storage_options_from_uri\n)\n\n# From dictionary\nopts = from_dict(\"s3\", {\"region\": \"us-west-2\"})\n\n# From environment variables\nopts = AwsStorageOptions.from_env()\n\n# Merge multiple options\nmerged = merge_storage_options(opts1, opts2, overwrite=True)\n\n# Infer protocol from URI\nprotocol = infer_protocol_from_uri(\"s3://bucket/path\")\n\n# Create from URI\nopts = storage_options_from_uri(\"s3://my-bucket/data\")\n</code></pre>"},{"location":"api-guide/#extended-io-operations","title":"Extended I/O Operations","text":"<p>fsspeckit adds rich I/O methods to all fsspec filesystems through monkey-patching.</p>"},{"location":"api-guide/#reading-operations","title":"Reading Operations","text":""},{"location":"api-guide/#json-operations","title":"JSON Operations","text":"<pre><code>from fsspeckit import filesystem\n\nfs = filesystem(\".\")\n\n# Read single JSON file\ndata = fs.read_json_file(\"data.json\")  # Returns dict\ndf = fs.read_json_file(\"data.json\", as_dataframe=True)  # Returns Polars DF\n\n# Read multiple JSON files with batching\nfor batch in fs.read_json(\"data/*.json\", batch_size=5):\n    # Process batch\n    pass\n\n# Read JSON Lines format\ndf = fs.read_json(\"data/lines.jsonl\", as_dataframe=True)\n\n# With threading\ndf = fs.read_json(\"data/*.json\", use_threads=True, num_threads=4)\n\n# Include source file path\ndf = fs.read_json(\"data/*.json\", include_file_path=True)\n</code></pre>"},{"location":"api-guide/#csv-operations","title":"CSV Operations","text":"<pre><code># Read single CSV\ndf = fs.read_csv_file(\"data.csv\")\n\n# Read multiple CSV files\ndf = fs.read_csv(\"data/*.csv\", concat=True)\n\n# Batch reading\nfor batch in fs.read_csv(\"data/*.csv\", batch_size=10):\n    pass\n\n# Optimize data types\ndf = fs.read_csv(\"data/*.csv\", opt_dtypes=True)\n\n# With parallelism\ndf = fs.read_csv(\"data/*.csv\", use_threads=True)\n</code></pre>"},{"location":"api-guide/#parquet-operations","title":"Parquet Operations","text":"<pre><code># Read single Parquet file\ntable = fs.read_parquet_file(\"data.parquet\")\n\n# Read multiple with schema unification\ntable = fs.read_parquet(\"data/*.parquet\", concat=True)\n\n# Batch reading\nfor batch in fs.read_parquet(\"data/*.parquet\", batch_size=20):\n    pass\n\n# With partitioning support\ntable = fs.read_parquet(\"partitioned_data/**/*.parquet\", concat=True)\n\n# Include file path column\ntable = fs.read_parquet(\"data/*.parquet\", include_file_path=True)\n</code></pre>"},{"location":"api-guide/#universal-reader","title":"Universal Reader","text":"<pre><code># Auto-detect format from file extension\ndf = fs.read_files(\"data/mixed/*\", format=\"auto\")\n\n# Explicit format\ndf = fs.read_files(\"data/*.csv\", format=\"csv\")\n\n# Control result type\ndf_polars = fs.read_files(\"data/*.parquet\", as_dataframe=True)\ntable_arrow = fs.read_files(\"data/*.parquet\", as_dataframe=False)\n</code></pre>"},{"location":"api-guide/#dataset-operations","title":"Dataset Operations","text":"<pre><code># Create PyArrow dataset\ndataset = fs.pyarrow_dataset(\"data/\")\n\n# Optimized for Parquet with metadata\ndataset = fs.pyarrow_parquet_dataset(\"partitioned_data/\")\n\n# Query dataset with filtering\nfiltered = dataset.to_table(filter=pyarrow.compute.greater(dataset.column(\"age\"), 25))\n\n# Schema inspection\nprint(dataset.schema)\n\n# Statistics and metadata\nprint(dataset.count_rows())\n</code></pre>"},{"location":"api-guide/#writing-operations","title":"Writing Operations","text":""},{"location":"api-guide/#parquet-writing","title":"Parquet Writing","text":"<pre><code>import polars as pl\n\n# From Polars DataFrame\ndf = pl.DataFrame({\"col1\": [1, 2, 3], \"col2\": [4, 5, 6]})\nfs.write_parquet(df, \"output.parquet\")\n\n# From Pandas\nimport pandas as pd\ndf = pd.DataFrame({\"a\": [1, 2], \"b\": [3, 4]})\nfs.write_parquet(df, \"output.parquet\")\n\n# Compression\nfs.write_parquet(df, \"output.parquet\", compression=\"zstd\")\n\n# Append mode\nfs.write_parquet(df, \"output.parquet\", mode=\"append\")\n</code></pre>"},{"location":"api-guide/#csv-writing","title":"CSV Writing","text":"<pre><code># Write DataFrame\ndf.write_csv(\"output.csv\")\n\n# Append to existing file\nfs.write_csv(new_df, \"output.csv\", mode=\"append\")\n</code></pre>"},{"location":"api-guide/#json-writing","title":"JSON Writing","text":"<pre><code># Write DataFrame to JSON\ndf.write_json(\"output.json\")\n\n# JSON Lines format\nfs.write_json(df, \"output.jsonl\", format=\"json_lines\")\n\n# Include file path metadata\nfs.write_json(df, \"output.json\", include_file_path=True)\n</code></pre>"},{"location":"api-guide/#pyarrow-dataset-writing","title":"PyArrow Dataset Writing","text":"<pre><code>import pyarrow as pa\n\n# Write partitioned dataset\ntable = pa.table({\"year\": [2023, 2023, 2024], \"value\": [10, 20, 30]})\nfs.write_pyarrow_dataset(\n    data=table,\n    path=\"partitioned_data\",\n    partition_by=[\"year\"],\n    format=\"parquet\",\n    compression=\"zstd\"\n)\n\n# Result structure: partitioned_data/year=2023/...parquet files\n</code></pre>"},{"location":"api-guide/#cache-management","title":"Cache Management","text":"<pre><code># Clear all caches\nfs.clear_cache()\n\n# Check cache size\nsize = fs.get_cache_size()\n\n# Sync cache (ensure data is written)\nfs.sync_cache()\n</code></pre>"},{"location":"api-guide/#helper-utilities","title":"Helper Utilities","text":"<p>Note: fsspeckit utilities are organized into domain packages. See Architecture for details.</p>"},{"location":"api-guide/#parallel-processing","title":"Parallel Processing","text":"<pre><code>from fsspeckit.common.misc import run_parallel\n\ndef process(item):\n    return len(item)\n\nresults = run_parallel(\n    process,\n    [\"item1\", \"item2\", \"item3\"],\n    n_jobs=4,\n    verbose=True\n)\n</code></pre>"},{"location":"api-guide/#type-conversions","title":"Type Conversions","text":"<pre><code>from fsspeckit.common.types import (\n    dict_to_dataframe,\n    to_pyarrow_table\n)\n\n# Dict to DataFrame\ndf = dict_to_dataframe({\"col1\": [1, 2], \"col2\": [3, 4]})\n\n# Any to PyArrow\ntable = to_pyarrow_table(df)\n</code></pre>"},{"location":"api-guide/#data-type-optimization","title":"Data Type Optimization","text":"<pre><code>import polars as pl\nfrom fsspeckit.common import opt_dtype_pl\n\n# Optimize DataFrame\ndf = pl.read_csv(\"data.csv\")\ndf_opt = opt_dtype_pl(df)\n\n# Or use extension\ndf_opt = df.opt_dtype\n</code></pre>"},{"location":"api-guide/#sql-filtering","title":"SQL Filtering","text":"<pre><code>from fsspeckit.sql.filters import sql2pyarrow_filter\nimport pyarrow as pa\n\nschema = pa.schema([(\"age\", pa.int32()), (\"name\", pa.string())])\nexpr = sql2pyarrow_filter(\"age &gt; 25 AND name = 'Alice'\", schema)\nfiltered_table = dataset.to_table(filter=expr)\n</code></pre>"},{"location":"api-guide/#file-synchronization","title":"File Synchronization","text":"<pre><code>from fsspeckit.common.misc import sync_dir\n\n# Sync directories\nsync_dir(\n    fs_source, \"/source/\",\n    fs_target, \"/target/\",\n    overwrite=False\n)\n</code></pre>"},{"location":"api-guide/#filesystem-classes","title":"Filesystem Classes","text":""},{"location":"api-guide/#custom-implementations","title":"Custom Implementations","text":""},{"location":"api-guide/#gitlabfilesystem","title":"<code>GitLabFileSystem</code>","text":"<p>Read-only filesystem for GitLab repositories:</p> <pre><code>from fsspeckit.core import filesystem\n\nfs = filesystem(\n    \"gitlab\",\n    storage_options={\n        \"project_name\": \"group/project\",\n        \"token\": \"glpat_xxx\",\n        \"ref\": \"main\"\n    }\n)\n\n# List files\nfiles = fs.ls(\"/\")\n\n# Read file\ncontent = fs.cat(\"README.md\")\n</code></pre>"},{"location":"api-guide/#monitoredsimplecachefilesystem","title":"<code>MonitoredSimpleCacheFileSystem</code>","text":"<p>Enhanced cache with logging and monitoring:</p> <pre><code># Automatically used when cached=True\nfs = filesystem(\"s3\", cached=True, verbose=True)\n\n# Monitor cache operations\nfs.sync_cache()\nsize = fs.get_cache_size()\n</code></pre>"},{"location":"api-guide/#working-with-dirfilesystem","title":"Working with DirFileSystem","text":"<pre><code>from fsspeckit import filesystem\n\n# Create DirFileSystem wrapper\nfs = filesystem(\"/data\", dirfs=True)\n\n# Access files within the base directory\nfs.ls(\"/subdir\")\n\n# Create hierarchical filesystem (base_fs parameter)\nparent_fs = filesystem(\"/datasets\", dirfs=True)\nchild_fs = filesystem(\"/datasets/project1\", dirfs=True, base_fs=parent_fs)\n\n# Files are accessible only within the base directory\n# Attempting to escape raises an error\n</code></pre>"},{"location":"api-guide/#configuration-methods","title":"Configuration Methods","text":"<p>All storage option classes provide conversion methods:</p> <pre><code>opts = AwsStorageOptions(...)\n\n# Convert to fsspec kwargs\nkwargs = opts.to_fsspec_kwargs()\n\n# Convert to filesystem\nfs = opts.to_filesystem()\n\n# Convert to object store kwargs (for deltalake, etc.)\nobj_store_kwargs = opts.to_object_store_kwargs()\n\n# Convert to YAML\nyaml_str = opts.to_yaml()\n\n# Load from YAML\nopts = AwsStorageOptions.from_yaml(yaml_str)\n\n# Convert to environment variables\nenv = opts.to_env()\n\n# Load from environment\nopts = AwsStorageOptions.from_env()\n</code></pre>"},{"location":"api-guide/#error-handling","title":"Error Handling","text":"<pre><code>from fsspeckit import filesystem\nfrom fsspeckit.storage_options import AwsStorageOptions\n\ntry:\n    fs = filesystem(\"s3\", storage_options={\"region\": \"invalid\"})\n    data = fs.cat(\"s3://bucket/file.txt\")\nexcept Exception as e:\n    print(f\"Error: {e}\")\n\n# Check optional dependencies\nfrom fsspeckit.common.misc import check_optional_dependency\ntry:\n    check_optional_dependency(\"deltalake\")\nexcept ImportError:\n    print(\"Install with: pip install deltalake\")\n</code></pre>"},{"location":"api-guide/#best-practices","title":"Best Practices","text":"<ol> <li>Use StorageOptions - Type-safe and consistent configuration</li> <li>Enable Caching - For remote filesystems, always consider caching</li> <li>Batch Processing - Use batch_size for large datasets</li> <li>Parallel I/O - Enable threading for multi-file operations</li> <li>Type Optimization - Use opt_dtypes=True to reduce memory</li> <li>Error Handling - Wrap filesystem operations in try/except</li> </ol>"},{"location":"api-guide/#see-also","title":"See Also","text":"<ul> <li>Advanced Usage - In-depth guides and patterns</li> <li>Utils Module - Utility functions reference</li> <li>Examples - Runnable example scripts</li> <li>Architecture - Design and implementation details</li> </ul>"},{"location":"architecture/","title":"Architecture Overview","text":"<p><code>fsspeckit</code> extends <code>fsspec</code> with enhanced filesystem utilities and storage option configurations for working with various data formats and storage backends. This document provides a technical reference for understanding the system's design and implementation patterns.</p>"},{"location":"architecture/#executive-overview","title":"Executive Overview","text":""},{"location":"architecture/#purpose-and-value-proposition","title":"Purpose and Value Proposition","text":"<p><code>fsspeckit</code> provides enhanced data processing capabilities through a modular, domain-driven architecture that focuses on filesystem operations, storage configuration, and cross-framework SQL filter translation. The system enables users to work with multiple storage backends and data processing frameworks through unified APIs.</p>"},{"location":"architecture/#core-architectural-principles","title":"Core Architectural Principles","text":"<ol> <li>Domain-Driven Design: Clear separation of concerns through domain-specific packages</li> <li>Backend Neutrality: Consistent interfaces across different storage providers</li> <li>Practical Utilities: Focus on implemented features rather than theoretical capabilities</li> <li>Backwards Compatibility: Migration path for existing users</li> <li>Type Safety: Strong typing and validation throughout the codebase</li> </ol>"},{"location":"architecture/#target-use-cases","title":"Target Use Cases","text":"<ul> <li>Multi-Cloud Data Access: Unified access to AWS S3, Azure Blob, Google Cloud Storage</li> <li>Dataset Operations: High-performance dataset operations with DuckDB and PyArrow</li> <li>Git Integration: Filesystem access to GitHub and GitLab repositories</li> <li>SQL Filter Translation: Cross-framework SQL expression conversion</li> <li>Storage Configuration: Environment-based storage option management</li> </ul>"},{"location":"architecture/#architectural-decision-records-adrs","title":"Architectural Decision Records (ADRs)","text":""},{"location":"architecture/#adr-001-domain-package-architecture","title":"ADR-001: Domain Package Architecture","text":"<p>Decision: Organize fsspeckit into domain-specific packages (core, storage_options, datasets, sql, common) rather than a monolithic structure.</p> <p>Rationale: - Separation of Concerns: Each domain has distinct responsibilities and user patterns - Discoverability: Users can easily find relevant functionality without searching large modules - Testing: Isolated testing for each domain with clear boundaries - Maintenance: Changes to one domain don't impact others</p> <p>Migration Path: Existing imports through <code>fsspeckit.utils</code> continue working while new code uses domain-specific imports.</p>"},{"location":"architecture/#adr-002-backend-neutral-planning-layer","title":"ADR-002: Backend-Neutral Planning Layer","text":"<p>Decision: Centralize merge and maintenance planning logic in the core package with backend-specific delegates.</p> <p>Rationale: - Consistency: All backends use identical merge semantics and validation - Maintainability: Single source of truth for business logic - Performance: Shared optimization strategies across implementations - Testing: Consistent behavior validation across all backends</p> <p>Implementation: Both DuckDB and PyArrow backends delegate to <code>core.merge</code> and <code>core.maintenance</code> for planning, validation, and statistics calculation.</p>"},{"location":"architecture/#adr-003-storage-options-factory-pattern","title":"ADR-003: Storage Options Factory Pattern","text":"<p>Decision: Implement factory pattern for storage configuration with environment-based setup.</p> <p>Rationale: - Portability: Code works across different cloud providers without changes - Configuration: Environment-based configuration for production deployments - Flexibility: Users can override defaults for specific requirements</p> <p>Implementation Pattern: <pre><code># Protocol-agnostic approach\nfrom fsspeckit.storage_options import storage_options_from_env\noptions = storage_options_from_env(\"s3\")\n\n# URI-based inference\nfrom fsspeckit.core.filesystem import filesystem\nfs = filesystem(\"s3://bucket/path\")  # Auto-detects protocol\n</code></pre></p>"},{"location":"architecture/#core-architecture-deep-dive","title":"Core Architecture Deep Dive","text":""},{"location":"architecture/#domain-package-breakdown","title":"Domain Package Breakdown","text":""},{"location":"architecture/#fsspeckitcore-foundation-layer","title":"<code>fsspeckit.core</code> - Foundation Layer","text":"<p>The core package provides fundamental filesystem APIs and path safety utilities:</p> <p>Key Components:</p> <ul> <li> <p><code>AbstractFileSystem</code> (<code>core/ext.py</code>): Extended base class with enhanced functionality   <pre><code>class AbstractFileSystem(fsspec.AbstractFileSystem):\n    \"\"\"Enhanced filesystem with smart path handling and protocol inference.\"\"\"\n</code></pre></p> </li> <li> <p><code>DirFileSystem</code>: Path-safe filesystem wrapper   <pre><code>class DirFileSystem(AbstractFileSystem):\n    \"\"\"Filesystem wrapper that restricts operations within specified directories.\"\"\"\n</code></pre></p> </li> <li> <p><code>filesystem()</code> function: Enhanced filesystem creation with URI inference   <pre><code>def filesystem(protocol: str, **storage_options) -&gt; AbstractFileSystem:\n    \"\"\"Create filesystem with protocol inference and validation.\"\"\"\n</code></pre></p> </li> </ul> <p>Integration Patterns: - Protocol detection and inference from URIs - Smart path normalization and validation - Directory confinement for security</p>"},{"location":"architecture/#fsspeckitstorage_options-configuration-layer","title":"<code>fsspeckit.storage_options</code> - Configuration Layer","text":"<p>Manages storage configurations for cloud and Git providers:</p> <p>Factory Pattern Implementation: <pre><code>def from_dict(protocol: str, storage_options: dict) -&gt; BaseStorageOptions\ndef from_env(protocol: str) -&gt; BaseStorageOptions\ndef storage_options_from_uri(uri: str) -&gt; BaseStorageOptions\n</code></pre></p> <p>Provider Implementations: - <code>AwsStorageOptions</code>: AWS S3 configuration with region, credentials, and endpoint settings - <code>GcsStorageOptions</code>: Google Cloud Storage setup - <code>AzureStorageOptions</code>: Azure Blob Storage configuration - <code>GitHubStorageOptions</code>: GitHub repository access with token authentication - <code>GitLabStorageOptions</code>: GitLab repository configuration</p> <p>Key Features: - YAML serialization for persistent configuration - Environment variable auto-configuration - Protocol inference from URIs - Unified interface across all providers</p>"},{"location":"architecture/#fsspeckitdatasets-data-processing-layer","title":"<code>fsspeckit.datasets</code> - Data Processing Layer","text":"<p>High-performance dataset operations for large-scale data processing:</p> <p>DuckDB Implementation: <pre><code>class DuckDBParquetHandler:\n    \"\"\"High-performance parquet operations with atomic guarantees.\"\"\"\n\n    def __init__(self, storage_options=None):\n        self.storage_options = storage_options\n\n    def write_parquet_dataset(self, table, path, **kwargs):\n        \"\"\"Atomic dataset writing with backup-and-restore.\"\"\"\n\n    def execute_sql(self, query, **kwargs):\n        \"\"\"Parameterized SQL execution with fsspec integration.\"\"\"\n</code></pre></p> <p>PyArrow Implementation: <pre><code># Optimization and compaction functions\ndef optimize_parquet_dataset_pyarrow(dataset_path, **kwargs):\n    \"\"\"Z-ordering and file size optimization.\"\"\"\n\ndef compact_parquet_dataset_pyarrow(dataset_path, **kwargs):\n    \"\"\"Dataset compaction with atomic operations.\"\"\"\n</code></pre></p> <p>Backend Integration: - Shared merge logic from <code>core.merge</code> - Common maintenance operations from <code>core.maintenance</code> - Consistent statistics and validation across backends</p>"},{"location":"architecture/#fsspeckitsql-query-translation-layer","title":"<code>fsspeckit.sql</code> - Query Translation Layer","text":"<p>SQL-to-filter translation for cross-framework compatibility:</p> <p>Core Functions: <pre><code>def sql2pyarrow_filter(string: str, schema: pa.Schema) -&gt; pc.Expression:\n    \"\"\"Convert SQL WHERE clause to PyArrow filter expression.\"\"\"\n\ndef sql2polars_filter(string: str, schema: pl.Schema) -&gt; pl.Expr:\n    \"\"\"Convert SQL WHERE clause to Polars filter expression.\"\"\"\n</code></pre></p> <p>Integration Points: - Cross-framework SQL expression translation - Schema-aware filter generation - Unified SQL parsing using sqlglot - Table name extraction for validation</p>"},{"location":"architecture/#fsspeckitcommon-shared-utilities-layer","title":"<code>fsspeckit.common</code> - Shared Utilities Layer","text":"<p>Cross-cutting utilities used across all domains:</p> <p>Parallel Processing: <pre><code>def run_parallel(\n    func: Callable,\n    data: Sequence[Any],\n    max_workers: Optional[int] = None,\n    progress: bool = True\n) -&gt; List[Any]:\n    \"\"\"Parallel execution with progress tracking and error handling.\"\"\"\n</code></pre></p> <p>Type Conversion: <pre><code>def convert_large_types_to_normal(table: pa.Table) -&gt; pa.Table:\n    \"\"\"Convert large string types to normal string types for compatibility.\"\"\"\n\ndef dict_to_dataframe(data: Dict[str, Any], library: str = \"polars\"):\n    \"\"\"Convert dictionaries to Polars/Pandas DataFrames.\"\"\"\n</code></pre></p> <p>File Operations: <pre><code>def sync_dir(src_fs, dst_fs, src_path: str, dst_path: str, **kwargs):\n    \"\"\"Synchronize directories between filesystems.\"\"\"\n\ndef extract_partitions(path: str, **kwargs) -&gt; Dict[str, str]:\n    \"\"\"Extract partition information from file paths.\"\"\"\n</code></pre></p>"},{"location":"architecture/#fsspeckitutils-backwards-compatibility-facade","title":"<code>fsspeckit.utils</code> - Backwards Compatibility Fa\u00e7ade","text":"<p>Re-exports selected helpers from domain packages for backwards compatibility:</p> <pre><code># Re-exports for backwards compatibility\nfrom ..common.misc import run_parallel\nfrom ..common.datetime import timestamp_from_string\nfrom ..common.types import dict_to_dataframe, to_pyarrow_table\n</code></pre> <p>Migration Strategy: - Immediate compatibility with existing code - Gradual migration to domain-specific imports - Deprecation warnings for discouraged patterns</p>"},{"location":"architecture/#integration-patterns","title":"Integration Patterns","text":""},{"location":"architecture/#cross-domain-communication","title":"Cross-Domain Communication","text":"<p>Import Patterns: <pre><code># Core \u2192 Storage Options\nfrom fsspeckit.storage_options.base import BaseStorageOptions\n\n# Datasets \u2192 Core Merge Logic\nfrom fsspeckit.core.merge import (\n    MergeStrategy, validate_merge_inputs,\n    calculate_merge_stats, check_null_keys\n)\n\n# Storage Options \u2192 Core Filesystem\nfrom fsspeckit.core.filesystem import filesystem\n</code></pre></p> <p>Configuration Flow: <pre><code># Environment-based configuration\nfrom fsspeckit.storage_options import storage_options_from_env\noptions = storage_options_from_env(\"s3\")\nfs = filesystem(\"s3\", storage_options=options.to_dict())\n\n# URI-based configuration\nfs = filesystem(\"s3://bucket/path\")  # Auto-detects and configures\n</code></pre></p>"},{"location":"architecture/#error-handling-architecture","title":"Error Handling Architecture","text":"<p>Consistent Exception Types: - <code>ValueError</code> for configuration and validation errors - <code>FileNotFoundError</code> for missing resources - <code>PermissionError</code> for access control issues - Custom exceptions for domain-specific errors</p> <p>Data Flow Patterns</p> <p>Typical Data Processing Pipeline: <pre><code># 1. Configuration Setup\nfrom fsspeckit.storage_options import storage_options_from_env\nfrom fsspeckit.core.filesystem import filesystem\nfrom fsspeckit.datasets import DuckDBParquetHandler\n\nstorage_options = storage_options_from_env(\"s3\")\nfs = filesystem(\"s3\", storage_options=storage_options.to_dict())\n\n# 2. Data Processing\nhandler = DuckDBParquetHandler(storage_options=storage_options.to_dict())\n\n# Data ingestion and processing\nhandler.write_parquet_dataset(data, \"s3://bucket/raw/\")\nresult = handler.execute_sql(\"\"\"\n    SELECT region, SUM(amount) as total\n    FROM parquet_scan('s3://bucket/raw/')\n    GROUP BY region\n\"\"\")\n\n# Output\nhandler.write_parquet_dataset(result, \"s3://bucket/processed/\")\n</code></pre></p> <p>Cross-Storage Operations: <pre><code># Sync between cloud providers\nfrom fsspeckit.core.filesystem import filesystem\nfrom fsspeckit.common import sync_dir\n\nsrc_fs = filesystem(\"s3\", storage_options=s3_options)\ndst_fs = filesystem(\"az\", storage_options=azure_options)\n\nsync_dir(\n    src_fs, dst_fs,\n    \"s3://bucket/data/\",\n    \"az://container/data/\",\n    progress=True\n)\n</code></pre></p>"},{"location":"architecture/#performance-and-scalability-architecture","title":"Performance and Scalability Architecture","text":""},{"location":"architecture/#caching-strategy","title":"Caching Strategy","text":"<p>Filesystem Level Caching: - Support for fsspec's built-in caching mechanisms - Optional directory structure preservation - Configurable cache size and location</p>"},{"location":"architecture/#parallel-processing-architecture","title":"Parallel Processing Architecture","text":"<p>Worker Pool Management: <pre><code>from fsspeckit.common import run_parallel\n\ndef process_file(file_path):\n    # Process individual file\n    return processed_data\n\n# Parallel execution with automatic resource management\nresults = run_parallel(process_file, file_list, max_workers=8)\n</code></pre></p> <p>Resource Optimization: - Automatic worker count detection based on CPU cores - Memory-aware chunking for large datasets - Progress tracking and error handling</p>"},{"location":"architecture/#memory-management","title":"Memory Management","text":"<p>Efficient Data Processing: - Streaming operations for large files - Chunked processing with configurable batch sizes - Type conversion for PyArrow compatibility</p>"},{"location":"architecture/#extension-points-and-customization","title":"Extension Points and Customization","text":""},{"location":"architecture/#adding-new-storage-providers","title":"Adding New Storage Providers","text":"<p>Custom Storage Options: <pre><code>from fsspeckit.storage_options.base import BaseStorageOptions\n\nclass CustomStorageOptions(BaseStorageOptions):\n    \"\"\"Custom storage provider configuration.\"\"\"\n\n    provider: str = \"custom\"\n    custom_endpoint: Optional[str] = None\n\n    def to_filesystem(self) -&gt; AbstractFileSystem:\n        \"\"\"Create filesystem instance.\"\"\"\n        return CustomFileSystem(\n            endpoint=self.custom_endpoint,\n            **self.get_storage_options()\n        )\n</code></pre></p>"},{"location":"architecture/#custom-processing-backends","title":"Custom Processing Backends","text":"<p>Extending Dataset Operations: <pre><code>class CustomDatasetHandler:\n    \"\"\"Custom dataset processing backend.\"\"\"\n\n    def __init__(self, storage_options=None):\n        self.storage_options = storage_options\n\n    def write_dataset(self, data, path, **kwargs):\n        \"\"\"Custom dataset writing logic.\"\"\"\n        pass\n\n    def read_dataset(self, path, **kwargs):\n        \"\"\"Custom dataset reading logic.\"\"\"\n        pass\n</code></pre></p>"},{"location":"architecture/#migration-guide","title":"Migration Guide","text":""},{"location":"architecture/#from-fsspec-utils-to-fsspeckit","title":"From fsspec-utils to fsspeckit","text":"<p>Step 1: Update Imports <pre><code># Old imports (still work via utils fa\u00e7ade)\nfrom fsspec_utils import run_parallel\n\n# New recommended imports\nfrom fsspeckit.common import run_parallel\n</code></pre></p> <p>Step 2: Update Configuration <pre><code># Old configuration style\nstorage_options = {\"key\": \"value\", \"secret\": \"secret\"}\n\n# New configuration style\nfrom fsspeckit.storage_options import AwsStorageOptions\nstorage_options = AwsStorageOptions(\n    access_key=\"key\",\n    secret_key=\"secret\"\n)\n</code></pre></p> <p>Step 3: Update Filesystem Creation <pre><code># Old method\nfs = fsspec.filesystem(\"s3\", **storage_options)\n\n# New method\nfrom fsspeckit.core.filesystem import filesystem\nfs = filesystem(\"s3\", storage_options=storage_options.to_dict())\n</code></pre></p>"},{"location":"architecture/#future-features-not-yet-implemented","title":"Future Features (Not Yet Implemented)","text":"<p>The following features are planned but not yet implemented:</p> <ul> <li>Performance Tracking: Built-in performance monitoring and metrics collection</li> <li>Plugin Registry: Dynamic plugin discovery and registration system</li> <li>Circuit Breaker Patterns: Advanced resilience patterns for distributed systems</li> <li>Delta Lake Integration: Delta Lake write helpers and compatibility</li> <li>Advanced Monitoring: Comprehensive observability and health checking</li> </ul>"},{"location":"architecture/#conclusion","title":"Conclusion","text":"<p>The fsspeckit architecture provides a practical foundation for data processing across multiple storage backends and processing frameworks. The domain-driven design ensures clear separation of concerns while maintaining consistent interfaces and behavior across all components.</p> <p>The modular architecture enables easy extension and customization while maintaining backwards compatibility for existing users. Built-in performance optimizations and cross-framework compatibility make fsspeckit suitable for data processing workflows.</p> <p>For specific implementation details and code examples, refer to the individual domain package documentation.</p>"},{"location":"contributing/","title":"Contributing to fsspeckit","text":"<p>We welcome contributions to <code>fsspeckit</code>! Your help makes this project better. This guide outlines how you can contribute, from reporting issues to submitting pull requests.</p>"},{"location":"contributing/#how-to-contribute","title":"How to Contribute","text":""},{"location":"contributing/#reporting-issues","title":"Reporting Issues","text":"<p>If you encounter any bugs, unexpected behavior, or have suggestions for new features, please open an issue on our GitHub Issues page.</p> <p>When reporting an issue, please include: - A clear and concise description of the problem. - Steps to reproduce the behavior. - Expected behavior. - Screenshots or error messages if applicable. - Your <code>fsspeckit</code> version and Python environment details.</p>"},{"location":"contributing/#submitting-pull-requests","title":"Submitting Pull Requests","text":"<p>We gladly accept pull requests for bug fixes, new features, and improvements. To submit a pull request:</p> <ol> <li>Fork the Repository: Start by forking the <code>fsspeckit</code> repository on GitHub.</li> <li>Clone Your Fork: Clone your forked repository to your local machine.     <pre><code>git clone https://github.com/your-username/fsspeckit.git\ncd fsspeckit\n</code></pre></li> <li>Create a New Branch: Create a new branch for your changes.     <pre><code>git checkout -b feature/your-feature-name\n# or\ngit checkout -b bugfix/issue-description\n</code></pre></li> <li>Make Your Changes: Implement your bug fix or feature.</li> <li>Write Tests: Ensure your changes are covered by appropriate unit tests.</li> <li>Run Tests: Verify all tests pass before submitting.     <pre><code>uv run pytest\n</code></pre></li> <li>Format Code: Ensure your code adheres to the project's style guidelines. The project uses <code>ruff</code> for linting and formatting.     <pre><code>uv run ruff check . --fix\nuv run ruff format .\n</code></pre></li> <li>Commit Your Changes: Write clear and concise commit messages.     <pre><code>git commit -m \"feat: Add new awesome feature\"\n</code></pre></li> <li>Push to Your Fork: Push your branch to your forked repository.     <pre><code>git push origin feature/your-feature-name\n</code></pre></li> <li>Open a Pull Request: Go to the original <code>fsspeckit</code> repository on GitHub and open a pull request from your new branch. Provide a detailed description of your changes.</li> </ol>"},{"location":"contributing/#development-setup","title":"Development Setup","text":"<p>To set up your development environment, follow these steps:</p> <ol> <li>Clone the repository:     <pre><code>git clone https://github.com/legout/fsspeckit.git\ncd fsspeckit\n</code></pre></li> <li>Install <code>uv</code>:     <code>fsspeckit</code> uses <code>uv</code> for dependency management and running commands. If you don't have <code>uv</code> installed, you can install it via <code>pip</code>:     <pre><code>pip install uv\n</code></pre></li> <li>Install Development Dependencies:     The project uses <code>uv</code> to manage dependencies. Install the <code>dev</code> dependency group which includes tools for testing, linting, and documentation generation.     <pre><code>uv pip install -e \".[dev]\"\n</code></pre>     This command installs the project in editable mode (<code>-e</code>) and includes all development-related dependencies specified in <code>pyproject.toml</code> under the <code>[project.optional-dependencies] dev</code> section.</li> </ol>"},{"location":"contributing/#best-practices-for-contributions","title":"Best Practices for Contributions","text":"<ul> <li>Code Style: Adhere to the existing code style. We use <code>ruff</code> for linting and formatting.</li> <li>Testing: All new features and bug fixes should be accompanied by relevant unit tests.</li> <li>Documentation: If your changes introduce new features or modify existing behavior, please update the documentation accordingly.</li> <li>Commit Messages: Write descriptive commit messages that explain the purpose of your changes.</li> <li>Atomic Commits: Try to keep your commits focused on a single logical change.</li> <li>Branch Naming: Use clear and concise branch names (e.g., <code>feature/new-feature</code>, <code>bugfix/fix-issue-123</code>).</li> </ul>"},{"location":"examples/","title":"Examples Guide","text":"<p>This page provides practical examples of using <code>fsspeckit</code> for real-world data processing tasks. All examples use the domain package structure and demonstrate the current implemented features.</p>"},{"location":"examples/#quick-start-examples","title":"Quick Start Examples","text":""},{"location":"examples/#basic-filesystem-creation","title":"Basic Filesystem Creation","text":"<pre><code>from fsspeckit.core.filesystem import filesystem\n\n# Auto-detect protocol from URI\nfs = filesystem(\"s3://bucket/path\")  # S3\nfs = filesystem(\"gs://bucket/path\")  # Google Cloud Storage\nfs = filesystem(\"az://container/path\")  # Azure Blob Storage\n\n# Local filesystem with specific options\nfs = filesystem(\"file\", auto_mkdir=True)\n</code></pre>"},{"location":"examples/#storage-options-configuration","title":"Storage Options Configuration","text":"<pre><code>from fsspeckit.storage_options import (\n    AwsStorageOptions,\n    GcsStorageOptions,\n    AzureStorageOptions,\n    storage_options_from_env\n)\n\n# Configure AWS from environment variables\naws_options = storage_options_from_env(\"s3\")\nfs = filesystem(\"s3\", storage_options=aws_options.to_dict())\n\n# Manual configuration\naws_options = AwsStorageOptions(\n    region=\"us-east-1\",\n    access_key_id=\"YOUR_ACCESS_KEY\",\n    secret_access_key=\"YOUR_SECRET_KEY\"\n)\nfs = aws_options.to_filesystem()\n</code></pre>"},{"location":"examples/#dataset-operations-examples","title":"Dataset Operations Examples","text":""},{"location":"examples/#duckdb-parquet-handler","title":"DuckDB Parquet Handler","text":"<pre><code>from fsspeckit.datasets import DuckDBParquetHandler\nimport polars as pl\n\n# Initialize with storage options\nstorage_options = {\"key\": \"value\", \"secret\": \"secret\"}\nhandler = DuckDBParquetHandler(storage_options=storage_options)\n\n# Create sample data\ndata = pl.DataFrame({\n    \"id\": [1, 2, 3, 4, 5],\n    \"category\": [\"A\", \"B\", \"A\", \"B\", \"A\"],\n    \"value\": [10.5, 20.3, 15.7, 25.1, 12.8],\n    \"timestamp\": [\"2023-01-01\", \"2023-01-02\", \"2023-01-03\", \"2023-01-04\", \"2023-01-05\"]\n})\n\n# Write dataset\nhandler.write_parquet_dataset(data, \"s3://bucket/my-dataset/\")\n\n# Execute SQL queries\nresult = handler.execute_sql(\"\"\"\n    SELECT\n        category,\n        COUNT(*) as count,\n        AVG(value) as avg_value,\n        SUM(value) as total_value\n    FROM parquet_scan('s3://bucket/my-dataset/')\n    GROUP BY category\n    ORDER BY category\n\"\"\")\n\nprint(result)\n</code></pre>"},{"location":"examples/#pyarrow-dataset-operations","title":"PyArrow Dataset Operations","text":"<pre><code>import pyarrow as pa\nimport pyarrow.parquet as pq\nfrom fsspeckit.datasets.pyarrow import (\n    merge_parquet_dataset_pyarrow,\n    optimize_parquet_dataset_pyarrow,\n    compact_parquet_dataset_pyarrow\n)\nfrom fsspeckit.core.filesystem import filesystem\n\n# Create filesystem\nfs = filesystem(\"s3\", storage_options=storage_options)\n\n# Merge multiple parquet datasets\nmerge_parquet_dataset_pyarrow(\n    dataset_path=\"s3://bucket/source-datasets/\",\n    output_path=\"s3://bucket/merged-dataset/\",\n    merge_strategy=\"schema_evolution\"\n)\n\n# Optimize dataset with Z-ordering\noptimize_parquet_dataset_pyarrow(\n    dataset_path=\"s3://bucket/large-dataset/\",\n    z_order_columns=[\"category\", \"timestamp\"],\n    target_file_size=\"256MB\"\n)\n\n# Compact small files\ncompact_parquet_dataset_pyarrow(\n    dataset_path=\"s3://bucket/fragmented-dataset/\",\n    target_file_size=\"128MB\"\n)\n</code></pre>"},{"location":"examples/#sql-filter-translation-examples","title":"SQL Filter Translation Examples","text":""},{"location":"examples/#cross-framept-sql-filtering","title":"Cross-Framept SQL Filtering","text":"<pre><code>import pyarrow as pa\nimport pyarrow.compute as pc\nimport polars as pl\nfrom fsspeckit.sql.filters import sql2pyarrow_filter, sql2polars_filter\n\n# Define schema\nschema = pa.schema([\n    (\"id\", pa.int64()),\n    (\"value\", pa.string()),\n    (\"category\", pa.string()),\n    (\"timestamp\", pa.timestamp(\"us\")),\n    (\"amount\", pa.float64())\n])\n\n# SQL filter examples\nsql_filters = [\n    \"id &gt; 100 AND category IN ('A', 'B', 'C')\",\n    \"value LIKE 'prefix%' AND amount &gt; 1000.0\",\n    \"timestamp &gt;= '2023-01-01' AND timestamp &lt;= '2023-12-31'\",\n    \"category = 'IMPORTANT' AND (amount BETWEEN 100 AND 1000)\"\n]\n\n# Convert to PyArrow filters\nfor sql_filter in sql_filters:\n    pyarrow_filter = sql2pyarrow_filter(sql_filter, schema)\n    print(f\"SQL: {sql_filter}\")\n    print(f\"PyArrow: {pyarrow_filter}\")\n    print()\n\n# Convert to Polars filters\npolars_schema = pl.Schema({\n    \"id\": pl.Int64,\n    \"value\": pl.String,\n    \"category\": pl.String,\n    \"timestamp\": pl.Datetime,\n    \"amount\": pl.Float64\n})\n\nfor sql_filter in sql_filters:\n    polars_filter = sql2polars_filter(sql_filter, polars_schema)\n    print(f\"SQL: {sql_filter}\")\n    print(f\"Polars: {polars_filter}\")\n    print()\n</code></pre>"},{"location":"examples/#practical-dataset-filtering","title":"Practical Dataset Filtering","text":"<pre><code>import pandas as pd\nimport pyarrow.parquet as pq\nfrom fsspeckit.sql.filters import sql2pyarrow_filter\n\n# Load a dataset\ndataset = pq.ParquetDataset(\"s3://bucket/large-dataset/\")\ntable = dataset.to_table()\n\n# Define your schema\nschema = table.schema\n\n# Use SQL to create filters\nsql_conditions = [\n    \"category = 'HIGH_PRIORITY'\",\n    \"amount &gt; 50000\",\n    \"timestamp &gt;= '2023-06-01'\",\n    \"status IN ('ACTIVE', 'PENDING')\"\n]\n\n# Apply filters incrementally\nfiltered_data = table\nfor condition in sql_conditions:\n    filter_expr = sql2pyarrow_filter(condition, schema)\n    filtered_data = filtered_data.filter(filter_expr)\n\nprint(f\"Original rows: {len(table)}\")\nprint(f\"Filtered rows: {len(filtered_data)}\")\n</code></pre>"},{"location":"examples/#storage-options-examples","title":"Storage Options Examples","text":""},{"location":"examples/#environment-based-configuration","title":"Environment-Based Configuration","text":"<pre><code>from fsspeckit.storage_options import storage_options_from_env\nfrom fsspeckit.core.filesystem import filesystem\nimport os\n\n# Set environment variables (in production, these would be set externally)\nos.environ[\"AWS_ACCESS_KEY_ID\"] = \"your_access_key\"\nos.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"your_secret_key\"\nos.environ[\"AWS_DEFAULT_REGION\"] = \"us-east-1\"\n\n# Load configuration from environment\naws_options = storage_options_from_env(\"s3\")\nprint(f\"Loaded AWS region: {aws_options.region}\")\n\n# Create filesystem\nfs = filesystem(\"s3\", storage_options=aws_options.to_dict())\n\n# Use the filesystem\nfiles = fs.ls(\"s3://your-bucket/\")\nprint(files)\n</code></pre>"},{"location":"examples/#multi-cloud-configuration","title":"Multi-Cloud Configuration","text":"<pre><code>from fsspeckit.storage_options import (\n    AwsStorageOptions,\n    GcsStorageOptions,\n    AzureStorageOptions\n)\n\n# AWS configuration\naws_config = AwsStorageOptions(\n    region=\"us-west-2\",\n    access_key_id=\"aws_key\",\n    secret_access_key=\"aws_secret\"\n)\n\n# Google Cloud configuration\ngcs_config = GcsStorageOptions(\n    project=\"gcp-project\",\n    token=\"path/to/service-account.json\"\n)\n\n# Azure configuration\nazure_config = AzureStorageOptions(\n    account_name=\"storageaccount\",\n    account_key=\"azure_key\"\n)\n\n# Create filesystems for each provider\naws_fs = aws_config.to_filesystem()\ngcs_fs = gcs_config.to_filesystem()\nazure_fs = azure_config.to_filesystem()\n\n# Use them interchangeably\nfor provider, fs in [(\"AWS\", aws_fs), (\"GCS\", gcs_fs), (\"Azure\", azure_fs)]:\n    print(f\"{provider} filesystem: {type(fs).__name__}\")\n</code></pre>"},{"location":"examples/#uri-based-configuration","title":"URI-Based Configuration","text":"<pre><code>from fsspeckit.storage_options import storage_options_from_uri\nfrom fsspeckit.core.filesystem import filesystem\n\n# Extract storage options from URIs\nuris = [\n    \"s3://bucket/path?region=us-east-1&amp;endpoint_url=https://s3.amazonaws.com\",\n    \"gs://bucket/path?project=my-gcp-project\",\n    \"az://container/path?account_name=mystorageaccount\"\n]\n\nfor uri in uris:\n    options = storage_options_from_uri(uri)\n    fs = filesystem(options.protocol, storage_options=options.to_dict())\n    print(f\"URI: {uri}\")\n    print(f\"Protocol: {options.protocol}\")\n    print(f\"Filesystem: {type(fs).__name__}\")\n    print()\n</code></pre>"},{"location":"examples/#common-utilities-examples","title":"Common Utilities Examples","text":""},{"location":"examples/#parallel-processing","title":"Parallel Processing","text":"<pre><code>from fsspeckit.common.misc import run_parallel\nimport time\n\ndef process_file(file_path):\n    \"\"\"Simulate processing a file\"\"\"\n    print(f\"Processing {file_path}...\")\n    time.sleep(0.1)  # Simulate work\n    return f\"Processed {file_path}\"\n\n# List of files to process\nfile_list = [f\"file_{i}.parquet\" for i in range(10)]\n\n# Process files in parallel\nresults = run_parallel(\n    func=process_file,\n    data=file_list,\n    max_workers=4,\n    progress=True\n)\n\nprint(\"\\nResults:\")\nfor result in results:\n    print(result)\n</code></pre>"},{"location":"examples/#type-conversion-and-utilities","title":"Type Conversion and Utilities","text":"<pre><code>import pyarrow as pa\nimport polars as pl\nfrom fsspeckit.common.types import (\n    convert_large_types_to_normal,\n    dict_to_dataframe,\n    to_pyarrow_table\n)\n\n# Convert large string types to normal\nlarge_string_table = pa.Table.from_pydict({\n    \"text\": pa.array([\"value1\", \"value2\", \"value3\"], type=pa.large_string())\n})\n\nprint(f\"Original schema: {large_string_table.schema}\")\nnormal_table = convert_large_types_to_normal(large_string_table)\nprint(f\"Normal schema: {normal_table.schema}\")\n\n# Convert dictionaries to DataFrames\ndata = {\n    \"id\": [1, 2, 3],\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n    \"score\": [95.5, 87.3, 92.1]\n}\n\n# Convert to Polars DataFrame\npl_df = dict_to_dataframe(data, library=\"polars\")\nprint(f\"Polars DataFrame:\\n{pl_df}\")\n\n# Convert to Pandas DataFrame\npd_df = dict_to_dataframe(data, library=\"pandas\")\nprint(f\"Pandas DataFrame:\\n{pd_df}\")\n\n# Convert to PyArrow Table\narrow_table = to_pyarrow_table(data)\nprint(f\"PyArrow Table:\\n{arrow_table}\")\n</code></pre>"},{"location":"examples/#directory-operations","title":"Directory Operations","text":"<pre><code>from fsspeckit.core.filesystem import filesystem, DirFileSystem\nfrom fsspeckit.common.misc import sync_dir, extract_partitions\nimport tempfile\nimport shutil\n\n# Create temporary directories for testing\nsrc_dir = tempfile.mkdtemp()\ndst_dir = tempfile.mkdtemp()\n\n# Create base filesystems\nsrc_fs = filesystem(\"file\")\ndst_fs = filesystem(\"file\")\n\n# Create safe filesystems that restrict operations\nsafe_src = DirFileSystem(fs=src_fs, path=src_dir)\nsafe_dst = DirFileSystem(fs=dst_fs, path=dst_dir)\n\n# Create some test files\nwith safe_src.open(\"test1.txt\", \"w\") as f:\n    f.write(\"Content 1\")\nwith safe_src.open(\"test2.txt\", \"w\") as f:\n    f.write(\"Content 2\")\n\n# List files\nprint(\"Source files:\", safe_src.ls(\"/\"))\n\n# Synchronize directories\nsync_dir(\n    src_fs=safe_src,\n    dst_fs=safe_dst,\n    src_path=\"/\",\n    dst_path=\"/\",\n    progress=True\n)\n\n# List destination files\nprint(\"Destination files:\", safe_dst.ls(\"/\"))\n\n# Extract partition information\npaths = [\n    \"data/year=2023/month=01/day=15/file.parquet\",\n    \"data/year=2023/month=01/day=16/file.parquet\",\n    \"data/year=2023/month=02/day=01/file.parquet\"\n]\n\nfor path in paths:\n    partitions = extract_partitions(path)\n    print(f\"Path: {path}\")\n    print(f\"Partitions: {partitions}\")\n\n# Cleanup\nshutil.rmtree(src_dir)\nshutil.rmtree(dst_dir)\n</code></pre>"},{"location":"examples/#error-handling-examples","title":"Error Handling Examples","text":""},{"location":"examples/#robust-file-operations","title":"Robust File Operations","text":"<pre><code>from fsspeckit.core.filesystem import filesystem, DirFileSystem\nfrom fsspeckit.storage_options import AwsStorageOptions\nimport os\n\ndef safe_file_operations():\n    \"\"\"Demonstrate safe file operations with error handling\"\"\"\n\n    try:\n        # Create filesystem with error handling\n        storage_options = AwsStorageOptions(\n            region=\"us-east-1\",\n            access_key_id=\"invalid_key\",  # This will cause authentication error\n            secret_access_key=\"invalid_secret\"\n        )\n\n        fs = storage_options.to_filesystem()\n\n        # Try to list files (will fail)\n        files = fs.ls(\"s3://bucket/\")\n\n    except Exception as e:\n        print(f\"Expected authentication error: {e}\")\n\n        # Fall back to local filesystem\n        fs = filesystem(\"file\")\n        print(\"Fell back to local filesystem\")\n\n    try:\n        # Create a safe filesystem that restricts operations\n        safe_dir = \"/tmp/fsspeckit_safe\"\n        os.makedirs(safe_dir, exist_ok=True)\n\n        safe_fs = DirFileSystem(fs=fs, path=safe_dir)\n\n        # This works\n        with safe_fs.open(\"test.txt\", \"w\") as f:\n            f.write(\"Safe content\")\n\n        # This should fail (attempting to access outside safe directory)\n        try:\n            safe_fs.open(\"../../../etc/passwd\", \"r\")\n        except (ValueError, PermissionError) as e:\n            print(f\"Security check worked: {e}\")\n\n    except Exception as e:\n        print(f\"Unexpected error: {e}\")\n\nsafe_file_operations()\n</code></pre>"},{"location":"examples/#performance-optimization-examples","title":"Performance Optimization Examples","text":""},{"location":"examples/#efficient-large-dataset-processing","title":"Efficient Large Dataset Processing","text":"<pre><code>import pyarrow as pa\nimport pyarrow.parquet as pq\nfrom fsspeckit.core.filesystem import filesystem\nfrom fsspeckit.datasets.pyarrow import process_dataset_in_batches\n\ndef process_batch_efficiently():\n    \"\"\"Show how to process large datasets efficiently\"\"\"\n\n    # Create filesystem with optimal settings\n    fs = filesystem(\"s3\", storage_options=storage_options)\n\n    # Define batch processing function\n    def process_batch(batch_table):\n        \"\"\"Process individual batch efficiently\"\"\"\n        # Example: calculate statistics\n        total_rows = len(batch_table)\n        if \"amount\" in batch_table.column_names:\n            total_amount = batch_table.column(\"amount\").to_pandas().sum()\n            return {\"rows\": total_rows, \"total_amount\": total_amount}\n        return {\"rows\": total_rows}\n\n    # Process dataset in batches\n    dataset_path = \"s3://bucket/large-dataset/\"\n\n    print(\"Processing large dataset in batches...\")\n    for i, result in enumerate(process_dataset_in_batches(\n        dataset_path=dataset_path,\n        batch_size=\"100MB\",\n        process_func=process_batch,\n        max_workers=4\n    )):\n        print(f\"Batch {i+1}: {result}\")\n\n# Uncomment to run (requires actual dataset)\n# process_batch_efficiently()\n</code></pre>"},{"location":"examples/#running-examples","title":"Running Examples","text":""},{"location":"examples/#prerequisites","title":"Prerequisites","text":"<pre><code># Install with all dependencies\npip install \"fsspeckit[aws,gcp,azure]\"\n\n# For SQL filter translation\npip install sqlglot\n\n# For dataset operations\npip install duckdb pyarrow polars\n</code></pre>"},{"location":"examples/#environment-setup","title":"Environment Setup","text":"<pre><code># Set AWS credentials (replace with your actual values)\nexport AWS_ACCESS_KEY_ID=\"your_access_key\"\nexport AWS_SECRET_ACCESS_KEY=\"your_secret_key\"\nexport AWS_DEFAULT_REGION=\"us-east-1\"\n\n# Set GCS credentials\nexport GOOGLE_APPLICATION_CREDENTIALS=\"/path/to/service-account.json\"\nexport GOOGLE_CLOUD_PROJECT=\"your-gcp-project\"\n\n# Set Azure credentials\nexport AZURE_STORAGE_ACCOUNT=\"your_storage_account\"\nexport AZURE_STORAGE_KEY=\"your_storage_key\"\n</code></pre>"},{"location":"examples/#testing-examples","title":"Testing Examples","text":"<pre><code># Test individual components\nfrom fsspeckit.storage_options import storage_options_from_env\nfrom fsspeckit.core.filesystem import filesystem\n\n# Test filesystem creation\ntry:\n    fs = filesystem(\"file\")  # Local filesystem should always work\n    print(\"\u2713 Local filesystem created successfully\")\nexcept Exception as e:\n    print(f\"\u2717 Local filesystem failed: {e}\")\n\n# Test storage options loading\ntry:\n    options = storage_options_from_env(\"s3\")  # May fail if no env vars set\n    print(\"\u2713 AWS storage options loaded\")\nexcept Exception as e:\n    print(f\"\u26a0 AWS storage options not available: {e}\")\n\n# Test SQL filter translation\nfrom fsspeckit.sql.filters import sql2pyarrow_filter\nimport pyarrow as pa\n\ntry:\n    schema = pa.schema([(\"id\", pa.int64()), (\"value\", pa.string())])\n    filter_expr = sql2pyarrow_filter(\"id &gt; 100\", schema)\n    print(\"\u2713 SQL filter translation working\")\nexcept Exception as e:\n    print(f\"\u2717 SQL filter translation failed: {e}\")\n</code></pre>"},{"location":"examples/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/#common-issues","title":"Common Issues","text":"<ol> <li>Import Errors: Install with <code>pip install \"fsspeckit[aws,gcp,azure]\"</code></li> <li>Authentication: Set environment variables for cloud providers</li> <li>Dependencies: Install optional dependencies: <code>pip install duckdb pyarrow polars sqlglot</code></li> <li>Network: Check connectivity to cloud services</li> <li>Permissions: Verify IAM roles and access policies</li> </ol>"},{"location":"examples/#getting-help","title":"Getting Help","text":"<ul> <li>Check the API Reference for detailed method documentation</li> <li>Review the Advanced Usage guide for complex scenarios</li> <li>Examine the source code in the <code>src/fsspeckit/</code> directory for implementation details</li> </ul>"},{"location":"installation/","title":"Installation","text":"<p><code>fsspeckit</code> can be installed using <code>pip</code>, the Python package installer.</p>"},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.11 or higher is required.</li> </ul>"},{"location":"installation/#install-with-pip","title":"Install with pip","text":"<p>To install <code>fsspeckit</code> using <code>pip</code>, run the following command:</p> <pre><code>pip install fsspeckit\n</code></pre>"},{"location":"installation/#optional-cloud-provider-support","title":"Optional Cloud Provider Support","text":"<p>Install with support for specific cloud providers:</p> <pre><code># AWS S3 support\npip install \"fsspeckit[aws]\"\n\n# Google Cloud Storage support\npip install \"fsspeckit[gcp]\"\n\n# Azure Storage support\npip install \"fsspeckit[azure]\"\n\n# All cloud providers\npip install \"fsspeckit[aws,gcp,azure]\"\n</code></pre>"},{"location":"installation/#upgrading","title":"Upgrading","text":"<p>To upgrade <code>fsspeckit</code> to the latest version, use:</p> <pre><code>pip install --upgrade fsspeckit\n</code></pre>"},{"location":"installation/#environment-management-with-uv-and-pixi","title":"Environment Management with <code>uv</code> and <code>pixi</code>","text":"<p>For robust dependency management and faster installations, we recommend using <code>uv</code> or <code>pixi</code>.</p>"},{"location":"installation/#using-uv","title":"Using <code>uv</code>","text":"<p><code>uv</code> is a fast Python package installer and resolver. To install <code>fsspeckit</code> with <code>uv</code>:</p> <pre><code>uv pip install fsspeckit\n</code></pre>"},{"location":"installation/#using-pixi","title":"Using <code>pixi</code>","text":"<p><code>pixi</code> is a modern package manager for Python and other languages. To add <code>fsspeckit</code> to your <code>pixi</code> project:</p> <pre><code>pixi add fsspeckit\n</code></pre>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter any issues during installation, consider the following:</p> <ul> <li>Python Version: Ensure you are using Python 3.11 or higher. You can check your Python version with <code>python --version</code>.</li> <li>Virtual Environments: It is highly recommended to use a virtual environment (e.g., <code>venv</code>, <code>conda</code>, <code>uv</code>, <code>pixi</code>) to avoid conflicts with system-wide packages.</li> <li>Permissions: If you encounter permission errors, you might need to run the installation command with <code>sudo</code> (e.g., <code>sudo pip install fsspeckit</code>), but this is generally not recommended in a virtual environment.</li> <li>Network Issues: Check your internet connection if the installation fails to download packages.</li> </ul> <p>For further assistance, please refer to the official fsspeckit GitHub repository or open an issue.</p>"},{"location":"quickstart/","title":"Quickstart","text":"<p>This guide will help you get started with <code>fsspeckit</code> by demonstrating the key features for working with various storage backends and data processing frameworks.</p>"},{"location":"quickstart/#installation","title":"Installation","text":"<p>Install <code>fsspeckit</code> with the dependencies you need:</p> <pre><code># Basic installation\npip install fsspeckit\n\n# With cloud storage support\npip install \"fsspeckit[aws,gcp,azure]\"\n\n# With all optional dependencies for data processing\npip install \"fsspeckit[aws,gcp,azure]\" duckdb pyarrow polars sqlglot\n</code></pre>"},{"location":"quickstart/#domain-package-structure","title":"Domain Package Structure","text":"<p><code>fsspeckit</code> is organized into domain-specific packages. Import from the appropriate package for your use case:</p> <pre><code># Filesystem creation and core functionality\nfrom fsspeckit.core.filesystem import filesystem\n\n# Storage configuration\nfrom fsspeckit.storage_options import AwsStorageOptions, storage_options_from_env\n\n# Dataset operations\nfrom fsspeckit.datasets import DuckDBParquetHandler\n\n# SQL filter translation\nfrom fsspeckit.sql.filters import sql2pyarrow_filter, sql2polars_filter\n\n# Common utilities\nfrom fsspeckit.common.misc import run_parallel\nfrom fsspeckit.common.types import dict_to_dataframe\n\n# Backwards compatibility (legacy)\nfrom fsspeckit.utils import DuckDBParquetHandler  # Still works\n</code></pre>"},{"location":"quickstart/#basic-usage-local-filesystem","title":"Basic Usage: Local Filesystem","text":"<pre><code>from fsspeckit.core.filesystem import filesystem\nimport os\n\n# Create a local filesystem\nfs = filesystem(\"file\")\n\n# Define a directory path\nlocal_dir = \"./my_data/\"\nos.makedirs(local_dir, exist_ok=True)\n\n# Create and write a file\nwith fs.open(f\"{local_dir}example.txt\", \"w\") as f:\n    f.write(\"Hello, fsspeckit!\")\n\n# Read the file\nwith fs.open(f\"{local_dir}example.txt\", \"r\") as f:\n    content = f.read()\nprint(f\"Content: {content}\")\n\n# List files in directory\nfiles = fs.ls(local_dir)\nprint(f\"Files: {files}\")\n</code></pre>"},{"location":"quickstart/#storage-options-configuration","title":"Storage Options Configuration","text":""},{"location":"quickstart/#environment-based-configuration","title":"Environment-Based Configuration","text":"<pre><code>from fsspeckit.storage_options import storage_options_from_env\nfrom fsspeckit.core.filesystem import filesystem\n\n# Set environment variables (or set them in your environment)\nimport os\nos.environ[\"AWS_ACCESS_KEY_ID\"] = \"your_access_key\"\nos.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"your_secret_key\"\nos.environ[\"AWS_DEFAULT_REGION\"] = \"us-east-1\"\n\n# Load AWS options from environment\naws_options = storage_options_from_env(\"s3\")\nfs = filesystem(\"s3\", storage_options=aws_options.to_dict())\n\nprint(f\"Created S3 filesystem in region: {aws_options.region}\")\n</code></pre>"},{"location":"quickstart/#manual-configuration","title":"Manual Configuration","text":"<pre><code>from fsspeckit.storage_options import AwsStorageOptions, GcsStorageOptions\n\n# Configure AWS S3\naws_options = AwsStorageOptions(\n    region=\"us-east-1\",\n    access_key_id=\"YOUR_ACCESS_KEY\",\n    secret_access_key=\"YOUR_SECRET_KEY\"\n)\n\n# Configure Google Cloud Storage\ngcs_options = GcsStorageOptions(\n    project=\"your-gcp-project\",\n    token=\"path/to/service-account.json\"\n)\n\n# Create filesystems\naws_fs = aws_options.to_filesystem()\ngcs_fs = gcs_options.to_filesystem()\n</code></pre>"},{"location":"quickstart/#protocol-inference","title":"Protocol Inference","text":"<p>The <code>filesystem()</code> function can automatically detect protocols from URIs:</p> <pre><code>from fsspeckit.core.filesystem import filesystem\n\n# Auto-detect protocols\ns3_fs = filesystem(\"s3://bucket/path\")      # S3\ngcs_fs = filesystem(\"gs://bucket/path\")      # Google Cloud Storage\naz_fs = filesystem(\"az://container/path\")    # Azure Blob Storage\ngithub_fs = filesystem(\"github://owner/repo\") # GitHub\n\n# All work with the same interface\nfor name, fs in [(\"S3\", s3_fs), (\"GCS\", gcs_fs)]:\n    try:\n        files = fs.ls(\"/\")\n        print(f\"{name} files: {len(files)}\")\n    except Exception as e:\n        print(f\"{name} error: {e}\")\n</code></pre>"},{"location":"quickstart/#dataset-operations","title":"Dataset Operations","text":""},{"location":"quickstart/#duckdb-parquet-handler","title":"DuckDB Parquet Handler","text":"<pre><code>from fsspeckit.datasets import DuckDBParquetHandler\nimport polars as pl\n\n# Initialize handler with storage options\nstorage_options = {\"key\": \"value\", \"secret\": \"secret\"}\nhandler = DuckDBParquetHandler(storage_options=storage_options)\n\n# Create sample data\ndata = pl.DataFrame({\n    \"id\": [1, 2, 3, 4],\n    \"category\": [\"A\", \"B\", \"A\", \"B\"],\n    \"value\": [10.5, 20.3, 15.7, 25.1]\n})\n\n# Write dataset\nhandler.write_parquet_dataset(data, \"s3://bucket/my-dataset/\")\n\n# Execute SQL queries\nresult = handler.execute_sql(\"\"\"\n    SELECT category, COUNT(*) as count, AVG(value) as avg_value\n    FROM parquet_scan('s3://bucket/my-dataset/')\n    GROUP BY category\n\"\"\")\n\nprint(result)\n</code></pre>"},{"location":"quickstart/#pyarrow-dataset-operations","title":"PyArrow Dataset Operations","text":"<pre><code>from fsspeckit.datasets.pyarrow import optimize_parquet_dataset_pyarrow\n\n# Optimize large datasets\noptimize_parquet_dataset_pyarrow(\n    dataset_path=\"s3://bucket/large-dataset/\",\n    z_order_columns=[\"category\", \"timestamp\"],\n    target_file_size=\"256MB\"\n)\n</code></pre>"},{"location":"quickstart/#sql-filter-translation","title":"SQL Filter Translation","text":"<p>Convert SQL WHERE clauses to framework-specific filter expressions:</p> <pre><code>import pyarrow as pa\nimport polars as pl\nfrom fsspeckit.sql.filters import sql2pyarrow_filter, sql2polars_filter\n\n# Define schemas\npyarrow_schema = pa.schema([\n    (\"id\", pa.int64()),\n    (\"category\", pa.string()),\n    (\"value\", pa.float64())\n])\n\npolars_schema = pl.Schema({\n    \"id\": pl.Int64,\n    \"category\": pl.String,\n    \"value\": pl.Float64\n})\n\n# SQL filter\nsql_filter = \"category IN ('A', 'B') AND value &gt; 15.0\"\n\n# Convert to PyArrow filter\npyarrow_filter = sql2pyarrow_filter(sql_filter, pyarrow_schema)\nprint(f\"PyArrow filter: {pyarrow_filter}\")\n\n# Convert to Polars filter\npolars_filter = sql2polars_filter(sql_filter, polars_schema)\nprint(f\"Polars filter: {polars_filter}\")\n\n# Apply filters to data\nimport pyarrow.parquet as pq\ndataset = pq.ParquetDataset(\"data.parquet\")\nfiltered_table = dataset.to_table(filter=pyarrow_filter)\n\ndf = pl.read_parquet(\"data.parquet\")\nfiltered_df = df.filter(polars_filter)\n</code></pre>"},{"location":"quickstart/#common-utilities","title":"Common Utilities","text":""},{"location":"quickstart/#parallel-processing","title":"Parallel Processing","text":"<pre><code>from fsspeckit.common.misc import run_parallel\n\ndef process_file(file_path):\n    # Process individual file\n    return f\"Processed {file_path}\"\n\n# Process files in parallel\nfile_list = [\"file1.parquet\", \"file2.parquet\", \"file3.parquet\"]\nresults = run_parallel(\n    func=process_file,\n    data=file_list,\n    max_workers=4,\n    progress=True\n)\n\nprint(results)\n</code></pre>"},{"location":"quickstart/#type-conversion","title":"Type Conversion","text":"<pre><code>from fsspeckit.common.types import dict_to_dataframe, convert_large_types_to_normal\nimport pyarrow as pa\n\n# Convert dictionaries to DataFrames\ndata = {\"id\": [1, 2, 3], \"name\": [\"Alice\", \"Bob\", \"Charlie\"]}\n\n# To Polars DataFrame\npl_df = dict_to_dataframe(data, library=\"polars\")\nprint(f\"Polars DataFrame:\\n{pl_df}\")\n\n# To Pandas DataFrame\npd_df = dict_to_dataframe(data, library=\"pandas\")\nprint(f\"Pandas DataFrame:\\n{pd_df}\")\n\n# Convert large string types to normal strings\nlarge_table = pa.Table.from_pydict({\n    \"text\": pa.array([\"value1\", \"value2\"], type=pa.large_string())\n})\nnormal_table = convert_large_types_to_normal(large_table)\nprint(f\"Original schema: {large_table.schema}\")\nprint(f\"Normal schema: {normal_table.schema}\")\n</code></pre>"},{"location":"quickstart/#safe-file-operations","title":"Safe File Operations","text":""},{"location":"quickstart/#directory-constrained-filesystem","title":"Directory-Constrained Filesystem","text":"<pre><code>from fsspeckit.core.filesystem import DirFileSystem, filesystem\n\n# Create base filesystem\nbase_fs = filesystem(\"file\")\n\n# Create safe filesystem confined to specific directory\nsafe_fs = DirFileSystem(fs=base_fs, path=\"/allowed/directory\")\n\ntry:\n    # This works - within allowed directory\n    with safe_fs.open(\"/allowed/directory/file.txt\", \"w\") as f:\n        f.write(\"Safe content\")\n\n    # This fails - outside allowed directory\n    safe_fs.open(\"/etc/passwd\", \"r\")  # Raises ValueError/PermissionError\n\nexcept (ValueError, PermissionError) as e:\n    print(f\"Security check worked: {e}\")\n</code></pre>"},{"location":"quickstart/#error-handling","title":"Error Handling","text":"<p>Always wrap filesystem operations in try-except blocks:</p> <pre><code>from fsspeckit.core.filesystem import filesystem\nfrom fsspeckit.storage_options import AwsStorageOptions\n\ntry:\n    # Try to create filesystem\n    storage_options = AwsStorageOptions(\n        region=\"us-east-1\",\n        access_key_id=\"invalid_key\",\n        secret_access_key=\"invalid_secret\"\n    )\n    fs = storage_options.to_filesystem()\n\n    # Try to use it\n    files = fs.ls(\"s3://bucket/\")\n\nexcept Exception as e:\n    print(f\"Operation failed: {e}\")\n\n    # Fall back to local filesystem\n    fs = filesystem(\"file\")\n    print(\"Fell back to local filesystem\")\n</code></pre>"},{"location":"quickstart/#next-steps","title":"Next Steps","text":""},{"location":"quickstart/#explore-more-features","title":"Explore More Features","text":"<ul> <li>Advanced Usage: Read the Advanced Usage guide for complex scenarios</li> <li>Examples: Check the Examples guide for practical code samples</li> <li>API Reference: Browse the API Reference for detailed documentation</li> </ul>"},{"location":"quickstart/#common-use-cases","title":"Common Use Cases","text":"<ol> <li>Cloud Data Processing: Use <code>storage_options_from_env()</code> for production deployments</li> <li>Dataset Operations: Use <code>DuckDBParquetHandler</code> for large-scale parquet operations</li> <li>SQL Filtering: Use <code>sql2pyarrow_filter()</code> and <code>sql2polars_filter()</code> for cross-framework compatibility</li> <li>Safe Operations: Use <code>DirFileSystem</code> for security-critical applications</li> <li>Performance: Use <code>run_parallel()</code> for concurrent file processing</li> </ol>"},{"location":"quickstart/#production-tips","title":"Production Tips","text":"<ol> <li>Use Domain Packages: Import from <code>fsspeckit.datasets</code>, <code>fsspeckit.storage_options</code>, etc. instead of utils</li> <li>Environment Configuration: Load credentials from environment variables in production</li> <li>Error Handling: Always wrap remote filesystem operations in try-except blocks</li> <li>Type Safety: Use structured <code>StorageOptions</code> classes instead of raw dictionaries</li> <li>Testing: Use <code>LocalStorageOptions</code> and <code>DirFileSystem</code> for isolated test environments</li> </ol> <p>For more detailed information, explore the other sections of the documentation.</p>"},{"location":"utils/","title":"Utilities Reference","text":"<p>This page documents utilities available in fsspeckit. The utilities are organized into domain packages for better discoverability and maintainability.</p> <p>Package Layout Overview: fsspeckit is organized into domain packages - see Architecture for details. Backwards Compatibility: All imports from <code>fsspeckit.utils</code> continue to work unchanged.</p>"},{"location":"utils/#cross-cutting-utilities-fsspeckitcommon","title":"Cross-Cutting Utilities (<code>fsspeckit.common</code>)","text":""},{"location":"utils/#logging","title":"Logging","text":""},{"location":"utils/#setup_logging","title":"<code>setup_logging()</code>","text":"<p>Configure logging throughout your application with loguru:</p> <pre><code>from fsspeckit.common.logging import setup_logging\n\n# Basic setup\nsetup_logging()\n\n# With custom level and format\nsetup_logging(level=\"DEBUG\", format_string=\"{time} | {level} | {message}\")\n\n# Control logging via environment variable\n# export fsspeckit_LOG_LEVEL=DEBUG\n</code></pre> <p>Environment Variables: - <code>fsspeckit_LOG_LEVEL</code> - Set the logging level (default: INFO)</p>"},{"location":"utils/#parallel-processing","title":"Parallel Processing","text":""},{"location":"utils/#run_parallel","title":"<code>run_parallel()</code>","text":"<p>Execute a function across multiple inputs using parallel threads with optional progress bar:</p> <pre><code>from fsspeckit.common.misc import run_parallel\n\ndef process_file(path, multiplier=1):\n    return len(path) * multiplier\n\nresults = run_parallel(\n    process_file,\n    [\"/path1\", \"/path2\", \"/path3\"],\n    multiplier=2,\n    n_jobs=4,\n    verbose=True,  # Show progress bar\n    backend=\"threading\"\n)\n</code></pre> <p>Parameters: - <code>func</code> - Function to apply to each item - <code>items</code> - List of inputs to process - <code>n_jobs</code> - Number of parallel threads (default: CPU count) - <code>backend</code> - Parallel backend: \"threading\" or \"process\" (default: \"threading\")</p>"},{"location":"utils/#file-system-operations","title":"File System Operations","text":""},{"location":"utils/#file-synchronization","title":"File Synchronization","text":"<pre><code>from fsspeckit.common.misc import sync_files, sync_dir\n\n# Sync individual files\nsync_files(\n    source_paths=[\"/data/file1.txt\", \"/data/file2.txt\"],\n    fs_target=filesystem(\"s3://bucket/\"),\n    verbose=True\n)\n\n# Sync directories recursively\nsync_dir(\n    source_dir=\"/data/\",\n    fs_target=filesystem(\"s3://bucket/\"),\n    pattern=\"*.parquet\",\n    delete=True\n)\n</code></pre>"},{"location":"utils/#path-utilities","title":"Path Utilities","text":"<pre><code>from fsspeckit.common.misc import get_partitions_from_path\n\n# Extract partition information from paths\npartitions = get_partitions_from_path(\"/data/year=2023/month=01/file.parquet\")\n# Returns: {'year': '2023', 'month': '01'}\n</code></pre>"},{"location":"utils/#type-conversion","title":"Type Conversion","text":""},{"location":"utils/#dataframe-conversion","title":"DataFrame Conversion","text":"<pre><code>from fsspeckit.common.types import dict_to_dataframe, to_pyarrow_table\n\n# Convert dict to DataFrame\ndata = {\"col1\": [1, 2, 3], \"col2\": [4, 5, 6]}\ndf = dict_to_dataframe(data)\n\n# Convert to PyArrow table\ntable = to_pyarrow_table(df)\n</code></pre>"},{"location":"utils/#datetime-utilities","title":"DateTime Utilities","text":"<pre><code>from fsspeckit.common.datetime import timestamp_from_string, get_timestamp_column\n\n# Parse timestamp strings\nts = timestamp_from_string(\"2023-01-15 10:30:00\")\nts_with_tz = timestamp_from_string(\"2023-01-15T10:30:00Z\", tz=\"UTC\")\n\n# Find timestamp column in DataFrame\ntimestamp_cols = get_timestamp_column(df)\n</code></pre>"},{"location":"utils/#polars-optimization","title":"Polars Optimization","text":"<pre><code>from fsspeckit.common.polars import opt_dtype_pl\n\n# Optimize DataFrame data types\ndf_optimized = opt_dtype_pl(df, shrink_numerics=True)\n\n# Optimize specific columns\ndf_optimized = opt_dtype_pl(df, columns=[\"numeric_col\"])\n</code></pre>"},{"location":"utils/#dataset-operations-fsspeckitdatasets","title":"Dataset Operations (<code>fsspeckit.datasets</code>)","text":""},{"location":"utils/#duckdb-dataset-handler","title":"DuckDB Dataset Handler","text":""},{"location":"utils/#duckdbparquethandler","title":"<code>DuckDBParquetHandler</code>","text":"<p>High-performance parquet dataset operations using DuckDB:</p> <pre><code>from fsspeckit.datasets import DuckDBParquetHandler\n\nwith DuckDBParquetHandler() as handler:\n    # Dataset maintenance operations\n    handler.compact_parquet_dataset(\n        path=\"/data/events/\",\n        target_rows_per_file=500_000\n    )\n\n    # Data analytics\n    handler.optimize_parquet_dataset(\n        path=\"/data/events/\",\n        zorder_columns=[\"user_id\", \"timestamp\"]\n    )\n</code></pre>"},{"location":"utils/#pyarrow-dataset-helpers","title":"PyArrow Dataset Helpers","text":""},{"location":"utils/#schema-management","title":"Schema Management","text":"<pre><code>from fsspeckit.datasets import cast_schema, convert_large_types_to_normal\n\n# Convert to standard schema\nstandard_schema = convert_large_types_normal(original_schema)\n\n# Cast table to schema\ntable_casted = cast_schema(table, target_schema)\n</code></pre>"},{"location":"utils/#data-type-optimization","title":"Data Type Optimization","text":"<pre><code>from fsspeckit.datasets import opt_dtype_pa\n\n# Optimize PyArrow table data types\ntable_optimized = opt_dtype_pa(table)\n</code></pre>"},{"location":"utils/#dataset-merging","title":"Dataset Merging","text":"<pre><code>from fsspeckit.datasets.pyarrow import merge_parquet_dataset_pyarrow\n\n# Merge multiple datasets\nmerge_parquet_pyarrow(\n    dataset_paths=[\"/data/part1/\", \"/data/part2/\"],\n    target_path=\"/data/merged/\",\n    key_columns=[\"id\"]\n)\n</code></pre>"},{"location":"utils/#sql-filtering-fsspeckitsql","title":"SQL Filtering (<code>fsspeckit.sql</code>)","text":""},{"location":"utils/#sql-to-expression-translation","title":"SQL to Expression Translation","text":""},{"location":"utils/#sql2pyarrow_filter","title":"<code>sql2pyarrow_filter()</code>","text":"<p>Convert SQL WHERE clauses to PyArrow filter expressions:</p> <pre><code>from fsspeckit.sql.filters import sql2pyarrow_filter\n\nimport pyarrow as pa\n\nschema = pa.schema([\n    (\"id\", pa.int64()),\n    (\"name\", pa.string()),\n    (\"timestamp\", pa.timestamp(\"us\")),\n    (\"value\", pa.float64())\n])\n\n# Convert SQL to PyArrow filter\nfilter_expr = sql2pyarrow_filter(\"name = 'test' AND value &gt; 100\", schema)\n</code></pre>"},{"location":"utils/#sql2polars_filter","title":"<code>sql2polars_filter()</code>","text":"<p>Convert SQL WHERE clauses to Polars expressions:</p> <pre><code>from fsspeckit.sql.filters import sql2polars_filter\n\nimport polars as pl\n\n# Convert SQL to Polars filter\nfilter_expr = sql2polars_filter(\"name == 'test' AND value &gt; 100\")\n</code></pre>"},{"location":"utils/#backwards-compatibility-fsspeckitutils","title":"Backwards Compatibility (<code>fsspeckit.utils</code>)","text":"<p>The <code>fsspeckit.utils</code> module provides a backwards-compatible fa\u00e7ade that re-exports selected helpers from the domain packages.</p>"},{"location":"utils/#legacy-import-examples","title":"Legacy Import Examples","text":"<pre><code># These imports continue to work for backwards compatibility\nfrom fsspeckit.utils import (\n    setup_logging,\n    run_parallel,\n    DuckDBParquetHandler,\n    sql2pyarrow_filter,\n    dict_to_dataframe,\n    to_pyarrow_table,\n    opt_dtype_pl,\n    opt_dtype_pa\n)\n</code></pre>"},{"location":"utils/#migration-recommendation","title":"Migration Recommendation","text":"<p>For new code, prefer importing directly from domain packages:</p> <pre><code># Recommended (new code)\nfrom fsspeckit.datasets import DuckDBParquetHandler\nfrom fsspeckit.common.logging import setup_logging\nfrom fsspeckit.sql.filters import sql2pyarrow_filter\n\n# Legacy (existing code - still works)\nfrom fsspeckit.utils import DuckDBParquetHandler, setup_logging, sql2pyarrow_filter\n</code></pre> <p>For detailed migration instructions, see the Migration Guide.</p>"},{"location":"api/","title":"<code>fsspeckit</code> API Reference","text":"<p>Welcome to the <code>fsspeckit</code> API reference documentation. This section provides detailed information on the various modules, classes, and functions available in the library.</p> <p>Package Structure: fsspeckit is organized into domain packages. See Architecture for details.</p>"},{"location":"api/#core-modules","title":"Core Modules","text":"<ul> <li> <p><code>fsspeckit.core.base</code></p> </li> <li> <p><code>fsspeckit.core.ext</code></p> </li> </ul>"},{"location":"api/#storage-options","title":"Storage Options","text":"<ul> <li> <p><code>fsspeckit.storage_options.base</code></p> </li> <li> <p><code>fsspeckit.storage_options.cloud</code></p> </li> <li> <p><code>fsspeckit.storage_options.core</code></p> </li> <li> <p><code>fsspeckit.storage_options.git</code></p> </li> </ul>"},{"location":"api/#utils-backwards-compatibility","title":"Utils (Backwards Compatibility)","text":"<ul> <li> <p><code>fsspeckit.utils.datetime</code> - Date and time utilities</p> </li> <li> <p><code>fsspeckit.utils.logging</code> - Logging configuration and utilities</p> </li> <li> <p><code>fsspeckit.utils.misc</code> - Miscellaneous utility functions</p> </li> <li> <p><code>fsspeckit.utils.polars</code> - Polars DataFrame utilities</p> </li> <li> <p><code>fsspeckit.utils.pyarrow</code> - PyArrow utilities and integrations</p> </li> <li> <p><code>fsspeckit.utils.sql</code> - SQL query and filter utilities</p> </li> <li> <p><code>fsspeckit.utils.types</code> - Type definitions and utilities</p> </li> </ul> <p>Note: API documentation for the new domain packages (datasets, sql, common) will be added in a future update. The utils modules above provide backwards compatibility for all functionality.</p>"},{"location":"api/fsspeckit.common/","title":"fsspeckit.common","text":""},{"location":"api/fsspeckit.common/#fsspeckit.common","title":"common","text":"<p>Cross-cutting utilities for fsspeckit.</p> <p>This package contains utilities that are shared across different components: - Datetime parsing and manipulation utilities - Logging configuration and helpers - General purpose utility functions - Polars DataFrame optimization and manipulation - Type conversion and data transformation utilities</p>"},{"location":"api/fsspeckit.common/#fsspeckit.common-functions","title":"Functions","text":""},{"location":"api/fsspeckit.common/#fsspeckit.common.dict_to_dataframe","title":"fsspeckit.common.dict_to_dataframe","text":"<pre><code>dict_to_dataframe(\n    data: Union[dict, list[dict]],\n    unique: Union[bool, list[str], str] = False,\n) -&gt; DataFrame\n</code></pre> <p>Convert a dictionary or list of dictionaries to a Polars DataFrame.</p> <p>Handles various input formats: - Single dict with list values \u2192 DataFrame rows - Single dict with scalar values \u2192 Single row DataFrame - List of dicts with scalar values \u2192 Multi-row DataFrame - List of dicts with list values \u2192 DataFrame with list columns</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[dict, list[dict]]</code> <p>Dictionary or list of dictionaries to convert.</p> required <code>unique</code> <code>Union[bool, list[str], str]</code> <p>If True, remove duplicate rows. Can also specify columns.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Polars DataFrame containing the converted data.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Single dict with list values\n&gt;&gt;&gt; data = {'a': [1, 2, 3], 'b': [4, 5, 6]}\n&gt;&gt;&gt; dict_to_dataframe(data)\nshape: (3, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 b   \u2502\n\u2502 --- \u2506 --- \u2502\n\u2502 i64 \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 4   \u2502\n\u2502 2   \u2506 5   \u2502\n\u2502 3   \u2506 6   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <pre><code>&gt;&gt;&gt; # Single dict with scalar values\n&gt;&gt;&gt; data = {'a': 1, 'b': 2}\n&gt;&gt;&gt; dict_to_dataframe(data)\nshape: (1, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 b   \u2502\n\u2502 --- \u2506 --- \u2502\n\u2502 i64 \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 2   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <pre><code>&gt;&gt;&gt; # List of dicts with scalar values\n&gt;&gt;&gt; data = [{'a': 1, 'b': 2}, {'a': 3, 'b': 4}]\n&gt;&gt;&gt; dict_to_dataframe(data)\nshape: (2, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 b   \u2502\n\u2502 --- \u2506 --- \u2502\n\u2502 i64 \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 2   \u2502\n\u2502 3   \u2506 4   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> Source code in <code>src/fsspeckit/common/types.py</code> <pre><code>def dict_to_dataframe(\n    data: Union[dict, list[dict]], unique: Union[bool, list[str], str] = False\n) -&gt; \"pl.DataFrame\":\n    \"\"\"Convert a dictionary or list of dictionaries to a Polars DataFrame.\n\n    Handles various input formats:\n    - Single dict with list values \u2192 DataFrame rows\n    - Single dict with scalar values \u2192 Single row DataFrame\n    - List of dicts with scalar values \u2192 Multi-row DataFrame\n    - List of dicts with list values \u2192 DataFrame with list columns\n\n    Args:\n        data: Dictionary or list of dictionaries to convert.\n        unique: If True, remove duplicate rows. Can also specify columns.\n\n    Returns:\n        Polars DataFrame containing the converted data.\n\n    Examples:\n        &gt;&gt;&gt; # Single dict with list values\n        &gt;&gt;&gt; data = {'a': [1, 2, 3], 'b': [4, 5, 6]}\n        &gt;&gt;&gt; dict_to_dataframe(data)\n        shape: (3, 2)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 a   \u2506 b   \u2502\n        \u2502 --- \u2506 --- \u2502\n        \u2502 i64 \u2506 i64 \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 1   \u2506 4   \u2502\n        \u2502 2   \u2506 5   \u2502\n        \u2502 3   \u2506 6   \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n\n        &gt;&gt;&gt; # Single dict with scalar values\n        &gt;&gt;&gt; data = {'a': 1, 'b': 2}\n        &gt;&gt;&gt; dict_to_dataframe(data)\n        shape: (1, 2)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 a   \u2506 b   \u2502\n        \u2502 --- \u2506 --- \u2502\n        \u2502 i64 \u2506 i64 \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 1   \u2506 2   \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n\n        &gt;&gt;&gt; # List of dicts with scalar values\n        &gt;&gt;&gt; data = [{'a': 1, 'b': 2}, {'a': 3, 'b': 4}]\n        &gt;&gt;&gt; dict_to_dataframe(data)\n        shape: (2, 2)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 a   \u2506 b   \u2502\n        \u2502 --- \u2506 --- \u2502\n        \u2502 i64 \u2506 i64 \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 1   \u2506 2   \u2502\n        \u2502 3   \u2506 4   \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n    \"\"\"\n    if isinstance(data, list):\n        # If it's a single-element list, just use the first element\n        if len(data) == 1:\n            data = data[0]\n        # If it's a list of dicts\n        else:\n            first_item = data[0]\n            # Check if the dict values are lists/tuples\n            if any(isinstance(v, (list, tuple)) for v in first_item.values()):\n                # Each dict becomes a row with list/tuple values\n                data = pl.DataFrame(data)\n            else:\n                # If values are scalars, convert list of dicts to DataFrame\n                data = pl.DataFrame(data)\n\n            if unique:\n                data = data.unique(\n                    subset=None if not isinstance(unique, (str, list)) else unique,\n                    maintain_order=True,\n                )\n            return data\n\n    # If it's a single dict\n    if isinstance(data, dict):\n        # Check if values are lists/tuples\n        if any(isinstance(v, (list, tuple)) for v in data.values()):\n            # Get the length of any list value (assuming all lists have same length)\n            length = len(next(v for v in data.values() if isinstance(v, (list, tuple))))\n            # Convert to DataFrame where each list element becomes a row\n            data = pl.DataFrame(\n                {\n                    k: v if isinstance(v, (list, tuple)) else [v] * length\n                    for k, v in data.items()\n                }\n            )\n        else:\n            # If values are scalars, wrap them in a list to create a single row\n            data = pl.DataFrame({k: [v] for k, v in data.items()})\n\n        if unique:\n            data = data.unique(\n                subset=None if not isinstance(unique, (str, list)) else unique,\n                maintain_order=True,\n            )\n        return data\n\n    raise ValueError(\"Input must be a dictionary or list of dictionaries\")\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.get_logger","title":"fsspeckit.common.get_logger","text":"<pre><code>get_logger(name: str = 'fsspeckit')\n</code></pre> <p>Get a logger instance for the given name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Logger name, typically the module name.</p> <code>'fsspeckit'</code> <p>Returns:</p> Type Description <p>Configured logger instance.</p> Example <p>logger = get_logger(name) logger.info(\"This is a log message\")</p> Source code in <code>src/fsspeckit/common/logging.py</code> <pre><code>def get_logger(name: str = \"fsspeckit\"):\n    \"\"\"Get a logger instance for the given name.\n\n    Args:\n        name: Logger name, typically the module name.\n\n    Returns:\n        Configured logger instance.\n\n    Example:\n        &gt;&gt;&gt; logger = get_logger(__name__)\n        &gt;&gt;&gt; logger.info(\"This is a log message\")\n    \"\"\"\n    return logger.bind(name=name)\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.get_partitions_from_path","title":"fsspeckit.common.get_partitions_from_path","text":"<pre><code>get_partitions_from_path(\n    path: str,\n    partitioning: Union[str, list[str], None] = None,\n) -&gt; list[tuple]\n</code></pre> <p>Extract dataset partitions from a file path.</p> <p>Parses file paths to extract partition information based on different partitioning schemes.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>File path potentially containing partition information.</p> required <code>partitioning</code> <code>Union[str, list[str], None]</code> <p>Partitioning scheme: - \"hive\": Hive-style partitioning (key=value) - str: Single partition column name - list[str]: Multiple partition column names - None: Return empty list</p> <code>None</code> <p>Returns:</p> Type Description <code>list[tuple]</code> <p>List of tuples containing (column, value) pairs.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Hive-style partitioning\n&gt;&gt;&gt; get_partitions_from_path(\"data/year=2023/month=01/file.parquet\", \"hive\")\n[('year', '2023'), ('month', '01')]\n</code></pre> <pre><code>&gt;&gt;&gt; # Single partition column\n&gt;&gt;&gt; get_partitions_from_path(\"data/2023/01/file.parquet\", \"year\")\n[('year', '2023')]\n</code></pre> <pre><code>&gt;&gt;&gt; # Multiple partition columns\n&gt;&gt;&gt; get_partitions_from_path(\"data/2023/01/file.parquet\", [\"year\", \"month\"])\n[('year', '2023'), ('month', '01')]\n</code></pre> Source code in <code>src/fsspeckit/common/misc.py</code> <pre><code>def get_partitions_from_path(\n    path: str, partitioning: Union[str, list[str], None] = None\n) -&gt; list[tuple]:\n    \"\"\"Extract dataset partitions from a file path.\n\n    Parses file paths to extract partition information based on\n    different partitioning schemes.\n\n    Args:\n        path: File path potentially containing partition information.\n        partitioning: Partitioning scheme:\n            - \"hive\": Hive-style partitioning (key=value)\n            - str: Single partition column name\n            - list[str]: Multiple partition column names\n            - None: Return empty list\n\n    Returns:\n        List of tuples containing (column, value) pairs.\n\n    Examples:\n        &gt;&gt;&gt; # Hive-style partitioning\n        &gt;&gt;&gt; get_partitions_from_path(\"data/year=2023/month=01/file.parquet\", \"hive\")\n        [('year', '2023'), ('month', '01')]\n\n        &gt;&gt;&gt; # Single partition column\n        &gt;&gt;&gt; get_partitions_from_path(\"data/2023/01/file.parquet\", \"year\")\n        [('year', '2023')]\n\n        &gt;&gt;&gt; # Multiple partition columns\n        &gt;&gt;&gt; get_partitions_from_path(\"data/2023/01/file.parquet\", [\"year\", \"month\"])\n        [('year', '2023'), ('month', '01')]\n    \"\"\"\n    if \".\" in path:\n        path = os.path.dirname(path)\n\n    parts = path.split(\"/\")\n\n    if isinstance(partitioning, str):\n        if partitioning == \"hive\":\n            return [tuple(p.split(\"=\")) for p in parts if \"=\" in p]\n        else:\n            return [(partitioning, parts[0])]\n    elif isinstance(partitioning, list):\n        return list(zip(partitioning, parts[-len(partitioning) :]))\n    else:\n        return []\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.opt_dtype_pl","title":"fsspeckit.common.opt_dtype_pl","text":"<pre><code>opt_dtype_pl(\n    df: DataFrame,\n    include: str | list[str] | None = None,\n    exclude: str | list[str] | None = None,\n    time_zone: str | None = None,\n    shrink_numerics: bool = False,\n    allow_unsigned: bool = True,\n    allow_null: bool = True,\n    sample_size: int | None = 1024,\n    sample_method: SampleMethod = \"first\",\n    strict: bool = False,\n    *,\n    force_timezone: str | None = None,\n) -&gt; DataFrame\n</code></pre> <p>Optimize data types of a Polars DataFrame for performance and memory efficiency.</p> <p>This function analyzes each column and converts it to the most appropriate data type based on content, handling string-to-type conversions and numeric type downcasting.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The Polars DataFrame to optimize.</p> required <code>include</code> <code>str | list[str] | None</code> <p>Column(s) to include in optimization (default: all columns).</p> <code>None</code> <code>exclude</code> <code>str | list[str] | None</code> <p>Column(s) to exclude from optimization.</p> <code>None</code> <code>time_zone</code> <code>str | None</code> <p>Optional time zone hint during datetime parsing.</p> <code>None</code> <code>shrink_numerics</code> <code>bool</code> <p>Whether to downcast numeric types when possible.</p> <code>False</code> <code>allow_unsigned</code> <code>bool</code> <p>Whether to allow unsigned integer types.</p> <code>True</code> <code>allow_null</code> <code>bool</code> <p>Whether to allow columns with all null values to be cast to Null type.</p> <code>True</code> <code>sample_size</code> <code>int | None</code> <p>Maximum number of cleaned values to inspect for regex-based inference. Use None to inspect the entire column.</p> <code>1024</code> <code>sample_method</code> <code>SampleMethod</code> <p>Which subset to inspect (<code>\"first\"</code> or <code>\"random\"</code>).</p> <code>'first'</code> <code>strict</code> <code>bool</code> <p>If True, will raise an error if any column cannot be optimized.</p> <code>False</code> <code>force_timezone</code> <code>str | None</code> <p>If set, ensure all parsed datetime columns end up with this timezone.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with optimized data types.</p> Source code in <code>src/fsspeckit/common/polars.py</code> <pre><code>def opt_dtype(\n    df: pl.DataFrame,\n    include: str | list[str] | None = None,\n    exclude: str | list[str] | None = None,\n    time_zone: str | None = None,\n    shrink_numerics: bool = False,\n    allow_unsigned: bool = True,\n    allow_null: bool = True,\n    sample_size: int | None = 1024,\n    sample_method: SampleMethod = \"first\",\n    strict: bool = False,\n    *,\n    force_timezone: str | None = None,\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Optimize data types of a Polars DataFrame for performance and memory efficiency.\n\n    This function analyzes each column and converts it to the most appropriate\n    data type based on content, handling string-to-type conversions and\n    numeric type downcasting.\n\n    Args:\n        df: The Polars DataFrame to optimize.\n        include: Column(s) to include in optimization (default: all columns).\n        exclude: Column(s) to exclude from optimization.\n        time_zone: Optional time zone hint during datetime parsing.\n        shrink_numerics: Whether to downcast numeric types when possible.\n        allow_unsigned: Whether to allow unsigned integer types.\n        allow_null: Whether to allow columns with all null values to be cast to Null type.\n        sample_size: Maximum number of cleaned values to inspect for regex-based inference. Use None to inspect the entire column.\n        sample_method: Which subset to inspect (`\"first\"` or `\"random\"`).\n        strict: If True, will raise an error if any column cannot be optimized.\n        force_timezone: If set, ensure all parsed datetime columns end up with this timezone.\n\n    Returns:\n        DataFrame with optimized data types.\n    \"\"\"\n    if sample_method not in (\"first\", \"random\"):\n        raise ValueError(\"sample_method must be 'first' or 'random'\")\n\n    if isinstance(df, pl.LazyFrame):\n        return opt_dtype(\n            df.collect(),\n            include=include,\n            exclude=exclude,\n            time_zone=time_zone,\n            shrink_numerics=shrink_numerics,\n            allow_unsigned=allow_unsigned,\n            allow_null=allow_null,\n            sample_size=sample_size,\n            sample_method=sample_method,\n            strict=strict,\n            force_timezone=force_timezone,\n        ).lazy()\n\n    # Normalize include/exclude parameters\n    if isinstance(include, str):\n        include = [include]\n    if isinstance(exclude, str):\n        exclude = [exclude]\n\n    # Determine columns to process\n    cols_to_process = df.columns\n    if include:\n        cols_to_process = [col for col in include if col in df.columns]\n    if exclude:\n        cols_to_process = [col for col in cols_to_process if col not in exclude]\n\n    # Generate optimization expressions for all columns\n    expressions = []\n    for col_name in cols_to_process:\n        try:\n            expressions.append(\n                _get_column_expr(\n                    df,\n                    col_name,\n                    shrink_numerics,\n                    allow_unsigned,\n                    allow_null,\n                    time_zone,\n                    force_timezone,\n                    sample_size,\n                    sample_method,\n                    strict,\n                )\n            )\n        except Exception as e:\n            if strict:\n                raise e\n            # If strict mode is off, just keep the original column\n            continue\n\n    # Apply all transformations at once if any exist\n    return df if not expressions else df.with_columns(expressions)\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.setup_logging","title":"fsspeckit.common.setup_logging","text":"<pre><code>setup_logging(\n    level: Optional[str] = None,\n    disable: bool = False,\n    format_string: Optional[str] = None,\n) -&gt; None\n</code></pre> <p>Configure the Loguru logger for fsspeckit.</p> <p>Removes the default handler and adds a new one targeting stderr with customizable level and format.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>Optional[str]</code> <p>Log level (DEBUG, INFO, WARNING, ERROR, CRITICAL).    If None, uses fsspeckit_LOG_LEVEL environment variable    or defaults to \"INFO\".</p> <code>None</code> <code>disable</code> <code>bool</code> <p>Whether to disable logging for fsspeckit package.</p> <code>False</code> <code>format_string</code> <code>Optional[str]</code> <p>Custom format string for log messages.           If None, uses a default comprehensive format.</p> <code>None</code> Example Source code in <code>src/fsspeckit/common/logging.py</code> <pre><code>def setup_logging(\n    level: Optional[str] = None,\n    disable: bool = False,\n    format_string: Optional[str] = None,\n) -&gt; None:\n    \"\"\"Configure the Loguru logger for fsspeckit.\n\n    Removes the default handler and adds a new one targeting stderr\n    with customizable level and format.\n\n    Args:\n        level: Log level (DEBUG, INFO, WARNING, ERROR, CRITICAL).\n               If None, uses fsspeckit_LOG_LEVEL environment variable\n               or defaults to \"INFO\".\n        disable: Whether to disable logging for fsspeckit package.\n        format_string: Custom format string for log messages.\n                      If None, uses a default comprehensive format.\n\n    Example:\n        &gt;&gt;&gt; # Basic setup\n        &gt;&gt;&gt; setup_logging()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Custom level and format\n        &gt;&gt;&gt; setup_logging(level=\"DEBUG\", format_string=\"{time} | {level} | {message}\")\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Disable logging\n        &gt;&gt;&gt; setup_logging(disable=True)\n    \"\"\"\n    # Determine log level\n    if level is None:\n        level = os.getenv(\"fsspeckit_LOG_LEVEL\", \"INFO\")\n\n    # Default format if none provided\n    if format_string is None:\n        format_string = (\n            \"&lt;green&gt;{time:YYYY-MM-DD HH:mm:ss.SSS}&lt;/green&gt; | \"\n            \"&lt;level&gt;{level: &lt;8}&lt;/level&gt; | \"\n            \"&lt;cyan&gt;{name}&lt;/cyan&gt;:&lt;cyan&gt;{function}&lt;/cyan&gt;:&lt;cyan&gt;{line}&lt;/cyan&gt; - \"\n            \"&lt;level&gt;{message}&lt;/level&gt;\"\n        )\n\n    # Remove the default handler added by Loguru\n    logger.remove()\n\n    # Add new handler with custom configuration\n    logger.add(\n        sys.stderr,\n        level=level.upper(),\n        format=format_string,\n    )\n\n    # Optionally disable logging for this package\n    if disable:\n        logger.disable(\"fsspeckit\")\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.setup_logging--basic-setup","title":"Basic setup","text":"<p>setup_logging()</p>"},{"location":"api/fsspeckit.common/#fsspeckit.common.setup_logging--custom-level-and-format","title":"Custom level and format","text":"<p>setup_logging(level=\"DEBUG\", format_string=\"{time} | {level} | {message}\")</p>"},{"location":"api/fsspeckit.common/#fsspeckit.common.setup_logging--disable-logging","title":"Disable logging","text":"<p>setup_logging(disable=True)</p>"},{"location":"api/fsspeckit.common/#fsspeckit.common.sync_dir","title":"fsspeckit.common.sync_dir","text":"<pre><code>sync_dir(\n    src_fs: AbstractFileSystem,\n    dst_fs: AbstractFileSystem,\n    src_path: str = \"\",\n    dst_path: str = \"\",\n    server_side: bool = True,\n    chunk_size: int = 8 * 1024 * 1024,\n    parallel: bool = False,\n    n_jobs: int = -1,\n    verbose: bool = True,\n) -&gt; dict[str, list[str]]\n</code></pre> <p>Sync two directories between different filesystems.</p> <p>Compares files in the source and destination directories, copies new or updated files from source to destination, and deletes stale files from destination.</p> <p>Parameters:</p> Name Type Description Default <code>src_fs</code> <code>AbstractFileSystem</code> <p>Source filesystem (fsspec AbstractFileSystem)</p> required <code>dst_fs</code> <code>AbstractFileSystem</code> <p>Destination filesystem (fsspec AbstractFileSystem)</p> required <code>src_path</code> <code>str</code> <p>Path in source filesystem to sync. Default is root ('').</p> <code>''</code> <code>dst_path</code> <code>str</code> <p>Path in destination filesystem to sync. Default is root ('').</p> <code>''</code> <code>chunk_size</code> <code>int</code> <p>Size of chunks to read/write files (in bytes). Default is 8MB.</p> <code>8 * 1024 * 1024</code> <code>parallel</code> <code>bool</code> <p>Whether to perform copy/delete operations in parallel. Default is False.</p> <code>False</code> <code>n_jobs</code> <code>int</code> <p>Number of parallel jobs if parallel=True. Default is -1 (all cores).</p> <code>-1</code> <code>verbose</code> <code>bool</code> <p>Whether to show progress bars. Default is True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, list[str]]</code> <p>Summary of added and deleted files</p> Source code in <code>src/fsspeckit/common/misc.py</code> <pre><code>def sync_dir(\n    src_fs: AbstractFileSystem,\n    dst_fs: AbstractFileSystem,\n    src_path: str = \"\",\n    dst_path: str = \"\",\n    server_side: bool = True,\n    chunk_size: int = 8 * 1024 * 1024,\n    parallel: bool = False,\n    n_jobs: int = -1,\n    verbose: bool = True,\n) -&gt; dict[str, list[str]]:\n    \"\"\"Sync two directories between different filesystems.\n\n    Compares files in the source and destination directories, copies new or updated files from source to destination,\n    and deletes stale files from destination.\n\n    Args:\n        src_fs: Source filesystem (fsspec AbstractFileSystem)\n        dst_fs: Destination filesystem (fsspec AbstractFileSystem)\n        src_path: Path in source filesystem to sync. Default is root ('').\n        dst_path: Path in destination filesystem to sync. Default is root ('').\n        chunk_size: Size of chunks to read/write files (in bytes). Default is 8MB.\n        parallel: Whether to perform copy/delete operations in parallel. Default is False.\n        n_jobs: Number of parallel jobs if parallel=True. Default is -1 (all cores).\n        verbose: Whether to show progress bars. Default is True.\n\n    Returns:\n        dict: Summary of added and deleted files\n    \"\"\"\n\n    src_mapper = src_fs.get_mapper(src_path)\n    dst_mapper = dst_fs.get_mapper(dst_path)\n\n    add_files = sorted(src_mapper.keys() - dst_mapper.keys())\n    delete_files = sorted(dst_mapper.keys() - src_mapper.keys())\n\n    return sync_files(\n        add_files=add_files,\n        delete_files=delete_files,\n        src_fs=src_fs,\n        dst_fs=dst_fs,\n        src_path=src_path,\n        dst_path=dst_path,\n        chunk_size=chunk_size,\n        server_side=server_side,\n        parallel=parallel,\n        n_jobs=n_jobs,\n        verbose=verbose,\n    )\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.sync_files","title":"fsspeckit.common.sync_files","text":"<pre><code>sync_files(\n    add_files: list[str],\n    delete_files: list[str],\n    src_fs: AbstractFileSystem,\n    dst_fs: AbstractFileSystem,\n    src_path: str = \"\",\n    dst_path: str = \"\",\n    server_side: bool = False,\n    chunk_size: int = 8 * 1024 * 1024,\n    parallel: bool = False,\n    n_jobs: int = -1,\n    verbose: bool = True,\n) -&gt; dict[str, list[str]]\n</code></pre> <p>Sync files between two filesystems by copying new files and deleting old ones.</p> <p>Parameters:</p> Name Type Description Default <code>add_files</code> <code>list[str]</code> <p>List of file paths to add (copy from source to destination)</p> required <code>delete_files</code> <code>list[str]</code> <p>List of file paths to delete from destination</p> required <code>src_fs</code> <code>AbstractFileSystem</code> <p>Source filesystem (fsspec AbstractFileSystem)</p> required <code>dst_fs</code> <code>AbstractFileSystem</code> <p>Destination filesystem (fsspec AbstractFileSystem)</p> required <code>src_path</code> <code>str</code> <p>Base path in source filesystem. Default is root ('').</p> <code>''</code> <code>dst_path</code> <code>str</code> <p>Base path in destination filesystem. Default is root ('').</p> <code>''</code> <code>server_side</code> <code>bool</code> <p>Whether to use server-side copy if supported. Default is False.</p> <code>False</code> <code>chunk_size</code> <code>int</code> <p>Size of chunks to read/write files (in bytes). Default is 8MB.</p> <code>8 * 1024 * 1024</code> <code>parallel</code> <code>bool</code> <p>Whether to perform copy/delete operations in parallel. Default is False.</p> <code>False</code> <code>n_jobs</code> <code>int</code> <p>Number of parallel jobs if parallel=True. Default is -1 (all cores).</p> <code>-1</code> <code>verbose</code> <code>bool</code> <p>Whether to show progress bars. Default is True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, list[str]]</code> <p>Summary of added and deleted files</p> Source code in <code>src/fsspeckit/common/misc.py</code> <pre><code>def sync_files(\n    add_files: list[str],\n    delete_files: list[str],\n    src_fs: AbstractFileSystem,\n    dst_fs: AbstractFileSystem,\n    src_path: str = \"\",\n    dst_path: str = \"\",\n    server_side: bool = False,\n    chunk_size: int = 8 * 1024 * 1024,\n    parallel: bool = False,\n    n_jobs: int = -1,\n    verbose: bool = True,\n) -&gt; dict[str, list[str]]:\n    \"\"\"Sync files between two filesystems by copying new files and deleting old ones.\n\n    Args:\n        add_files: List of file paths to add (copy from source to destination)\n        delete_files: List of file paths to delete from destination\n        src_fs: Source filesystem (fsspec AbstractFileSystem)\n        dst_fs: Destination filesystem (fsspec AbstractFileSystem)\n        src_path: Base path in source filesystem. Default is root ('').\n        dst_path: Base path in destination filesystem. Default is root ('').\n        server_side: Whether to use server-side copy if supported. Default is False.\n        chunk_size: Size of chunks to read/write files (in bytes). Default is 8MB.\n        parallel: Whether to perform copy/delete operations in parallel. Default is False.\n        n_jobs: Number of parallel jobs if parallel=True. Default is -1 (all cores).\n        verbose: Whether to show progress bars. Default is True.\n\n    Returns:\n        dict: Summary of added and deleted files\n    \"\"\"\n    CHUNK = chunk_size\n    RETRIES = 3\n\n    server_side = check_fs_identical(src_fs, dst_fs) and server_side\n\n    src_mapper = src_fs.get_mapper(src_path)\n    dst_mapper = dst_fs.get_mapper(dst_path)\n\n    def server_side_copy_file(key, src_mapper, dst_mapper, RETRIES):\n        last_exc = None\n        for attempt in range(1, RETRIES + 1):\n            try:\n                dst_mapper[key] = src_mapper[key]\n                break\n            except Exception as e:\n                last_exc = e\n\n    def copy_file(key, src_fs, dst_fs, src_path, dst_path, CHUNK, RETRIES):\n        last_exc = None\n        for attempt in range(1, RETRIES + 1):\n            try:\n                with (\n                    src_fs.open(posixpath.join(src_path, key), \"rb\") as r,\n                    dst_fs.open(posixpath.join(dst_path, key), \"wb\") as w,\n                ):\n                    while True:\n                        chunk = r.read(CHUNK)\n                        if not chunk:\n                            break\n                        w.write(chunk)\n                break\n            except Exception as e:\n                last_exc = e\n\n    def delete_file(key, dst_fs, dst_path, RETRIES):\n        last_exc = None\n        for attempt in range(1, RETRIES + 1):\n            try:\n                dst_fs.rm(posixpath.join(dst_path, key))\n                break\n            except Exception as e:\n                last_exc = e\n\n    if len(add_files):\n        # Copy new files\n        if parallel:\n            if server_side:\n                try:\n                    run_parallel(\n                        server_side_copy_file,\n                        add_files,\n                        src_mapper=src_mapper,\n                        dst_mapper=dst_mapper,\n                        RETRIES=RETRIES,\n                        n_jobs=n_jobs,\n                        verbose=verbose,\n                    )\n                except Exception:\n                    # Fallback to client-side copy if server-side fails\n                    run_parallel(\n                        copy_file,\n                        add_files,\n                        src_fs=src_fs,\n                        dst_fs=dst_fs,\n                        src_path=src_path,\n                        dst_path=dst_path,\n                        CHUNK=CHUNK,\n                        RETRIES=RETRIES,\n                        n_jobs=n_jobs,\n                        verbose=verbose,\n                    )\n            else:\n                run_parallel(\n                    copy_file,\n                    add_files,\n                    src_fs=src_fs,\n                    dst_fs=dst_fs,\n                    src_path=src_path,\n                    dst_path=dst_path,\n                    CHUNK=CHUNK,\n                    RETRIES=RETRIES,\n                    n_jobs=n_jobs,\n                    verbose=verbose,\n                )\n        else:\n            if verbose:\n                for key in track(\n                    add_files,\n                    description=\"Copying new files...\",\n                    total=len(add_files),\n                ):\n                    if server_side:\n                        try:\n                            server_side_copy_file(key, src_mapper, dst_mapper, RETRIES)\n                        except Exception:\n                            copy_file(\n                                key, src_fs, dst_fs, src_path, dst_path, CHUNK, RETRIES\n                            )\n                    else:\n                        copy_file(\n                            key, src_fs, dst_fs, src_path, dst_path, CHUNK, RETRIES\n                        )\n            else:\n                for key in add_files:\n                    if server_side:\n                        try:\n                            server_side_copy_file(key, src_mapper, dst_mapper, RETRIES)\n                        except Exception:\n                            copy_file(\n                                key, src_fs, dst_fs, src_path, dst_path, CHUNK, RETRIES\n                            )\n                    else:\n                        copy_file(\n                            key, src_fs, dst_fs, src_path, dst_path, CHUNK, RETRIES\n                        )\n\n    if len(delete_files):\n        # Delete old files from destination\n        if parallel:\n            run_parallel(\n                delete_file,\n                delete_files,\n                dst_fs=dst_fs,\n                dst_path=dst_path,\n                RETRIES=RETRIES,\n                n_jobs=n_jobs,\n                verbose=verbose,\n            )\n        else:\n            if verbose:\n                for key in track(\n                    delete_files,\n                    description=\"Deleting stale files...\",\n                    total=len(delete_files),\n                ):\n                    delete_file(key, dst_fs, dst_path, RETRIES)\n            else:\n                for key in delete_files:\n                    delete_file(key, dst_fs, dst_path, RETRIES)\n\n    return {\"added_files\": add_files, \"deleted_files\": delete_files}\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.timestamp_from_string","title":"fsspeckit.common.timestamp_from_string  <code>cached</code>","text":"<pre><code>timestamp_from_string(\n    timestamp_str: str,\n    tz: str | None = None,\n    naive: bool = False,\n) -&gt; datetime | date | time\n</code></pre> <p>Converts a timestamp string (ISO 8601 format) into a datetime, date, or time object using only standard Python libraries.</p> <p>Handles strings with or without timezone information (e.g., '2023-01-01T10:00:00+02:00', '2023-01-01', '10:00:00'). Supports timezone offsets like '+HH:MM' or '+HHMM'. For named timezones (e.g., 'Europe/Paris'), requires Python 3.9+ and the 'tzdata' package to be installed.</p> <p>Parameters:</p> Name Type Description Default <code>timestamp_str</code> <code>str</code> <p>The string representation of the timestamp (ISO 8601 format).</p> required <code>tz</code> <code>str</code> <p>Target timezone identifier (e.g., 'UTC', '+02:00', 'Europe/Paris'). If provided, the output datetime/time will be localized or converted to this timezone. Defaults to None.</p> <code>None</code> <code>naive</code> <code>bool</code> <p>If True, return a naive datetime/time (no timezone info), even if the input string or <code>tz</code> parameter specifies one. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>datetime | date | time</code> <p>Union[dt.datetime, dt.date, dt.time]: The parsed datetime, date, or time object.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the timestamp string format is invalid or the timezone is         invalid/unsupported.</p> Source code in <code>src/fsspeckit/common/datetime.py</code> <pre><code>@lru_cache(maxsize=128)\ndef timestamp_from_string(\n    timestamp_str: str,\n    tz: str | None = None,\n    naive: bool = False,\n) -&gt; dt.datetime | dt.date | dt.time:\n    \"\"\"\n    Converts a timestamp string (ISO 8601 format) into a datetime, date, or time object\n    using only standard Python libraries.\n\n    Handles strings with or without timezone information (e.g., '2023-01-01T10:00:00+02:00',\n    '2023-01-01', '10:00:00'). Supports timezone offsets like '+HH:MM' or '+HHMM'.\n    For named timezones (e.g., 'Europe/Paris'), requires Python 3.9+ and the 'tzdata'\n    package to be installed.\n\n    Args:\n        timestamp_str (str): The string representation of the timestamp (ISO 8601 format).\n        tz (str, optional): Target timezone identifier (e.g., 'UTC', '+02:00', 'Europe/Paris').\n            If provided, the output datetime/time will be localized or converted to this timezone.\n            Defaults to None.\n        naive (bool, optional): If True, return a naive datetime/time (no timezone info),\n            even if the input string or `tz` parameter specifies one. Defaults to False.\n\n    Returns:\n        Union[dt.datetime, dt.date, dt.time]: The parsed datetime, date, or time object.\n\n    Raises:\n        ValueError: If the timestamp string format is invalid or the timezone is\n                    invalid/unsupported.\n    \"\"\"\n\n    # Regex to parse timezone offsets like +HH:MM or +HHMM\n    _TZ_OFFSET_REGEX = re.compile(r\"([+-])(\\d{2}):?(\\d{2})\")\n\n    def _parse_tz_offset(tz_str: str) -&gt; dt.tzinfo | None:\n        \"\"\"Parses a timezone offset string into a timezone object.\"\"\"\n        match = _TZ_OFFSET_REGEX.fullmatch(tz_str)\n        if match:\n            sign, hours, minutes = match.groups()\n            offset_seconds = (int(hours) * 3600 + int(minutes) * 60) * (\n                -1 if sign == \"-\" else 1\n            )\n            if abs(offset_seconds) &gt;= 24 * 3600:\n                raise ValueError(f\"Invalid timezone offset: {tz_str}\")\n            return dt.timezone(dt.timedelta(seconds=offset_seconds), name=tz_str)\n        return None\n\n    def _get_tzinfo(tz_identifier: str | None) -&gt; dt.tzinfo | None:\n        \"\"\"Gets a tzinfo object from a string (offset or IANA name).\"\"\"\n        if tz_identifier is None:\n            return None\n        if tz_identifier.upper() == \"UTC\":\n            return dt.timezone.utc\n\n        # Try parsing as offset first\n        offset_tz = _parse_tz_offset(tz_identifier)\n        if offset_tz:\n            return offset_tz\n\n        # Try parsing as IANA name using zoneinfo (if available)\n        if ZoneInfo:\n            try:\n                return ZoneInfo(tz_identifier)\n            except ZoneInfoNotFoundError:\n                raise ValueError(\n                    f\"Timezone '{tz_identifier}' not found. Install 'tzdata' or use offset format.\"\n                )\n            except Exception as e:  # Catch other potential zoneinfo errors\n                raise ValueError(f\"Error loading timezone '{tz_identifier}': {e}\")\n        else:\n            # zoneinfo not available\n            raise ValueError(\n                f\"Invalid timezone: '{tz_identifier}'. Use offset format (e.g., '+02:00') \"\n                \"or run Python 3.9+ with 'tzdata' installed for named timezones.\"\n            )\n\n    target_tz: dt.tzinfo | None = _get_tzinfo(tz)\n    parsed_obj: dt.datetime | dt.date | dt.time | None = None\n\n    # Preprocess: Replace space separator, strip whitespace\n    processed_str = timestamp_str.strip().replace(\" \", \"T\")\n\n    # Attempt parsing (datetime, date, time) using fromisoformat\n    try:\n        # Python &lt; 3.11 fromisoformat has limitations (e.g., no Z, no +HHMM offset)\n        # This implementation assumes Python 3.11+ for full ISO 8601 support via fromisoformat\n        # or that input strings use formats compatible with older versions (e.g., +HH:MM)\n        parsed_obj = dt.datetime.fromisoformat(processed_str)\n    except ValueError:\n        try:\n            parsed_obj = dt.date.fromisoformat(processed_str)\n        except ValueError:\n            try:\n                # Time parsing needs care, especially with offsets in older Python\n                parsed_obj = dt.time.fromisoformat(processed_str)\n            except ValueError:\n                # Add fallback for simple HH:MM:SS if needed, though less robust\n                # try:\n                #     parsed_obj = dt.datetime.strptime(processed_str, \"%H:%M:%S\").time()\n                # except ValueError:\n                raise ValueError(f\"Invalid timestamp format: '{timestamp_str}'\")\n\n    # Apply timezone logic if we have a datetime or time object\n    if isinstance(parsed_obj, (dt.datetime, dt.time)):\n        is_aware = (\n            parsed_obj.tzinfo is not None\n            and parsed_obj.tzinfo.utcoffset(\n                parsed_obj if isinstance(parsed_obj, dt.datetime) else None\n            )\n            is not None\n        )\n\n        if target_tz:\n            if is_aware:\n                # Convert existing aware object to target timezone (only for datetime)\n                if isinstance(parsed_obj, dt.datetime):\n                    parsed_obj = parsed_obj.astimezone(target_tz)\n                # else: dt.time cannot be converted without a date context. Keep original tz.\n            else:\n                # Localize naive object to target timezone\n                parsed_obj = parsed_obj.replace(tzinfo=target_tz)\n            is_aware = True  # Object is now considered aware\n\n        # Handle naive flag: remove tzinfo if requested\n        if naive and is_aware:\n            parsed_obj = parsed_obj.replace(tzinfo=None)\n\n    # If it's a date object, tz/naive flags are ignored\n    elif isinstance(parsed_obj, dt.date):\n        pass\n\n    return parsed_obj\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.to_pyarrow_table","title":"fsspeckit.common.to_pyarrow_table","text":"<pre><code>to_pyarrow_table(\n    data: Union[\n        DataFrame,\n        LazyFrame,\n        DataFrame,\n        dict,\n        list[Union[DataFrame, LazyFrame, DataFrame, dict]],\n    ],\n    concat: bool = False,\n    unique: Union[bool, list[str], str] = False,\n) -&gt; Table\n</code></pre> <p>Convert various data formats to PyArrow Table.</p> <p>Handles conversion from Polars DataFrames, Pandas DataFrames, dictionaries, and lists of these types to PyArrow Tables.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[DataFrame, LazyFrame, DataFrame, dict, list[Union[DataFrame, LazyFrame, DataFrame, dict]]]</code> <p>Input data to convert.</p> required <code>concat</code> <code>bool</code> <p>Whether to concatenate multiple inputs into single table.</p> <code>False</code> <code>unique</code> <code>Union[bool, list[str], str]</code> <p>Whether to remove duplicates. Can specify columns.</p> <code>False</code> <p>Returns:</p> Type Description <code>Table</code> <p>PyArrow Table containing the converted data.</p> Example <p>df = pl.DataFrame({\"a\": [1, 2], \"b\": [3, 4]}) table = to_pyarrow_table(df) print(table.schema) a: int64 b: int64</p> Source code in <code>src/fsspeckit/common/types.py</code> <pre><code>def to_pyarrow_table(\n    data: Union[\n        \"pl.DataFrame\",\n        \"pl.LazyFrame\",\n        \"pd.DataFrame\",\n        dict,\n        list[Union[\"pl.DataFrame\", \"pl.LazyFrame\", \"pd.DataFrame\", dict]],\n    ],\n    concat: bool = False,\n    unique: Union[bool, list[str], str] = False,\n) -&gt; \"pa.Table\":\n    \"\"\"Convert various data formats to PyArrow Table.\n\n    Handles conversion from Polars DataFrames, Pandas DataFrames,\n    dictionaries, and lists of these types to PyArrow Tables.\n\n    Args:\n        data: Input data to convert.\n        concat: Whether to concatenate multiple inputs into single table.\n        unique: Whether to remove duplicates. Can specify columns.\n\n    Returns:\n        PyArrow Table containing the converted data.\n\n    Example:\n        &gt;&gt;&gt; df = pl.DataFrame({\"a\": [1, 2], \"b\": [3, 4]})\n        &gt;&gt;&gt; table = to_pyarrow_table(df)\n        &gt;&gt;&gt; print(table.schema)\n        a: int64\n        b: int64\n    \"\"\"\n    # Convert dict to DataFrame first\n    if isinstance(data, dict):\n        data = dict_to_dataframe(data)\n    if isinstance(data, list):\n        if isinstance(data[0], dict):\n            data = dict_to_dataframe(data, unique=unique)\n\n    # Ensure data is a list for uniform processing\n    if not isinstance(data, list):\n        data = [data]\n\n    # Collect lazy frames\n    if isinstance(data[0], pl.LazyFrame):\n        data = [dd.collect() for dd in data]\n\n    # Convert based on the first item's type\n    if isinstance(data[0], pl.DataFrame):\n        if concat:\n            data = pl.concat(data, how=\"diagonal_relaxed\")\n            if unique:\n                data = data.unique(\n                    subset=None if not isinstance(unique, (str, list)) else unique,\n                    maintain_order=True,\n                )\n            data = data.to_arrow()\n            data = data.cast(convert_large_types_to_normal(data.schema))\n        else:\n            data = [dd.to_arrow() for dd in data]\n            data = [dd.cast(convert_large_types_to_normal(dd.schema)) for dd in data]\n\n    elif isinstance(data[0], pd.DataFrame):\n        data = [pa.Table.from_pandas(dd, preserve_index=False) for dd in data]\n        if concat:\n            data = pa.concat_tables(data, promote_options=\"permissive\")\n            if unique:\n                data = (\n                    pl.from_arrow(data)\n                    .unique(\n                        subset=None if not isinstance(unique, (str, list)) else unique,\n                        maintain_order=True,\n                    )\n                    .to_arrow()\n                )\n                data = data.cast(convert_large_types_to_normal(data.schema))\n\n    elif isinstance(data[0], (pa.RecordBatch, Generator)):\n        if concat:\n            data = pa.Table.from_batches(data)\n            if unique:\n                data = (\n                    pl.from_arrow(data)\n                    .unique(\n                        subset=None if not isinstance(unique, (str, list)) else unique,\n                        maintain_order=True,\n                    )\n                    .to_arrow()\n                )\n                data = data.cast(convert_large_types_to_normal(data.schema))\n        else:\n            data = [pa.Table.from_batches([dd]) for dd in data]\n\n    return data\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common-modules","title":"Modules","text":""},{"location":"api/fsspeckit.common/#fsspeckit.common.datetime","title":"fsspeckit.common.datetime","text":""},{"location":"api/fsspeckit.common/#fsspeckit.common.datetime-functions","title":"Functions","text":""},{"location":"api/fsspeckit.common/#fsspeckit.common.datetime.timestamp_from_string","title":"fsspeckit.common.datetime.timestamp_from_string  <code>cached</code>","text":"<pre><code>timestamp_from_string(\n    timestamp_str: str,\n    tz: str | None = None,\n    naive: bool = False,\n) -&gt; datetime | date | time\n</code></pre> <p>Converts a timestamp string (ISO 8601 format) into a datetime, date, or time object using only standard Python libraries.</p> <p>Handles strings with or without timezone information (e.g., '2023-01-01T10:00:00+02:00', '2023-01-01', '10:00:00'). Supports timezone offsets like '+HH:MM' or '+HHMM'. For named timezones (e.g., 'Europe/Paris'), requires Python 3.9+ and the 'tzdata' package to be installed.</p> <p>Parameters:</p> Name Type Description Default <code>timestamp_str</code> <code>str</code> <p>The string representation of the timestamp (ISO 8601 format).</p> required <code>tz</code> <code>str</code> <p>Target timezone identifier (e.g., 'UTC', '+02:00', 'Europe/Paris'). If provided, the output datetime/time will be localized or converted to this timezone. Defaults to None.</p> <code>None</code> <code>naive</code> <code>bool</code> <p>If True, return a naive datetime/time (no timezone info), even if the input string or <code>tz</code> parameter specifies one. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>datetime | date | time</code> <p>Union[dt.datetime, dt.date, dt.time]: The parsed datetime, date, or time object.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the timestamp string format is invalid or the timezone is         invalid/unsupported.</p> Source code in <code>src/fsspeckit/common/datetime.py</code> <pre><code>@lru_cache(maxsize=128)\ndef timestamp_from_string(\n    timestamp_str: str,\n    tz: str | None = None,\n    naive: bool = False,\n) -&gt; dt.datetime | dt.date | dt.time:\n    \"\"\"\n    Converts a timestamp string (ISO 8601 format) into a datetime, date, or time object\n    using only standard Python libraries.\n\n    Handles strings with or without timezone information (e.g., '2023-01-01T10:00:00+02:00',\n    '2023-01-01', '10:00:00'). Supports timezone offsets like '+HH:MM' or '+HHMM'.\n    For named timezones (e.g., 'Europe/Paris'), requires Python 3.9+ and the 'tzdata'\n    package to be installed.\n\n    Args:\n        timestamp_str (str): The string representation of the timestamp (ISO 8601 format).\n        tz (str, optional): Target timezone identifier (e.g., 'UTC', '+02:00', 'Europe/Paris').\n            If provided, the output datetime/time will be localized or converted to this timezone.\n            Defaults to None.\n        naive (bool, optional): If True, return a naive datetime/time (no timezone info),\n            even if the input string or `tz` parameter specifies one. Defaults to False.\n\n    Returns:\n        Union[dt.datetime, dt.date, dt.time]: The parsed datetime, date, or time object.\n\n    Raises:\n        ValueError: If the timestamp string format is invalid or the timezone is\n                    invalid/unsupported.\n    \"\"\"\n\n    # Regex to parse timezone offsets like +HH:MM or +HHMM\n    _TZ_OFFSET_REGEX = re.compile(r\"([+-])(\\d{2}):?(\\d{2})\")\n\n    def _parse_tz_offset(tz_str: str) -&gt; dt.tzinfo | None:\n        \"\"\"Parses a timezone offset string into a timezone object.\"\"\"\n        match = _TZ_OFFSET_REGEX.fullmatch(tz_str)\n        if match:\n            sign, hours, minutes = match.groups()\n            offset_seconds = (int(hours) * 3600 + int(minutes) * 60) * (\n                -1 if sign == \"-\" else 1\n            )\n            if abs(offset_seconds) &gt;= 24 * 3600:\n                raise ValueError(f\"Invalid timezone offset: {tz_str}\")\n            return dt.timezone(dt.timedelta(seconds=offset_seconds), name=tz_str)\n        return None\n\n    def _get_tzinfo(tz_identifier: str | None) -&gt; dt.tzinfo | None:\n        \"\"\"Gets a tzinfo object from a string (offset or IANA name).\"\"\"\n        if tz_identifier is None:\n            return None\n        if tz_identifier.upper() == \"UTC\":\n            return dt.timezone.utc\n\n        # Try parsing as offset first\n        offset_tz = _parse_tz_offset(tz_identifier)\n        if offset_tz:\n            return offset_tz\n\n        # Try parsing as IANA name using zoneinfo (if available)\n        if ZoneInfo:\n            try:\n                return ZoneInfo(tz_identifier)\n            except ZoneInfoNotFoundError:\n                raise ValueError(\n                    f\"Timezone '{tz_identifier}' not found. Install 'tzdata' or use offset format.\"\n                )\n            except Exception as e:  # Catch other potential zoneinfo errors\n                raise ValueError(f\"Error loading timezone '{tz_identifier}': {e}\")\n        else:\n            # zoneinfo not available\n            raise ValueError(\n                f\"Invalid timezone: '{tz_identifier}'. Use offset format (e.g., '+02:00') \"\n                \"or run Python 3.9+ with 'tzdata' installed for named timezones.\"\n            )\n\n    target_tz: dt.tzinfo | None = _get_tzinfo(tz)\n    parsed_obj: dt.datetime | dt.date | dt.time | None = None\n\n    # Preprocess: Replace space separator, strip whitespace\n    processed_str = timestamp_str.strip().replace(\" \", \"T\")\n\n    # Attempt parsing (datetime, date, time) using fromisoformat\n    try:\n        # Python &lt; 3.11 fromisoformat has limitations (e.g., no Z, no +HHMM offset)\n        # This implementation assumes Python 3.11+ for full ISO 8601 support via fromisoformat\n        # or that input strings use formats compatible with older versions (e.g., +HH:MM)\n        parsed_obj = dt.datetime.fromisoformat(processed_str)\n    except ValueError:\n        try:\n            parsed_obj = dt.date.fromisoformat(processed_str)\n        except ValueError:\n            try:\n                # Time parsing needs care, especially with offsets in older Python\n                parsed_obj = dt.time.fromisoformat(processed_str)\n            except ValueError:\n                # Add fallback for simple HH:MM:SS if needed, though less robust\n                # try:\n                #     parsed_obj = dt.datetime.strptime(processed_str, \"%H:%M:%S\").time()\n                # except ValueError:\n                raise ValueError(f\"Invalid timestamp format: '{timestamp_str}'\")\n\n    # Apply timezone logic if we have a datetime or time object\n    if isinstance(parsed_obj, (dt.datetime, dt.time)):\n        is_aware = (\n            parsed_obj.tzinfo is not None\n            and parsed_obj.tzinfo.utcoffset(\n                parsed_obj if isinstance(parsed_obj, dt.datetime) else None\n            )\n            is not None\n        )\n\n        if target_tz:\n            if is_aware:\n                # Convert existing aware object to target timezone (only for datetime)\n                if isinstance(parsed_obj, dt.datetime):\n                    parsed_obj = parsed_obj.astimezone(target_tz)\n                # else: dt.time cannot be converted without a date context. Keep original tz.\n            else:\n                # Localize naive object to target timezone\n                parsed_obj = parsed_obj.replace(tzinfo=target_tz)\n            is_aware = True  # Object is now considered aware\n\n        # Handle naive flag: remove tzinfo if requested\n        if naive and is_aware:\n            parsed_obj = parsed_obj.replace(tzinfo=None)\n\n    # If it's a date object, tz/naive flags are ignored\n    elif isinstance(parsed_obj, dt.date):\n        pass\n\n    return parsed_obj\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.logging","title":"fsspeckit.common.logging","text":"<p>Logging configuration utilities for fsspeckit.</p>"},{"location":"api/fsspeckit.common/#fsspeckit.common.logging-functions","title":"Functions","text":""},{"location":"api/fsspeckit.common/#fsspeckit.common.logging.get_logger","title":"fsspeckit.common.logging.get_logger","text":"<pre><code>get_logger(name: str = 'fsspeckit')\n</code></pre> <p>Get a logger instance for the given name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Logger name, typically the module name.</p> <code>'fsspeckit'</code> <p>Returns:</p> Type Description <p>Configured logger instance.</p> Example <p>logger = get_logger(name) logger.info(\"This is a log message\")</p> Source code in <code>src/fsspeckit/common/logging.py</code> <pre><code>def get_logger(name: str = \"fsspeckit\"):\n    \"\"\"Get a logger instance for the given name.\n\n    Args:\n        name: Logger name, typically the module name.\n\n    Returns:\n        Configured logger instance.\n\n    Example:\n        &gt;&gt;&gt; logger = get_logger(__name__)\n        &gt;&gt;&gt; logger.info(\"This is a log message\")\n    \"\"\"\n    return logger.bind(name=name)\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.logging.setup_logging","title":"fsspeckit.common.logging.setup_logging","text":"<pre><code>setup_logging(\n    level: Optional[str] = None,\n    disable: bool = False,\n    format_string: Optional[str] = None,\n) -&gt; None\n</code></pre> <p>Configure the Loguru logger for fsspeckit.</p> <p>Removes the default handler and adds a new one targeting stderr with customizable level and format.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>Optional[str]</code> <p>Log level (DEBUG, INFO, WARNING, ERROR, CRITICAL).    If None, uses fsspeckit_LOG_LEVEL environment variable    or defaults to \"INFO\".</p> <code>None</code> <code>disable</code> <code>bool</code> <p>Whether to disable logging for fsspeckit package.</p> <code>False</code> <code>format_string</code> <code>Optional[str]</code> <p>Custom format string for log messages.           If None, uses a default comprehensive format.</p> <code>None</code> Example Source code in <code>src/fsspeckit/common/logging.py</code> <pre><code>def setup_logging(\n    level: Optional[str] = None,\n    disable: bool = False,\n    format_string: Optional[str] = None,\n) -&gt; None:\n    \"\"\"Configure the Loguru logger for fsspeckit.\n\n    Removes the default handler and adds a new one targeting stderr\n    with customizable level and format.\n\n    Args:\n        level: Log level (DEBUG, INFO, WARNING, ERROR, CRITICAL).\n               If None, uses fsspeckit_LOG_LEVEL environment variable\n               or defaults to \"INFO\".\n        disable: Whether to disable logging for fsspeckit package.\n        format_string: Custom format string for log messages.\n                      If None, uses a default comprehensive format.\n\n    Example:\n        &gt;&gt;&gt; # Basic setup\n        &gt;&gt;&gt; setup_logging()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Custom level and format\n        &gt;&gt;&gt; setup_logging(level=\"DEBUG\", format_string=\"{time} | {level} | {message}\")\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Disable logging\n        &gt;&gt;&gt; setup_logging(disable=True)\n    \"\"\"\n    # Determine log level\n    if level is None:\n        level = os.getenv(\"fsspeckit_LOG_LEVEL\", \"INFO\")\n\n    # Default format if none provided\n    if format_string is None:\n        format_string = (\n            \"&lt;green&gt;{time:YYYY-MM-DD HH:mm:ss.SSS}&lt;/green&gt; | \"\n            \"&lt;level&gt;{level: &lt;8}&lt;/level&gt; | \"\n            \"&lt;cyan&gt;{name}&lt;/cyan&gt;:&lt;cyan&gt;{function}&lt;/cyan&gt;:&lt;cyan&gt;{line}&lt;/cyan&gt; - \"\n            \"&lt;level&gt;{message}&lt;/level&gt;\"\n        )\n\n    # Remove the default handler added by Loguru\n    logger.remove()\n\n    # Add new handler with custom configuration\n    logger.add(\n        sys.stderr,\n        level=level.upper(),\n        format=format_string,\n    )\n\n    # Optionally disable logging for this package\n    if disable:\n        logger.disable(\"fsspeckit\")\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.logging.setup_logging--basic-setup","title":"Basic setup","text":"<p>setup_logging()</p>"},{"location":"api/fsspeckit.common/#fsspeckit.common.logging.setup_logging--custom-level-and-format","title":"Custom level and format","text":"<p>setup_logging(level=\"DEBUG\", format_string=\"{time} | {level} | {message}\")</p>"},{"location":"api/fsspeckit.common/#fsspeckit.common.logging.setup_logging--disable-logging","title":"Disable logging","text":"<p>setup_logging(disable=True)</p>"},{"location":"api/fsspeckit.common/#fsspeckit.common.misc","title":"fsspeckit.common.misc","text":"<p>Miscellaneous utility functions for fsspeckit.</p>"},{"location":"api/fsspeckit.common/#fsspeckit.common.misc-functions","title":"Functions","text":""},{"location":"api/fsspeckit.common/#fsspeckit.common.misc.check_fs_identical","title":"fsspeckit.common.misc.check_fs_identical","text":"<pre><code>check_fs_identical(\n    fs1: AbstractFileSystem, fs2: AbstractFileSystem\n) -&gt; bool\n</code></pre> <p>Check if two fsspec filesystems are identical.</p> <p>Parameters:</p> Name Type Description Default <code>fs1</code> <code>AbstractFileSystem</code> <p>First filesystem (fsspec AbstractFileSystem)</p> required <code>fs2</code> <code>AbstractFileSystem</code> <p>Second filesystem (fsspec AbstractFileSystem)</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if filesystems are identical, False otherwise</p> Source code in <code>src/fsspeckit/common/misc.py</code> <pre><code>def check_fs_identical(fs1: AbstractFileSystem, fs2: AbstractFileSystem) -&gt; bool:\n    \"\"\"Check if two fsspec filesystems are identical.\n\n    Args:\n        fs1: First filesystem (fsspec AbstractFileSystem)\n        fs2: Second filesystem (fsspec AbstractFileSystem)\n\n    Returns:\n        bool: True if filesystems are identical, False otherwise\n    \"\"\"\n\n    def _get_root_fs(fs: AbstractFileSystem) -&gt; AbstractFileSystem:\n        while hasattr(fs, \"fs\"):\n            fs = fs.fs\n        return fs\n\n    fs1 = _get_root_fs(fs1)\n    fs2 = _get_root_fs(fs2)\n    return fs1 == fs2\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.misc.check_optional_dependency","title":"fsspeckit.common.misc.check_optional_dependency","text":"<pre><code>check_optional_dependency(\n    package_name: str, feature_name: str\n) -&gt; None\n</code></pre> <p>Check if an optional dependency is available.</p> <p>Parameters:</p> Name Type Description Default <code>package_name</code> <code>str</code> <p>Name of the package to check</p> required <code>feature_name</code> <code>str</code> <p>Name of the feature that requires this package</p> required <p>Raises:</p> Type Description <code>ImportError</code> <p>If the package is not available</p> Source code in <code>src/fsspeckit/common/misc.py</code> <pre><code>def check_optional_dependency(package_name: str, feature_name: str) -&gt; None:\n    \"\"\"Check if an optional dependency is available.\n\n    Args:\n        package_name: Name of the package to check\n        feature_name: Name of the feature that requires this package\n\n    Raises:\n        ImportError: If the package is not available\n    \"\"\"\n    if not importlib.util.find_spec(package_name):\n        raise ImportError(\n            f\"{package_name} is required for {feature_name}. \"\n            f\"Install with: pip install fsspeckit[full]\"\n        )\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.misc.get_partitions_from_path","title":"fsspeckit.common.misc.get_partitions_from_path","text":"<pre><code>get_partitions_from_path(\n    path: str,\n    partitioning: Union[str, list[str], None] = None,\n) -&gt; list[tuple]\n</code></pre> <p>Extract dataset partitions from a file path.</p> <p>Parses file paths to extract partition information based on different partitioning schemes.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>File path potentially containing partition information.</p> required <code>partitioning</code> <code>Union[str, list[str], None]</code> <p>Partitioning scheme: - \"hive\": Hive-style partitioning (key=value) - str: Single partition column name - list[str]: Multiple partition column names - None: Return empty list</p> <code>None</code> <p>Returns:</p> Type Description <code>list[tuple]</code> <p>List of tuples containing (column, value) pairs.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Hive-style partitioning\n&gt;&gt;&gt; get_partitions_from_path(\"data/year=2023/month=01/file.parquet\", \"hive\")\n[('year', '2023'), ('month', '01')]\n</code></pre> <pre><code>&gt;&gt;&gt; # Single partition column\n&gt;&gt;&gt; get_partitions_from_path(\"data/2023/01/file.parquet\", \"year\")\n[('year', '2023')]\n</code></pre> <pre><code>&gt;&gt;&gt; # Multiple partition columns\n&gt;&gt;&gt; get_partitions_from_path(\"data/2023/01/file.parquet\", [\"year\", \"month\"])\n[('year', '2023'), ('month', '01')]\n</code></pre> Source code in <code>src/fsspeckit/common/misc.py</code> <pre><code>def get_partitions_from_path(\n    path: str, partitioning: Union[str, list[str], None] = None\n) -&gt; list[tuple]:\n    \"\"\"Extract dataset partitions from a file path.\n\n    Parses file paths to extract partition information based on\n    different partitioning schemes.\n\n    Args:\n        path: File path potentially containing partition information.\n        partitioning: Partitioning scheme:\n            - \"hive\": Hive-style partitioning (key=value)\n            - str: Single partition column name\n            - list[str]: Multiple partition column names\n            - None: Return empty list\n\n    Returns:\n        List of tuples containing (column, value) pairs.\n\n    Examples:\n        &gt;&gt;&gt; # Hive-style partitioning\n        &gt;&gt;&gt; get_partitions_from_path(\"data/year=2023/month=01/file.parquet\", \"hive\")\n        [('year', '2023'), ('month', '01')]\n\n        &gt;&gt;&gt; # Single partition column\n        &gt;&gt;&gt; get_partitions_from_path(\"data/2023/01/file.parquet\", \"year\")\n        [('year', '2023')]\n\n        &gt;&gt;&gt; # Multiple partition columns\n        &gt;&gt;&gt; get_partitions_from_path(\"data/2023/01/file.parquet\", [\"year\", \"month\"])\n        [('year', '2023'), ('month', '01')]\n    \"\"\"\n    if \".\" in path:\n        path = os.path.dirname(path)\n\n    parts = path.split(\"/\")\n\n    if isinstance(partitioning, str):\n        if partitioning == \"hive\":\n            return [tuple(p.split(\"=\")) for p in parts if \"=\" in p]\n        else:\n            return [(partitioning, parts[0])]\n    elif isinstance(partitioning, list):\n        return list(zip(partitioning, parts[-len(partitioning) :]))\n    else:\n        return []\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.misc.path_to_glob","title":"fsspeckit.common.misc.path_to_glob","text":"<pre><code>path_to_glob(path: str, format: str | None = None) -&gt; str\n</code></pre> <p>Convert a path to a glob pattern for file matching.</p> <p>Intelligently converts paths to glob patterns that match files of the specified format, handling various directory and wildcard patterns.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Base path to convert. Can include wildcards (* or ). Examples: \"data/\", \"data/*.json\", \"data/\"</p> required <code>format</code> <code>str | None</code> <p>File format to match (without dot). If None, inferred from path. Examples: \"json\", \"csv\", \"parquet\"</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Glob pattern that matches files of specified format. Examples: \"data/**/.json\", \"data/.csv\"</p> Example Source code in <code>src/fsspeckit/common/misc.py</code> <pre><code>def path_to_glob(path: str, format: str | None = None) -&gt; str:\n    \"\"\"Convert a path to a glob pattern for file matching.\n\n    Intelligently converts paths to glob patterns that match files of the specified\n    format, handling various directory and wildcard patterns.\n\n    Args:\n        path: Base path to convert. Can include wildcards (* or **).\n            Examples: \"data/\", \"data/*.json\", \"data/**\"\n        format: File format to match (without dot). If None, inferred from path.\n            Examples: \"json\", \"csv\", \"parquet\"\n\n    Returns:\n        str: Glob pattern that matches files of specified format.\n            Examples: \"data/**/*.json\", \"data/*.csv\"\n\n    Example:\n        &gt;&gt;&gt; # Basic directory\n        &gt;&gt;&gt; path_to_glob(\"data\", \"json\")\n        'data/**/*.json'\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # With wildcards\n        &gt;&gt;&gt; path_to_glob(\"data/**\", \"csv\")\n        'data/**/*.csv'\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Format inference\n        &gt;&gt;&gt; path_to_glob(\"data/file.parquet\")\n        'data/file.parquet'\n    \"\"\"\n    path = path.rstrip(\"/\")\n    if format is None:\n        if \".json\" in path:\n            format = \"json\"\n        elif \".csv\" in path:\n            format = \"csv\"\n        elif \".parquet\" in path:\n            format = \"parquet\"\n\n    if format in path:\n        return path\n    else:\n        if path.endswith(\"**\"):\n            return posixpath.join(path, f\"*.{format}\")\n        elif path.endswith(\"*\"):\n            if path.endswith(\"*/*\"):\n                return path + f\".{format}\"\n            return posixpath.join(path.rstrip(\"/*\"), f\"*.{format}\")\n        return posixpath.join(path, f\"**/*.{format}\")\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.misc.path_to_glob--basic-directory","title":"Basic directory","text":"<p>path_to_glob(\"data\", \"json\") 'data/**/*.json'</p>"},{"location":"api/fsspeckit.common/#fsspeckit.common.misc.path_to_glob--with-wildcards","title":"With wildcards","text":"<p>path_to_glob(\"data/\", \"csv\") 'data//*.csv'</p>"},{"location":"api/fsspeckit.common/#fsspeckit.common.misc.path_to_glob--format-inference","title":"Format inference","text":"<p>path_to_glob(\"data/file.parquet\") 'data/file.parquet'</p>"},{"location":"api/fsspeckit.common/#fsspeckit.common.misc.sync_dir","title":"fsspeckit.common.misc.sync_dir","text":"<pre><code>sync_dir(\n    src_fs: AbstractFileSystem,\n    dst_fs: AbstractFileSystem,\n    src_path: str = \"\",\n    dst_path: str = \"\",\n    server_side: bool = True,\n    chunk_size: int = 8 * 1024 * 1024,\n    parallel: bool = False,\n    n_jobs: int = -1,\n    verbose: bool = True,\n) -&gt; dict[str, list[str]]\n</code></pre> <p>Sync two directories between different filesystems.</p> <p>Compares files in the source and destination directories, copies new or updated files from source to destination, and deletes stale files from destination.</p> <p>Parameters:</p> Name Type Description Default <code>src_fs</code> <code>AbstractFileSystem</code> <p>Source filesystem (fsspec AbstractFileSystem)</p> required <code>dst_fs</code> <code>AbstractFileSystem</code> <p>Destination filesystem (fsspec AbstractFileSystem)</p> required <code>src_path</code> <code>str</code> <p>Path in source filesystem to sync. Default is root ('').</p> <code>''</code> <code>dst_path</code> <code>str</code> <p>Path in destination filesystem to sync. Default is root ('').</p> <code>''</code> <code>chunk_size</code> <code>int</code> <p>Size of chunks to read/write files (in bytes). Default is 8MB.</p> <code>8 * 1024 * 1024</code> <code>parallel</code> <code>bool</code> <p>Whether to perform copy/delete operations in parallel. Default is False.</p> <code>False</code> <code>n_jobs</code> <code>int</code> <p>Number of parallel jobs if parallel=True. Default is -1 (all cores).</p> <code>-1</code> <code>verbose</code> <code>bool</code> <p>Whether to show progress bars. Default is True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, list[str]]</code> <p>Summary of added and deleted files</p> Source code in <code>src/fsspeckit/common/misc.py</code> <pre><code>def sync_dir(\n    src_fs: AbstractFileSystem,\n    dst_fs: AbstractFileSystem,\n    src_path: str = \"\",\n    dst_path: str = \"\",\n    server_side: bool = True,\n    chunk_size: int = 8 * 1024 * 1024,\n    parallel: bool = False,\n    n_jobs: int = -1,\n    verbose: bool = True,\n) -&gt; dict[str, list[str]]:\n    \"\"\"Sync two directories between different filesystems.\n\n    Compares files in the source and destination directories, copies new or updated files from source to destination,\n    and deletes stale files from destination.\n\n    Args:\n        src_fs: Source filesystem (fsspec AbstractFileSystem)\n        dst_fs: Destination filesystem (fsspec AbstractFileSystem)\n        src_path: Path in source filesystem to sync. Default is root ('').\n        dst_path: Path in destination filesystem to sync. Default is root ('').\n        chunk_size: Size of chunks to read/write files (in bytes). Default is 8MB.\n        parallel: Whether to perform copy/delete operations in parallel. Default is False.\n        n_jobs: Number of parallel jobs if parallel=True. Default is -1 (all cores).\n        verbose: Whether to show progress bars. Default is True.\n\n    Returns:\n        dict: Summary of added and deleted files\n    \"\"\"\n\n    src_mapper = src_fs.get_mapper(src_path)\n    dst_mapper = dst_fs.get_mapper(dst_path)\n\n    add_files = sorted(src_mapper.keys() - dst_mapper.keys())\n    delete_files = sorted(dst_mapper.keys() - src_mapper.keys())\n\n    return sync_files(\n        add_files=add_files,\n        delete_files=delete_files,\n        src_fs=src_fs,\n        dst_fs=dst_fs,\n        src_path=src_path,\n        dst_path=dst_path,\n        chunk_size=chunk_size,\n        server_side=server_side,\n        parallel=parallel,\n        n_jobs=n_jobs,\n        verbose=verbose,\n    )\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.misc.sync_files","title":"fsspeckit.common.misc.sync_files","text":"<pre><code>sync_files(\n    add_files: list[str],\n    delete_files: list[str],\n    src_fs: AbstractFileSystem,\n    dst_fs: AbstractFileSystem,\n    src_path: str = \"\",\n    dst_path: str = \"\",\n    server_side: bool = False,\n    chunk_size: int = 8 * 1024 * 1024,\n    parallel: bool = False,\n    n_jobs: int = -1,\n    verbose: bool = True,\n) -&gt; dict[str, list[str]]\n</code></pre> <p>Sync files between two filesystems by copying new files and deleting old ones.</p> <p>Parameters:</p> Name Type Description Default <code>add_files</code> <code>list[str]</code> <p>List of file paths to add (copy from source to destination)</p> required <code>delete_files</code> <code>list[str]</code> <p>List of file paths to delete from destination</p> required <code>src_fs</code> <code>AbstractFileSystem</code> <p>Source filesystem (fsspec AbstractFileSystem)</p> required <code>dst_fs</code> <code>AbstractFileSystem</code> <p>Destination filesystem (fsspec AbstractFileSystem)</p> required <code>src_path</code> <code>str</code> <p>Base path in source filesystem. Default is root ('').</p> <code>''</code> <code>dst_path</code> <code>str</code> <p>Base path in destination filesystem. Default is root ('').</p> <code>''</code> <code>server_side</code> <code>bool</code> <p>Whether to use server-side copy if supported. Default is False.</p> <code>False</code> <code>chunk_size</code> <code>int</code> <p>Size of chunks to read/write files (in bytes). Default is 8MB.</p> <code>8 * 1024 * 1024</code> <code>parallel</code> <code>bool</code> <p>Whether to perform copy/delete operations in parallel. Default is False.</p> <code>False</code> <code>n_jobs</code> <code>int</code> <p>Number of parallel jobs if parallel=True. Default is -1 (all cores).</p> <code>-1</code> <code>verbose</code> <code>bool</code> <p>Whether to show progress bars. Default is True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, list[str]]</code> <p>Summary of added and deleted files</p> Source code in <code>src/fsspeckit/common/misc.py</code> <pre><code>def sync_files(\n    add_files: list[str],\n    delete_files: list[str],\n    src_fs: AbstractFileSystem,\n    dst_fs: AbstractFileSystem,\n    src_path: str = \"\",\n    dst_path: str = \"\",\n    server_side: bool = False,\n    chunk_size: int = 8 * 1024 * 1024,\n    parallel: bool = False,\n    n_jobs: int = -1,\n    verbose: bool = True,\n) -&gt; dict[str, list[str]]:\n    \"\"\"Sync files between two filesystems by copying new files and deleting old ones.\n\n    Args:\n        add_files: List of file paths to add (copy from source to destination)\n        delete_files: List of file paths to delete from destination\n        src_fs: Source filesystem (fsspec AbstractFileSystem)\n        dst_fs: Destination filesystem (fsspec AbstractFileSystem)\n        src_path: Base path in source filesystem. Default is root ('').\n        dst_path: Base path in destination filesystem. Default is root ('').\n        server_side: Whether to use server-side copy if supported. Default is False.\n        chunk_size: Size of chunks to read/write files (in bytes). Default is 8MB.\n        parallel: Whether to perform copy/delete operations in parallel. Default is False.\n        n_jobs: Number of parallel jobs if parallel=True. Default is -1 (all cores).\n        verbose: Whether to show progress bars. Default is True.\n\n    Returns:\n        dict: Summary of added and deleted files\n    \"\"\"\n    CHUNK = chunk_size\n    RETRIES = 3\n\n    server_side = check_fs_identical(src_fs, dst_fs) and server_side\n\n    src_mapper = src_fs.get_mapper(src_path)\n    dst_mapper = dst_fs.get_mapper(dst_path)\n\n    def server_side_copy_file(key, src_mapper, dst_mapper, RETRIES):\n        last_exc = None\n        for attempt in range(1, RETRIES + 1):\n            try:\n                dst_mapper[key] = src_mapper[key]\n                break\n            except Exception as e:\n                last_exc = e\n\n    def copy_file(key, src_fs, dst_fs, src_path, dst_path, CHUNK, RETRIES):\n        last_exc = None\n        for attempt in range(1, RETRIES + 1):\n            try:\n                with (\n                    src_fs.open(posixpath.join(src_path, key), \"rb\") as r,\n                    dst_fs.open(posixpath.join(dst_path, key), \"wb\") as w,\n                ):\n                    while True:\n                        chunk = r.read(CHUNK)\n                        if not chunk:\n                            break\n                        w.write(chunk)\n                break\n            except Exception as e:\n                last_exc = e\n\n    def delete_file(key, dst_fs, dst_path, RETRIES):\n        last_exc = None\n        for attempt in range(1, RETRIES + 1):\n            try:\n                dst_fs.rm(posixpath.join(dst_path, key))\n                break\n            except Exception as e:\n                last_exc = e\n\n    if len(add_files):\n        # Copy new files\n        if parallel:\n            if server_side:\n                try:\n                    run_parallel(\n                        server_side_copy_file,\n                        add_files,\n                        src_mapper=src_mapper,\n                        dst_mapper=dst_mapper,\n                        RETRIES=RETRIES,\n                        n_jobs=n_jobs,\n                        verbose=verbose,\n                    )\n                except Exception:\n                    # Fallback to client-side copy if server-side fails\n                    run_parallel(\n                        copy_file,\n                        add_files,\n                        src_fs=src_fs,\n                        dst_fs=dst_fs,\n                        src_path=src_path,\n                        dst_path=dst_path,\n                        CHUNK=CHUNK,\n                        RETRIES=RETRIES,\n                        n_jobs=n_jobs,\n                        verbose=verbose,\n                    )\n            else:\n                run_parallel(\n                    copy_file,\n                    add_files,\n                    src_fs=src_fs,\n                    dst_fs=dst_fs,\n                    src_path=src_path,\n                    dst_path=dst_path,\n                    CHUNK=CHUNK,\n                    RETRIES=RETRIES,\n                    n_jobs=n_jobs,\n                    verbose=verbose,\n                )\n        else:\n            if verbose:\n                for key in track(\n                    add_files,\n                    description=\"Copying new files...\",\n                    total=len(add_files),\n                ):\n                    if server_side:\n                        try:\n                            server_side_copy_file(key, src_mapper, dst_mapper, RETRIES)\n                        except Exception:\n                            copy_file(\n                                key, src_fs, dst_fs, src_path, dst_path, CHUNK, RETRIES\n                            )\n                    else:\n                        copy_file(\n                            key, src_fs, dst_fs, src_path, dst_path, CHUNK, RETRIES\n                        )\n            else:\n                for key in add_files:\n                    if server_side:\n                        try:\n                            server_side_copy_file(key, src_mapper, dst_mapper, RETRIES)\n                        except Exception:\n                            copy_file(\n                                key, src_fs, dst_fs, src_path, dst_path, CHUNK, RETRIES\n                            )\n                    else:\n                        copy_file(\n                            key, src_fs, dst_fs, src_path, dst_path, CHUNK, RETRIES\n                        )\n\n    if len(delete_files):\n        # Delete old files from destination\n        if parallel:\n            run_parallel(\n                delete_file,\n                delete_files,\n                dst_fs=dst_fs,\n                dst_path=dst_path,\n                RETRIES=RETRIES,\n                n_jobs=n_jobs,\n                verbose=verbose,\n            )\n        else:\n            if verbose:\n                for key in track(\n                    delete_files,\n                    description=\"Deleting stale files...\",\n                    total=len(delete_files),\n                ):\n                    delete_file(key, dst_fs, dst_path, RETRIES)\n            else:\n                for key in delete_files:\n                    delete_file(key, dst_fs, dst_path, RETRIES)\n\n    return {\"added_files\": add_files, \"deleted_files\": delete_files}\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.polars","title":"fsspeckit.common.polars","text":""},{"location":"api/fsspeckit.common/#fsspeckit.common.polars-functions","title":"Functions","text":""},{"location":"api/fsspeckit.common/#fsspeckit.common.polars.drop_null_columns","title":"fsspeckit.common.polars.drop_null_columns","text":"<pre><code>drop_null_columns(\n    df: DataFrame | LazyFrame,\n) -&gt; DataFrame | LazyFrame\n</code></pre> <p>Remove columns with all null values from the DataFrame.</p> Source code in <code>src/fsspeckit/common/polars.py</code> <pre><code>def drop_null_columns(df: pl.DataFrame | pl.LazyFrame) -&gt; pl.DataFrame | pl.LazyFrame:\n    \"\"\"Remove columns with all null values from the DataFrame.\"\"\"\n    return df.select([col for col in df.columns if df[col].null_count() &lt; df.height])\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.polars.opt_dtype","title":"fsspeckit.common.polars.opt_dtype","text":"<pre><code>opt_dtype(\n    df: DataFrame,\n    include: str | list[str] | None = None,\n    exclude: str | list[str] | None = None,\n    time_zone: str | None = None,\n    shrink_numerics: bool = False,\n    allow_unsigned: bool = True,\n    allow_null: bool = True,\n    sample_size: int | None = 1024,\n    sample_method: SampleMethod = \"first\",\n    strict: bool = False,\n    *,\n    force_timezone: str | None = None,\n) -&gt; DataFrame\n</code></pre> <p>Optimize data types of a Polars DataFrame for performance and memory efficiency.</p> <p>This function analyzes each column and converts it to the most appropriate data type based on content, handling string-to-type conversions and numeric type downcasting.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The Polars DataFrame to optimize.</p> required <code>include</code> <code>str | list[str] | None</code> <p>Column(s) to include in optimization (default: all columns).</p> <code>None</code> <code>exclude</code> <code>str | list[str] | None</code> <p>Column(s) to exclude from optimization.</p> <code>None</code> <code>time_zone</code> <code>str | None</code> <p>Optional time zone hint during datetime parsing.</p> <code>None</code> <code>shrink_numerics</code> <code>bool</code> <p>Whether to downcast numeric types when possible.</p> <code>False</code> <code>allow_unsigned</code> <code>bool</code> <p>Whether to allow unsigned integer types.</p> <code>True</code> <code>allow_null</code> <code>bool</code> <p>Whether to allow columns with all null values to be cast to Null type.</p> <code>True</code> <code>sample_size</code> <code>int | None</code> <p>Maximum number of cleaned values to inspect for regex-based inference. Use None to inspect the entire column.</p> <code>1024</code> <code>sample_method</code> <code>SampleMethod</code> <p>Which subset to inspect (<code>\"first\"</code> or <code>\"random\"</code>).</p> <code>'first'</code> <code>strict</code> <code>bool</code> <p>If True, will raise an error if any column cannot be optimized.</p> <code>False</code> <code>force_timezone</code> <code>str | None</code> <p>If set, ensure all parsed datetime columns end up with this timezone.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with optimized data types.</p> Source code in <code>src/fsspeckit/common/polars.py</code> <pre><code>def opt_dtype(\n    df: pl.DataFrame,\n    include: str | list[str] | None = None,\n    exclude: str | list[str] | None = None,\n    time_zone: str | None = None,\n    shrink_numerics: bool = False,\n    allow_unsigned: bool = True,\n    allow_null: bool = True,\n    sample_size: int | None = 1024,\n    sample_method: SampleMethod = \"first\",\n    strict: bool = False,\n    *,\n    force_timezone: str | None = None,\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Optimize data types of a Polars DataFrame for performance and memory efficiency.\n\n    This function analyzes each column and converts it to the most appropriate\n    data type based on content, handling string-to-type conversions and\n    numeric type downcasting.\n\n    Args:\n        df: The Polars DataFrame to optimize.\n        include: Column(s) to include in optimization (default: all columns).\n        exclude: Column(s) to exclude from optimization.\n        time_zone: Optional time zone hint during datetime parsing.\n        shrink_numerics: Whether to downcast numeric types when possible.\n        allow_unsigned: Whether to allow unsigned integer types.\n        allow_null: Whether to allow columns with all null values to be cast to Null type.\n        sample_size: Maximum number of cleaned values to inspect for regex-based inference. Use None to inspect the entire column.\n        sample_method: Which subset to inspect (`\"first\"` or `\"random\"`).\n        strict: If True, will raise an error if any column cannot be optimized.\n        force_timezone: If set, ensure all parsed datetime columns end up with this timezone.\n\n    Returns:\n        DataFrame with optimized data types.\n    \"\"\"\n    if sample_method not in (\"first\", \"random\"):\n        raise ValueError(\"sample_method must be 'first' or 'random'\")\n\n    if isinstance(df, pl.LazyFrame):\n        return opt_dtype(\n            df.collect(),\n            include=include,\n            exclude=exclude,\n            time_zone=time_zone,\n            shrink_numerics=shrink_numerics,\n            allow_unsigned=allow_unsigned,\n            allow_null=allow_null,\n            sample_size=sample_size,\n            sample_method=sample_method,\n            strict=strict,\n            force_timezone=force_timezone,\n        ).lazy()\n\n    # Normalize include/exclude parameters\n    if isinstance(include, str):\n        include = [include]\n    if isinstance(exclude, str):\n        exclude = [exclude]\n\n    # Determine columns to process\n    cols_to_process = df.columns\n    if include:\n        cols_to_process = [col for col in include if col in df.columns]\n    if exclude:\n        cols_to_process = [col for col in cols_to_process if col not in exclude]\n\n    # Generate optimization expressions for all columns\n    expressions = []\n    for col_name in cols_to_process:\n        try:\n            expressions.append(\n                _get_column_expr(\n                    df,\n                    col_name,\n                    shrink_numerics,\n                    allow_unsigned,\n                    allow_null,\n                    time_zone,\n                    force_timezone,\n                    sample_size,\n                    sample_method,\n                    strict,\n                )\n            )\n        except Exception as e:\n            if strict:\n                raise e\n            # If strict mode is off, just keep the original column\n            continue\n\n    # Apply all transformations at once if any exist\n    return df if not expressions else df.with_columns(expressions)\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.types","title":"fsspeckit.common.types","text":"<p>Type conversion and data transformation utilities.</p>"},{"location":"api/fsspeckit.common/#fsspeckit.common.types-functions","title":"Functions","text":""},{"location":"api/fsspeckit.common/#fsspeckit.common.types.dict_to_dataframe","title":"fsspeckit.common.types.dict_to_dataframe","text":"<pre><code>dict_to_dataframe(\n    data: Union[dict, list[dict]],\n    unique: Union[bool, list[str], str] = False,\n) -&gt; DataFrame\n</code></pre> <p>Convert a dictionary or list of dictionaries to a Polars DataFrame.</p> <p>Handles various input formats: - Single dict with list values \u2192 DataFrame rows - Single dict with scalar values \u2192 Single row DataFrame - List of dicts with scalar values \u2192 Multi-row DataFrame - List of dicts with list values \u2192 DataFrame with list columns</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[dict, list[dict]]</code> <p>Dictionary or list of dictionaries to convert.</p> required <code>unique</code> <code>Union[bool, list[str], str]</code> <p>If True, remove duplicate rows. Can also specify columns.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Polars DataFrame containing the converted data.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Single dict with list values\n&gt;&gt;&gt; data = {'a': [1, 2, 3], 'b': [4, 5, 6]}\n&gt;&gt;&gt; dict_to_dataframe(data)\nshape: (3, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 b   \u2502\n\u2502 --- \u2506 --- \u2502\n\u2502 i64 \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 4   \u2502\n\u2502 2   \u2506 5   \u2502\n\u2502 3   \u2506 6   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <pre><code>&gt;&gt;&gt; # Single dict with scalar values\n&gt;&gt;&gt; data = {'a': 1, 'b': 2}\n&gt;&gt;&gt; dict_to_dataframe(data)\nshape: (1, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 b   \u2502\n\u2502 --- \u2506 --- \u2502\n\u2502 i64 \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 2   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <pre><code>&gt;&gt;&gt; # List of dicts with scalar values\n&gt;&gt;&gt; data = [{'a': 1, 'b': 2}, {'a': 3, 'b': 4}]\n&gt;&gt;&gt; dict_to_dataframe(data)\nshape: (2, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 b   \u2502\n\u2502 --- \u2506 --- \u2502\n\u2502 i64 \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 2   \u2502\n\u2502 3   \u2506 4   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> Source code in <code>src/fsspeckit/common/types.py</code> <pre><code>def dict_to_dataframe(\n    data: Union[dict, list[dict]], unique: Union[bool, list[str], str] = False\n) -&gt; \"pl.DataFrame\":\n    \"\"\"Convert a dictionary or list of dictionaries to a Polars DataFrame.\n\n    Handles various input formats:\n    - Single dict with list values \u2192 DataFrame rows\n    - Single dict with scalar values \u2192 Single row DataFrame\n    - List of dicts with scalar values \u2192 Multi-row DataFrame\n    - List of dicts with list values \u2192 DataFrame with list columns\n\n    Args:\n        data: Dictionary or list of dictionaries to convert.\n        unique: If True, remove duplicate rows. Can also specify columns.\n\n    Returns:\n        Polars DataFrame containing the converted data.\n\n    Examples:\n        &gt;&gt;&gt; # Single dict with list values\n        &gt;&gt;&gt; data = {'a': [1, 2, 3], 'b': [4, 5, 6]}\n        &gt;&gt;&gt; dict_to_dataframe(data)\n        shape: (3, 2)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 a   \u2506 b   \u2502\n        \u2502 --- \u2506 --- \u2502\n        \u2502 i64 \u2506 i64 \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 1   \u2506 4   \u2502\n        \u2502 2   \u2506 5   \u2502\n        \u2502 3   \u2506 6   \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n\n        &gt;&gt;&gt; # Single dict with scalar values\n        &gt;&gt;&gt; data = {'a': 1, 'b': 2}\n        &gt;&gt;&gt; dict_to_dataframe(data)\n        shape: (1, 2)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 a   \u2506 b   \u2502\n        \u2502 --- \u2506 --- \u2502\n        \u2502 i64 \u2506 i64 \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 1   \u2506 2   \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n\n        &gt;&gt;&gt; # List of dicts with scalar values\n        &gt;&gt;&gt; data = [{'a': 1, 'b': 2}, {'a': 3, 'b': 4}]\n        &gt;&gt;&gt; dict_to_dataframe(data)\n        shape: (2, 2)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 a   \u2506 b   \u2502\n        \u2502 --- \u2506 --- \u2502\n        \u2502 i64 \u2506 i64 \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 1   \u2506 2   \u2502\n        \u2502 3   \u2506 4   \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n    \"\"\"\n    if isinstance(data, list):\n        # If it's a single-element list, just use the first element\n        if len(data) == 1:\n            data = data[0]\n        # If it's a list of dicts\n        else:\n            first_item = data[0]\n            # Check if the dict values are lists/tuples\n            if any(isinstance(v, (list, tuple)) for v in first_item.values()):\n                # Each dict becomes a row with list/tuple values\n                data = pl.DataFrame(data)\n            else:\n                # If values are scalars, convert list of dicts to DataFrame\n                data = pl.DataFrame(data)\n\n            if unique:\n                data = data.unique(\n                    subset=None if not isinstance(unique, (str, list)) else unique,\n                    maintain_order=True,\n                )\n            return data\n\n    # If it's a single dict\n    if isinstance(data, dict):\n        # Check if values are lists/tuples\n        if any(isinstance(v, (list, tuple)) for v in data.values()):\n            # Get the length of any list value (assuming all lists have same length)\n            length = len(next(v for v in data.values() if isinstance(v, (list, tuple))))\n            # Convert to DataFrame where each list element becomes a row\n            data = pl.DataFrame(\n                {\n                    k: v if isinstance(v, (list, tuple)) else [v] * length\n                    for k, v in data.items()\n                }\n            )\n        else:\n            # If values are scalars, wrap them in a list to create a single row\n            data = pl.DataFrame({k: [v] for k, v in data.items()})\n\n        if unique:\n            data = data.unique(\n                subset=None if not isinstance(unique, (str, list)) else unique,\n                maintain_order=True,\n            )\n        return data\n\n    raise ValueError(\"Input must be a dictionary or list of dictionaries\")\n</code></pre>"},{"location":"api/fsspeckit.common/#fsspeckit.common.types.to_pyarrow_table","title":"fsspeckit.common.types.to_pyarrow_table","text":"<pre><code>to_pyarrow_table(\n    data: Union[\n        DataFrame,\n        LazyFrame,\n        DataFrame,\n        dict,\n        list[Union[DataFrame, LazyFrame, DataFrame, dict]],\n    ],\n    concat: bool = False,\n    unique: Union[bool, list[str], str] = False,\n) -&gt; Table\n</code></pre> <p>Convert various data formats to PyArrow Table.</p> <p>Handles conversion from Polars DataFrames, Pandas DataFrames, dictionaries, and lists of these types to PyArrow Tables.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[DataFrame, LazyFrame, DataFrame, dict, list[Union[DataFrame, LazyFrame, DataFrame, dict]]]</code> <p>Input data to convert.</p> required <code>concat</code> <code>bool</code> <p>Whether to concatenate multiple inputs into single table.</p> <code>False</code> <code>unique</code> <code>Union[bool, list[str], str]</code> <p>Whether to remove duplicates. Can specify columns.</p> <code>False</code> <p>Returns:</p> Type Description <code>Table</code> <p>PyArrow Table containing the converted data.</p> Example <p>df = pl.DataFrame({\"a\": [1, 2], \"b\": [3, 4]}) table = to_pyarrow_table(df) print(table.schema) a: int64 b: int64</p> Source code in <code>src/fsspeckit/common/types.py</code> <pre><code>def to_pyarrow_table(\n    data: Union[\n        \"pl.DataFrame\",\n        \"pl.LazyFrame\",\n        \"pd.DataFrame\",\n        dict,\n        list[Union[\"pl.DataFrame\", \"pl.LazyFrame\", \"pd.DataFrame\", dict]],\n    ],\n    concat: bool = False,\n    unique: Union[bool, list[str], str] = False,\n) -&gt; \"pa.Table\":\n    \"\"\"Convert various data formats to PyArrow Table.\n\n    Handles conversion from Polars DataFrames, Pandas DataFrames,\n    dictionaries, and lists of these types to PyArrow Tables.\n\n    Args:\n        data: Input data to convert.\n        concat: Whether to concatenate multiple inputs into single table.\n        unique: Whether to remove duplicates. Can specify columns.\n\n    Returns:\n        PyArrow Table containing the converted data.\n\n    Example:\n        &gt;&gt;&gt; df = pl.DataFrame({\"a\": [1, 2], \"b\": [3, 4]})\n        &gt;&gt;&gt; table = to_pyarrow_table(df)\n        &gt;&gt;&gt; print(table.schema)\n        a: int64\n        b: int64\n    \"\"\"\n    # Convert dict to DataFrame first\n    if isinstance(data, dict):\n        data = dict_to_dataframe(data)\n    if isinstance(data, list):\n        if isinstance(data[0], dict):\n            data = dict_to_dataframe(data, unique=unique)\n\n    # Ensure data is a list for uniform processing\n    if not isinstance(data, list):\n        data = [data]\n\n    # Collect lazy frames\n    if isinstance(data[0], pl.LazyFrame):\n        data = [dd.collect() for dd in data]\n\n    # Convert based on the first item's type\n    if isinstance(data[0], pl.DataFrame):\n        if concat:\n            data = pl.concat(data, how=\"diagonal_relaxed\")\n            if unique:\n                data = data.unique(\n                    subset=None if not isinstance(unique, (str, list)) else unique,\n                    maintain_order=True,\n                )\n            data = data.to_arrow()\n            data = data.cast(convert_large_types_to_normal(data.schema))\n        else:\n            data = [dd.to_arrow() for dd in data]\n            data = [dd.cast(convert_large_types_to_normal(dd.schema)) for dd in data]\n\n    elif isinstance(data[0], pd.DataFrame):\n        data = [pa.Table.from_pandas(dd, preserve_index=False) for dd in data]\n        if concat:\n            data = pa.concat_tables(data, promote_options=\"permissive\")\n            if unique:\n                data = (\n                    pl.from_arrow(data)\n                    .unique(\n                        subset=None if not isinstance(unique, (str, list)) else unique,\n                        maintain_order=True,\n                    )\n                    .to_arrow()\n                )\n                data = data.cast(convert_large_types_to_normal(data.schema))\n\n    elif isinstance(data[0], (pa.RecordBatch, Generator)):\n        if concat:\n            data = pa.Table.from_batches(data)\n            if unique:\n                data = (\n                    pl.from_arrow(data)\n                    .unique(\n                        subset=None if not isinstance(unique, (str, list)) else unique,\n                        maintain_order=True,\n                    )\n                    .to_arrow()\n                )\n                data = data.cast(convert_large_types_to_normal(data.schema))\n        else:\n            data = [pa.Table.from_batches([dd]) for dd in data]\n\n    return data\n</code></pre>"},{"location":"api/fsspeckit.core.base/","title":"<code>fsspeckit.core.base</code> API Documentation","text":"<p>This module provides core filesystem functionalities and utilities, including custom cache mappers, enhanced cached filesystems, and a GitLab filesystem implementation.</p>"},{"location":"api/fsspeckit.core.base/#filenamecachemapper","title":"<code>FileNameCacheMapper</code>","text":"<p>Maps remote file paths to local cache paths while preserving directory structure.</p> <p>This cache mapper maintains the original file path structure in the cache directory, creating necessary subdirectories as needed.</p> <p>Attributes:</p> <ul> <li><code>directory</code> (<code>str</code>): Base directory for cached files</li> </ul> <p>Example:</p> <pre><code>from fsspeckit.core.base import FileNameCacheMapper\n\n# Create cache mapper for S3 files\nmapper = FileNameCacheMapper(\"/tmp/cache\")\n\n# Map remote path to cache path\ncache_path = mapper(\"bucket/data/file.csv\")\nprint(cache_path)  # Preserves structure\n</code></pre>"},{"location":"api/fsspeckit.core.base/#__init__","title":"<code>__init__()</code>","text":"<p>Initialize cache mapper with base directory.</p> Parameter Type Description <code>directory</code> <code>str</code> Base directory where cached files will be stored"},{"location":"api/fsspeckit.core.base/#__call__","title":"<code>__call__()</code>","text":"<p>Map remote file path to cache file path.</p> <p>Creates necessary subdirectories in the cache directory to maintain the original path structure.</p> Parameter Type Description <code>path</code> <code>str</code> Original file path from remote filesystem Returns Type Description <code>str</code> <code>str</code> Cache file path that preserves original structure <p>Example:</p> <pre><code>from fsspeckit.core.base import FileNameCacheMapper\n\nmapper = FileNameCacheMapper(\"/tmp/cache\")\n# Maps maintain directory structure\nprint(mapper(\"data/nested/file.txt\"))\n</code></pre>"},{"location":"api/fsspeckit.core.base/#monitoredsimplecachefilesystem","title":"<code>MonitoredSimpleCacheFileSystem</code>","text":"<p>Enhanced caching filesystem with monitoring and improved path handling.</p> <p>This filesystem extends <code>SimpleCacheFileSystem</code> to provide:</p> <ul> <li>Verbose logging of cache operations</li> <li>Improved path mapping for cache files</li> <li>Enhanced synchronization capabilities</li> <li>Better handling of parallel operations</li> </ul> <p>Attributes:</p> <ul> <li><code>_verbose</code> (<code>bool</code>): Whether to print verbose cache operations</li> <li><code>_mapper</code> (<code>FileNameCacheMapper</code>): Maps remote paths to cache paths</li> <li><code>storage</code> (<code>list[str]</code>): List of cache storage locations</li> <li><code>fs</code> (<code>AbstractFileSystem</code>): Underlying filesystem being cached</li> </ul> <p>Example:</p> <pre><code>from fsspec import filesystem\nfrom fsspeckit.core.base import MonitoredSimpleCacheFileSystem\n\ns3_fs = filesystem(\"s3\")\ncached_fs = MonitoredSimpleCacheFileSystem(\n    fs=s3_fs,\n    cache_storage=\"/tmp/cache\",\n    verbose=True\n)\n# Use cached_fs like any other filesystem\nfiles = cached_fs.ls(\"my-bucket/\")\n</code></pre>"},{"location":"api/fsspeckit.core.base/#__init___1","title":"<code>__init__()</code>","text":"<p>Initialize monitored cache filesystem.</p> Parameter Type Description <code>fs</code> <code>Optional[fsspec.AbstractFileSystem]</code> Underlying filesystem to cache. If None, creates a local filesystem. <code>cache_storage</code> <code>Union[str, list[str]]</code> Cache storage location(s). Can be string path or list of paths. <code>verbose</code> <code>bool</code> Whether to enable verbose logging of cache operations. <code>**kwargs</code> <code>Any</code> Additional arguments passed to <code>SimpleCacheFileSystem</code>. <p>Example:</p> <pre><code># Cache S3 filesystem\ns3_fs = filesystem(\"s3\")\ncached = MonitoredSimpleCacheFileSystem(\n    fs=s3_fs,\n    cache_storage=\"/tmp/s3_cache\",\n    verbose=True\n)\n</code></pre>"},{"location":"api/fsspeckit.core.base/#_check_cache","title":"<code>_check_cache()</code>","text":"<p>Check if file exists in cache and return cache path if found.</p> Parameter Type Description <code>path</code> <code>str</code> Remote file path to check Returns Type Descript <code>Optional[str]</code> <code>str</code> or <code>None</code> Cache file path if found, None otherwise <p>Example:</p> <pre><code>from fsspeckit.core.base import MonitoredSimpleCacheFileSystem\nfrom fsspec import filesystem\n\ncached_fs = MonitoredSimpleCacheFileSystem(fs=filesystem(\"s3\"), cache_storage=\"/tmp/cache\")\n# Check if a file is in cache\ncache_path = cached_fs._check_cache(\"my-bucket/data/file.txt\")\nif cache_path:\n    print(f\"File found in cache at: {cache_path}\")\nelse:\n    print(\"File not in cache.\")\n</code></pre>"},{"location":"api/fsspeckit.core.base/#_check_file","title":"<code>_check_file()</code>","text":"<p>Ensure file is in cache, downloading if necessary.</p> Parameter Type Description <code>path</code> <code>str</code> Remote file path Returns Type Description <code>str</code> <code>str</code> Local cache path for the file <p>Example:</p> <pre><code>from fsspeckit.core.base import MonitoredSimpleCacheFileSystem\nfrom fsspec import filesystem\n\ncached_fs = MonitoredSimpleCacheFileSystem(fs=filesystem(\"s3\"), cache_storage=\"/tmp/cache\")\n# Ensure file is in cache (downloads if not present)\nlocal_path = cached_fs._check_file(\"my-bucket/data/large_file.parquet\")\nprint(f\"File available locally at: {local_path}\")\n</code></pre>"},{"location":"api/fsspeckit.core.base/#gitlabfilesystem","title":"<code>GitLabFileSystem</code>","text":"<p>Filesystem interface for GitLab repositories.</p> <p>Provides read-only access to files in GitLab repositories, including:</p> <ul> <li>Public and private repositories</li> <li>Self-hosted GitLab instances</li> <li>Branch/tag/commit selection</li> <li>Token-based authentication</li> </ul> <p>Attributes:</p> <ul> <li><code>protocol</code> (<code>str</code>): Always \"gitlab\"</li> <li><code>base_url</code> (<code>str</code>): GitLab instance URL</li> <li><code>project_id</code> (<code>str</code>): Project ID</li> <li><code>project_name</code> (<code>str</code>): Project name/path</li> <li><code>ref</code> (<code>str</code>): Git reference (branch, tag, commit)</li> <li><code>token</code> (<code>str</code>): Access token</li> <li><code>api_version</code> (<code>str</code>): API version</li> </ul> <p>Example:</p> <pre><code># Public repository\nfs = GitLabFileSystem(\n    project_name=\"group/project\",\n    ref=\"main\"\n)\nfiles = fs.ls(\"/\")\n\n# Private repository with token\nfs = GitLabFileSystem(\n    project_id=\"12345\",\n    token=\"glpat_xxxx\",\n    ref=\"develop\"\n)\ncontent = fs.cat(\"README.md\")\n</code></pre>"},{"location":"api/fsspeckit.core.base/#__init___2","title":"<code>__init__()</code>","text":"<p>Initialize GitLab filesystem.</p> Parameter Type Description <code>base_url</code> <code>str</code> GitLab instance URL <code>project_id</code> <code>Optional[Union[str, int]]</code> Project ID number <code>project_name</code> <code>Optional[str]</code> Project name/path (alternative to project_id) <code>ref</code> <code>str</code> Git reference (branch, tag, or commit SHA) <code>token</code> <code>Optional[str]</code> GitLab personal access token <code>api_version</code> <code>str</code> API version to use <p>| <code>**kwargs</code> | <code>Any</code> | Additional filesystem arguments |</p> Raises Type Description <code>ValueError</code> <code>ValueError</code> If neither <code>project_id</code> nor <code>project_name</code> is provided <p>Example:</p> <pre><code>from fsspeckit.core.base import GitLabFileSystem\n\n# Access a public repository\nfs_public = GitLabFileSystem(\n    project_name=\"gitlab-org/gitlab\",\n    ref=\"master\"\n)\nprint(fs_public.ls(\"README.md\"))\n\n# Access a private repository (replace with your token and project info)\n# fs_private = GitLabFileSystem(\n#     project_id=\"12345\",\n# #    token=\"your_private_token\",\n#     ref=\"main\"\n# )\n# print(fs_private.ls(\"/\"))\n</code></pre>"},{"location":"api/fsspeckit.core.base/#_get_file_content","title":"<code>_get_file_content()</code>","text":"<p>Get file content from GitLab API.</p> Parameter Type Description <code>path</code> <code>str</code> File path in repository Returns Type Description <code>bytes</code> <code>bytes</code> File content as bytes <p>Example:</p> <pre><code>from fsspeckit.core.base import GitLabFileSystem\n\nfs = GitLabFileSystem(project_name=\"gitlab-org/gitlab\")\ncontent = fs.cat(\"README.md\")\nprint(content[:50])\n</code></pre> Raises Type Description <code>FileNotFoundError</code> <code>FileNotFoundError</code> If file doesn't exist <code>requests.HTTPError</code> <code>requests.HTTPError</code> For other HTTP errors"},{"location":"api/fsspeckit.core.base/#_open","title":"<code>_open()</code>","text":"<p>Open file for reading.</p> Parameter Type Description <code>path</code> <code>str</code> File path to open <code>mode</code> <code>str</code> File mode (only 'rb' and 'r' supported) <code>block_size</code> <code>Optional[int]</code> Block size for reading (unused) <code>cache_options</code> <code>Optional[dict]</code> Cache options (unused) <code>**kwargs</code> <code>Any</code> Additional options Returns Type Description File-like object File-like object File-like object for reading Raises Type Description <code>ValueError</code> <code>ValueError</code> If mode is not supported"},{"location":"api/fsspeckit.core.base/#cat","title":"<code>cat()</code>","text":"<p>Get file contents as bytes.</p> Parameter Type Description <code>path</code> <code>str</code> File path <code>**kwargs</code> <code>Any</code> Additional options Returns Type Description <code>bytes</code> <code>bytes</code> File content as bytes"},{"location":"api/fsspeckit.core.base/#ls","title":"<code>ls()</code>","text":"<p>List directory contents.</p> Parameter Type Description <code>path</code> <code>str</code> Directory path to list <code>detail</code> <code>bool</code> Whether to return detailed information <code>**kwargs</code> <code>Any</code> Additional options Returns Type Description <code>list</code> <code>list</code> List of files/directories or their details"},{"location":"api/fsspeckit.core.base/#exists","title":"<code>exists()</code>","text":"<p>Check if file or directory exists.</p> Parameter Type Description <code>path</code> <code>str</code> Path to check <code>**kwargs</code> <code>Any</code> Additional options Returns Type Description <code>bool</code> <code>bool</code> True if path exists, False otherwise"},{"location":"api/fsspeckit.core.base/#info","title":"<code>info()</code>","text":"<p>Get file information.</p> Parameter Type Description <code>path</code> <code>str</code> File path <code>**kwargs</code> <code>Any</code> Additional options Returns Type Description <code>dict</code> <code>dict</code> Dictionary with file information Raises Type Description <code>FileNotFoundError</code> <code>FileNotFoundError</code> If file not found"},{"location":"api/fsspeckit.core.base/#filesystem","title":"<code>filesystem()</code>","text":"<p>Get filesystem instance with enhanced configuration options.</p> <p>Creates filesystem instances with support for storage options classes, intelligent caching, and protocol inference from paths.</p> Parameter Type Description <code>protocol_or_path</code> <code>str</code> Filesystem protocol (e.g., \"s3\", \"file\") or path with protocol prefix <code>storage_options</code> <code>Optional[Union[BaseStorageOptions, dict]]</code> Storage configuration as <code>BaseStorageOptions</code> instance or dict <code>cached</code> <code>bool</code> Whether to wrap filesystem in caching layer <code>cache_storage</code> <code>Optional[str]</code> Cache directory path (if <code>cached=True</code>) <code>verbose</code> <code>bool</code> Enable verbose logging for cache operations <code>dirfs</code> <code>bool</code> Whether to wrap the filesystem in a <code>DirFileSystem</code>. Defaults to <code>True</code>. <code>base_fs</code> <code>AbstractFileSystem</code> An existing filesystem to wrap. <code>**kwargs</code> <code>Any</code> Additional filesystem arguments <p>| Ret | :------ | :--- | :---------- | | <code>AbstractFileSystem</code> | <code>fsspec.AbstractFileSystem</code> | Configured filesystem instance |</p> <p>Example:</p> <pre><code># Basic local filesystem\nfs = filesystem(\"file\")\n\n# S3 with storage options\nfrom fsspeckit.storage_options.cloud import AwsStorageOptions\nopts = AwsStorageOptions(region=\"us-west-2\")\nfs = filesystem(\"s3\", storage_options=opts, cached=True)\n\n# Infer protocol from path\nfs = filesystem(\"s3://my-bucket/\", cached=True)\n\n# GitLab filesystem\nfs = filesystem(\"gitlab\", storage_options={\n    \"project_name\": \"group/project\",\n    \"token\": \"glpat_xxxx\"\n})\n</code></pre>"},{"location":"api/fsspeckit.core.base/#get_filesystem","title":"<code>get_filesystem()</code>","text":"<p>Get filesystem instance with enhanced configuration options.</p> <p>Deprecated</p> <p>Use <code>filesystem</code> instead. This function will be removed in a future version.</p> <p>Creates filesystem instances with support for storage options classes, intelligent caching, and protocol inference from paths.</p> Parameter Type Description <code>protocol_or_path</code> <code>str</code> Filesystem protocol (e.g., \"s3\", \"file\") or path with protocol prefix <code>storage_options</code> <code>Optional[Union[BaseStorageOptions, dict]]</code> Storage configuration as <code>BaseStorageOptions</code> instance or dict <code>cached</code> <code>bool</code> Whether to wrap filesystem in caching layer <code>cache_storage</code> <code>Optional[str]</code> Cache directory path (if <code>cached=True</code>) <code>verbose</code> <code>bool</code> Enable verbose logging for cache operations <code>**kwargs</code> <code>Any</code> Additional filesystem arguments Returns Type Description <code>fsspec.AbstractFileSystem</code> <code>fsspec.AbstractFileSystem</code> Configured filesystem instance <p>Example:</p> <pre><code># Basic local filesystem\nfs = get_filesystem(\"file\")\n\n# S3 with storage options\nfrom fsspeckit.storage_options.cloud import AwsStorageOptions\nopts = AwsStorageOptions(region=\"us-west-2\")\nfs = get_filesystem(\"s3\", storage_options=opts, cached=True)\n\n# Infer protocol from path\nfs = get_filesystem(\"s3://my-bucket/\", cached=True)\n\n# GitLab filesystem\nfs = get_filesystem(\"gitlab\", storage_options={\n    \"project_name\": \"group/project\",\n    \"token\": \"glpat_xxxx\"\n})\n</code></pre>"},{"location":"api/fsspeckit.core.ext/","title":"<code>fsspeckit.core.ext</code> API Documentation","text":"<p>This module provides extended functionalities for <code>fsspec.AbstractFileSystem</code>, including methods for reading and writing various file formats (JSON, CSV, Parquet) with advanced options like batch processing, parallelization, and data type optimization. It also includes functions for creating PyArrow datasets.</p>"},{"location":"api/fsspeckit.core.ext/#path_to_glob","title":"<code>path_to_glob()</code>","text":"<p>Convert a path to a glob pattern for file matching.</p> <p>Intelligently converts paths to glob patterns that match files of the specified format, handling various directory and wildcard patterns.</p> Parameter Type Description <code>path</code> <code>str</code> Base path to convert. Can include wildcards (<code>*</code> or <code>**</code>). Examples: \"data/\", \"data/.json\", \"data/*\" <code>format</code> <code>str | None</code> File format to match (without dot). If None, inferred from path. Examples: \"json\", \"csv\", \"parquet\" Returns Type Description <code>str</code> <code>str</code> Glob pattern that matches files of specified format. Examples: \"data/**/.json\", \"data/.csv\" <p>Example:</p> <pre><code># Basic directory\npath_to_glob(\"data\", \"json\")\n# 'data/**/*.json'\n\n# With wildcards\npath_to_glob(\"data/**\", \"csv\")\n# 'data/**/*.csv'\n\n# Format inference\npath_to_glob(\"data/file.parquet\")\n# 'data/file.parquet'\n</code></pre>"},{"location":"api/fsspeckit.core.ext/#read_json_file","title":"<code>read_json_file()</code>","text":"<p>Read a single JSON file from any filesystem.</p> <p>A public wrapper around <code>_read_json_file</code> providing a clean interface for reading individual JSON files.</p> Parameter Type Description <code>self</code> <code>AbstractFileSystem</code> Filesystem instance to use for reading <code>path</code> <code>str</code> Path to JSON file to read <code>include_file_path</code> <code>bool</code> Whether to return dict with filepath as key <code>jsonlines</code> <code>bool</code> Whether to read as JSON Lines format Returns Type Description <code>dict</code> or <code>list[dict]</code> <code>dict</code> or <code>list[dict]</code> Parsed JSON data. For regular JSON, returns a dict. For JSON Lines, returns a list of dicts. If <code>include_file_path=True</code>, returns <code>{filepath: data}</code>. <p>Example:</p> <pre><code>from fsspec.implementations.local import LocalFileSystem\n\nfs = LocalFileSystem()\n# Read regular JSON\ndata = fs.read_json_file(\"config.json\")\nprint(data[\"setting\"])\n# 'value'\n\n# Read JSON Lines with filepath\ndata = fs.read_json_file(\n    \"logs.jsonl\",\n    include_file_path=True,\n    jsonlines=True\n)\nprint(list(data.keys())[0])\n# 'logs.jsonl'\n</code></pre>"},{"location":"api/fsspeckit.core.ext/#read_json","title":"<code>read_json()</code>","text":"<p>Read JSON data from one or more files with powerful options.</p> <p>Provides a flexible interface for reading JSON data with support for:</p> <ul> <li>Single file or multiple files</li> <li>Regular JSON or JSON Lines format</li> <li>Batch processing for large datasets</li> <li>Parallel processing</li> <li>DataFrame conversion</li> <li>File path tracking</li> </ul> Parameter Type Description <code>path</code> <code>str</code> or <code>list[str]</code> Path(s) to JSON file(s). Can be: - Single path string (globs supported) - List of path strings <code>batch_size</code> <code>int | None</code> If set, enables batch reading with this many files per batch <code>include_file_path</code> <code>bool</code> Include source filepath in output <code>jsonlines</code> <code>bool</code> Whether to read as JSON Lines format <code>as_dataframe</code> <code>bool</code> Convert output to Polars DataFrame(s) <code>concat</code> <code>bool</code> Combine multiple files/batches into single result <code>use_threads</code> <code>bool</code> Enable parallel file reading <code>verbose</code> <code>bool</code> Print progress information <code>opt_dtypes</code> <code>bool</code> Optimize DataFrame dtypes for performance <code>**kwargs</code> <code>Any</code> Additional arguments passed to DataFrame conversion Returns Type Description <code>dict</code> or <code>list[dict]</code> or <code>pl.DataFrame</code> or <code>list[pl.DataFrame]</code> or <code>Generator</code> Various types depending on arguments: - <code>dict</code>: Single JSON file as dictionary - <code>list[dict]</code>: Multiple JSON files as list of dictionaries - <code>pl.DataFrame</code>: Single or concatenated DataFrame - <code>list[pl.DataFrame]</code>: List of Dataframes (if <code>concat=False</code>) - <code>Generator</code>: If <code>batch_size</code> set, yields batches of above types <p>Example:</p> <pre><code>from fsspec.implementations.local import LocalFileSystem\n\nfs = LocalFileSystem()\n# Read all JSON files in directory\ndf = fs.read_json(\n    \"data/*.json\",\n    as_dataframe=True,\n    concat=True\n)\nprint(df.shape)\n# (1000, 5)  # Combined data from all files\n\n# Batch process large dataset\nfor batch_df in fs.read_json(\n    \"logs/*.jsonl\",\n    batch_size=100,\n    jsonlines=True,\n    include_file_path=True\n):\n    print(f\"Processing {len(batch_df)} records\")\n\n# Parallel read with custom options\ndfs = fs.read_json(\n    [\"file1.json\", \"file2.json\"],\n    use_threads=True,\n    concat=False,\n    verbose=True\n)\nprint(f\"Read {len(dfs)} files\")\n</code></pre>"},{"location":"api/fsspeckit.core.ext/#read_csv_file","title":"<code>read_csv_file()</code>","text":"<p>Read a single CSV file from any filesystem.</p> <p>Internal function that handles reading individual CSV files and optionally adds the source filepath as a column.</p> Parameter Type Description <code>self</code> <code>AbstractFileSystem</code> Filesystem instance to use for reading <code>path</code> <code>str</code> Path to CSV file <code>include_file_path</code> <code>bool</code> Add source filepath as a column <code>opt_dtypes</code> <code>bool</code> Optimize DataFrame dtypes <code>**kwargs</code> <code>Any</code> Additional arguments passed to <code>pl.read_csv()</code> Returns Type Description <code>pl.DataFrame</code> <code>pl.DataFrame</code> DataFrame containing CSV data <p>Example:</p> <pre><code>from fsspec.implementations.local import LocalFileSystem\n\nfs = LocalFileSystem()\n# This example assumes _read_csv_file is an internal method or needs to be called differently.\n# For public use, you would typically use fs.read_csv().\n# df = fs.read_csv_file(\n#     \"data.csv\",\n#     include_file_path=True,\n#     delimiter=\"|\"\n# )\n# print(\"file_path\" in df.columns)\n# True\n</code></pre>"},{"location":"api/fsspeckit.core.ext/#read_csv","title":"<code>read_csv()</code>","text":"<p>Read CSV data from one or more files with powerful options.</p> <p>Provides a flexible interface for reading CSV files with support for:</p> <ul> <li>Single file or multiple files</li> <li>Batch processing for large datasets</li> <li>Parallel</li> <li>File path tracking</li> <li>Polars DataFrame output</li> </ul> Parameter Type Description <code>path</code> <code>str</code> or <code>list[str]</code> Path(s) to CSV file(s). Can be: - Single path string (globs supported) - List of path strings <code>batch_size</code> <code>int | None</code> If set, enables batch reading with this many files per batch <code>include_file_path</code> <code>bool</code> Add source filepath as a column <code>concat</code> <code>bool</code> Combine multiple files/batches into single DataFrame <code>use_threads</code> <code>bool</code> Enable parallel file reading <code>verbose</code> <code>bool</code> Print progress information <code>**kwargs</code> <code>Any</code> Additional arguments passed to <code>pl.read_csv()</code> Returns Type Description <code>pl.DataFrame</code> or <code>list[pl.DataFrame]</code> or <code>Generator</code> Various types depending on arguments: - <code>pl.DataFrame</code>: Single or concatenated DataFrame - <code>list[pl.DataFrame]</code>: List of DataFrames (if <code>concat=False</code>) - <code>Generator</code>: If <code>batch_size</code> set, yields batches of above types <p>Example:</p> <pre><code>from fsspec.implementations.local import LocalFileSystem\n\nfs = LocalFileSystem()\n# Read all CSVs in directory\ndf = fs.read_csv(\n    \"data/*.csv\",\n    include_file_path=True\n)\nprint(df.columns)\n# ['file_path', 'col1', 'col2', ...]\n\n# Batch process large dataset\nfor batch_df in fs.read_csv(\n    \"logs/*.csv\",\n    batch_size=100,\n    use_threads=True,\n    verbose=True\n):\n    print(f\"Processing {len(batch_df)} rows\")\n\n# Multiple files without concatenation\ndfs = fs.read_csv(\n    [\"file1.csv\", \"file2.csv\"],\n    concat=False,\n    use_threads=True\n)\nprint(f\"Read {len(dfs)} files\")\n</code></pre>"},{"location":"api/fsspeckit.core.ext/#read_parquet_file","title":"<code>read_parquet_file()</code>","text":"<p>Read a single Parquet file from any filesystem.</p> <p>Internal function that handles reading individual Parquet files and optionally adds the source filepath as a column.</p> Parameter Type Description <code>self</code> <code>AbstractFileSystem</code> Filesystem instance to use for reading <code>path</code> <code>str</code> Path to Parquet file <code>include_file_path</code> <code>bool</code> Add source filepath as a column <code>opt_dtypes</code> <code>bool</code> Optimize DataFrame dtypes <code>**kwargs</code> <code>Any</code> Additional arguments passed to <code>pq.read_table()</code> Returns Type Description <code>pa.Table</code> <code>pa.Table</code> PyArrow Table containing Parquet data <p>Example:</p> <pre><code>from fsspec.implementations.local import LocalFileSystem\n\nfs = LocalFileSystem()\n# This example assumes _read_parquet_file is an internal method or needs to be called differently.\n# For public use, you would typically use fs.read_parquet().\n# table = fs.read_parquet_file(\n#     \"data.parquet\",\n#     include_file_path=True,\n#     use_threads=True\n# )\n# print(\"file_path\" in table.column_names)\n# True\n</code></pre>"},{"location":"api/fsspeckit.core.ext/#read_parquet","title":"<code>read_parquet()</code>","text":"<p>Read Parquet data with advanced features and optimizations.</p> <p>Provides a high-performance interface for reading Parquet files with support for:</p> <ul> <li>Single file or multiple files</li> <li>Batch processing for large datasets</li> <li>Parallel processing</li> <li>File path tracking</li> <li>Automatic concatenation</li> <li>PyArrow Table output</li> </ul> <p>The function automatically uses optimal reading strategies:</p> <ul> <li>Direct dataset reading for simple cases</li> <li>Parallel processing for multiple files</li> <li>Batched reading for memory efficiency</li> </ul> Parameter Type Description <code>path</code> <code>str</code> or <code>list[str]</code> Path(s) to Parquet file(s). Can be: - Single path string (globs supported) - List of path strings - Directory containing _metadata file <code>batch_size</code> <code>int | None</code> If set, enables batch reading with this many files per batch <code>include_file_path</code> <code>bool</code> Add source filepath as a column <code>concat</code> <code>bool</code> Combine multiple files/batches into single Table <code>use_threads</code> <code>bool</code> Enable parallel file reading <code>verbose</code> <code>bool</code> Print progress information <code>opt_dtypes</code> <code>bool</code> Optimize Table dtypes for performance <code>**kwargs</code> <code>Any</code> Additional arguments passed to <code>pq.read_table()</code> Returns Type Description <code>pa.Table</code> or <code>list[pa.Table]</code> or <code>Generator</code> Various types depending on arguments: - <code>pa.Table</code>: Single or concatenated Table - <code>list[pa.Table]</code>: List of Tables (if <code>concat=False</code>) - <code>Generator</code>: If <code>batch_size</code> set, yields batches of above types <p>Example:</p> <pre><code>from fsspec.implementations.local import LocalFileSystem\n\nfs = LocalFileSystem()\n# Read all Parquet files in directory\ntable = fs.read_parquet(\n    \"data/*.parquet\",\n    include_file_path=True\n)\nprint(table.column_names)\n# ['file_path', 'col1', 'col2', ...]\n\n# Batch process large dataset\nfor batch in fs.read_parquet(\n    \"data/*.parquet\",\n    batch_size=100,\n    use_threads=True\n):\n    print(f\"Processing {batch.num_rows} rows\")\n\n# Read from directory with metadata\ntable = fs.read_parquet(\n    \"data/\",  # Contains _metadata\n    use_threads=True\n)\nprint(f\"Total rows: {table.num_rows}\")\n</code></pre>"},{"location":"api/fsspeckit.core.ext/#read_files","title":"<code>read_files()</code>","text":"<p>Universal interface for reading data files of any supported format.</p> <p>A unified API that automatically delegates to the appropriate reading function based on file format, while preserving all advanced features like:</p> <ul> <li>Batch processing</li> <li>Parallel reading</li> <li>File path tracking</li> <li>Format-specific optimizations</li> </ul> Parameter Type Description <code>path</code> <code>str</code> or <code>list[str]</code> Path(s) to data file(s). Can be: - Single path string (globs supported) - List of path strings <code>format</code> <code>str</code> File format to read. Supported values: - \"json\": Regular JSON or JSON Lines - \"csv\": CSV files - \"parquet\": Parquet files <code>batch_size</code> <code>int | None</code> If set, enables batch reading with this many files per batch <code>include_file_path</code> <code>bool</code> Add source filepath as column/field <code>concat</code> <code>bool</code> Combine multiple files/batches into single result <code>jsonlines</code> <code>bool</code> For JSON format, whether to read as JSON Lines <code>use_threads</code> <code>bool</code> Enable parallel file reading <code>verbose</code> <code>bool</code> Print progress information <code>opt_dtypes</code> <code>bool</code> Optimize DataFrame/Arrow Table dtypes for performance <code>**kwargs</code> <code>Any</code> Additional format-specific arguments Returns Type Description <code>pl.DataFrame</code> or <code>pa.Table</code> or <code>list[pl.DataFrame]</code> or <code>list[pa.Table]</code> or <code>Generator</code> Various types depending on format and arguments: - <code>pl.DataFrame</code>: For CSV and optionally JSON - <code>pa.Table</code>: For Parquet - <code>list[pl.DataFrame</code> or <code>pa.Table]</code>: Without concatenation - <code>Generator</code>: If <code>batch_size</code> set, yields batches <p>Example:</p> <pre><code>from fsspec.implementations.local import LocalFileSystem\n\nfs = LocalFileSystem()\n# Read CSV files\ndf = fs.read_files(\n    \"data/*.csv\",\n    format=\"csv\",\n    include_file_path=True\n)\nprint(type(df))\n# &lt;class 'polars.DataFrame'&gt;\n\n# Batch process Parquet files\nfor batch in fs.read_files(\n    \"data/*.parquet\",\n    format=\"parquet\",\n    batch_size=100,\n    use_threads=True\n):\n    print(f\"Batch type: {type(batch)}\")\n\n# Read JSON Lines\ndf = fs.read_files(\n    \"logs/*.jsonl\",\n    format=\"json\",\n    jsonlines=True,\n    concat=True\n)\nprint(df.columns)\n</code></pre>"},{"location":"api/fsspeckit.core.ext/#pyarrow_dataset","title":"<code>pyarrow_dataset()</code>","text":"<p>Create a PyArrow dataset from files in any supported format.</p> <p>Creates a dataset that provides optimized reading and querying capabilities including:</p> <ul> <li>Schema inference and enforcement</li> <li>Partition discovery and pruning</li> <li>Predicate pushdown</li> <li>Column projection</li> </ul> Parameter Type Description <code>path</code> <code>str</code> Base path to dataset files <code>format</code> <code>str</code> File format. Currently supports: - \"parquet\" (default) - \"csv\" - \"json\" (experimental) <code>schema</code> <code>pa.Schema | None</code> Optional schema to enforce. If None, inferred from data. <code>partitioning</code> <code>str</code> or <code>list[str]</code> or <code>pds.Partitioning</code> How the dataset is partitioned. Can be: - <code>str</code>: Single partition field - <code>list[str]</code>: Multiple partition fields - <code>pds.Partitioning</code>: Custom partitioning scheme <code>**kwargs</code> <code>Any</code> Additional arguments for dataset creation Returns Type Description <code>pds.Dataset</code> <code>pds.Dataset</code> PyArrow dataset instance <p>Example:</p> <pre><code>from fsspec.implementations.local import LocalFileSystem\n\nfs = LocalFileSystem()\n# Simple Parquet dataset\nds = fs.pyarrow_dataset(\"data/\")\nprint(ds.schema)\n\n# Partitioned dataset\nds = fs.pyarrow_dataset(\n    \"events/\",\n    partitioning=[\"year\", \"month\"]\n)\n# Query with partition pruning\ntable = ds.to_table(\n    filter=(ds.field(\"year\") == 2024)\n)\n\n# CSV with schema\nds = fs.pyarrow_dataset(\n    \"logs/\",\n    format=\"csv\",\n    schema=pa.schema([\n        (\"timestamp\", pa.timestamp(\"s\")),\n        (\"level\", pa.string()),\n        (\"message\", pa.string())\n    ])\n)\n</code></pre>"},{"location":"api/fsspeckit.core.ext/#pyarrow_parquet_dataset","title":"<code>pyarrow_parquet_dataset()</code>","text":"<p>Create a PyArrow dataset optimized for Parquet files.</p> <p>Creates a dataset specifically for Parquet data, automatically handling <code>_metadata</code> files for optimized reading.</p> <p>This function is particularly useful for:</p> <ul> <li>Datasets with existing <code>_metadata</code> files</li> <li>Multi-file datasets that should be treated as one</li> <li>Partitioned Parquet datasets</li> </ul> Parameter Type Description <code>path</code> <code>str</code> Path to dataset directory or <code>_metadata</code> file <code>schema</code> <code>pa.Schema | None</code> Optional schema to enforce. If None, inferred from data. <code>partitioning</code> <code>str</code> or <code>list[str]</code> or <code>pds.Partitioning</code> How the dataset is partitioned. Can be: - <code>str</code>: Single partition field - <code>list[str]</code>: Multiple partition fields - <code>pds.Partitioning</code>: Custom partitioning scheme <code>**kwargs</code> <code>Any</code> Additional dataset arguments Returns Type Description <code>pds.Dataset</code> <code>pds.Dataset</code> PyArrow dataset instance <p>Example:</p> <pre><code>from fsspec.implementations.local import LocalFileSystem\n\nfs = LocalFileSystem()\n# Dataset with _metadata\nds = fs.pyarrow_parquet_dataset(\"data/_metadata\")\nprint(ds.files)  # Shows all data files\n\n# Partitioned dataset directory\nds = fs.pyarrow_parquet_dataset(\n    \"sales/\",\n    partitioning=[\"year\", \"region\"]\n)\n# Query with partition pruning\ntable = ds.to_table(\n    filter=(\n        (ds.field(\"year\") == 2024) &amp;\n        (ds.field(\"region\") == \"EMEA\")\n    )\n)\n</code></pre>"},{"location":"api/fsspeckit.core.filesystem/","title":"fsspeckit.core.filesystem","text":""},{"location":"api/fsspeckit.core.filesystem/#fsspeckit.core.filesystem","title":"filesystem","text":"<p>Core filesystem functionality and utilities.</p>"},{"location":"api/fsspeckit.core.filesystem/#fsspeckit.core.filesystem-classes","title":"Classes","text":""},{"location":"api/fsspeckit.core.filesystem/#fsspeckit.core.filesystem.FileNameCacheMapper","title":"fsspeckit.core.filesystem.FileNameCacheMapper","text":"<pre><code>FileNameCacheMapper(directory: str)\n</code></pre> <p>               Bases: <code>AbstractCacheMapper</code></p> <p>Maps remote file paths to local cache paths while preserving directory structure.</p> <p>This cache mapper maintains the original file path structure in the cache directory, creating necessary subdirectories as needed.</p> <p>Attributes:</p> Name Type Description <code>directory</code> <code>str</code> <p>Base directory for cached files</p> Example <p>Initialize cache mapper with base directory.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>Base directory where cached files will be stored</p> required Source code in <code>src/fsspeckit/core/filesystem.py</code> <pre><code>def __init__(self, directory: str):\n    \"\"\"Initialize cache mapper with base directory.\n\n    Args:\n        directory: Base directory where cached files will be stored\n    \"\"\"\n    self.directory = directory\n</code></pre>"},{"location":"api/fsspeckit.core.filesystem/#fsspeckit.core.filesystem.FileNameCacheMapper--create-cache-mapper-for-s3-files","title":"Create cache mapper for S3 files","text":"<p>mapper = FileNameCacheMapper(\"/tmp/cache\")</p>"},{"location":"api/fsspeckit.core.filesystem/#fsspeckit.core.filesystem.FileNameCacheMapper--map-remote-path-to-cache-path","title":"Map remote path to cache path","text":"<p>cache_path = mapper(\"bucket/data/file.csv\") print(cache_path)  # Preserves structure 'bucket/data/file.csv'</p>"},{"location":"api/fsspeckit.core.filesystem/#fsspeckit.core.filesystem.FileNameCacheMapper-functions","title":"Functions","text":""},{"location":"api/fsspeckit.core.filesystem/#fsspeckit.core.filesystem.FileNameCacheMapper.__call__","title":"fsspeckit.core.filesystem.FileNameCacheMapper.__call__","text":"<pre><code>__call__(path: str) -&gt; str\n</code></pre> <p>Map remote file path to cache file path.</p> <p>Creates necessary subdirectories in the cache directory to maintain the original path structure.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Original file path from remote filesystem</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Cache file path that preserves original structure</p> Example <p>mapper = FileNameCacheMapper(\"/tmp/cache\")</p> Source code in <code>src/fsspeckit/core/filesystem.py</code> <pre><code>def __call__(self, path: str) -&gt; str:\n    \"\"\"Map remote file path to cache file path.\n\n    Creates necessary subdirectories in the cache directory to maintain\n    the original path structure.\n\n    Args:\n        path: Original file path from remote filesystem\n\n    Returns:\n        str: Cache file path that preserves original structure\n\n    Example:\n        &gt;&gt;&gt; mapper = FileNameCacheMapper(\"/tmp/cache\")\n        &gt;&gt;&gt; # Maps maintain directory structure\n        &gt;&gt;&gt; print(mapper(\"data/nested/file.txt\"))\n        'data/nested/file.txt'\n    \"\"\"\n    # os.makedirs(\n    #     posixpath.dirname(posixpath.join(self.directory, path)), exist_ok=True\n    # )\n    os.makedirs(\n        posixpath.dirname(posixpath.join(self.directory, path)), exist_ok=True\n    )\n\n    return path\n</code></pre>"},{"location":"api/fsspeckit.core.filesystem/#fsspeckit.core.filesystem.FileNameCacheMapper.__call__--maps-maintain-directory-structure","title":"Maps maintain directory structure","text":"<p>print(mapper(\"data/nested/file.txt\")) 'data/nested/file.txt'</p>"},{"location":"api/fsspeckit.core.filesystem/#fsspeckit.core.filesystem.GitLabFileSystem","title":"fsspeckit.core.filesystem.GitLabFileSystem","text":"<pre><code>GitLabFileSystem(\n    base_url: str = \"https://gitlab.com\",\n    project_id: Optional[Union[str, int]] = None,\n    project_name: Optional[str] = None,\n    ref: str = \"main\",\n    token: Optional[str] = None,\n    api_version: str = \"v4\",\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>AbstractFileSystem</code></p> <p>Filesystem interface for GitLab repositories.</p> <p>Provides read-only access to files in GitLab repositories, including: - Public and private repositories - Self-hosted GitLab instances - Branch/tag/commit selection - Token-based authentication</p> <p>Attributes:</p> Name Type Description <code>protocol</code> <code>str</code> <p>Always \"gitlab\"</p> <code>base_url</code> <code>str</code> <p>GitLab instance URL</p> <code>project_id</code> <code>str</code> <p>Project ID</p> <code>project_name</code> <code>str</code> <p>Project name/path</p> <code>ref</code> <code>str</code> <p>Git reference (branch, tag, commit)</p> <code>token</code> <code>str</code> <p>Access token</p> <code>api_version</code> <code>str</code> <p>API version</p> Example <p>Initialize GitLab filesystem.</p> <p>Parameters:</p> Name Type Description Default <code>base_url</code> <code>str</code> <p>GitLab instance URL</p> <code>'https://gitlab.com'</code> <code>project_id</code> <code>Optional[Union[str, int]]</code> <p>Project ID number</p> <code>None</code> <code>project_name</code> <code>Optional[str]</code> <p>Project name/path (alternative to project_id)</p> <code>None</code> <code>ref</code> <code>str</code> <p>Git reference (branch, tag, or commit SHA)</p> <code>'main'</code> <code>token</code> <code>Optional[str]</code> <p>GitLab personal access token</p> <code>None</code> <code>api_version</code> <code>str</code> <p>API version to use</p> <code>'v4'</code> <code>**kwargs</code> <p>Additional filesystem arguments</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither project_id nor project_name is provided</p> Source code in <code>src/fsspeckit/core/filesystem.py</code> <pre><code>def __init__(\n    self,\n    base_url: str = \"https://gitlab.com\",\n    project_id: Optional[Union[str, int]] = None,\n    project_name: Optional[str] = None,\n    ref: str = \"main\",\n    token: Optional[str] = None,\n    api_version: str = \"v4\",\n    **kwargs,\n):\n    \"\"\"Initialize GitLab filesystem.\n\n    Args:\n        base_url: GitLab instance URL\n        project_id: Project ID number\n        project_name: Project name/path (alternative to project_id)\n        ref: Git reference (branch, tag, or commit SHA)\n        token: GitLab personal access token\n        api_version: API version to use\n        **kwargs: Additional filesystem arguments\n\n    Raises:\n        ValueError: If neither project_id nor project_name is provided\n    \"\"\"\n    super().__init__(**kwargs)\n\n    if project_id is None and project_name is None:\n        raise ValueError(\"Either project_id or project_name must be provided\")\n\n    self.base_url = base_url.rstrip(\"/\")\n    self.project_id = str(project_id) if project_id else None\n    self.project_name = project_name\n    self.ref = ref\n    self.token = token\n    self.api_version = api_version\n\n    # Build API URL\n    self.api_url = f\"{self.base_url}/api/{self.api_version}\"\n\n    # Determine project identifier for API calls\n    if self.project_id:\n        self.project_identifier = self.project_id\n    else:\n        # URL encode project name\n        self.project_identifier = urllib.parse.quote(self.project_name, safe=\"\")\n\n    # Setup session with authentication\n    self.session = requests.Session()\n    if self.token:\n        self.session.headers[\"Private-Token\"] = self.token\n</code></pre>"},{"location":"api/fsspeckit.core.filesystem/#fsspeckit.core.filesystem.GitLabFileSystem--public-repository","title":"Public repository","text":"<p>fs = GitLabFileSystem( ...     project_name=\"group/project\", ...     ref=\"main\" ... ) files = fs.ls(\"/\")</p>"},{"location":"api/fsspeckit.core.filesystem/#fsspeckit.core.filesystem.GitLabFileSystem--private-repository-with-token","title":"Private repository with token","text":"<p>fs = GitLabFileSystem( ...     project_id=\"12345\", ...     token=\"glpat_xxxx\", ...     ref=\"develop\" ... ) content = fs.cat(\"README.md\")</p>"},{"location":"api/fsspeckit.core.filesystem/#fsspeckit.core.filesystem.GitLabFileSystem-functions","title":"Functions","text":""},{"location":"api/fsspeckit.core.filesystem/#fsspeckit.core.filesystem.GitLabFileSystem.cat","title":"fsspeckit.core.filesystem.GitLabFileSystem.cat","text":"<pre><code>cat(path: str, **kwargs) -&gt; bytes\n</code></pre> <p>Get file contents as bytes.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>File path</p> required <code>**kwargs</code> <p>Additional options</p> <code>{}</code> <p>Returns:</p> Type Description <code>bytes</code> <p>File content as bytes</p> Source code in <code>src/fsspeckit/core/filesystem.py</code> <pre><code>def cat(self, path: str, **kwargs) -&gt; bytes:\n    \"\"\"Get file contents as bytes.\n\n    Args:\n        path: File path\n        **kwargs: Additional options\n\n    Returns:\n        File content as bytes\n    \"\"\"\n    return self._get_file_content(path)\n</code></pre>"},{"location":"api/fsspeckit.core.filesystem/#fsspeckit.core.filesystem.GitLabFileSystem.exists","title":"fsspeckit.core.filesystem.GitLabFileSystem.exists","text":"<pre><code>exists(path: str, **kwargs) -&gt; bool\n</code></pre> <p>Check if file or directory exists.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to check</p> required <code>**kwargs</code> <p>Additional options</p> <code>{}</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if path exists, False otherwise</p> Source code in <code>src/fsspeckit/core/filesystem.py</code> <pre><code>def exists(self, path: str, **kwargs) -&gt; bool:\n    \"\"\"Check if file or directory exists.\n\n    Args:\n        path: Path to check\n        **kwargs: Additional options\n\n    Returns:\n        True if path exists, False otherwise\n    \"\"\"\n    try:\n        self._get_file_content(path)\n        return True\n    except FileNotFoundError:\n        return False\n</code></pre>"},{"location":"api/fsspeckit.core.filesystem/#fsspeckit.core.filesystem.GitLabFileSystem.info","title":"fsspeckit.core.filesystem.GitLabFileSystem.info","text":"<pre><code>info(path: str, **kwargs) -&gt; dict\n</code></pre> <p>Get file information.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>File path</p> required <code>**kwargs</code> <p>Additional options</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with file information</p> Source code in <code>src/fsspeckit/core/filesystem.py</code> <pre><code>def info(self, path: str, **kwargs) -&gt; dict:\n    \"\"\"Get file information.\n\n    Args:\n        path: File path\n        **kwargs: Additional options\n\n    Returns:\n        Dictionary with file information\n    \"\"\"\n    # For simplicity, we'll use the file content request\n    # In a production implementation, you might want to use a more efficient endpoint\n    try:\n        content = self._get_file_content(path)\n        return {\n            \"name\": path,\n            \"size\": len(content),\n            \"type\": \"file\",\n        }\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"File not found: {path}\")\n</code></pre>"},{"location":"api/fsspeckit.core.filesystem/#fsspeckit.core.filesystem.GitLabFileSystem.ls","title":"fsspeckit.core.filesystem.GitLabFileSystem.ls","text":"<pre><code>ls(path: str = '', detail: bool = True, **kwargs) -&gt; list\n</code></pre> <p>List directory contents.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Directory path to list</p> <code>''</code> <code>detail</code> <code>bool</code> <p>Whether to return detailed information</p> <code>True</code> <code>**kwargs</code> <p>Additional options</p> <code>{}</code> <p>Returns:</p> Type Description <code>list</code> <p>List of files/directories or their details</p> Source code in <code>src/fsspeckit/core/filesystem.py</code> <pre><code>def ls(self, path: str = \"\", detail: bool = True, **kwargs) -&gt; list:\n    \"\"\"List directory contents.\n\n    Args:\n        path: Directory path to list\n        detail: Whether to return detailed information\n        **kwargs: Additional options\n\n    Returns:\n        List of files/directories or their details\n    \"\"\"\n    path = path.lstrip(\"/\")\n\n    url = f\"{self.api_url}/projects/{self.project_identifier}/repository/tree\"\n    params = {\"ref\": self.ref, \"path\": path, \"recursive\": False}\n\n    response = self.session.get(url, params=params)\n    response.raise_for_status()\n\n    items = response.json()\n\n    if detail:\n        return [\n            {\n                \"name\": posixpath.join(path, item[\"name\"])\n                if path\n                else item[\"name\"],\n                \"size\": None,  # GitLab API doesn't provide size in tree endpoint\n                \"type\": \"directory\" if item[\"type\"] == \"tree\" else \"file\",\n                \"id\": item[\"id\"],\n            }\n            for item in items\n        ]\n    else:\n        return [\n            posixpath.join(path, item[\"name\"]) if path else item[\"name\"]\n            for item in items\n        ]\n</code></pre>"},{"location":"api/fsspeckit.core.filesystem/#fsspeckit.core.filesystem.MonitoredSimpleCacheFileSystem","title":"fsspeckit.core.filesystem.MonitoredSimpleCacheFileSystem","text":"<pre><code>MonitoredSimpleCacheFileSystem(\n    fs: Optional[AbstractFileSystem] = None,\n    cache_storage: str = \"~/.cache/fsspec\",\n    verbose: bool = False,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>SimpleCacheFileSystem</code></p> <p>Initialize monitored cache filesystem.</p> <p>Parameters:</p> Name Type Description Default <code>fs</code> <code>Optional[AbstractFileSystem]</code> <p>Underlying filesystem to cache. If None, creates a local filesystem.</p> <code>None</code> <code>cache_storage</code> <code>str</code> <p>Cache storage location(s). Can be string path or list of paths.</p> <code>'~/.cache/fsspec'</code> <code>verbose</code> <code>bool</code> <p>Whether to enable verbose logging of cache operations.</p> <code>False</code> <code>**kwargs</code> <p>Additional arguments passed to SimpleCacheFileSystem.</p> <code>{}</code> Example Source code in <code>src/fsspeckit/core/filesystem.py</code> <pre><code>def __init__(\n    self,\n    fs: Optional[fsspec.AbstractFileSystem] = None,\n    cache_storage: str = \"~/.cache/fsspec\",\n    verbose: bool = False,\n    **kwargs,\n):\n    \"\"\"Initialize monitored cache filesystem.\n\n    Args:\n        fs: Underlying filesystem to cache. If None, creates a local filesystem.\n        cache_storage: Cache storage location(s). Can be string path or list of paths.\n        verbose: Whether to enable verbose logging of cache operations.\n        **kwargs: Additional arguments passed to SimpleCacheFileSystem.\n\n    Example:\n        &gt;&gt;&gt; # Cache S3 filesystem\n        &gt;&gt;&gt; s3_fs = filesystem(\"s3\")\n        &gt;&gt;&gt; cached = MonitoredSimpleCacheFileSystem(\n        ...     fs=s3_fs,\n        ...     cache_storage=\"/tmp/s3_cache\",\n        ...     verbose=True\n        ... )\n    \"\"\"\n    self._verbose = verbose\n\n    # # Initialize with expanded cache storage paths\n    # expanded_storage = os.path.expanduser(cache_storage)\n    # super().__init__(\n    #     fs=fs,\n    #     cache_storage=expanded_storage,\n    #     cache_mapper=FileNameCacheMapper(expanded_storage),\n    #     **kwargs,\n    # )\n    # kwargs[\"cache_storage\"] = os.path.join(kwargs.get(\"cache_storage\"), \"123\")\n    super().__init__(fs=fs, cache_storage=cache_storage, **kwargs)\n    self._mapper = FileNameCacheMapper(cache_storage)\n\n    if self._verbose:\n        logger.info(f\"Initialized cache filesystem with storage: {cache_storage}\")\n</code></pre>"},{"location":"api/fsspeckit.core.filesystem/#fsspeckit.core.filesystem.MonitoredSimpleCacheFileSystem--cache-s3-filesystem","title":"Cache S3 filesystem","text":"<p>s3_fs = filesystem(\"s3\") cached = MonitoredSimpleCacheFileSystem( ...     fs=s3_fs, ...     cache_storage=\"/tmp/s3_cache\", ...     verbose=True ... )</p>"},{"location":"api/fsspeckit.core.filesystem/#fsspeckit.core.filesystem.MonitoredSimpleCacheFileSystem-functions","title":"Functions","text":""},{"location":"api/fsspeckit.core.filesystem/#fsspeckit.core.filesystem.MonitoredSimpleCacheFileSystem.open","title":"fsspeckit.core.filesystem.MonitoredSimpleCacheFileSystem.open","text":"<pre><code>open(path, mode='rb', **kwargs)\n</code></pre> <p>Open a file. If the file's path does not match the cache regex, bypass the caching and read directly from the underlying filesystem.</p> Source code in <code>src/fsspeckit/core/filesystem.py</code> <pre><code>def open(self, path, mode=\"rb\", **kwargs):\n    \"\"\"\n    Open a file. If the file's path does not match the cache regex, bypass the\n    caching and read directly from the underlying filesystem.\n    \"\"\"\n    # if not ICEBERG_FILE_REGEX.search(path):\n    #     # bypass caching.\n    #     return self.fs.open(path, mode=mode, **kwargs)\n\n    return super().open(path, mode=mode, **kwargs)\n</code></pre>"},{"location":"api/fsspeckit.core.filesystem/#fsspeckit.core.filesystem.MonitoredSimpleCacheFileSystem.size","title":"fsspeckit.core.filesystem.MonitoredSimpleCacheFileSystem.size","text":"<pre><code>size(path: str) -&gt; int\n</code></pre> <p>Get size of file in bytes.</p> <p>Checks cache first, falls back to remote filesystem.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to file</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Size of file in bytes</p> Example <p>fs = MonitoredSimpleCacheFileSystem( ...     fs=remote_fs, ...     cache_storage=\"/tmp/cache\" ... ) size = fs.size(\"large_file.dat\") print(f\"File size: {size} bytes\")</p> Source code in <code>src/fsspeckit/core/filesystem.py</code> <pre><code>def size(self, path: str) -&gt; int:\n    \"\"\"Get size of file in bytes.\n\n    Checks cache first, falls back to remote filesystem.\n\n    Args:\n        path: Path to file\n\n    Returns:\n        int: Size of file in bytes\n\n    Example:\n        &gt;&gt;&gt; fs = MonitoredSimpleCacheFileSystem(\n        ...     fs=remote_fs,\n        ...     cache_storage=\"/tmp/cache\"\n        ... )\n        &gt;&gt;&gt; size = fs.size(\"large_file.dat\")\n        &gt;&gt;&gt; print(f\"File size: {size} bytes\")\n    \"\"\"\n    cached_file = self._check_file(self._strip_protocol(path))\n    if cached_file is None:\n        return self.fs.size(path)\n    else:\n        return posixpath.getsize(cached_file)\n</code></pre>"},{"location":"api/fsspeckit.core.filesystem/#fsspeckit.core.filesystem.MonitoredSimpleCacheFileSystem.sync_cache","title":"fsspeckit.core.filesystem.MonitoredSimpleCacheFileSystem.sync_cache","text":"<pre><code>sync_cache(reload: bool = False) -&gt; None\n</code></pre> <p>Synchronize cache with remote filesystem.</p> <p>Downloads all files in remote path to cache if not present.</p> <p>Parameters:</p> Name Type Description Default <code>reload</code> <code>bool</code> <p>Whether to force reload all files, ignoring existing cache</p> <code>False</code> Example <p>fs = MonitoredSimpleCacheFileSystem( ...     fs=remote_fs, ...     cache_storage=\"/tmp/cache\" ... )</p> Source code in <code>src/fsspeckit/core/filesystem.py</code> <pre><code>def sync_cache(self, reload: bool = False) -&gt; None:\n    \"\"\"Synchronize cache with remote filesystem.\n\n    Downloads all files in remote path to cache if not present.\n\n    Args:\n        reload: Whether to force reload all files, ignoring existing cache\n\n    Example:\n        &gt;&gt;&gt; fs = MonitoredSimpleCacheFileSystem(\n        ...     fs=remote_fs,\n        ...     cache_storage=\"/tmp/cache\"\n        ... )\n        &gt;&gt;&gt; # Initial sync\n        &gt;&gt;&gt; fs.sync_cache()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Force reload all files\n        &gt;&gt;&gt; fs.sync_cache(reload=True)\n    \"\"\"\n    if reload:\n        if hasattr(self, \"clear_cache\"):\n            self.clear_cache()\n\n    files = self.glob(\"**/*\")\n    [self.open(f, mode=\"rb\").close() for f in files if self.isfile(f)]\n</code></pre>"},{"location":"api/fsspeckit.core.filesystem/#fsspeckit.core.filesystem.MonitoredSimpleCacheFileSystem.sync_cache--initial-sync","title":"Initial sync","text":"<p>fs.sync_cache()</p>"},{"location":"api/fsspeckit.core.filesystem/#fsspeckit.core.filesystem.MonitoredSimpleCacheFileSystem.sync_cache--force-reload-all-files","title":"Force reload all files","text":"<p>fs.sync_cache(reload=True)</p>"},{"location":"api/fsspeckit.core.filesystem/#fsspeckit.core.filesystem-functions","title":"Functions","text":""},{"location":"api/fsspeckit.core.filesystem/#fsspeckit.core.filesystem.filesystem","title":"fsspeckit.core.filesystem.filesystem","text":"<pre><code>filesystem(\n    protocol_or_path: str | None = \"\",\n    storage_options: Optional[\n        Union[BaseStorageOptions, dict]\n    ] = None,\n    cached: bool = False,\n    cache_storage: Optional[str] = None,\n    verbose: bool = False,\n    dirfs: bool = True,\n    base_fs: AbstractFileSystem = None,\n    use_listings_cache=True,\n    skip_instance_cache=False,\n    **kwargs,\n) -&gt; AbstractFileSystem\n</code></pre> <p>Get filesystem instance with enhanced configuration options.</p> <p>Creates filesystem instances with support for storage options classes, intelligent caching, and protocol inference from paths.</p> <p>Parameters:</p> Name Type Description Default <code>protocol_or_path</code> <code>str | None</code> <p>Filesystem protocol (e.g., \"s3\", \"file\") or path with protocol prefix</p> <code>''</code> <code>storage_options</code> <code>Optional[Union[BaseStorageOptions, dict]]</code> <p>Storage configuration as BaseStorageOptions instance or dict</p> <code>None</code> <code>cached</code> <code>bool</code> <p>Whether to wrap filesystem in caching layer</p> <code>False</code> <code>cache_storage</code> <code>Optional[str]</code> <p>Cache directory path (if cached=True)</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Enable verbose logging for cache operations</p> <code>False</code> <code>dirfs</code> <code>bool</code> <p>Whether to wrap filesystem in DirFileSystem</p> <code>True</code> <code>base_fs</code> <code>AbstractFileSystem</code> <p>Base filesystem instance to use</p> <code>None</code> <code>use_listings_cache</code> <p>Whether to enable directory-listing cache</p> <code>True</code> <code>skip_instance_cache</code> <p>Whether to skip fsspec instance caching</p> <code>False</code> <code>**kwargs</code> <p>Additional filesystem arguments</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>AbstractFileSystem</code> <code>AbstractFileSystem</code> <p>Configured filesystem instance</p> Example Source code in <code>src/fsspeckit/core/filesystem.py</code> <pre><code>def filesystem(\n    protocol_or_path: str | None = \"\",\n    storage_options: Optional[Union[BaseStorageOptions, dict]] = None,\n    cached: bool = False,\n    cache_storage: Optional[str] = None,\n    verbose: bool = False,\n    dirfs: bool = True,\n    base_fs: AbstractFileSystem = None,\n    use_listings_cache=True,  # \u2190 disable directory-listing cache\n    skip_instance_cache=False,\n    **kwargs,\n) -&gt; AbstractFileSystem:\n    \"\"\"Get filesystem instance with enhanced configuration options.\n\n    Creates filesystem instances with support for storage options classes,\n    intelligent caching, and protocol inference from paths.\n\n    Args:\n        protocol_or_path: Filesystem protocol (e.g., \"s3\", \"file\") or path with protocol prefix\n        storage_options: Storage configuration as BaseStorageOptions instance or dict\n        cached: Whether to wrap filesystem in caching layer\n        cache_storage: Cache directory path (if cached=True)\n        verbose: Enable verbose logging for cache operations\n        dirfs: Whether to wrap filesystem in DirFileSystem\n        base_fs: Base filesystem instance to use\n        use_listings_cache: Whether to enable directory-listing cache\n        skip_instance_cache: Whether to skip fsspec instance caching\n        **kwargs: Additional filesystem arguments\n\n    Returns:\n        AbstractFileSystem: Configured filesystem instance\n\n    Example:\n        &gt;&gt;&gt; # Basic local filesystem\n        &gt;&gt;&gt; fs = filesystem(\"file\")\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # S3 with storage options\n        &gt;&gt;&gt; from fsspeckit.storage import AwsStorageOptions\n        &gt;&gt;&gt; opts = AwsStorageOptions(region=\"us-west-2\")\n        &gt;&gt;&gt; fs = filesystem(\"s3\", storage_options=opts, cached=True)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Infer protocol from path\n        &gt;&gt;&gt; fs = filesystem(\"s3://my-bucket/\", cached=True)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # GitLab filesystem\n        &gt;&gt;&gt; fs = filesystem(\"gitlab\", storage_options={\n        ...     \"project_name\": \"group/project\",\n        ...     \"token\": \"glpat_xxxx\"\n        ... })\n    \"\"\"\n    if isinstance(protocol_or_path, Path):\n        protocol_or_path = protocol_or_path.as_posix()\n\n    raw_input = _ensure_string(protocol_or_path)\n    protocol_from_kwargs = kwargs.pop(\"protocol\", None)\n\n    provided_protocol: str | None = None\n    base_path_input: str = \"\"\n\n    if raw_input:\n        provided_protocol, remainder = split_protocol(raw_input)\n        if provided_protocol:\n            base_path_input = remainder or \"\"\n        else:\n            base_path_input = remainder or raw_input\n            if base_fs is None and base_path_input in known_implementations:\n                provided_protocol = base_path_input\n                base_path_input = \"\"\n    else:\n        base_path_input = \"\"\n\n    base_path_input = base_path_input.replace(\"\\\\\", \"/\")\n\n    if (\n        base_fs is None\n        and base_path_input\n        and (provided_protocol or protocol_from_kwargs) in {None, \"file\", \"local\"}\n    ):\n        detected_parent, is_file = _detect_local_file_path(base_path_input)\n        if is_file:\n            base_path_input = detected_parent\n\n    base_path = _normalize_path(base_path_input)\n    cache_path_hint = base_path\n\n    if base_fs is not None:\n        if not dirfs:\n            raise ValueError(\"dirfs must be True when providing base_fs\")\n\n        base_is_dir = isinstance(base_fs, DirFileSystem)\n        underlying_fs = base_fs.fs if base_is_dir else base_fs\n        underlying_protocols = _protocol_set(underlying_fs.protocol)\n        requested_protocol = provided_protocol or protocol_from_kwargs\n\n        if requested_protocol and not _protocol_matches(\n            requested_protocol, underlying_protocols\n        ):\n            raise ValueError(\n                f\"Protocol '{requested_protocol}' does not match base filesystem protocol \"\n                f\"{sorted(underlying_protocols)}\"\n            )\n\n        sep = getattr(underlying_fs, \"sep\", \"/\") or \"/\"\n        base_root = base_fs.path if base_is_dir else \"\"\n        base_root_norm = _normalize_path(base_root, sep)\n        cache_path_hint = base_root_norm\n\n        fs: AbstractFileSystem\n        path_for_cache = base_root_norm\n\n        if requested_protocol:\n            absolute_target = _strip_for_fs(underlying_fs, raw_input)\n            absolute_target = _normalize_path(absolute_target, sep)\n\n            if (\n                base_is_dir\n                and base_root_norm\n                and not _is_within(base_root_norm, absolute_target, sep)\n            ):\n                raise ValueError(\n                    f\"Requested path '{absolute_target}' is outside the base directory \"\n                    f\"'{base_root_norm}'\"\n                )\n\n            if base_is_dir and absolute_target == base_root_norm:\n                fs = base_fs\n            else:\n                fs = DirFileSystem(path=absolute_target, fs=underlying_fs)\n\n            path_for_cache = absolute_target\n        else:\n            rel_input = base_path\n            if rel_input:\n                segments = [segment for segment in rel_input.split(sep) if segment]\n                if any(segment == \"..\" for segment in segments):\n                    raise ValueError(\n                        \"Relative paths must not escape the base filesystem root\"\n                    )\n\n                candidate = _normalize_path(rel_input, sep)\n                absolute_target = _smart_join(base_root_norm, candidate, sep)\n\n                if (\n                    base_is_dir\n                    and base_root_norm\n                    and not _is_within(base_root_norm, absolute_target, sep)\n                ):\n                    raise ValueError(\n                        f\"Resolved path '{absolute_target}' is outside the base \"\n                        f\"directory '{base_root_norm}'\"\n                    )\n\n                if base_is_dir and absolute_target == base_root_norm:\n                    fs = base_fs\n                else:\n                    fs = DirFileSystem(path=absolute_target, fs=underlying_fs)\n\n                path_for_cache = absolute_target\n            else:\n                fs = base_fs\n                path_for_cache = base_root_norm\n\n        cache_path_hint = path_for_cache\n\n        if cached:\n            if getattr(fs, \"is_cache_fs\", False):\n                return fs\n            storage = cache_storage\n            if storage is None:\n                storage = _default_cache_storage(cache_path_hint or None)\n            cached_fs = MonitoredSimpleCacheFileSystem(\n                fs=fs, cache_storage=storage, verbose=verbose\n            )\n            cached_fs.is_cache_fs = True\n            return cached_fs\n\n        if not hasattr(fs, \"is_cache_fs\"):\n            fs.is_cache_fs = False\n        return fs\n\n    protocol = provided_protocol or protocol_from_kwargs\n    if protocol is None:\n        if isinstance(storage_options, dict):\n            protocol = storage_options.get(\"protocol\")\n        else:\n            protocol = getattr(storage_options, \"protocol\", None)\n\n    protocol = protocol or \"file\"\n    protocol = protocol.lower()\n\n    if protocol in {\"file\", \"local\"}:\n        fs = fsspec_filesystem(\n            protocol,\n            use_listings_cache=use_listings_cache,\n            skip_instance_cache=skip_instance_cache,\n        )\n        if dirfs:\n            dir_path: str | Path = base_path or Path.cwd()\n            fs = DirFileSystem(path=dir_path, fs=fs)\n            cache_path_hint = _ensure_string(dir_path)\n        if not hasattr(fs, \"is_cache_fs\"):\n            fs.is_cache_fs = False\n    else:\n        storage_opts = storage_options\n        if isinstance(storage_opts, dict):\n            storage_opts = storage_options_from_dict(protocol, storage_opts)\n        if storage_opts is None:\n            storage_opts = storage_options_from_dict(protocol, kwargs)\n        fs = storage_opts.to_filesystem(\n            use_listings_cache=use_listings_cache,\n            skip_instance_cache=skip_instance_cache,\n        )\n        if dirfs and base_path:\n            fs = DirFileSystem(path=base_path, fs=fs)\n            cache_path_hint = base_path\n        if not hasattr(fs, \"is_cache_fs\"):\n            fs.is_cache_fs = False\n\n    if cached:\n        if getattr(fs, \"is_cache_fs\", False):\n            return fs\n        storage = cache_storage\n        if storage is None:\n            storage = _default_cache_storage(cache_path_hint or None)\n        cached_fs = MonitoredSimpleCacheFileSystem(\n            fs=fs, cache_storage=storage, verbose=verbose\n        )\n        cached_fs.is_cache_fs = True\n        return cached_fs\n\n    if not hasattr(fs, \"is_cache_fs\"):\n        fs.is_cache_fs = False\n\n    return fs\n</code></pre>"},{"location":"api/fsspeckit.core.filesystem/#fsspeckit.core.filesystem.filesystem--basic-local-filesystem","title":"Basic local filesystem","text":"<p>fs = filesystem(\"file\")</p>"},{"location":"api/fsspeckit.core.filesystem/#fsspeckit.core.filesystem.filesystem--s3-with-storage-options","title":"S3 with storage options","text":"<p>from fsspeckit.storage import AwsStorageOptions opts = AwsStorageOptions(region=\"us-west-2\") fs = filesystem(\"s3\", storage_options=opts, cached=True)</p>"},{"location":"api/fsspeckit.core.filesystem/#fsspeckit.core.filesystem.filesystem--infer-protocol-from-path","title":"Infer protocol from path","text":"<p>fs = filesystem(\"s3://my-bucket/\", cached=True)</p>"},{"location":"api/fsspeckit.core.filesystem/#fsspeckit.core.filesystem.filesystem--gitlab-filesystem","title":"GitLab filesystem","text":"<p>fs = filesystem(\"gitlab\", storage_options={ ...     \"project_name\": \"group/project\", ...     \"token\": \"glpat_xxxx\" ... })</p>"},{"location":"api/fsspeckit.core.filesystem/#fsspeckit.core.filesystem.get_filesystem","title":"fsspeckit.core.filesystem.get_filesystem","text":"<pre><code>get_filesystem(\n    protocol_or_path: str | None = None,\n    storage_options: Optional[\n        Union[BaseStorageOptions, dict]\n    ] = None,\n    cached: bool = False,\n    cache_storage: Optional[str] = None,\n    verbose: bool = False,\n    **kwargs,\n) -&gt; AbstractFileSystem\n</code></pre> <p>Get filesystem instance with enhanced configuration options.</p> <p>.. deprecated:: 0.1.0     Use :func:<code>filesystem</code> instead. This function will be removed in a future version.</p> <p>Creates filesystem instances with support for storage options classes, intelligent caching, and protocol inference from paths.</p> <p>Parameters:</p> Name Type Description Default <code>protocol_or_path</code> <code>str | None</code> <p>Filesystem protocol (e.g., \"s3\", \"file\") or path with protocol prefix</p> <code>None</code> <code>storage_options</code> <code>Optional[Union[BaseStorageOptions, dict]]</code> <p>Storage configuration as BaseStorageOptions instance or dict</p> <code>None</code> <code>cached</code> <code>bool</code> <p>Whether to wrap filesystem in caching layer</p> <code>False</code> <code>cache_storage</code> <code>Optional[str]</code> <p>Cache directory path (if cached=True)</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Enable verbose logging for cache operations</p> <code>False</code> <code>**kwargs</code> <p>Additional filesystem arguments</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>AbstractFileSystem</code> <code>AbstractFileSystem</code> <p>Configured filesystem instance</p> Example Source code in <code>src/fsspeckit/core/filesystem.py</code> <pre><code>def get_filesystem(\n    protocol_or_path: str | None = None,\n    storage_options: Optional[Union[BaseStorageOptions, dict]] = None,\n    cached: bool = False,\n    cache_storage: Optional[str] = None,\n    verbose: bool = False,\n    **kwargs,\n) -&gt; fsspec.AbstractFileSystem:\n    \"\"\"Get filesystem instance with enhanced configuration options.\n\n    .. deprecated:: 0.1.0\n        Use :func:`filesystem` instead. This function will be removed in a future version.\n\n    Creates filesystem instances with support for storage options classes,\n    intelligent caching, and protocol inference from paths.\n\n    Args:\n        protocol_or_path: Filesystem protocol (e.g., \"s3\", \"file\") or path with protocol prefix\n        storage_options: Storage configuration as BaseStorageOptions instance or dict\n        cached: Whether to wrap filesystem in caching layer\n        cache_storage: Cache directory path (if cached=True)\n        verbose: Enable verbose logging for cache operations\n        **kwargs: Additional filesystem arguments\n\n    Returns:\n        AbstractFileSystem: Configured filesystem instance\n\n    Example:\n        &gt;&gt;&gt; # Basic local filesystem\n        &gt;&gt;&gt; fs = get_filesystem(\"file\")\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # S3 with storage options\n        &gt;&gt;&gt; from fsspeckit.storage import AwsStorageOptions\n        &gt;&gt;&gt; opts = AwsStorageOptions(region=\"us-west-2\")\n        &gt;&gt;&gt; fs = get_filesystem(\"s3\", storage_options=opts, cached=True)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Infer protocol from path\n        &gt;&gt;&gt; fs = get_filesystem(\"s3://my-bucket/\", cached=True)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # GitLab filesystem\n        &gt;&gt;&gt; fs = get_filesystem(\"gitlab\", storage_options={\n        ...     \"project_name\": \"group/project\",\n        ...     \"token\": \"glpat_xxxx\"\n        ... })\n    \"\"\"\n    warnings.warn(\n        \"get_filesystem() is deprecated and will be removed in a future version. \"\n        \"Use filesystem() instead.\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n    return filesystem(\n        protocol_or_path=protocol_or_path,\n        storage_options=storage_options,\n        cached=cached,\n        cache_storage=cache_storage,\n        verbose=verbose,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/fsspeckit.core.filesystem/#fsspeckit.core.filesystem.get_filesystem--basic-local-filesystem","title":"Basic local filesystem","text":"<p>fs = get_filesystem(\"file\")</p>"},{"location":"api/fsspeckit.core.filesystem/#fsspeckit.core.filesystem.get_filesystem--s3-with-storage-options","title":"S3 with storage options","text":"<p>from fsspeckit.storage import AwsStorageOptions opts = AwsStorageOptions(region=\"us-west-2\") fs = get_filesystem(\"s3\", storage_options=opts, cached=True)</p>"},{"location":"api/fsspeckit.core.filesystem/#fsspeckit.core.filesystem.get_filesystem--infer-protocol-from-path","title":"Infer protocol from path","text":"<p>fs = get_filesystem(\"s3://my-bucket/\", cached=True)</p>"},{"location":"api/fsspeckit.core.filesystem/#fsspeckit.core.filesystem.get_filesystem--gitlab-filesystem","title":"GitLab filesystem","text":"<p>fs = get_filesystem(\"gitlab\", storage_options={ ...     \"project_name\": \"group/project\", ...     \"token\": \"glpat_xxxx\" ... })</p>"},{"location":"api/fsspeckit.core.filesystem/#fsspeckit.core.filesystem.setup_filesystem_logging","title":"fsspeckit.core.filesystem.setup_filesystem_logging","text":"<pre><code>setup_filesystem_logging()\n</code></pre> <p>Setup logging configuration for filesystem operations.</p> <p>This function configures logging for filesystem-related operations, providing appropriate log levels and formatters for debugging and monitoring filesystem activity.</p> Source code in <code>src/fsspeckit/core/filesystem.py</code> <pre><code>def setup_filesystem_logging():\n    \"\"\"Setup logging configuration for filesystem operations.\n\n    This function configures logging for filesystem-related operations,\n    providing appropriate log levels and formatters for debugging\n    and monitoring filesystem activity.\n    \"\"\"\n    # Implementation would go here - this is a placeholder\n    # to match what was referenced in the plan\n    pass\n</code></pre>"},{"location":"api/fsspeckit.core.maintenance/","title":"fsspeckit.core.maintenance","text":""},{"location":"api/fsspeckit.core.maintenance/#fsspeckit.core.maintenance","title":"maintenance","text":"<p>Backend-neutral maintenance layer for parquet dataset operations.</p> <p>This module provides shared functionality for dataset discovery, statistics, and grouping algorithms used by both DuckDB and PyArrow maintenance operations. It serves as the authoritative implementation for maintenance planning, ensuring consistent behavior across different backends.</p> <p>Key responsibilities: 1. Dataset discovery and file-level statistics 2. Compaction grouping algorithms with streaming execution 3. Optimization planning with z-order validation 4. Canonical statistics structures 5. Partition filtering and edge case handling</p> <p>Architecture: - Functions accept both dict format (legacy) and FileInfo objects for backward compatibility - All planning functions return structured results with canonical MaintenanceStats - Backend implementations delegate to this core for consistent behavior - Streaming design avoids materializing entire datasets in memory</p> <p>Core components: - FileInfo: Canonical file information with validation - MaintenanceStats: Canonical statistics structure across backends - CompactionGroup: Logical grouping of files for processing - collect_dataset_stats: Dataset discovery with partition filtering - plan_compaction_groups: Shared compaction planning algorithm - plan_optimize_groups: Shared optimization planning with z-order validation</p> <p>Usage: Backend functions should delegate to this module rather than implementing their own discovery and planning logic. This ensures that DuckDB and PyArrow produce identical grouping decisions and statistics structures.</p>"},{"location":"api/fsspeckit.core.maintenance/#fsspeckit.core.maintenance-classes","title":"Classes","text":""},{"location":"api/fsspeckit.core.maintenance/#fsspeckit.core.maintenance.CompactionGroup","title":"fsspeckit.core.maintenance.CompactionGroup  <code>dataclass</code>","text":"<pre><code>CompactionGroup(files: list[FileInfo])\n</code></pre> <p>A group of files to be compacted or optimized together.</p> <p>This dataclass represents a logical grouping of files that will be processed together during maintenance operations. It enables streaming execution by bounding the amount of data processed at once.</p> <p>Attributes:</p> Name Type Description <code>files</code> <code>list[FileInfo]</code> <p>List of FileInfo objects in this group.</p> <code>total_size_bytes</code> <code>int</code> <p>Total size of all files in this group (computed).</p> <code>total_rows</code> <code>int</code> <p>Total rows across all files in this group (computed).</p> Note <p>Must contain at least one file. The total_size_bytes and total_rows are computed during initialization and used for planning decisions. This structure enables per-group streaming processing without materializing entire datasets.</p>"},{"location":"api/fsspeckit.core.maintenance/#fsspeckit.core.maintenance.FileInfo","title":"fsspeckit.core.maintenance.FileInfo  <code>dataclass</code>","text":"<pre><code>FileInfo(path: str, size_bytes: int, num_rows: int)\n</code></pre> <p>Information about a single parquet file with validation.</p> <p>This canonical data structure represents file metadata across all backends. It enables consistent file information handling and size-based planning.</p> <p>Attributes:</p> Name Type Description <code>path</code> <code>str</code> <p>File path relative to the dataset root.</p> <code>size_bytes</code> <code>int</code> <p>File size in bytes; must be &gt;= 0.</p> <code>num_rows</code> <code>int</code> <p>Number of rows in the file; must be &gt;= 0.</p> Note <p>The size_bytes and num_rows values are validated to be non-negative. This class is used throughout the maintenance planning pipeline for consistent file metadata representation.</p>"},{"location":"api/fsspeckit.core.maintenance/#fsspeckit.core.maintenance.MaintenanceStats","title":"fsspeckit.core.maintenance.MaintenanceStats  <code>dataclass</code>","text":"<pre><code>MaintenanceStats(\n    before_file_count: int,\n    after_file_count: int,\n    before_total_bytes: int,\n    after_total_bytes: int,\n    compacted_file_count: int,\n    rewritten_bytes: int,\n    compression_codec: str | None = None,\n    dry_run: bool = False,\n    zorder_columns: list[str] | None = None,\n    planned_groups: list[list[str]] | None = None,\n)\n</code></pre> <p>Canonical statistics structure for maintenance operations.</p> <p>This dataclass provides the authoritative statistics format for all maintenance operations across DuckDB and PyArrow backends. It ensures consistent reporting and enables unified testing and validation.</p> <p>Attributes:</p> Name Type Description <code>before_file_count</code> <code>int</code> <p>Number of files before the operation.</p> <code>after_file_count</code> <code>int</code> <p>Number of files after the operation.</p> <code>before_total_bytes</code> <code>int</code> <p>Total bytes before the operation.</p> <code>after_total_bytes</code> <code>int</code> <p>Total bytes after the operation.</p> <code>compacted_file_count</code> <code>int</code> <p>Number of files that were compacted/rewritten.</p> <code>rewritten_bytes</code> <code>int</code> <p>Total bytes rewritten during the operation.</p> <code>compression_codec</code> <code>str | None</code> <p>Compression codec used (None if unchanged).</p> <code>dry_run</code> <code>bool</code> <p>Whether this was a dry run operation.</p> <code>zorder_columns</code> <code>list[str] | None</code> <p>Z-order columns used (for optimization operations).</p> <code>planned_groups</code> <code>list[list[str]] | None</code> <p>File groupings planned during dry run.</p> Note <p>All numeric fields are validated to be non-negative. The to_dict() method provides backward compatibility with existing code expecting dictionary format.</p>"},{"location":"api/fsspeckit.core.maintenance/#fsspeckit.core.maintenance.MaintenanceStats-functions","title":"Functions","text":""},{"location":"api/fsspeckit.core.maintenance/#fsspeckit.core.maintenance.MaintenanceStats.to_dict","title":"fsspeckit.core.maintenance.MaintenanceStats.to_dict","text":"<pre><code>to_dict() -&gt; dict[str, Any]\n</code></pre> <p>Convert to dictionary format for backward compatibility.</p> Source code in <code>src/fsspeckit/core/maintenance.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert to dictionary format for backward compatibility.\"\"\"\n    result = {\n        \"before_file_count\": self.before_file_count,\n        \"after_file_count\": self.after_file_count,\n        \"before_total_bytes\": self.before_total_bytes,\n        \"after_total_bytes\": self.after_total_bytes,\n        \"compacted_file_count\": self.compacted_file_count,\n        \"rewritten_bytes\": self.rewritten_bytes,\n        \"compression_codec\": self.compression_codec,\n        \"dry_run\": self.dry_run,\n    }\n\n    if self.zorder_columns is not None:\n        result[\"zorder_columns\"] = self.zorder_columns\n    if self.planned_groups is not None:\n        result[\"planned_groups\"] = self.planned_groups\n\n    return result\n</code></pre>"},{"location":"api/fsspeckit.core.maintenance/#fsspeckit.core.maintenance-functions","title":"Functions","text":""},{"location":"api/fsspeckit.core.maintenance/#fsspeckit.core.maintenance.collect_dataset_stats","title":"fsspeckit.core.maintenance.collect_dataset_stats","text":"<pre><code>collect_dataset_stats(\n    path: str,\n    filesystem: AbstractFileSystem | None = None,\n    partition_filter: list[str] | None = None,\n) -&gt; dict[str, Any]\n</code></pre> <p>Collect file-level statistics for a parquet dataset.</p> <p>This function walks the given dataset directory on the provided filesystem, discovers parquet files (recursively), and returns basic statistics.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Root directory of the parquet dataset.</p> required <code>filesystem</code> <code>AbstractFileSystem | None</code> <p>Optional fsspec filesystem. If omitted, a local \"file\" filesystem is used.</p> <code>None</code> <code>partition_filter</code> <code>list[str] | None</code> <p>Optional list of partition prefix filters (e.g. [\"date=2025-11-04\"]). Only files whose path relative to <code>path</code> starts with one of these prefixes are included.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict with keys:</p> <code>dict[str, Any]</code> <ul> <li><code>files</code>: list of <code>{\"path\", \"size_bytes\", \"num_rows\"}</code> dicts</li> </ul> <code>dict[str, Any]</code> <ul> <li><code>total_bytes</code>: sum of file sizes</li> </ul> <code>dict[str, Any]</code> <ul> <li><code>total_rows</code>: sum of row counts</li> </ul> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the path does not exist or no parquet files match the optional partition filter.</p> Source code in <code>src/fsspeckit/core/maintenance.py</code> <pre><code>def collect_dataset_stats(\n    path: str,\n    filesystem: AbstractFileSystem | None = None,\n    partition_filter: list[str] | None = None,\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Collect file-level statistics for a parquet dataset.\n\n    This function walks the given dataset directory on the provided filesystem,\n    discovers parquet files (recursively), and returns basic statistics.\n\n    Args:\n        path: Root directory of the parquet dataset.\n        filesystem: Optional fsspec filesystem. If omitted, a local \"file\"\n            filesystem is used.\n        partition_filter: Optional list of partition prefix filters\n            (e.g. [\"date=2025-11-04\"]). Only files whose path relative to\n            ``path`` starts with one of these prefixes are included.\n\n    Returns:\n        Dict with keys:\n        - ``files``: list of ``{\"path\", \"size_bytes\", \"num_rows\"}`` dicts\n        - ``total_bytes``: sum of file sizes\n        - ``total_rows``: sum of row counts\n\n    Raises:\n        FileNotFoundError: If the path does not exist or no parquet files\n            match the optional partition filter.\n    \"\"\"\n    import pyarrow.parquet as pq\n\n    fs = filesystem or fsspec_filesystem(\"file\")\n\n    if not fs.exists(path):\n        raise FileNotFoundError(f\"Dataset path '{path}' does not exist\")\n\n    root = Path(path)\n\n    # Discover parquet files recursively via a manual stack walk so we can\n    # respect partition_filter prefixes on the logical relative path.\n    files: list[str] = []\n    stack: list[str] = [path]\n    while stack:\n        current_dir = stack.pop()\n        try:\n            entries = fs.ls(current_dir, detail=False)\n        except Exception:\n            continue\n\n        for entry in entries:\n            if entry.endswith(\".parquet\"):\n                files.append(entry)\n            else:\n                try:\n                    if fs.isdir(entry):\n                        stack.append(entry)\n                except Exception:\n                    continue\n\n    if partition_filter:\n        normalized_filters = [p.rstrip(\"/\") for p in partition_filter]\n        filtered_files: list[str] = []\n        for filename in files:\n            rel = Path(filename).relative_to(root).as_posix()\n            if any(rel.startswith(prefix) for prefix in normalized_filters):\n                filtered_files.append(filename)\n        files = filtered_files\n\n    if not files:\n        raise FileNotFoundError(\n            f\"No parquet files found under '{path}' matching filter\"\n        )\n\n    file_infos: list[dict[str, Any]] = []\n    total_bytes = 0\n    total_rows = 0\n\n    for filename in files:\n        size_bytes = 0\n        try:\n            info = fs.info(filename)\n            if isinstance(info, dict):\n                size_bytes = int(info.get(\"size\", 0))\n        except Exception:\n            size_bytes = 0\n\n        num_rows = 0\n        try:\n            with fs.open(filename, \"rb\") as fh:\n                pf = pq.ParquetFile(fh)\n                num_rows = pf.metadata.num_rows\n        except Exception:\n            # As a fallback, attempt a minimal table read to estimate rows.\n            try:\n                with fs.open(filename, \"rb\") as fh:\n                    table = pq.read_table(fh)\n                num_rows = table.num_rows\n            except Exception:\n                num_rows = 0\n\n        total_bytes += size_bytes\n        total_rows += num_rows\n        file_infos.append(\n            {\"path\": filename, \"size_bytes\": size_bytes, \"num_rows\": num_rows}\n        )\n\n    return {\"files\": file_infos, \"total_bytes\": total_bytes, \"total_rows\": total_rows}\n</code></pre>"},{"location":"api/fsspeckit.core.maintenance/#fsspeckit.core.maintenance.plan_compaction_groups","title":"fsspeckit.core.maintenance.plan_compaction_groups","text":"<pre><code>plan_compaction_groups(\n    file_infos: list[dict[str, Any]] | list[FileInfo],\n    target_mb_per_file: int | None,\n    target_rows_per_file: int | None,\n) -&gt; dict[str, Any]\n</code></pre> <p>Plan compaction groups based on size and row thresholds.</p> <p>Parameters:</p> Name Type Description Default <code>file_infos</code> <code>list[dict[str, Any]] | list[FileInfo]</code> <p>List of file information dictionaries or FileInfo objects.</p> required <code>target_mb_per_file</code> <code>int | None</code> <p>Target size in megabytes per output file.</p> required <code>target_rows_per_file</code> <code>int | None</code> <p>Target number of rows per output file.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with:</p> <code>dict[str, Any]</code> <ul> <li>groups: List of CompactionGroup objects to be compacted</li> </ul> <code>dict[str, Any]</code> <ul> <li>untouched_files: List of FileInfo objects not requiring compaction</li> </ul> <code>dict[str, Any]</code> <ul> <li>planned_stats: MaintenanceStats object for the planned operation</li> </ul> <code>dict[str, Any]</code> <ul> <li>planned_groups: List of file paths per group (for backward compatibility)</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If both target_mb_per_file and target_rows_per_file are None or &lt;= 0.</p> Source code in <code>src/fsspeckit/core/maintenance.py</code> <pre><code>def plan_compaction_groups(\n    file_infos: list[dict[str, Any]] | list[FileInfo],\n    target_mb_per_file: int | None,\n    target_rows_per_file: int | None,\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Plan compaction groups based on size and row thresholds.\n\n    Args:\n        file_infos: List of file information dictionaries or FileInfo objects.\n        target_mb_per_file: Target size in megabytes per output file.\n        target_rows_per_file: Target number of rows per output file.\n\n    Returns:\n        Dictionary with:\n        - groups: List of CompactionGroup objects to be compacted\n        - untouched_files: List of FileInfo objects not requiring compaction\n        - planned_stats: MaintenanceStats object for the planned operation\n        - planned_groups: List of file paths per group (for backward compatibility)\n\n    Raises:\n        ValueError: If both target_mb_per_file and target_rows_per_file are None or &lt;= 0.\n    \"\"\"\n    # Validate inputs\n    if target_mb_per_file is None and target_rows_per_file is None:\n        raise ValueError(\n            \"Must provide at least one of target_mb_per_file or target_rows_per_file\"\n        )\n    if target_mb_per_file is not None and target_mb_per_file &lt;= 0:\n        raise ValueError(\"target_mb_per_file must be &gt; 0\")\n    if target_rows_per_file is not None and target_rows_per_file &lt;= 0:\n        raise ValueError(\"target_rows_per_file must be &gt; 0\")\n\n    # Convert to FileInfo objects if needed\n    if file_infos and isinstance(file_infos[0], dict):\n        files = [FileInfo(fi[\"path\"], fi[\"size_bytes\"], fi[\"num_rows\"]) for fi in file_infos]\n    else:\n        files = file_infos  # type: ignore\n\n    size_threshold_bytes = (\n        target_mb_per_file * 1024 * 1024 if target_mb_per_file is not None else None\n    )\n\n    # Separate candidate files (eligible for compaction) from large files.\n    candidates: list[FileInfo] = []\n    large_files: list[FileInfo] = []\n    for file_info in files:\n        size_bytes = file_info.size_bytes\n        if size_threshold_bytes is None or size_bytes &lt; size_threshold_bytes:\n            candidates.append(file_info)\n        else:\n            large_files.append(file_info)\n\n    # Build groups based on thresholds.\n    groups: list[list[FileInfo]] = []\n    current_group: list[FileInfo] = []\n    current_size = 0\n    current_rows = 0\n\n    def flush_group() -&gt; None:\n        nonlocal current_group, current_size, current_rows\n        if current_group:\n            groups.append(current_group)\n            current_group = []\n            current_size = 0\n            current_rows = 0\n\n    for file_info in sorted(candidates, key=lambda x: x.size_bytes):\n        size_bytes = file_info.size_bytes\n        num_rows = file_info.num_rows\n        would_exceed_size = (\n            size_threshold_bytes is not None\n            and current_size + size_bytes &gt; size_threshold_bytes\n            and current_group\n        )\n        would_exceed_rows = (\n            target_rows_per_file is not None\n            and current_rows + num_rows &gt; target_rows_per_file\n            and current_group\n        )\n        if would_exceed_size or would_exceed_rows:\n            flush_group()\n        current_group.append(file_info)\n        current_size += size_bytes\n        current_rows += num_rows\n    flush_group()\n\n    # Only compact groups that contain more than one file; singleton groups\n    # would just rewrite an existing file.\n    finalized_groups: list[CompactionGroup] = [\n        CompactionGroup(files=group) for group in groups if len(group) &gt; 1\n    ]\n\n    # Calculate statistics\n    before_file_count = len(files)\n    before_total_bytes = sum(f.size_bytes for f in files)\n\n    compacted_file_count = sum(len(group.files) for group in finalized_groups)\n    untouched_files = large_files + [\n        file_info for file_info in candidates\n        if not any(file_info in group.files for group in finalized_groups)\n    ]\n\n    after_file_count = len(untouched_files) + len(finalized_groups)\n\n    # Estimate after_total_bytes (assume minimal compression change for planning)\n    compacted_bytes = sum(group.total_size_bytes for group in finalized_groups)\n    untouched_bytes = sum(f.size_bytes for f in untouched_files)\n    after_total_bytes = untouched_bytes + compacted_bytes  # Rough estimate\n\n    rewritten_bytes = compacted_bytes\n\n    # Create compatibility structures\n    planned_groups = [group.file_paths() for group in finalized_groups]\n\n    planned_stats = MaintenanceStats(\n        before_file_count=before_file_count,\n        after_file_count=after_file_count,\n        before_total_bytes=before_total_bytes,\n        after_total_bytes=after_total_bytes,\n        compacted_file_count=compacted_file_count,\n        rewritten_bytes=rewritten_bytes,\n        compression_codec=None,  # Will be set by backend\n        dry_run=True,\n        planned_groups=planned_groups,\n    )\n\n    return {\n        \"groups\": finalized_groups,\n        \"untouched_files\": untouched_files,\n        \"planned_stats\": planned_stats,\n        \"planned_groups\": planned_groups,\n    }\n</code></pre>"},{"location":"api/fsspeckit.core.maintenance/#fsspeckit.core.maintenance.plan_optimize_groups","title":"fsspeckit.core.maintenance.plan_optimize_groups","text":"<pre><code>plan_optimize_groups(\n    file_infos: list[dict[str, Any]] | list[FileInfo],\n    zorder_columns: list[str],\n    target_mb_per_file: int | None = None,\n    target_rows_per_file: int | None = None,\n    sample_schema: Any = None,\n) -&gt; dict[str, Any]\n</code></pre> <p>Plan optimization groups with z-order validation.</p> <p>Parameters:</p> Name Type Description Default <code>file_infos</code> <code>list[dict[str, Any]] | list[FileInfo]</code> <p>List of file information dictionaries or FileInfo objects.</p> required <code>zorder_columns</code> <code>list[str]</code> <p>List of columns to use for z-order clustering.</p> required <code>target_mb_per_file</code> <code>int | None</code> <p>Target size in megabytes per output file.</p> <code>None</code> <code>target_rows_per_file</code> <code>int | None</code> <p>Target number of rows per output file.</p> <code>None</code> <code>sample_schema</code> <code>Any</code> <p>PyArrow schema or object with column_names method for validation.           If None, schema validation will be skipped.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with:</p> <code>dict[str, Any]</code> <ul> <li>groups: List of CompactionGroup objects to be optimized</li> </ul> <code>dict[str, Any]</code> <ul> <li>untouched_files: List of FileInfo objects not requiring optimization</li> </ul> <code>dict[str, Any]</code> <ul> <li>planned_stats: MaintenanceStats object for the planned operation</li> </ul> <code>dict[str, Any]</code> <ul> <li>planned_groups: List of file paths per group (for backward compatibility)</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If thresholds are invalid or zorder_columns is empty.</p> Source code in <code>src/fsspeckit/core/maintenance.py</code> <pre><code>def plan_optimize_groups(\n    file_infos: list[dict[str, Any]] | list[FileInfo],\n    zorder_columns: list[str],\n    target_mb_per_file: int | None = None,\n    target_rows_per_file: int | None = None,\n    sample_schema: Any = None,\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Plan optimization groups with z-order validation.\n\n    Args:\n        file_infos: List of file information dictionaries or FileInfo objects.\n        zorder_columns: List of columns to use for z-order clustering.\n        target_mb_per_file: Target size in megabytes per output file.\n        target_rows_per_file: Target number of rows per output file.\n        sample_schema: PyArrow schema or object with column_names method for validation.\n                      If None, schema validation will be skipped.\n\n    Returns:\n        Dictionary with:\n        - groups: List of CompactionGroup objects to be optimized\n        - untouched_files: List of FileInfo objects not requiring optimization\n        - planned_stats: MaintenanceStats object for the planned operation\n        - planned_groups: List of file paths per group (for backward compatibility)\n\n    Raises:\n        ValueError: If thresholds are invalid or zorder_columns is empty.\n    \"\"\"\n    # Validate inputs\n    if not zorder_columns:\n        raise ValueError(\"zorder_columns must be a non-empty list\")\n    if target_mb_per_file is not None and target_mb_per_file &lt;= 0:\n        raise ValueError(\"target_mb_per_file must be &gt; 0\")\n    if target_rows_per_file is not None and target_rows_per_file &lt;= 0:\n        raise ValueError(\"target_rows_per_file must be &gt; 0\")\n\n    # Validate zorder columns against schema if provided\n    if sample_schema is not None:\n        try:\n            available_cols = set(sample_schema.column_names)\n            missing = [col for col in zorder_columns if col not in available_cols]\n            if missing:\n                raise ValueError(\n                    f\"Missing z-order columns: {', '.join(missing)}. \"\n                    f\"Available columns: {', '.join(sorted(available_cols))}\"\n                )\n        except AttributeError:\n            # sample_schema doesn't have column_names, skip validation\n            pass\n\n    # Convert to FileInfo objects if needed\n    if file_infos and isinstance(file_infos[0], dict):\n        files = [FileInfo(fi[\"path\"], fi[\"size_bytes\"], fi[\"num_rows\"]) for fi in file_infos]\n    else:\n        files = file_infos  # type: ignore\n\n    # For optimization, we typically want to process all files unless they're\n    # already large enough to be left alone\n    size_threshold_bytes = (\n        target_mb_per_file * 1024 * 1024 if target_mb_per_file is not None else None\n    )\n\n    # Separate candidate files from large files\n    candidates: list[FileInfo] = []\n    large_files: list[FileInfo] = []\n    for file_info in files:\n        size_bytes = file_info.size_bytes\n        if size_threshold_bytes is None or size_bytes &lt; size_threshold_bytes:\n            candidates.append(file_info)\n        else:\n            large_files.append(file_info)\n\n    # Group files for optimization - similar to compaction but more aggressive\n    # since optimization typically rewrites all eligible files\n    groups: list[list[FileInfo]] = []\n    current_group: list[FileInfo] = []\n    current_size = 0\n    current_rows = 0\n\n    def flush_group() -&gt; None:\n        nonlocal current_group, current_size, current_rows\n        if current_group:\n            groups.append(current_group)\n            current_group = []\n            current_size = 0\n            current_rows = 0\n\n    # Sort files for more consistent optimization\n    for file_info in sorted(candidates, key=lambda x: x.size_bytes):\n        size_bytes = file_info.size_bytes\n        num_rows = file_info.num_rows\n        would_exceed_size = (\n            size_threshold_bytes is not None\n            and current_size + size_bytes &gt; size_threshold_bytes\n            and current_group\n        )\n        would_exceed_rows = (\n            target_rows_per_file is not None\n            and current_rows + num_rows &gt; target_rows_per_file\n            and current_group\n        )\n        if would_exceed_size or would_exceed_rows:\n            flush_group()\n        current_group.append(file_info)\n        current_size += size_bytes\n        current_rows += num_rows\n    flush_group()\n\n    # Include single-file groups for optimization (unlike compaction)\n    # because optimization needs to reorder all eligible files\n    finalized_groups: list[CompactionGroup] = []\n    for group in groups:\n        if len(group) &gt; 0:  # Include single files too\n            finalized_groups.append(CompactionGroup(files=group))\n\n    # Calculate statistics\n    before_file_count = len(files)\n    before_total_bytes = sum(f.size_bytes for f in files)\n\n    optimized_file_count = sum(len(group.files) for group in finalized_groups)\n    untouched_files = large_files  # Only large files are left untouched in optimization\n\n    after_file_count = len(untouched_files) + len(finalized_groups)\n\n    # Estimate after_total_bytes (optimization may improve compression)\n    optimized_bytes = sum(group.total_size_bytes for group in finalized_groups)\n    untouched_bytes = sum(f.size_bytes for f in untouched_files)\n    after_total_bytes = untouched_bytes + optimized_bytes  # Rough estimate\n\n    rewritten_bytes = optimized_bytes\n\n    # Create compatibility structures\n    planned_groups = [group.file_paths() for group in finalized_groups]\n\n    planned_stats = MaintenanceStats(\n        before_file_count=before_file_count,\n        after_file_count=after_file_count,\n        before_total_bytes=before_total_bytes,\n        after_total_bytes=after_total_bytes,\n        compacted_file_count=optimized_file_count,\n        rewritten_bytes=rewritten_bytes,\n        compression_codec=None,  # Will be set by backend\n        dry_run=True,\n        zorder_columns=zorder_columns,\n        planned_groups=planned_groups,\n    )\n\n    return {\n        \"groups\": finalized_groups,\n        \"untouched_files\": untouched_files,\n        \"planned_stats\": planned_stats,\n        \"planned_groups\": planned_groups,\n    }\n</code></pre>"},{"location":"api/fsspeckit.core.merge/","title":"fsspeckit.core.merge","text":""},{"location":"api/fsspeckit.core.merge/#fsspeckit.core.merge","title":"merge","text":"<p>Backend-neutral merge layer for parquet dataset operations.</p> <p>This module provides shared functionality for merge operations used by both DuckDB and PyArrow merge implementations.</p> <p>Key responsibilities: 1. Merge validation and normalization 2. Strategy semantics and definitions 3. Key validation and schema compatibility checking 4. Shared statistics calculation 5. NULL-key detection and edge case handling</p>"},{"location":"api/fsspeckit.core.merge/#fsspeckit.core.merge-classes","title":"Classes","text":""},{"location":"api/fsspeckit.core.merge/#fsspeckit.core.merge.MergePlan","title":"fsspeckit.core.merge.MergePlan  <code>dataclass</code>","text":"<pre><code>MergePlan(\n    strategy: MergeStrategy,\n    key_columns: list[str],\n    source_count: int,\n    target_exists: bool,\n    dedup_order_by: list[str] | None = None,\n    key_columns_valid: bool = True,\n    schema_compatible: bool = True,\n    null_keys_detected: bool = False,\n    allow_target_empty: bool = True,\n    allow_source_empty: bool = True,\n)\n</code></pre> <p>Plan for executing a merge operation.</p>"},{"location":"api/fsspeckit.core.merge/#fsspeckit.core.merge.MergeStats","title":"fsspeckit.core.merge.MergeStats  <code>dataclass</code>","text":"<pre><code>MergeStats(\n    strategy: MergeStrategy,\n    source_count: int,\n    target_count_before: int,\n    target_count_after: int,\n    inserted: int,\n    updated: int,\n    deleted: int,\n    total_processed: int = 0,\n)\n</code></pre> <p>Canonical statistics structure for merge operations.</p>"},{"location":"api/fsspeckit.core.merge/#fsspeckit.core.merge.MergeStats-functions","title":"Functions","text":""},{"location":"api/fsspeckit.core.merge/#fsspeckit.core.merge.MergeStats.to_dict","title":"fsspeckit.core.merge.MergeStats.to_dict","text":"<pre><code>to_dict() -&gt; dict[str, Any]\n</code></pre> <p>Convert to dictionary format for backward compatibility.</p> Source code in <code>src/fsspeckit/core/merge.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert to dictionary format for backward compatibility.\"\"\"\n    return {\n        \"inserted\": self.inserted,\n        \"updated\": self.updated,\n        \"deleted\": self.deleted,\n        \"total\": self.target_count_after,\n        \"source_count\": self.source_count,\n        \"target_count_before\": self.target_count_before,\n        \"target_count_after\": self.target_count_after,\n        \"total_processed\": self.total_processed,\n        \"strategy\": self.strategy.value,\n    }\n</code></pre>"},{"location":"api/fsspeckit.core.merge/#fsspeckit.core.merge.MergeStrategy","title":"fsspeckit.core.merge.MergeStrategy","text":"<p>               Bases: <code>Enum</code></p> <p>Supported merge strategies with consistent semantics across backends.</p>"},{"location":"api/fsspeckit.core.merge/#fsspeckit.core.merge.MergeStrategy-attributes","title":"Attributes","text":""},{"location":"api/fsspeckit.core.merge/#fsspeckit.core.merge.MergeStrategy.DEDUPLICATE","title":"fsspeckit.core.merge.MergeStrategy.DEDUPLICATE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DEDUPLICATE = 'deduplicate'\n</code></pre> <p>Remove duplicates from source, then upsert.</p>"},{"location":"api/fsspeckit.core.merge/#fsspeckit.core.merge.MergeStrategy.FULL_MERGE","title":"fsspeckit.core.merge.MergeStrategy.FULL_MERGE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FULL_MERGE = 'full_merge'\n</code></pre> <p>Insert, update, and delete (full sync with source).</p>"},{"location":"api/fsspeckit.core.merge/#fsspeckit.core.merge.MergeStrategy.INSERT","title":"fsspeckit.core.merge.MergeStrategy.INSERT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>INSERT = 'insert'\n</code></pre> <p>Insert only new records, ignore existing records.</p>"},{"location":"api/fsspeckit.core.merge/#fsspeckit.core.merge.MergeStrategy.UPDATE","title":"fsspeckit.core.merge.MergeStrategy.UPDATE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>UPDATE = 'update'\n</code></pre> <p>Update only existing records, ignore new records.</p>"},{"location":"api/fsspeckit.core.merge/#fsspeckit.core.merge.MergeStrategy.UPSERT","title":"fsspeckit.core.merge.MergeStrategy.UPSERT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>UPSERT = 'upsert'\n</code></pre> <p>Insert new records, update existing records.</p>"},{"location":"api/fsspeckit.core.merge/#fsspeckit.core.merge-functions","title":"Functions","text":""},{"location":"api/fsspeckit.core.merge/#fsspeckit.core.merge.calculate_merge_stats","title":"fsspeckit.core.merge.calculate_merge_stats","text":"<pre><code>calculate_merge_stats(\n    strategy: MergeStrategy,\n    source_count: int,\n    target_count_before: int,\n    target_count_after: int,\n) -&gt; MergeStats\n</code></pre> <p>Calculate merge operation statistics.</p> <p>Parameters:</p> Name Type Description Default <code>strategy</code> <code>MergeStrategy</code> <p>Merge strategy that was used.</p> required <code>source_count</code> <code>int</code> <p>Number of rows in source data.</p> required <code>target_count_before</code> <code>int</code> <p>Number of rows in target before merge.</p> required <code>target_count_after</code> <code>int</code> <p>Number of rows in target after merge.</p> required <p>Returns:</p> Type Description <code>MergeStats</code> <p>MergeStats with calculated statistics.</p> Source code in <code>src/fsspeckit/core/merge.py</code> <pre><code>def calculate_merge_stats(\n    strategy: MergeStrategy,\n    source_count: int,\n    target_count_before: int,\n    target_count_after: int,\n) -&gt; MergeStats:\n    \"\"\"\n    Calculate merge operation statistics.\n\n    Args:\n        strategy: Merge strategy that was used.\n        source_count: Number of rows in source data.\n        target_count_before: Number of rows in target before merge.\n        target_count_after: Number of rows in target after merge.\n\n    Returns:\n        MergeStats with calculated statistics.\n    \"\"\"\n    stats = MergeStats(\n        strategy=strategy,\n        source_count=source_count,\n        target_count_before=target_count_before,\n        target_count_after=target_count_after,\n        inserted=0,\n        updated=0,\n        deleted=0,\n    )\n\n    if strategy == MergeStrategy.INSERT:\n        # INSERT: only additions, no updates or deletes\n        stats.inserted = target_count_after - target_count_before\n        stats.updated = 0\n        stats.deleted = 0\n\n    elif strategy == MergeStrategy.UPDATE:\n        # UPDATE: no additions or deletes (all existing potentially updated)\n        stats.inserted = 0\n        stats.updated = target_count_before if target_count_before &gt; 0 else 0\n        stats.deleted = 0\n\n    elif strategy == MergeStrategy.FULL_MERGE:\n        # FULL_MERGE: source replaces target completely\n        stats.inserted = source_count\n        stats.updated = 0\n        stats.deleted = target_count_before\n\n    else:  # UPSERT or DEDUPLICATE\n        # UPSERT/DEDUPLICATE: additions and updates\n        net_change = target_count_after - target_count_before\n        stats.inserted = max(0, net_change)\n        stats.updated = source_count - stats.inserted\n        stats.deleted = 0\n\n    # Update total_processed\n    stats.total_processed = stats.inserted + stats.updated\n\n    return stats\n</code></pre>"},{"location":"api/fsspeckit.core.merge/#fsspeckit.core.merge.check_null_keys","title":"fsspeckit.core.merge.check_null_keys","text":"<pre><code>check_null_keys(\n    source_table: Table,\n    target_table: Table | None,\n    key_columns: list[str],\n) -&gt; None\n</code></pre> <p>Check for NULL values in key columns.</p> <p>Parameters:</p> Name Type Description Default <code>source_table</code> <code>Table</code> <p>Source data table.</p> required <code>target_table</code> <code>Table | None</code> <p>Target data table, None if target doesn't exist.</p> required <code>key_columns</code> <code>list[str]</code> <p>List of key column names.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If NULL values found in key columns.</p> Source code in <code>src/fsspeckit/core/merge.py</code> <pre><code>def check_null_keys(\n    source_table: pa.Table,\n    target_table: pa.Table | None,\n    key_columns: list[str],\n) -&gt; None:\n    \"\"\"\n    Check for NULL values in key columns.\n\n    Args:\n        source_table: Source data table.\n        target_table: Target data table, None if target doesn't exist.\n        key_columns: List of key column names.\n\n    Raises:\n        ValueError: If NULL values found in key columns.\n    \"\"\"\n    # Check source for NULL keys\n    for key_col in key_columns:\n        source_col = source_table.column(key_col)\n        if source_col.null_count &gt; 0:\n            raise ValueError(\n                f\"Key column '{key_col}' contains {source_col.null_count} NULL values in source. \"\n                f\"Key columns must not have NULLs.\"\n            )\n\n    # Check target for NULL keys if it exists\n    if target_table is not None:\n        for key_col in key_columns:\n            target_col = target_table.column(key_col)\n            if target_col.null_count &gt; 0:\n                raise ValueError(\n                    f\"Key column '{key_col}' contains {target_col.null_count} NULL values in target. \"\n                    f\"Key columns must not have NULLs.\"\n                )\n</code></pre>"},{"location":"api/fsspeckit.core.merge/#fsspeckit.core.merge.get_canonical_merge_strategies","title":"fsspeckit.core.merge.get_canonical_merge_strategies","text":"<pre><code>get_canonical_merge_strategies() -&gt; list[str]\n</code></pre> <p>Get the list of canonical merge strategy names.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of strategy names in canonical order.</p> Source code in <code>src/fsspeckit/core/merge.py</code> <pre><code>def get_canonical_merge_strategies() -&gt; list[str]:\n    \"\"\"\n    Get the list of canonical merge strategy names.\n\n    Returns:\n        List of strategy names in canonical order.\n    \"\"\"\n    return [strategy.value for strategy in MergeStrategy]\n</code></pre>"},{"location":"api/fsspeckit.core.merge/#fsspeckit.core.merge.normalize_key_columns","title":"fsspeckit.core.merge.normalize_key_columns","text":"<pre><code>normalize_key_columns(\n    key_columns: list[str] | str,\n) -&gt; list[str]\n</code></pre> <p>Normalize key columns to a consistent list format.</p> <p>Parameters:</p> Name Type Description Default <code>key_columns</code> <code>list[str] | str</code> <p>Key column(s) as string or list of strings.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of key column names.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If key_columns is empty or contains invalid values.</p> Source code in <code>src/fsspeckit/core/merge.py</code> <pre><code>def normalize_key_columns(key_columns: list[str] | str) -&gt; list[str]:\n    \"\"\"\n    Normalize key columns to a consistent list format.\n\n    Args:\n        key_columns: Key column(s) as string or list of strings.\n\n    Returns:\n        List of key column names.\n\n    Raises:\n        ValueError: If key_columns is empty or contains invalid values.\n    \"\"\"\n    if isinstance(key_columns, str):\n        if not key_columns.strip():\n            raise ValueError(\"key_columns cannot be empty string\")\n        return [key_columns.strip()]\n\n    if not key_columns:\n        raise ValueError(\"key_columns cannot be empty\")\n\n    # Filter and validate each column name\n    normalized = []\n    for col in key_columns:\n        if not isinstance(col, str):\n            raise ValueError(f\"key_columns must be strings, got {type(col)}\")\n        stripped = col.strip()\n        if not stripped:\n            raise ValueError(\"key_columns cannot contain empty strings\")\n        normalized.append(stripped)\n\n    if not normalized:\n        raise ValueError(\"key_columns cannot be empty after normalization\")\n\n    return normalized\n</code></pre>"},{"location":"api/fsspeckit.core.merge/#fsspeckit.core.merge.validate_merge_inputs","title":"fsspeckit.core.merge.validate_merge_inputs","text":"<pre><code>validate_merge_inputs(\n    source_schema: Schema,\n    target_schema: Schema | None,\n    key_columns: list[str],\n    strategy: MergeStrategy,\n) -&gt; MergePlan\n</code></pre> <p>Validate merge inputs and create a merge plan.</p> <p>Parameters:</p> Name Type Description Default <code>source_schema</code> <code>Schema</code> <p>Schema of the source data.</p> required <code>target_schema</code> <code>Schema | None</code> <p>Schema of the target dataset, None if target doesn't exist.</p> required <code>key_columns</code> <code>list[str]</code> <p>List of key column names for matching records.</p> required <code>strategy</code> <code>MergeStrategy</code> <p>Merge strategy to use.</p> required <p>Returns:</p> Type Description <code>MergePlan</code> <p>MergePlan with validation results and execution details.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If validation fails with specific error messages.</p> Source code in <code>src/fsspeckit/core/merge.py</code> <pre><code>def validate_merge_inputs(\n    source_schema: pa.Schema,\n    target_schema: pa.Schema | None,\n    key_columns: list[str],\n    strategy: MergeStrategy,\n) -&gt; MergePlan:\n    \"\"\"\n    Validate merge inputs and create a merge plan.\n\n    Args:\n        source_schema: Schema of the source data.\n        target_schema: Schema of the target dataset, None if target doesn't exist.\n        key_columns: List of key column names for matching records.\n        strategy: Merge strategy to use.\n\n    Returns:\n        MergePlan with validation results and execution details.\n\n    Raises:\n        ValueError: If validation fails with specific error messages.\n    \"\"\"\n    # Normalize key columns\n    normalized_keys = normalize_key_columns(key_columns)\n\n    # Check key columns exist in source\n    source_columns = set(source_schema.names)\n    missing_source_keys = [col for col in normalized_keys if col not in source_columns]\n    if missing_source_keys:\n        raise ValueError(\n            f\"Key column(s) missing from source: {', '.join(missing_source_keys)}. \"\n            f\"Available columns: {', '.join(sorted(source_columns))}\"\n        )\n\n    # Initialize validation flags\n    keys_valid = True\n    schema_compatible = True\n    null_keys_possible = False\n\n    # Check target schema if it exists\n    target_exists = target_schema is not None\n    if target_exists:\n        target_columns = set(target_schema.names)\n\n        # Check key columns exist in target\n        missing_target_keys = [col for col in normalized_keys if col not in target_columns]\n        if missing_target_keys:\n            raise ValueError(\n                f\"Key column(s) missing from target: {', '.join(missing_target_keys)}. \"\n                f\"Available columns: {', '.join(sorted(target_columns))}\"\n            )\n\n        # Check schema compatibility\n        for field in source_schema:\n            if field.name in target_columns:\n                target_field = target_schema.field(field.name)\n                if field.type != target_field.type:\n                    schema_compatible = False\n                    break\n\n        # Check for column mismatches\n        source_only = source_columns - target_columns\n        target_only = target_columns - source_columns\n        if source_only or target_only:\n            schema_compatible = False\n\n    # Check if NULL keys are possible based on schema nullability\n    for key_col in normalized_keys:\n        source_field = source_schema.field(key_col)\n        if source_field.nullable:\n            null_keys_possible = True\n            break\n\n    # Determine if empty target/source are allowed based on strategy\n    allow_target_empty = True  # All strategies allow empty target\n    allow_source_empty = strategy != MergeStrategy.UPDATE  # UPDATE needs source records\n\n    return MergePlan(\n        strategy=strategy,\n        key_columns=normalized_keys,\n        source_count=0,  # Will be set by caller\n        target_exists=target_exists,\n        dedup_order_by=None,  # Will be set by caller if needed\n        key_columns_valid=keys_valid,\n        schema_compatible=schema_compatible,\n        null_keys_detected=null_keys_possible,\n        allow_target_empty=allow_target_empty,\n        allow_source_empty=allow_source_empty,\n    )\n</code></pre>"},{"location":"api/fsspeckit.core.merge/#fsspeckit.core.merge.validate_strategy_compatibility","title":"fsspeckit.core.merge.validate_strategy_compatibility","text":"<pre><code>validate_strategy_compatibility(\n    strategy: MergeStrategy,\n    source_count: int,\n    target_exists: bool,\n) -&gt; None\n</code></pre> <p>Validate that the chosen strategy is compatible with the data state.</p> <p>Parameters:</p> Name Type Description Default <code>strategy</code> <code>MergeStrategy</code> <p>Merge strategy to validate.</p> required <code>source_count</code> <code>int</code> <p>Number of rows in source data.</p> required <code>target_exists</code> <code>bool</code> <p>Whether target dataset exists.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If strategy is incompatible with the data state.</p> Source code in <code>src/fsspeckit/core/merge.py</code> <pre><code>def validate_strategy_compatibility(\n    strategy: MergeStrategy,\n    source_count: int,\n    target_exists: bool,\n) -&gt; None:\n    \"\"\"\n    Validate that the chosen strategy is compatible with the data state.\n\n    Args:\n        strategy: Merge strategy to validate.\n        source_count: Number of rows in source data.\n        target_exists: Whether target dataset exists.\n\n    Raises:\n        ValueError: If strategy is incompatible with the data state.\n    \"\"\"\n    if strategy == MergeStrategy.UPDATE and source_count == 0:\n        # UPDATE strategy with empty source doesn't make sense\n        raise ValueError(\"UPDATE strategy requires non-empty source data\")\n\n    if strategy == MergeStrategy.FULL_MERGE and not target_exists:\n        # FULL_MERGE on non-existent target is equivalent to just writing source\n        # This is more of a warning situation, but we'll allow it\n        pass\n\n    # Other strategies are generally compatible with any state\n    pass\n</code></pre>"},{"location":"api/fsspeckit.datasets/","title":"fsspeckit.datasets","text":""},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets","title":"datasets","text":"<p>Dataset-level operations for fsspeckit.</p> <p>This package contains dataset-specific functionality including: - DuckDB parquet handlers for high-performance dataset operations - PyArrow utilities for schema management and type conversion - Dataset merging and optimization tools</p>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets-classes","title":"Classes","text":""},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.DuckDBParquetHandler","title":"fsspeckit.datasets.DuckDBParquetHandler","text":"<pre><code>DuckDBParquetHandler(\n    storage_options: BaseStorageOptions | None = None,\n    filesystem: AbstractFileSystem | None = None,\n)\n</code></pre> <p>Handler for parquet operations using DuckDB with fsspec integration.</p> <p>This class provides methods for reading and writing parquet files and datasets using DuckDB's high-performance parquet engine. It integrates with fsspec filesystems to support local and remote storage (S3, GCS, Azure, etc.).</p> <p>The handler can be initialized with either storage options or an existing filesystem instance. For remote filesystems, the fsspec filesystem is registered in DuckDB using <code>.register_filesystem(fs)</code> to enable direct access to remote paths.</p> <p>Parameters:</p> Name Type Description Default <code>storage_options</code> <code>BaseStorageOptions | None</code> <p>Storage configuration options (e.g., AwsStorageOptions). If provided, a filesystem is created from these options.</p> <code>None</code> <code>filesystem</code> <code>AbstractFileSystem | None</code> <p>An existing fsspec filesystem instance. Takes precedence over storage_options if both are provided.</p> <code>None</code> <p>Examples:</p> <p>Basic usage with local filesystem:</p> <pre><code>&gt;&gt;&gt; from fsspeckit.utils import DuckDBParquetHandler\n&gt;&gt;&gt; import pyarrow as pa\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create sample data\n&gt;&gt;&gt; table = pa.table({'a': [1, 2, 3], 'b': ['x', 'y', 'z']})\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Write and read parquet file\n&gt;&gt;&gt; with DuckDBParquetHandler() as handler:\n...     handler.write_parquet(table, \"/tmp/data.parquet\")\n...     result = handler.read_parquet(\"/tmp/data.parquet\")\n...     print(result)\n</code></pre> <p>Using with AWS S3:</p> <pre><code>&gt;&gt;&gt; from fsspeckit.storage_options import AwsStorageOptions\n&gt;&gt;&gt; from fsspeckit.utils import DuckDBParquetHandler\n&gt;&gt;&gt;\n&gt;&gt;&gt; options = AwsStorageOptions(\n...     access_key_id=\"YOUR_KEY\",\n...     secret_access_key=\"YOUR_SECRET\",\n...     region=\"us-east-1\"\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; with DuckDBParquetHandler(storage_options=options) as handler:\n...     # Read from S3\n...     table = handler.read_parquet(\"s3://bucket/data.parquet\")\n...\n...     # Execute SQL query on S3 data\n...     result = handler.execute_sql(\n...         \"SELECT * FROM parquet_scan('s3://bucket/data.parquet') WHERE col &gt; 10\"\n...     )\n</code></pre> <p>Using with existing filesystem:</p> <pre><code>&gt;&gt;&gt; from fsspeckit import filesystem\n&gt;&gt;&gt; from fsspeckit.utils import DuckDBParquetHandler\n&gt;&gt;&gt;\n&gt;&gt;&gt; fs = filesystem(\"file\")\n&gt;&gt;&gt; with DuckDBParquetHandler(filesystem=fs) as handler:\n...     result = handler.read_parquet(\"/path/to/data.parquet\")\n</code></pre> <p>SQL query execution:</p> <pre><code>&gt;&gt;&gt; with DuckDBParquetHandler() as handler:\n...     handler.write_parquet(table, \"/tmp/data.parquet\")\n...\n...     # Simple query\n...     result = handler.execute_sql(\n...         \"SELECT a, b FROM parquet_scan('/tmp/data.parquet') WHERE a &gt; 1\"\n...     )\n...\n...     # Parameterized query\n...     result = handler.execute_sql(\n...         \"SELECT * FROM parquet_scan('/tmp/data.parquet') WHERE a BETWEEN ? AND ?\",\n...         parameters=[1, 3]\n...     )\n...\n...     # Aggregation query\n...     result = handler.execute_sql(\n...         '''\n...         SELECT b, COUNT(*) as count, AVG(a) as avg_a\n...         FROM parquet_scan('/tmp/data.parquet')\n...         GROUP BY b\n...         '''\n...     )\n</code></pre> <p>Reading specific columns:</p> <pre><code>&gt;&gt;&gt; with DuckDBParquetHandler() as handler:\n...     # Only read columns 'a' and 'b'\n...     result = handler.read_parquet(\"/tmp/data.parquet\", columns=[\"a\", \"b\"])\n</code></pre> <p>Writing with compression:</p> <pre><code>&gt;&gt;&gt; with DuckDBParquetHandler() as handler:\n...     handler.write_parquet(table, \"/tmp/data.parquet\", compression=\"gzip\")\n...     handler.write_parquet(table, \"/tmp/data2.parquet\", compression=\"zstd\")\n</code></pre> <p>Initialize the DuckDB parquet handler.</p> <p>Parameters:</p> Name Type Description Default <code>storage_options</code> <code>BaseStorageOptions | None</code> <p>Storage configuration options. If provided, a filesystem is created from these options.</p> <code>None</code> <code>filesystem</code> <code>AbstractFileSystem | None</code> <p>An existing fsspec filesystem instance. Takes precedence over storage_options if both are provided.</p> <code>None</code> Source code in <code>src/fsspeckit/datasets/duckdb.py</code> <pre><code>def __init__(\n    self,\n    storage_options: \"BaseStorageOptions | None\" = None,\n    filesystem: AbstractFileSystem | None = None,\n) -&gt; None:\n    \"\"\"Initialize the DuckDB parquet handler.\n\n    Args:\n        storage_options: Storage configuration options. If provided, a filesystem\n            is created from these options.\n        filesystem: An existing fsspec filesystem instance. Takes precedence over\n            storage_options if both are provided.\n    \"\"\"\n    self._connection: duckdb.DuckDBPyConnection | None = None\n    self._filesystem: AbstractFileSystem | None = None\n    self._storage_options = storage_options\n\n    # Determine which filesystem to use\n    if filesystem is not None:\n        self._filesystem = filesystem\n    elif storage_options is not None:\n        self._filesystem = storage_options.to_filesystem()\n    else:\n        # Default to local filesystem\n        self._filesystem = fsspec_filesystem(\"file\")\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.DuckDBParquetHandler-functions","title":"Functions","text":""},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.DuckDBParquetHandler.__del__","title":"fsspeckit.datasets.DuckDBParquetHandler.__del__","text":"<pre><code>__del__() -&gt; None\n</code></pre> <p>Cleanup on deletion.</p> Source code in <code>src/fsspeckit/datasets/duckdb.py</code> <pre><code>def __del__(self) -&gt; None:\n    \"\"\"Cleanup on deletion.\"\"\"\n    self.close()\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.DuckDBParquetHandler.__enter__","title":"fsspeckit.datasets.DuckDBParquetHandler.__enter__","text":"<pre><code>__enter__() -&gt; DuckDBParquetHandler\n</code></pre> <p>Enter context manager.</p> <p>Returns:</p> Type Description <code>DuckDBParquetHandler</code> <p>Self for use in with statement.</p> Source code in <code>src/fsspeckit/datasets/duckdb.py</code> <pre><code>def __enter__(self) -&gt; \"DuckDBParquetHandler\":\n    \"\"\"Enter context manager.\n\n    Returns:\n        Self for use in with statement.\n    \"\"\"\n    self._ensure_connection()\n    return self\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.DuckDBParquetHandler.__exit__","title":"fsspeckit.datasets.DuckDBParquetHandler.__exit__","text":"<pre><code>__exit__(exc_type: Any, exc_val: Any, exc_tb: Any) -&gt; None\n</code></pre> <p>Exit context manager and close connection.</p> <p>Parameters:</p> Name Type Description Default <code>exc_type</code> <code>Any</code> <p>Exception type if an exception occurred.</p> required <code>exc_val</code> <code>Any</code> <p>Exception value if an exception occurred.</p> required <code>exc_tb</code> <code>Any</code> <p>Exception traceback if an exception occurred.</p> required Source code in <code>src/fsspeckit/datasets/duckdb.py</code> <pre><code>def __exit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -&gt; None:\n    \"\"\"Exit context manager and close connection.\n\n    Args:\n        exc_type: Exception type if an exception occurred.\n        exc_val: Exception value if an exception occurred.\n        exc_tb: Exception traceback if an exception occurred.\n    \"\"\"\n    self.close()\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.DuckDBParquetHandler.close","title":"fsspeckit.datasets.DuckDBParquetHandler.close","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close the DuckDB connection.</p> <p>This method is called automatically when using the context manager. Manual calls are only needed when not using the context manager pattern.</p> Source code in <code>src/fsspeckit/datasets/duckdb.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Close the DuckDB connection.\n\n    This method is called automatically when using the context manager.\n    Manual calls are only needed when not using the context manager pattern.\n    \"\"\"\n    if self._connection is not None:\n        self._connection.close()\n        self._connection = None\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.DuckDBParquetHandler.compact_parquet_dataset","title":"fsspeckit.datasets.DuckDBParquetHandler.compact_parquet_dataset","text":"<pre><code>compact_parquet_dataset(\n    path: str,\n    target_mb_per_file: int | None = None,\n    target_rows_per_file: int | None = None,\n    partition_filter: list[str] | None = None,\n    compression: str | None = None,\n    dry_run: bool = False,\n) -&gt; dict[str, Any]\n</code></pre> <p>Compact a parquet dataset directory into fewer larger files using shared planning.</p> <p>This function delegates compaction planning to the shared core module, ensuring consistent behavior across DuckDB and PyArrow backends.</p> <p>Groups small files based on size (MB) and/or row thresholds, rewrites grouped files into new parquet files, optionally changing compression. Supports dry-run mode returning planned groups without writing.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Dataset directory path.</p> required <code>target_mb_per_file</code> <code>int | None</code> <p>Desired approximate size per output file (MB).</p> <code>None</code> <code>target_rows_per_file</code> <code>int | None</code> <p>Desired maximum rows per output file.</p> <code>None</code> <code>partition_filter</code> <code>list[str] | None</code> <p>Optional list of partition prefixes to restrict scope.</p> <code>None</code> <code>compression</code> <code>str | None</code> <p>Optional compression codec; defaults to existing or 'snappy'.</p> <code>None</code> <code>dry_run</code> <code>bool</code> <p>If True, plan only without modifying files.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Statistics dict following canonical MaintenanceStats format, including</p> <code>dict[str, Any]</code> <p>before/after counts and optional plan when dry_run=True.</p> Note <p>This function delegates dataset discovery and compaction planning to the shared <code>fsspeckit.core.maintenance</code> module for consistent behavior with the PyArrow backend.</p> Source code in <code>src/fsspeckit/datasets/duckdb.py</code> <pre><code>def compact_parquet_dataset(\n    self,\n    path: str,\n    target_mb_per_file: int | None = None,\n    target_rows_per_file: int | None = None,\n    partition_filter: list[str] | None = None,\n    compression: str | None = None,\n    dry_run: bool = False,\n) -&gt; dict[str, Any]:\n    \"\"\"Compact a parquet dataset directory into fewer larger files using shared planning.\n\n    This function delegates compaction planning to the shared core module,\n    ensuring consistent behavior across DuckDB and PyArrow backends.\n\n    Groups small files based on size (MB) and/or row thresholds, rewrites grouped\n    files into new parquet files, optionally changing compression. Supports\n    dry-run mode returning planned groups without writing.\n\n    Args:\n        path: Dataset directory path.\n        target_mb_per_file: Desired approximate size per output file (MB).\n        target_rows_per_file: Desired maximum rows per output file.\n        partition_filter: Optional list of partition prefixes to restrict scope.\n        compression: Optional compression codec; defaults to existing or 'snappy'.\n        dry_run: If True, plan only without modifying files.\n\n    Returns:\n        Statistics dict following canonical MaintenanceStats format, including\n        before/after counts and optional plan when dry_run=True.\n\n    Note:\n        This function delegates dataset discovery and compaction planning to the\n        shared ``fsspeckit.core.maintenance`` module for consistent behavior\n        with the PyArrow backend.\n    \"\"\"\n    from fsspeckit.core.maintenance import plan_compaction_groups, MaintenanceStats\n\n    if self._filesystem is None:\n        raise FileNotFoundError(\"Filesystem not initialized for compaction\")\n    filesystem = self._filesystem\n\n    # Get dataset stats using shared logic\n    stats_before = self._collect_dataset_stats(path, partition_filter)\n    files = stats_before[\"files\"]\n\n    # Use shared compaction planning\n    plan_result = plan_compaction_groups(\n        file_infos=files,\n        target_mb_per_file=target_mb_per_file,\n        target_rows_per_file=target_rows_per_file,\n    )\n\n    groups = plan_result[\"groups\"]\n    planned_stats = plan_result[\"planned_stats\"]\n\n    # Update planned stats with compression info\n    planned_stats.compression_codec = compression\n    planned_stats.dry_run = dry_run\n\n    if dry_run or not groups:\n        return planned_stats.to_dict()\n\n    # Execute compaction using DuckDB\n    conn = self._ensure_connection()\n    rewritten_bytes = 0\n\n    for group in groups:\n        # Read group into Arrow table using DuckDB for efficiency\n        paths = [file_info.path for file_info in group.files]\n        try:\n            scan_list = \",\".join([f\"'{p}'\" for p in paths])\n            table = conn.execute(\n                f\"SELECT * FROM parquet_scan([{scan_list}])\"\n            ).arrow()\n            if hasattr(table, \"read_all\"):\n                table = table.read_all()\n        except Exception:\n            # Fallback to pyarrow\n            import pyarrow.parquet as pq\n\n            tables = []\n            for p in paths:\n                with filesystem.open(p, \"rb\") as fh:\n                    tables.append(pq.read_table(fh))\n            table = pa.concat_tables(tables)\n\n        # Write compacted file\n        out_name = self._generate_unique_filename(\"compact-{}.parquet\")\n        out_path = str(Path(path) / out_name)\n        self.write_parquet(table, out_path, compression=compression or \"snappy\")\n\n        rewritten_bytes += group.total_size_bytes\n\n        # Remove original files\n        for file_info in group.files:\n            try:\n                filesystem.rm(file_info.path)\n            except Exception as e:\n                print(f\"Warning: failed to delete '{file_info.path}': {e}\")\n\n    # Recompute stats after compaction\n    stats_after = self._collect_dataset_stats(path, partition_filter=None)\n\n    # Create final stats\n    final_stats = MaintenanceStats(\n        before_file_count=planned_stats.before_file_count,\n        after_file_count=len(stats_after[\"files\"]),\n        before_total_bytes=planned_stats.before_total_bytes,\n        after_total_bytes=stats_after[\"total_bytes\"],\n        compacted_file_count=planned_stats.compacted_file_count,\n        rewritten_bytes=rewritten_bytes,\n        compression_codec=compression or \"snappy\",\n        dry_run=False,\n    )\n\n    return final_stats.to_dict()\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.DuckDBParquetHandler.execute_sql","title":"fsspeckit.datasets.DuckDBParquetHandler.execute_sql","text":"<pre><code>execute_sql(\n    query: str, parameters: list[Any] | None = None\n) -&gt; Table\n</code></pre> <p>Execute SQL query on parquet data and return results.</p> <p>Executes a SQL query using DuckDB and returns the results as a PyArrow table. The query can reference parquet files using the <code>parquet_scan()</code> function. Supports parameterized queries for safe value substitution.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>SQL query string. Use <code>parquet_scan('path')</code> to reference parquet files.</p> required <code>parameters</code> <code>list[Any] | None</code> <p>Optional list of parameter values for parameterized queries. Use <code>?</code> placeholders in the query string.</p> <code>None</code> <p>Returns:</p> Type Description <code>Table</code> <p>PyArrow table containing the query results.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If DuckDB encounters a SQL syntax error or query execution error.</p> <p>Examples:</p> <p>Simple query:</p> <pre><code>&gt;&gt;&gt; handler = DuckDBParquetHandler()\n&gt;&gt;&gt; result = handler.execute_sql(\n...     \"SELECT * FROM parquet_scan('/tmp/data.parquet') WHERE age &gt; 30\"\n... )\n</code></pre> <p>Parameterized query:</p> <pre><code>&gt;&gt;&gt; result = handler.execute_sql(\n...     \"SELECT * FROM parquet_scan('/tmp/data.parquet') WHERE age BETWEEN ? AND ?\",\n...     parameters=[25, 40]\n... )\n</code></pre> <p>Aggregation query:</p> <pre><code>&gt;&gt;&gt; result = handler.execute_sql(\n...     '''\n...     SELECT category, COUNT(*) as count, AVG(price) as avg_price\n...     FROM parquet_scan('/tmp/data.parquet')\n...     GROUP BY category\n...     ORDER BY count DESC\n...     '''\n... )\n</code></pre> <p>Join multiple parquet files:</p> <pre><code>&gt;&gt;&gt; result = handler.execute_sql(\n...     '''\n...     SELECT a.*, b.name\n...     FROM parquet_scan('/tmp/data1.parquet') a\n...     JOIN parquet_scan('/tmp/data2.parquet') b\n...     ON a.id = b.id\n...     '''\n... )\n</code></pre> <p>Window functions:</p> <pre><code>&gt;&gt;&gt; result = handler.execute_sql(\n...     '''\n...     SELECT\n...         date,\n...         revenue,\n...         AVG(revenue) OVER (ORDER BY date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) as moving_avg\n...     FROM parquet_scan('/tmp/sales.parquet')\n...     '''\n... )\n</code></pre> Source code in <code>src/fsspeckit/datasets/duckdb.py</code> <pre><code>def execute_sql(\n    self,\n    query: str,\n    parameters: list[Any] | None = None,\n) -&gt; pa.Table:\n    \"\"\"Execute SQL query on parquet data and return results.\n\n    Executes a SQL query using DuckDB and returns the results as a PyArrow table.\n    The query can reference parquet files using the `parquet_scan()` function.\n    Supports parameterized queries for safe value substitution.\n\n    Args:\n        query: SQL query string. Use `parquet_scan('path')` to reference parquet files.\n        parameters: Optional list of parameter values for parameterized queries.\n            Use `?` placeholders in the query string.\n\n    Returns:\n        PyArrow table containing the query results.\n\n    Raises:\n        Exception: If DuckDB encounters a SQL syntax error or query execution error.\n\n    Examples:\n        Simple query:\n        &gt;&gt;&gt; handler = DuckDBParquetHandler()\n        &gt;&gt;&gt; result = handler.execute_sql(\n        ...     \"SELECT * FROM parquet_scan('/tmp/data.parquet') WHERE age &gt; 30\"\n        ... )\n\n        Parameterized query:\n        &gt;&gt;&gt; result = handler.execute_sql(\n        ...     \"SELECT * FROM parquet_scan('/tmp/data.parquet') WHERE age BETWEEN ? AND ?\",\n        ...     parameters=[25, 40]\n        ... )\n\n        Aggregation query:\n        &gt;&gt;&gt; result = handler.execute_sql(\n        ...     '''\n        ...     SELECT category, COUNT(*) as count, AVG(price) as avg_price\n        ...     FROM parquet_scan('/tmp/data.parquet')\n        ...     GROUP BY category\n        ...     ORDER BY count DESC\n        ...     '''\n        ... )\n\n        Join multiple parquet files:\n        &gt;&gt;&gt; result = handler.execute_sql(\n        ...     '''\n        ...     SELECT a.*, b.name\n        ...     FROM parquet_scan('/tmp/data1.parquet') a\n        ...     JOIN parquet_scan('/tmp/data2.parquet') b\n        ...     ON a.id = b.id\n        ...     '''\n        ... )\n\n        Window functions:\n        &gt;&gt;&gt; result = handler.execute_sql(\n        ...     '''\n        ...     SELECT\n        ...         date,\n        ...         revenue,\n        ...         AVG(revenue) OVER (ORDER BY date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) as moving_avg\n        ...     FROM parquet_scan('/tmp/sales.parquet')\n        ...     '''\n        ... )\n    \"\"\"\n    conn = self._ensure_connection()\n\n    try:\n        if parameters is not None:\n            # Execute parameterized query\n            result = conn.execute(query, parameters).arrow()\n        else:\n            # Execute regular query\n            result = conn.execute(query).arrow()\n        # Convert RecordBatchReader to Table\n        if hasattr(result, \"read_all\"):\n            result = result.read_all()\n        return result\n    except Exception as e:\n        raise Exception(f\"Failed to execute SQL query: {e}\\nQuery: {query}\") from e\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.DuckDBParquetHandler.merge_parquet_dataset","title":"fsspeckit.datasets.DuckDBParquetHandler.merge_parquet_dataset","text":"<pre><code>merge_parquet_dataset(\n    source: Table | str,\n    target_path: str,\n    key_columns: list[str] | str,\n    strategy: MergeStrategy = \"upsert\",\n    dedup_order_by: list[str] | None = None,\n    compression: str = \"snappy\",\n    progress_callback: Callable[[str, int, int], None]\n    | None = None,\n) -&gt; dict[str, int]\n</code></pre> <p>Merge source data into target parquet dataset using specified strategy.</p> <p>Performs intelligent merge operations on parquet datasets with support for UPSERT, INSERT-only, UPDATE-only, FULL_MERGE (sync), and DEDUPLICATE strategies. Uses DuckDB's SQL engine for efficient merging with QUALIFY for deduplication.</p> <p>This implementation uses shared merge validation and semantics from fsspeckit.core.merge to ensure consistent behavior across all backends. All merge strategies share the same semantics, validation rules, and statistical calculations as the PyArrow implementation.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Table | str</code> <p>Source data as PyArrow table or path to parquet dataset.</p> required <code>target_path</code> <code>str</code> <p>Path to target parquet dataset directory.</p> required <code>key_columns</code> <code>list[str] | str</code> <p>Column(s) to use for matching records. Can be single column name (string) or list of column names for composite keys.</p> required <code>strategy</code> <code>MergeStrategy</code> <p>Merge strategy to use: - \"upsert\": Insert new records, update existing (default) - \"insert\": Insert only new records, ignore existing - \"update\": Update only existing records, ignore new - \"full_merge\": Insert, update, and delete (full sync with source) - \"deduplicate\": Remove duplicates from source, then upsert</p> <code>'upsert'</code> <code>dedup_order_by</code> <code>list[str] | None</code> <p>Columns to use for ordering when deduplicating (for \"deduplicate\" strategy). Keeps record with highest value. If None, uses first occurrence.</p> <code>None</code> <code>compression</code> <code>str</code> <p>Compression codec for output. Default is \"snappy\".</p> <code>'snappy'</code> <code>progress_callback</code> <code>Callable[[str, int, int], None] | None</code> <p>Optional callback function for progress tracking. Called with (stage, current, total) where stage is a string description.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, int]</code> <p>Dictionary with merge statistics: - \"inserted\": Number of records inserted - \"updated\": Number of records updated - \"deleted\": Number of records deleted - \"total\": Total records in merged dataset</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If strategy invalid, key columns missing, or NULL keys present.</p> <code>TypeError</code> <p>If source/target schemas incompatible.</p> <code>Exception</code> <p>If merge operation fails.</p> <p>Examples:</p> <p>UPSERT - insert new and update existing:</p> <pre><code>&gt;&gt;&gt; with DuckDBParquetHandler() as handler:\n...     stats = handler.merge_parquet_dataset(\n...         source=new_data_table,\n...         target_path=\"/data/customers/\",\n...         key_columns=[\"customer_id\"],\n...         strategy=\"upsert\"\n...     )\n...     print(f\"Inserted: {stats['inserted']}, Updated: {stats['updated']}\")\n</code></pre> <p>INSERT - add only new records:</p> <pre><code>&gt;&gt;&gt; stats = handler.merge_parquet_dataset(\n...     source=\"/staging/new_orders/\",\n...     target_path=\"/data/orders/\",\n...     key_columns=[\"order_id\"],\n...     strategy=\"insert\"\n... )\n... print(f\"Added {stats['inserted']} new orders\")\n</code></pre> <p>UPDATE - update existing only:</p> <pre><code>&gt;&gt;&gt; stats = handler.merge_parquet_dataset(\n...     source=product_updates,\n...     target_path=\"/data/products/\",\n...     key_columns=[\"product_id\"],\n...     strategy=\"update\"\n... )\n</code></pre> <p>FULL_MERGE - complete synchronization:</p> <pre><code>&gt;&gt;&gt; stats = handler.merge_parquet_dataset(\n...     source=authoritative_data,\n...     target_path=\"/data/inventory/\",\n...     key_columns=[\"item_id\"],\n...     strategy=\"full_merge\"\n... )\n... print(f\"Synced: +{stats['inserted']} -{stats['deleted']}\")\n</code></pre> <p>DEDUPLICATE - remove duplicates first:</p> <pre><code>&gt;&gt;&gt; stats = handler.merge_parquet_dataset(\n...     source=raw_data_with_dups,\n...     target_path=\"/data/transactions/\",\n...     key_columns=[\"transaction_id\"],\n...     strategy=\"deduplicate\",\n...     dedup_order_by=[\"timestamp\"]  # Keep latest\n... )\n</code></pre> <p>Composite key:</p> <pre><code>&gt;&gt;&gt; stats = handler.merge_parquet_dataset(\n...     source=updates,\n...     target_path=\"/data/sales/\",\n...     key_columns=[\"customer_id\", \"order_date\"],\n...     strategy=\"upsert\"\n... )\n</code></pre> Note <p>This implementation uses shared merge validation and semantics from fsspeckit.core.merge to ensure consistent behavior across all backends. Atomic operations are performed using temporary directories with backup-and-restore for error recovery.</p> Source code in <code>src/fsspeckit/datasets/duckdb.py</code> <pre><code>def merge_parquet_dataset(\n    self,\n    source: pa.Table | str,\n    target_path: str,\n    key_columns: list[str] | str,\n    strategy: MergeStrategy = \"upsert\",\n    dedup_order_by: list[str] | None = None,\n    compression: str = \"snappy\",\n    progress_callback: Callable[[str, int, int], None] | None = None,\n) -&gt; dict[str, int]:\n    \"\"\"Merge source data into target parquet dataset using specified strategy.\n\n    Performs intelligent merge operations on parquet datasets with support for\n    UPSERT, INSERT-only, UPDATE-only, FULL_MERGE (sync), and DEDUPLICATE strategies.\n    Uses DuckDB's SQL engine for efficient merging with QUALIFY for deduplication.\n\n    This implementation uses shared merge validation and semantics from fsspeckit.core.merge\n    to ensure consistent behavior across all backends. All merge strategies share the same\n    semantics, validation rules, and statistical calculations as the PyArrow implementation.\n\n    Args:\n        source: Source data as PyArrow table or path to parquet dataset.\n        target_path: Path to target parquet dataset directory.\n        key_columns: Column(s) to use for matching records. Can be single column\n            name (string) or list of column names for composite keys.\n        strategy: Merge strategy to use:\n            - \"upsert\": Insert new records, update existing (default)\n            - \"insert\": Insert only new records, ignore existing\n            - \"update\": Update only existing records, ignore new\n            - \"full_merge\": Insert, update, and delete (full sync with source)\n            - \"deduplicate\": Remove duplicates from source, then upsert\n        dedup_order_by: Columns to use for ordering when deduplicating (for\n            \"deduplicate\" strategy). Keeps record with highest value. If None,\n            uses first occurrence.\n        compression: Compression codec for output. Default is \"snappy\".\n        progress_callback: Optional callback function for progress tracking.\n            Called with (stage, current, total) where stage is a string description.\n\n    Returns:\n        Dictionary with merge statistics:\n            - \"inserted\": Number of records inserted\n            - \"updated\": Number of records updated\n            - \"deleted\": Number of records deleted\n            - \"total\": Total records in merged dataset\n\n    Raises:\n        ValueError: If strategy invalid, key columns missing, or NULL keys present.\n        TypeError: If source/target schemas incompatible.\n        Exception: If merge operation fails.\n\n    Examples:\n        UPSERT - insert new and update existing:\n        &gt;&gt;&gt; with DuckDBParquetHandler() as handler:\n        ...     stats = handler.merge_parquet_dataset(\n        ...         source=new_data_table,\n        ...         target_path=\"/data/customers/\",\n        ...         key_columns=[\"customer_id\"],\n        ...         strategy=\"upsert\"\n        ...     )\n        ...     print(f\"Inserted: {stats['inserted']}, Updated: {stats['updated']}\")\n\n        INSERT - add only new records:\n        &gt;&gt;&gt; stats = handler.merge_parquet_dataset(\n        ...     source=\"/staging/new_orders/\",\n        ...     target_path=\"/data/orders/\",\n        ...     key_columns=[\"order_id\"],\n        ...     strategy=\"insert\"\n        ... )\n        ... print(f\"Added {stats['inserted']} new orders\")\n\n        UPDATE - update existing only:\n        &gt;&gt;&gt; stats = handler.merge_parquet_dataset(\n        ...     source=product_updates,\n        ...     target_path=\"/data/products/\",\n        ...     key_columns=[\"product_id\"],\n        ...     strategy=\"update\"\n        ... )\n\n        FULL_MERGE - complete synchronization:\n        &gt;&gt;&gt; stats = handler.merge_parquet_dataset(\n        ...     source=authoritative_data,\n        ...     target_path=\"/data/inventory/\",\n        ...     key_columns=[\"item_id\"],\n        ...     strategy=\"full_merge\"\n        ... )\n        ... print(f\"Synced: +{stats['inserted']} -{stats['deleted']}\")\n\n        DEDUPLICATE - remove duplicates first:\n        &gt;&gt;&gt; stats = handler.merge_parquet_dataset(\n        ...     source=raw_data_with_dups,\n        ...     target_path=\"/data/transactions/\",\n        ...     key_columns=[\"transaction_id\"],\n        ...     strategy=\"deduplicate\",\n        ...     dedup_order_by=[\"timestamp\"]  # Keep latest\n        ... )\n\n        Composite key:\n        &gt;&gt;&gt; stats = handler.merge_parquet_dataset(\n        ...     source=updates,\n        ...     target_path=\"/data/sales/\",\n        ...     key_columns=[\"customer_id\", \"order_date\"],\n        ...     strategy=\"upsert\"\n        ... )\n\n    Note:\n        This implementation uses shared merge validation and semantics from fsspeckit.core.merge\n        to ensure consistent behavior across all backends. Atomic operations are performed using\n        temporary directories with backup-and-restore for error recovery.\n    \"\"\"\n    # Convert string strategy to core enum and validate\n    try:\n        core_strategy = CoreMergeStrategy(strategy)\n    except ValueError:\n        valid_strategies = {s.value for s in CoreMergeStrategy}\n        raise ValueError(\n            f\"Invalid strategy: '{strategy}'. Must be one of: {', '.join(sorted(valid_strategies))}\"\n        )\n\n    # Normalize key_columns using shared helper\n    normalized_keys = normalize_key_columns(key_columns)\n\n    conn = self._ensure_connection()\n\n    # Report progress start\n    if progress_callback:\n        progress_callback(\"Loading source data\", 0, 1)\n\n    # Load source data\n    if isinstance(source, str):\n        # Source is path to parquet dataset\n        source_table = self.read_parquet(source)\n    else:\n        # Source is PyArrow table\n        source_table = source\n\n    # Report progress for source loading\n    if progress_callback:\n        progress_callback(\"Loading target data\", 1, 4)\n\n    # Load target data (create empty if doesn't exist)\n    target_schema = None\n    target_table = None\n    if self._filesystem is not None and self._filesystem.exists(target_path):\n        target_table = self.read_parquet(target_path)\n        target_schema = target_table.schema\n\n    # Report progress for validation\n    if progress_callback:\n        progress_callback(\"Validating inputs\", 2, 4)\n\n    # Validate inputs using shared helpers\n    merge_plan = validate_merge_inputs(\n        source_table.schema, target_schema, normalized_keys, core_strategy\n    )\n    merge_plan.source_count = source_table.num_rows\n\n    # Validate strategy compatibility\n    validate_strategy_compatibility(\n        core_strategy, source_table.num_rows, target_table is not None\n    )\n\n    # Check for NULL keys using shared helper\n    check_null_keys(source_table, target_table, normalized_keys)\n\n    # Calculate pre-merge counts for statistics\n    target_count_before = target_table.num_rows if target_table else 0\n    source_count = source_table.num_rows\n\n    # Create empty target table if needed\n    if target_table is None:\n        target_table = pa.table(\n            {\n                col: pa.array([], type=source_table.schema.field(col).type)\n                for col in source_table.schema.names\n            }\n        )\n\n    # Register tables in DuckDB\n    conn.register(\"source_data\", source_table)\n    conn.register(\"target_dataset\", target_table)\n\n    # Report progress for merge execution\n    if progress_callback:\n        progress_callback(\"Executing merge strategy\", 3, 4)\n\n    # Configure DuckDB for optimal merge performance\n    conn.execute(\"SET memory_limit='1GB'\")\n    conn.execute(\"SET threads=4\")\n\n    # Enable DuckDB's parallel processing for large datasets\n    if source_table.num_rows &gt; 100000:\n        conn.execute(\"SET enable_progress_bar=true\")\n        conn.execute(\"SET preserve_insertion_order=false\")\n\n    # Execute merge based on strategy\n    merged_table = self._execute_merge_strategy(\n        conn, core_strategy, normalized_keys, dedup_order_by\n    )\n\n    # Calculate statistics using shared helper\n    merge_stats = calculate_merge_stats(\n        core_strategy, source_count, target_count_before, merged_table.num_rows\n    )\n    stats = merge_stats.to_dict()\n\n    # Report progress for writing results\n    if progress_callback:\n        progress_callback(\"Writing merged results\", 4, 4)\n\n    # Write merged result to temporary directory first, then move for atomicity\n    import tempfile\n    import shutil\n\n    with tempfile.TemporaryDirectory(prefix=f\"merge_{uuid.uuid4().hex[:8]}_\") as temp_dir:\n        temp_path = Path(temp_dir) / \"merged_dataset\"\n        temp_path_str = str(temp_path)\n\n        # Write merged result to temporary location\n        self.write_parquet_dataset(\n            merged_table, temp_path_str, mode=\"overwrite\", compression=compression\n        )\n\n        # Atomic move: if target exists, remove it first, then move temp data\n        if self._filesystem is not None and self._filesystem.exists(target_path):\n            # Make a backup in case something goes wrong\n            backup_path = f\"{target_path}.backup.{uuid.uuid4().hex[:8]}\"\n            try:\n                # Rename existing target to backup\n                self._filesystem.mv(target_path, backup_path)\n                # Move temp data to final location\n                self._filesystem.mv(temp_path_str, target_path)\n                # Remove backup after successful move\n                self._filesystem.rm(backup_path, recursive=True)\n            except Exception as e:\n                # Try to restore backup if move failed\n                if self._filesystem.exists(backup_path):\n                    self._filesystem.mv(backup_path, target_path)\n                raise Exception(f\"Atomic merge failed: {e}\")\n        else:\n            # No existing target, just move temp data\n            self._filesystem.mv(temp_path_str, target_path)\n\n    # Cleanup\n    try:\n        conn.unregister(\"source_data\")\n        conn.unregister(\"target_dataset\")\n        conn.unregister(\"merged_result\")\n    except Exception:\n        pass\n\n    return stats\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.DuckDBParquetHandler.optimize_parquet_dataset","title":"fsspeckit.datasets.DuckDBParquetHandler.optimize_parquet_dataset","text":"<pre><code>optimize_parquet_dataset(\n    path: str,\n    zorder_columns: list[str],\n    target_mb_per_file: int | None = None,\n    target_rows_per_file: int | None = None,\n    partition_filter: list[str] | None = None,\n    compression: str | None = None,\n    dry_run: bool = False,\n) -&gt; dict[str, Any]\n</code></pre> <p>Optimize a parquet dataset by clustering (approximate z-order) using shared planning.</p> <p>This function delegates optimization planning to the shared core module, ensuring consistent behavior across DuckDB and PyArrow backends.</p> <p>Reads dataset, orders rows by given columns, optionally groups into sized chunks similar to compaction, rewrites dataset (overwrite semantics). Supports dry-run.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Dataset directory path.</p> required <code>zorder_columns</code> <code>list[str]</code> <p>Columns to cluster by (must exist).</p> required <code>target_mb_per_file</code> <code>int | None</code> <p>Optional desired size per output file.</p> <code>None</code> <code>target_rows_per_file</code> <code>int | None</code> <p>Optional desired row cap per output file.</p> <code>None</code> <code>partition_filter</code> <code>list[str] | None</code> <p>Optional list of partition prefixes.</p> <code>None</code> <code>compression</code> <code>str | None</code> <p>Optional compression codec for output files.</p> <code>None</code> <code>dry_run</code> <code>bool</code> <p>If True, plan only.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Statistics dict following canonical MaintenanceStats format; may include</p> <code>dict[str, Any]</code> <p>planned grouping if dry-run=True.</p> Note <p>This function delegates optimization planning and validation to the shared <code>fsspeckit.core.maintenance.plan_optimize_groups</code> function, ensuring consistent behavior with the PyArrow backend.</p> Source code in <code>src/fsspeckit/datasets/duckdb.py</code> <pre><code>def optimize_parquet_dataset(\n    self,\n    path: str,\n    zorder_columns: list[str],\n    target_mb_per_file: int | None = None,\n    target_rows_per_file: int | None = None,\n    partition_filter: list[str] | None = None,\n    compression: str | None = None,\n    dry_run: bool = False,\n) -&gt; dict[str, Any]:\n    \"\"\"Optimize a parquet dataset by clustering (approximate z-order) using shared planning.\n\n    This function delegates optimization planning to the shared core module,\n    ensuring consistent behavior across DuckDB and PyArrow backends.\n\n    Reads dataset, orders rows by given columns, optionally groups into sized chunks\n    similar to compaction, rewrites dataset (overwrite semantics). Supports dry-run.\n\n    Args:\n        path: Dataset directory path.\n        zorder_columns: Columns to cluster by (must exist).\n        target_mb_per_file: Optional desired size per output file.\n        target_rows_per_file: Optional desired row cap per output file.\n        partition_filter: Optional list of partition prefixes.\n        compression: Optional compression codec for output files.\n        dry_run: If True, plan only.\n\n    Returns:\n        Statistics dict following canonical MaintenanceStats format; may include\n        planned grouping if dry-run=True.\n\n    Note:\n        This function delegates optimization planning and validation to the\n        shared ``fsspeckit.core.maintenance.plan_optimize_groups`` function,\n        ensuring consistent behavior with the PyArrow backend.\n    \"\"\"\n    from fsspeckit.core.maintenance import plan_optimize_groups, MaintenanceStats\n\n    if not zorder_columns:\n        raise ValueError(\"zorder_columns must be a non-empty list\")\n    if self._filesystem is None:\n        raise FileNotFoundError(\"Filesystem not initialized for optimization\")\n    filesystem = self._filesystem\n\n    # Get dataset stats using shared logic\n    stats_before = self._collect_dataset_stats(path, partition_filter)\n    files = stats_before[\"files\"]\n\n    # Load a sample table to inspect schema for z-order validation\n    sample_table = self.read_parquet(files[0][\"path\"])  # first file\n\n    # Use shared optimization planning with schema validation\n    plan_result = plan_optimize_groups(\n        file_infos=files,\n        zorder_columns=zorder_columns,\n        target_mb_per_file=target_mb_per_file,\n        target_rows_per_file=target_rows_per_file,\n        sample_schema=sample_table.schema,\n    )\n\n    groups = plan_result[\"groups\"]\n    planned_stats = plan_result[\"planned_stats\"]\n\n    # Update planned stats with compression info\n    planned_stats.compression_codec = compression\n    planned_stats.dry_run = dry_run\n\n    if dry_run or not groups:\n        return planned_stats.to_dict()\n\n    # Execute optimization using DuckDB\n    conn = self._ensure_connection()\n    compression_codec = compression or \"snappy\"\n    rewritten_bytes = 0\n\n    def _quote_identifier(identifier: str) -&gt; str:\n        escaped = identifier.replace('\"', '\"\"')\n        return f'\"{escaped}\"'\n\n    # Helper function to build ORDER BY clause with NULL handling\n    def _build_order_clause(columns: list[str]) -&gt; str:\n        order_parts = []\n        for col in columns:\n            quoted = _quote_identifier(col)\n            # Put NULLs last\n            order_parts.append(f\"({quoted} IS NULL) ASC\")\n            order_parts.append(f\"{quoted} ASC\")\n        return \", \".join(order_parts)\n\n    order_clause = _build_order_clause(zorder_columns)\n\n    # Process each group separately for more memory-efficient operation\n    written_paths: list[str] = []\n    for group_idx, group in enumerate(groups):\n        # Read group files and sort by z-order columns\n        paths = [file_info.path for file_info in group.files]\n        all_paths_sql = \",\".join([f\"'{p}'\" for p in paths])\n\n        query = f\"SELECT * FROM parquet_scan([{all_paths_sql}]) ORDER BY {order_clause}\"\n        ordered_table = conn.execute(query).arrow()\n        if hasattr(ordered_table, \"read_all\"):\n            ordered_table = ordered_table.read_all()\n\n        # Apply chunking within the group if needed\n        if target_rows_per_file and target_rows_per_file &gt; 0:\n            # Row-based splitting\n            num_rows = ordered_table.num_rows\n            chunks = []\n            for start in range(0, num_rows, target_rows_per_file):\n                end = min(start + target_rows_per_file, num_rows)\n                chunk = ordered_table.slice(start, end - start)\n                if chunk.num_rows &gt; 0:\n                    chunks.append(chunk)\n        else:\n            chunks = [ordered_table]\n\n        # Write optimized chunks\n        for chunk_idx, chunk in enumerate(chunks):\n            if len(groups) == 1:\n                # Single group case - use simple naming\n                filename = f\"optimized-{chunk_idx:05d}.parquet\"\n            else:\n                # Multiple groups - include group index\n                filename = f\"optimized-{group_idx:02d}-{chunk_idx:05d}.parquet\"\n\n            out_path = str(Path(path) / filename)\n            self.write_parquet(chunk, out_path, compression=compression_codec)\n            written_paths.append(out_path)\n\n        rewritten_bytes += group.total_size_bytes\n\n        # Remove original files in this group\n        for file_info in group.files:\n            try:\n                filesystem.rm(file_info.path)\n            except Exception:\n                pass\n\n    # Recompute stats after optimization\n    stats_after = self._collect_dataset_stats(path, partition_filter=partition_filter)\n\n    # Create final stats\n    final_stats = MaintenanceStats(\n        before_file_count=planned_stats.before_file_count,\n        after_file_count=len(stats_after[\"files\"]),\n        before_total_bytes=planned_stats.before_total_bytes,\n        after_total_bytes=stats_after[\"total_bytes\"],\n        compacted_file_count=planned_stats.compacted_file_count,\n        rewritten_bytes=rewritten_bytes,\n        compression_codec=compression_codec,\n        dry_run=False,\n        zorder_columns=zorder_columns,\n    )\n\n    return final_stats.to_dict()\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.DuckDBParquetHandler.read_parquet","title":"fsspeckit.datasets.DuckDBParquetHandler.read_parquet","text":"<pre><code>read_parquet(\n    path: str, columns: list[str] | None = None\n) -&gt; Table\n</code></pre> <p>Read parquet file or dataset directory.</p> <p>Reads a single parquet file or all parquet files in a directory and returns the data as a PyArrow table. Supports column projection for efficient reading of large datasets.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to parquet file or directory containing parquet files. Can be local path or remote URI (s3://, gs://, etc.).</p> required <code>columns</code> <code>list[str] | None</code> <p>Optional list of column names to read. If None, reads all columns. Specifying columns improves performance for large datasets.</p> <code>None</code> <p>Returns:</p> Type Description <code>Table</code> <p>PyArrow table containing the parquet data.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the specified path does not exist.</p> <code>Exception</code> <p>If DuckDB encounters an error reading the parquet file.</p> <p>Examples:</p> <p>Read entire parquet file:</p> <pre><code>&gt;&gt;&gt; handler = DuckDBParquetHandler()\n&gt;&gt;&gt; table = handler.read_parquet(\"/path/to/data.parquet\")\n</code></pre> <p>Read with column selection:</p> <pre><code>&gt;&gt;&gt; table = handler.read_parquet(\"/path/to/data.parquet\", columns=[\"col1\", \"col2\"])\n</code></pre> <p>Read parquet dataset directory:</p> <pre><code>&gt;&gt;&gt; table = handler.read_parquet(\"/path/to/dataset/\")\n</code></pre> <p>Read from S3:</p> <pre><code>&gt;&gt;&gt; from fsspeckit.storage_options import AwsStorageOptions\n&gt;&gt;&gt; handler = DuckDBParquetHandler(storage_options=AwsStorageOptions(...))\n&gt;&gt;&gt; table = handler.read_parquet(\"s3://bucket/data.parquet\")\n</code></pre> Source code in <code>src/fsspeckit/datasets/duckdb.py</code> <pre><code>def read_parquet(\n    self,\n    path: str,\n    columns: list[str] | None = None,\n) -&gt; pa.Table:\n    \"\"\"Read parquet file or dataset directory.\n\n    Reads a single parquet file or all parquet files in a directory and\n    returns the data as a PyArrow table. Supports column projection for\n    efficient reading of large datasets.\n\n    Args:\n        path: Path to parquet file or directory containing parquet files.\n            Can be local path or remote URI (s3://, gs://, etc.).\n        columns: Optional list of column names to read. If None, reads all columns.\n            Specifying columns improves performance for large datasets.\n\n    Returns:\n        PyArrow table containing the parquet data.\n\n    Raises:\n        FileNotFoundError: If the specified path does not exist.\n        Exception: If DuckDB encounters an error reading the parquet file.\n\n    Examples:\n        Read entire parquet file:\n        &gt;&gt;&gt; handler = DuckDBParquetHandler()\n        &gt;&gt;&gt; table = handler.read_parquet(\"/path/to/data.parquet\")\n\n        Read with column selection:\n        &gt;&gt;&gt; table = handler.read_parquet(\"/path/to/data.parquet\", columns=[\"col1\", \"col2\"])\n\n        Read parquet dataset directory:\n        &gt;&gt;&gt; table = handler.read_parquet(\"/path/to/dataset/\")\n\n        Read from S3:\n        &gt;&gt;&gt; from fsspeckit.storage_options import AwsStorageOptions\n        &gt;&gt;&gt; handler = DuckDBParquetHandler(storage_options=AwsStorageOptions(...))\n        &gt;&gt;&gt; table = handler.read_parquet(\"s3://bucket/data.parquet\")\n    \"\"\"\n    conn = self._ensure_connection()\n\n    # Check if path exists before executing DuckDB query\n    if self._filesystem is not None:\n        if not self._filesystem.exists(path):\n            raise FileNotFoundError(f\"Parquet path '{path}' does not exist\")\n\n    # Build column selection clause\n    columns_clause = \"*\" if columns is None else \", \".join(columns)\n\n    # Build query to read parquet\n    query = f\"SELECT {columns_clause} FROM parquet_scan('{path}')\"\n\n    try:\n        # Execute query and return as PyArrow table\n        result = conn.execute(query).arrow()\n        # Convert RecordBatchReader to Table\n        if hasattr(result, \"read_all\"):\n            result = result.read_all()\n        return result\n    except Exception as e:\n        # Preserve original error type and message when possible\n        error_msg = str(e)\n        if (\n            \"does not exist\" in error_msg.lower()\n            or \"not found\" in error_msg.lower()\n        ):\n            raise FileNotFoundError(\n                f\"Parquet path '{path}' does not exist: {error_msg}\"\n            ) from e\n        else:\n            # Re-raise with original exception type preserved\n            raise type(e)(\n                f\"Failed to read parquet from '{path}': {error_msg}\"\n            ) from e\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.DuckDBParquetHandler.write_parquet","title":"fsspeckit.datasets.DuckDBParquetHandler.write_parquet","text":"<pre><code>write_parquet(\n    table: Table, path: str, compression: str = \"snappy\"\n) -&gt; None\n</code></pre> <p>Write PyArrow table to parquet file.</p> <p>Writes a PyArrow table to a parquet file with configurable compression. Automatically creates parent directories if they don't exist.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>Table</code> <p>PyArrow table to write.</p> required <code>path</code> <code>str</code> <p>Output path for parquet file. Can be local path or remote URI.</p> required <code>compression</code> <code>str</code> <p>Compression codec to use. Supported values: \"snappy\", \"gzip\", \"lz4\", \"zstd\", \"brotli\", \"uncompressed\". Default is \"snappy\".</p> <code>'snappy'</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If DuckDB encounters an error writing the parquet file.</p> <p>Examples:</p> <p>Write with default compression:</p> <pre><code>&gt;&gt;&gt; import pyarrow as pa\n&gt;&gt;&gt; table = pa.table({'a': [1, 2, 3], 'b': ['x', 'y', 'z']})\n&gt;&gt;&gt; handler = DuckDBParquetHandler()\n&gt;&gt;&gt; handler.write_parquet(table, \"/tmp/output.parquet\")\n</code></pre> <p>Write with gzip compression:</p> <pre><code>&gt;&gt;&gt; handler.write_parquet(table, \"/tmp/output.parquet\", compression=\"gzip\")\n</code></pre> <p>Write to nested directory:</p> <pre><code>&gt;&gt;&gt; handler.write_parquet(table, \"/tmp/2024/01/15/data.parquet\")\n</code></pre> <p>Write to S3:</p> <pre><code>&gt;&gt;&gt; from fsspeckit.storage_options import AwsStorageOptions\n&gt;&gt;&gt; handler = DuckDBParquetHandler(storage_options=AwsStorageOptions(...))\n&gt;&gt;&gt; handler.write_parquet(table, \"s3://bucket/output.parquet\")\n</code></pre> Source code in <code>src/fsspeckit/datasets/duckdb.py</code> <pre><code>def write_parquet(\n    self,\n    table: pa.Table,\n    path: str,\n    compression: str = \"snappy\",\n) -&gt; None:\n    \"\"\"Write PyArrow table to parquet file.\n\n    Writes a PyArrow table to a parquet file with configurable compression.\n    Automatically creates parent directories if they don't exist.\n\n    Args:\n        table: PyArrow table to write.\n        path: Output path for parquet file. Can be local path or remote URI.\n        compression: Compression codec to use. Supported values: \"snappy\", \"gzip\",\n            \"lz4\", \"zstd\", \"brotli\", \"uncompressed\". Default is \"snappy\".\n\n    Raises:\n        Exception: If DuckDB encounters an error writing the parquet file.\n\n    Examples:\n        Write with default compression:\n        &gt;&gt;&gt; import pyarrow as pa\n        &gt;&gt;&gt; table = pa.table({'a': [1, 2, 3], 'b': ['x', 'y', 'z']})\n        &gt;&gt;&gt; handler = DuckDBParquetHandler()\n        &gt;&gt;&gt; handler.write_parquet(table, \"/tmp/output.parquet\")\n\n        Write with gzip compression:\n        &gt;&gt;&gt; handler.write_parquet(table, \"/tmp/output.parquet\", compression=\"gzip\")\n\n        Write to nested directory:\n        &gt;&gt;&gt; handler.write_parquet(table, \"/tmp/2024/01/15/data.parquet\")\n\n        Write to S3:\n        &gt;&gt;&gt; from fsspeckit.storage_options import AwsStorageOptions\n        &gt;&gt;&gt; handler = DuckDBParquetHandler(storage_options=AwsStorageOptions(...))\n        &gt;&gt;&gt; handler.write_parquet(table, \"s3://bucket/output.parquet\")\n    \"\"\"\n    conn = self._ensure_connection()\n\n    # Ensure parent directory exists and is not a file\n    parent_path = str(Path(path).parent)\n    if self._filesystem is not None:\n        # Check if parent path exists and is a file (not directory)\n        if self._filesystem.exists(parent_path):\n            if not self._filesystem.isdir(parent_path):\n                raise NotADirectoryError(\n                    f\"Parent directory '{parent_path}' exists but is a file. Cannot create file '{path}'.\"\n                )\n\n        try:\n            if not self._filesystem.exists(parent_path):\n                self._filesystem.makedirs(parent_path, exist_ok=True)\n        except Exception:\n            # Some filesystems may not support exists/makedirs on remote paths\n            # DuckDB will handle the path directly\n            pass\n\n    try:\n        # Register the table in DuckDB\n        conn.register(\"temp_table\", table)\n\n        # Use COPY command to write parquet\n        query = f\"COPY temp_table TO '{path}' (FORMAT PARQUET, COMPRESSION '{compression}')\"\n        conn.execute(query)\n\n        # Unregister the temporary table\n        conn.unregister(\"temp_table\")\n\n    except Exception as e:\n        # Clean up on error\n        try:\n            conn.unregister(\"temp_table\")\n        except Exception:\n            pass\n        raise Exception(f\"Failed to write parquet to '{path}': {e}\") from e\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.DuckDBParquetHandler.write_parquet_dataset","title":"fsspeckit.datasets.DuckDBParquetHandler.write_parquet_dataset","text":"<pre><code>write_parquet_dataset(\n    table: Table,\n    path: str,\n    mode: Literal[\"overwrite\", \"append\"] = \"append\",\n    max_rows_per_file: int | None = None,\n    compression: str = \"snappy\",\n    basename_template: str = \"part-{}.parquet\",\n) -&gt; None\n</code></pre> <p>Write PyArrow table to parquet dataset directory with unique filenames.</p> <p>Writes a PyArrow table to a directory as one or more parquet files with automatically generated unique filenames. Supports overwrite and append modes for managing existing datasets, and can split large tables across multiple files.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>Table</code> <p>PyArrow table to write.</p> required <code>path</code> <code>str</code> <p>Output directory path for the dataset. Can be local or remote URI.</p> required <code>mode</code> <code>Literal['overwrite', 'append']</code> <p>Write mode. \"append\" (default) adds files without deleting existing ones. \"overwrite\" deletes existing parquet files before writing.</p> <code>'append'</code> <code>max_rows_per_file</code> <code>int | None</code> <p>Optional maximum rows per file. If specified and table has more rows, splits into multiple files. If None, writes single file.</p> <code>None</code> <code>compression</code> <code>str</code> <p>Compression codec. Supported: \"snappy\", \"gzip\", \"lz4\", \"zstd\", \"brotli\", \"uncompressed\". Default is \"snappy\".</p> <code>'snappy'</code> <code>basename_template</code> <code>str</code> <p>Template for filenames with {} placeholder for unique ID. Default is \"part-{}.parquet\". The {} will be replaced with a short UUID.</p> <code>'part-{}.parquet'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If mode is invalid or max_rows_per_file &lt;= 0.</p> <code>Exception</code> <p>If filesystem operations or writing fails.</p> <p>Examples:</p> <p>Basic dataset write with unique filename:</p> <pre><code>&gt;&gt;&gt; import pyarrow as pa\n&gt;&gt;&gt; table = pa.table({'a': [1, 2, 3], 'b': ['x', 'y', 'z']})\n&gt;&gt;&gt; with DuckDBParquetHandler() as handler:\n...     handler.write_parquet_dataset(table, \"/tmp/dataset/\")\n...     # Creates: /tmp/dataset/part-a1b2c3d4.parquet\n</code></pre> <p>Append mode (incremental updates):</p> <pre><code>&gt;&gt;&gt; # First write\n&gt;&gt;&gt; handler.write_parquet_dataset(table1, \"/data/sales/\", mode=\"append\")\n&gt;&gt;&gt; # Second write (adds new file)\n&gt;&gt;&gt; handler.write_parquet_dataset(table2, \"/data/sales/\", mode=\"append\")\n&gt;&gt;&gt; # Read combined dataset\n&gt;&gt;&gt; result = handler.read_parquet(\"/data/sales/\")\n</code></pre> <p>Overwrite mode (replace dataset):</p> <pre><code>&gt;&gt;&gt; handler.write_parquet_dataset(\n...     new_table,\n...     \"/data/output/\",\n...     mode=\"overwrite\"  # Deletes existing parquet files\n... )\n</code></pre> <p>Split large table across multiple files:</p> <pre><code>&gt;&gt;&gt; large_table = pa.table({'id': range(10000), 'value': range(10000)})\n&gt;&gt;&gt; handler.write_parquet_dataset(\n...     large_table,\n...     \"/data/output/\",\n...     max_rows_per_file=2500  # Creates 4 files\n... )\n</code></pre> <p>Custom filename template:</p> <pre><code>&gt;&gt;&gt; handler.write_parquet_dataset(\n...     table,\n...     \"/data/output/\",\n...     basename_template=\"data_{}.parquet\"\n... )\n... # Creates: data_a1b2c3d4.parquet\n</code></pre> <p>With compression:</p> <pre><code>&gt;&gt;&gt; handler.write_parquet_dataset(\n...     table,\n...     \"/data/output/\",\n...     compression=\"gzip\"\n... )\n</code></pre> <p>Remote storage (S3):</p> <pre><code>&gt;&gt;&gt; from fsspeckit.storage_options import AwsStorageOptions\n&gt;&gt;&gt; handler = DuckDBParquetHandler(storage_options=AwsStorageOptions(...))\n&gt;&gt;&gt; handler.write_parquet_dataset(table, \"s3://bucket/dataset/\")\n</code></pre> Source code in <code>src/fsspeckit/datasets/duckdb.py</code> <pre><code>def write_parquet_dataset(\n    self,\n    table: pa.Table,\n    path: str,\n    mode: Literal[\"overwrite\", \"append\"] = \"append\",\n    max_rows_per_file: int | None = None,\n    compression: str = \"snappy\",\n    basename_template: str = \"part-{}.parquet\",\n) -&gt; None:\n    \"\"\"Write PyArrow table to parquet dataset directory with unique filenames.\n\n    Writes a PyArrow table to a directory as one or more parquet files with\n    automatically generated unique filenames. Supports overwrite and append modes\n    for managing existing datasets, and can split large tables across multiple files.\n\n    Args:\n        table: PyArrow table to write.\n        path: Output directory path for the dataset. Can be local or remote URI.\n        mode: Write mode. \"append\" (default) adds files without deleting existing ones.\n            \"overwrite\" deletes existing parquet files before writing.\n        max_rows_per_file: Optional maximum rows per file. If specified and table\n            has more rows, splits into multiple files. If None, writes single file.\n        compression: Compression codec. Supported: \"snappy\", \"gzip\", \"lz4\", \"zstd\",\n            \"brotli\", \"uncompressed\". Default is \"snappy\".\n        basename_template: Template for filenames with {} placeholder for unique ID.\n            Default is \"part-{}.parquet\". The {} will be replaced with a short UUID.\n\n    Raises:\n        ValueError: If mode is invalid or max_rows_per_file &lt;= 0.\n        Exception: If filesystem operations or writing fails.\n\n    Examples:\n        Basic dataset write with unique filename:\n        &gt;&gt;&gt; import pyarrow as pa\n        &gt;&gt;&gt; table = pa.table({'a': [1, 2, 3], 'b': ['x', 'y', 'z']})\n        &gt;&gt;&gt; with DuckDBParquetHandler() as handler:\n        ...     handler.write_parquet_dataset(table, \"/tmp/dataset/\")\n        ...     # Creates: /tmp/dataset/part-a1b2c3d4.parquet\n\n        Append mode (incremental updates):\n        &gt;&gt;&gt; # First write\n        &gt;&gt;&gt; handler.write_parquet_dataset(table1, \"/data/sales/\", mode=\"append\")\n        &gt;&gt;&gt; # Second write (adds new file)\n        &gt;&gt;&gt; handler.write_parquet_dataset(table2, \"/data/sales/\", mode=\"append\")\n        &gt;&gt;&gt; # Read combined dataset\n        &gt;&gt;&gt; result = handler.read_parquet(\"/data/sales/\")\n\n        Overwrite mode (replace dataset):\n        &gt;&gt;&gt; handler.write_parquet_dataset(\n        ...     new_table,\n        ...     \"/data/output/\",\n        ...     mode=\"overwrite\"  # Deletes existing parquet files\n        ... )\n\n        Split large table across multiple files:\n        &gt;&gt;&gt; large_table = pa.table({'id': range(10000), 'value': range(10000)})\n        &gt;&gt;&gt; handler.write_parquet_dataset(\n        ...     large_table,\n        ...     \"/data/output/\",\n        ...     max_rows_per_file=2500  # Creates 4 files\n        ... )\n\n        Custom filename template:\n        &gt;&gt;&gt; handler.write_parquet_dataset(\n        ...     table,\n        ...     \"/data/output/\",\n        ...     basename_template=\"data_{}.parquet\"\n        ... )\n        ... # Creates: data_a1b2c3d4.parquet\n\n        With compression:\n        &gt;&gt;&gt; handler.write_parquet_dataset(\n        ...     table,\n        ...     \"/data/output/\",\n        ...     compression=\"gzip\"\n        ... )\n\n        Remote storage (S3):\n        &gt;&gt;&gt; from fsspeckit.storage_options import AwsStorageOptions\n        &gt;&gt;&gt; handler = DuckDBParquetHandler(storage_options=AwsStorageOptions(...))\n        &gt;&gt;&gt; handler.write_parquet_dataset(table, \"s3://bucket/dataset/\")\n    \"\"\"\n    # Validate inputs\n    if mode not in (\"overwrite\", \"append\"):\n        raise ValueError(\n            f\"Invalid mode: '{mode}'. Must be 'overwrite' or 'append'.\"\n        )\n\n    if max_rows_per_file is not None and max_rows_per_file &lt;= 0:\n        raise ValueError(f\"max_rows_per_file must be &gt; 0, got {max_rows_per_file}\")\n\n    conn = self._ensure_connection()\n\n    # Ensure directory exists\n    if self._filesystem is not None:\n        # Check if path exists and is a file (not directory)\n        if self._filesystem.exists(path):\n            if not self._filesystem.isdir(path):\n                raise NotADirectoryError(\n                    f\"Dataset path '{path}' exists but is a file. Dataset paths must be directories.\"\n                )\n\n        try:\n            if not self._filesystem.exists(path):\n                self._filesystem.makedirs(path, exist_ok=True)\n        except Exception as e:\n            raise Exception(\n                f\"Failed to create dataset directory '{path}': {e}\"\n            ) from e\n\n    # Handle overwrite mode - clear existing parquet files\n    if mode == \"overwrite\":\n        self._clear_dataset(path)\n\n    # Determine how many files to write\n    if max_rows_per_file is not None and table.num_rows &gt; max_rows_per_file:\n        # Split table into multiple files\n        num_files = (table.num_rows + max_rows_per_file - 1) // max_rows_per_file\n\n        for i in range(num_files):\n            start_idx = i * max_rows_per_file\n            end_idx = min((i + 1) * max_rows_per_file, table.num_rows)\n            slice_table = table.slice(start_idx, end_idx - start_idx)\n\n            # Generate unique filename\n            filename = self._generate_unique_filename(basename_template)\n            file_path = str(Path(path) / filename)\n\n            # Write slice to file\n            try:\n                conn.register(\"temp_table\", slice_table)\n                query = f\"COPY temp_table TO '{file_path}' (FORMAT PARQUET, COMPRESSION '{compression}')\"\n                conn.execute(query)\n                conn.unregister(\"temp_table\")\n            except Exception as e:\n                try:\n                    conn.unregister(\"temp_table\")\n                except Exception:\n                    pass\n                raise Exception(\n                    f\"Failed to write parquet file '{file_path}': {e}\"\n                ) from e\n    else:\n        # Write single file\n        filename = self._generate_unique_filename(basename_template)\n        file_path = str(Path(path) / filename)\n\n        try:\n            conn.register(\"temp_table\", table)\n            query = f\"COPY temp_table TO '{file_path}' (FORMAT PARQUET, COMPRESSION '{compression}')\"\n            conn.execute(query)\n            conn.unregister(\"temp_table\")\n        except Exception as e:\n            try:\n                conn.unregister(\"temp_table\")\n            except Exception:\n                pass\n            raise Exception(\n                f\"Failed to write parquet file '{file_path}': {e}\"\n            ) from e\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets-functions","title":"Functions","text":""},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.cast_schema","title":"fsspeckit.datasets.cast_schema","text":"<pre><code>cast_schema(table: Table, schema: Schema) -&gt; Table\n</code></pre> <p>Cast a PyArrow table to a given schema, updating the schema to match the table's columns.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>Table</code> <p>The PyArrow table to cast.</p> required <code>schema</code> <code>Schema</code> <p>The target schema to cast the table to.</p> required <p>Returns:</p> Type Description <code>Table</code> <p>pa.Table: A new PyArrow table with the specified schema.</p> Source code in <code>src/fsspeckit/datasets/pyarrow.py</code> <pre><code>def cast_schema(table: pa.Table, schema: pa.Schema) -&gt; pa.Table:\n    \"\"\"\n    Cast a PyArrow table to a given schema, updating the schema to match the table's columns.\n\n    Args:\n        table (pa.Table): The PyArrow table to cast.\n        schema (pa.Schema): The target schema to cast the table to.\n\n    Returns:\n        pa.Table: A new PyArrow table with the specified schema.\n    \"\"\"\n    table_columns = set(table.schema.names)\n    for field in schema:\n        if field.name not in table_columns:\n            table = table.append_column(\n                field.name, pa.nulls(table.num_rows, type=field.type)\n            )\n    return table.cast(schema)\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.collect_dataset_stats_pyarrow","title":"fsspeckit.datasets.collect_dataset_stats_pyarrow","text":"<pre><code>collect_dataset_stats_pyarrow(\n    path: str,\n    filesystem: AbstractFileSystem | None = None,\n    partition_filter: list[str] | None = None,\n) -&gt; dict[str, Any]\n</code></pre> <p>Collect file-level statistics for a parquet dataset using shared core logic.</p> <p>This function delegates to the shared <code>fsspeckit.core.maintenance.collect_dataset_stats</code> function, ensuring consistent dataset discovery and statistics across both DuckDB and PyArrow backends.</p> <p>The helper walks the given dataset directory on the provided filesystem, discovers parquet files (recursively), and returns basic statistics:</p> <ul> <li>Per-file path, size in bytes, and number of rows</li> <li>Aggregated total bytes and total rows</li> </ul> <p>The function is intentionally streaming/metadata-driven and never materializes the full dataset as a single :class:<code>pyarrow.Table</code>.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Root directory of the parquet dataset.</p> required <code>filesystem</code> <code>AbstractFileSystem | None</code> <p>Optional fsspec filesystem. If omitted, a local \"file\" filesystem is used.</p> <code>None</code> <code>partition_filter</code> <code>list[str] | None</code> <p>Optional list of partition prefix filters (e.g. [\"date=2025-11-04\"]). Only files whose path relative to <code>path</code> starts with one of these prefixes are included.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict with keys:</p> <code>dict[str, Any]</code> <ul> <li><code>files</code>: list of <code>{\"path\", \"size_bytes\", \"num_rows\"}</code> dicts</li> </ul> <code>dict[str, Any]</code> <ul> <li><code>total_bytes</code>: sum of file sizes</li> </ul> <code>dict[str, Any]</code> <ul> <li><code>total_rows</code>: sum of row counts</li> </ul> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the path does not exist or no parquet files match the optional partition filter.</p> Note <p>This is a thin wrapper around the shared core function. See :func:<code>fsspeckit.core.maintenance.collect_dataset_stats</code> for the authoritative implementation.</p> Source code in <code>src/fsspeckit/datasets/pyarrow.py</code> <pre><code>def collect_dataset_stats_pyarrow(\n    path: str,\n    filesystem: AbstractFileSystem | None = None,\n    partition_filter: list[str] | None = None,\n) -&gt; dict[str, Any]:\n    \"\"\"Collect file-level statistics for a parquet dataset using shared core logic.\n\n    This function delegates to the shared ``fsspeckit.core.maintenance.collect_dataset_stats``\n    function, ensuring consistent dataset discovery and statistics across both DuckDB\n    and PyArrow backends.\n\n    The helper walks the given dataset directory on the provided filesystem,\n    discovers parquet files (recursively), and returns basic statistics:\n\n    - Per-file path, size in bytes, and number of rows\n    - Aggregated total bytes and total rows\n\n    The function is intentionally streaming/metadata-driven and never\n    materializes the full dataset as a single :class:`pyarrow.Table`.\n\n    Args:\n        path: Root directory of the parquet dataset.\n        filesystem: Optional fsspec filesystem. If omitted, a local \"file\"\n            filesystem is used.\n        partition_filter: Optional list of partition prefix filters\n            (e.g. [\"date=2025-11-04\"]). Only files whose path relative to\n            ``path`` starts with one of these prefixes are included.\n\n    Returns:\n        Dict with keys:\n\n        - ``files``: list of ``{\"path\", \"size_bytes\", \"num_rows\"}`` dicts\n        - ``total_bytes``: sum of file sizes\n        - ``total_rows``: sum of row counts\n\n    Raises:\n        FileNotFoundError: If the path does not exist or no parquet files\n            match the optional partition filter.\n\n    Note:\n        This is a thin wrapper around the shared core function. See\n        :func:`fsspeckit.core.maintenance.collect_dataset_stats` for the\n        authoritative implementation.\n    \"\"\"\n    from fsspeckit.core.maintenance import collect_dataset_stats\n\n    return collect_dataset_stats(\n        path=path,\n        filesystem=filesystem,\n        partition_filter=partition_filter,\n    )\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.compact_parquet_dataset_pyarrow","title":"fsspeckit.datasets.compact_parquet_dataset_pyarrow","text":"<pre><code>compact_parquet_dataset_pyarrow(\n    path: str,\n    target_mb_per_file: int | None = None,\n    target_rows_per_file: int | None = None,\n    partition_filter: list[str] | None = None,\n    compression: str | None = None,\n    dry_run: bool = False,\n    filesystem: AbstractFileSystem | None = None,\n) -&gt; dict[str, Any]\n</code></pre> <p>Compact a parquet dataset directory into fewer larger files using PyArrow and shared planning.</p> <p>Groups small files based on size (MB) and/or row thresholds, rewrites grouped files into new parquet files, and optionally changes compression. Supports a dry-run mode that returns the compaction plan without modifying files.</p> <p>The implementation uses the shared core planning algorithm for consistent behavior across backends. It processes data in a group-based, streaming fashion: it reads only the files in a given group into memory when processing that group and never materializes the entire dataset as a single table.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Dataset root directory (local path or fsspec URL).</p> required <code>target_mb_per_file</code> <code>int | None</code> <p>Optional max output size per file; must be &gt; 0.</p> <code>None</code> <code>target_rows_per_file</code> <code>int | None</code> <p>Optional max rows per output file; must be &gt; 0.</p> <code>None</code> <code>partition_filter</code> <code>list[str] | None</code> <p>Optional list of partition prefixes (e.g. <code>[\"date=2025-11-15\"]</code>) used to limit both stats collection and rewrites to matching paths.</p> <code>None</code> <code>compression</code> <code>str | None</code> <p>Optional parquet compression codec; defaults to <code>\"snappy\"</code>.</p> <code>None</code> <code>dry_run</code> <code>bool</code> <p>When <code>True</code> the function returns a plan + before/after stats without reading or writing any parquet data.</p> <code>False</code> <code>filesystem</code> <code>AbstractFileSystem | None</code> <p>Optional <code>fsspec.AbstractFileSystem</code> to reuse existing FS clients.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A stats dictionary describing before/after file counts, total bytes,</p> <code>dict[str, Any]</code> <p>rewritten bytes, and optional <code>planned_groups</code> when <code>dry_run</code> is enabled.</p> <code>dict[str, Any]</code> <p>The structure follows the canonical <code>MaintenanceStats</code> format from the shared core.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If thresholds are invalid or no files match partition filter.</p> <code>FileNotFoundError</code> <p>If the path does not exist.</p> Example <p>result = compact_parquet_dataset_pyarrow( ...     \"/path/to/dataset\", ...     target_mb_per_file=64, ...     dry_run=True ... ) print(f\"Files before: {result['before_file_count']}\") print(f\"Files after: {result['after_file_count']}\")</p> Note <p>This function delegates dataset discovery and compaction planning to the shared <code>fsspeckit.core.maintenance</code> module, ensuring consistent behavior across DuckDB and PyArrow backends.</p> Source code in <code>src/fsspeckit/datasets/pyarrow.py</code> <pre><code>def compact_parquet_dataset_pyarrow(\n    path: str,\n    target_mb_per_file: int | None = None,\n    target_rows_per_file: int | None = None,\n    partition_filter: list[str] | None = None,\n    compression: str | None = None,\n    dry_run: bool = False,\n    filesystem: AbstractFileSystem | None = None,\n) -&gt; dict[str, Any]:\n    \"\"\"Compact a parquet dataset directory into fewer larger files using PyArrow and shared planning.\n\n    Groups small files based on size (MB) and/or row thresholds, rewrites grouped\n    files into new parquet files, and optionally changes compression. Supports a\n    dry-run mode that returns the compaction plan without modifying files.\n\n    The implementation uses the shared core planning algorithm for consistent\n    behavior across backends. It processes data in a group-based, streaming fashion:\n    it reads only the files in a given group into memory when processing that group\n    and never materializes the entire dataset as a single table.\n\n    Args:\n        path: Dataset root directory (local path or fsspec URL).\n        target_mb_per_file: Optional max output size per file; must be &gt; 0.\n        target_rows_per_file: Optional max rows per output file; must be &gt; 0.\n        partition_filter: Optional list of partition prefixes (e.g. ``[\"date=2025-11-15\"]``)\n            used to limit both stats collection and rewrites to matching paths.\n        compression: Optional parquet compression codec; defaults to ``\"snappy\"``.\n        dry_run: When ``True`` the function returns a plan + before/after stats\n            without reading or writing any parquet data.\n        filesystem: Optional ``fsspec.AbstractFileSystem`` to reuse existing FS clients.\n\n    Returns:\n        A stats dictionary describing before/after file counts, total bytes,\n        rewritten bytes, and optional ``planned_groups`` when ``dry_run`` is enabled.\n        The structure follows the canonical ``MaintenanceStats`` format from the shared core.\n\n    Raises:\n        ValueError: If thresholds are invalid or no files match partition filter.\n        FileNotFoundError: If the path does not exist.\n\n    Example:\n        &gt;&gt;&gt; result = compact_parquet_dataset_pyarrow(\n        ...     \"/path/to/dataset\",\n        ...     target_mb_per_file=64,\n        ...     dry_run=True\n        ... )\n        &gt;&gt;&gt; print(f\"Files before: {result['before_file_count']}\")\n        &gt;&gt;&gt; print(f\"Files after: {result['after_file_count']}\")\n\n    Note:\n        This function delegates dataset discovery and compaction planning to the\n        shared ``fsspeckit.core.maintenance`` module, ensuring consistent behavior\n        across DuckDB and PyArrow backends.\n    \"\"\"\n    from fsspeckit.core.maintenance import plan_compaction_groups, MaintenanceStats\n\n    # Get dataset stats using shared logic\n    stats = collect_dataset_stats_pyarrow(\n        path=path, filesystem=filesystem, partition_filter=partition_filter\n    )\n    files = stats[\"files\"]\n\n    # Use shared compaction planning\n    plan_result = plan_compaction_groups(\n        file_infos=files,\n        target_mb_per_file=target_mb_per_file,\n        target_rows_per_file=target_rows_per_file,\n    )\n\n    groups = plan_result[\"groups\"]\n    planned_stats = plan_result[\"planned_stats\"]\n\n    # Update planned stats with compression info\n    planned_stats.compression_codec = compression\n    planned_stats.dry_run = dry_run\n\n    if dry_run or not groups:\n        return planned_stats.to_dict()\n\n    # Execute compaction using PyArrow\n    fs = filesystem or fsspec_filesystem(\"file\")\n    codec = compression or \"snappy\"\n    rewritten_bytes_live = 0\n\n    # Process each group: read, concatenate, write a new file, then delete\n    # originals for that group. This ensures peak memory is bounded by the\n    # group size.\n    for group_idx, group in enumerate(groups):\n        paths = [file_info.path for file_info in group.files]\n        tables: list[pa.Table] = []\n        for filename in paths:\n            with fs.open(filename, \"rb\") as fh:\n                tables.append(pq.read_table(fh))\n        combined = pa.concat_tables(tables, promote=True)\n\n        out_name = f\"compact-{group_idx:05d}.parquet\"\n        out_path = str(Path(path) / out_name)\n        with fs.open(out_path, \"wb\") as fh:\n            pq.write_table(combined, fh, compression=codec)\n\n        rewritten_bytes_live += group.total_size_bytes\n\n        # Remove original files in this group\n        for file_info in group.files:\n            try:\n                fs.rm(file_info.path)\n            except Exception:\n                # Best-effort cleanup; leave a warning to the caller.\n                print(f\"Warning: failed to delete '{file_info.path}' after compaction\")\n\n    # Recompute stats after compaction for the affected subset.\n    stats_after = collect_dataset_stats_pyarrow(\n        path=path, filesystem=fs, partition_filter=partition_filter\n    )\n\n    # Create final stats\n    final_stats = MaintenanceStats(\n        before_file_count=planned_stats.before_file_count,\n        after_file_count=len(stats_after[\"files\"]),\n        before_total_bytes=planned_stats.before_total_bytes,\n        after_total_bytes=stats_after[\"total_bytes\"],\n        compacted_file_count=planned_stats.compacted_file_count,\n        rewritten_bytes=rewritten_bytes_live,\n        compression_codec=codec,\n        dry_run=False,\n    )\n\n    return final_stats.to_dict()\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.convert_large_types_to_normal","title":"fsspeckit.datasets.convert_large_types_to_normal","text":"<pre><code>convert_large_types_to_normal(schema: Schema) -&gt; Schema\n</code></pre> <p>Convert large types in a PyArrow schema to their standard types.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>Schema</code> <p>The PyArrow schema to convert.</p> required <p>Returns:</p> Type Description <code>Schema</code> <p>pa.Schema: A new PyArrow schema with large types converted to standard types.</p> Source code in <code>src/fsspeckit/datasets/pyarrow.py</code> <pre><code>def convert_large_types_to_normal(schema: pa.Schema) -&gt; pa.Schema:\n    \"\"\"\n    Convert large types in a PyArrow schema to their standard types.\n\n    Args:\n        schema (pa.Schema): The PyArrow schema to convert.\n\n    Returns:\n        pa.Schema: A new PyArrow schema with large types converted to standard types.\n    \"\"\"\n    # Define mapping of large types to standard types\n    type_mapping = {\n        pa.large_string(): pa.string(),\n        pa.large_binary(): pa.binary(),\n        pa.large_utf8(): pa.utf8(),\n        pa.large_list(pa.null()): pa.list_(pa.null()),\n        pa.large_list_view(pa.null()): pa.list_view(pa.null()),\n    }\n    # Convert fields\n    new_fields = []\n    for field in schema:\n        field_type = field.type\n        # Check if type exists in mapping\n        if field_type in type_mapping:\n            new_field = pa.field(\n                name=field.name,\n                type=type_mapping[field_type],\n                nullable=field.nullable,\n                metadata=field.metadata,\n            )\n            new_fields.append(new_field)\n        # Handle large lists with nested types\n        elif isinstance(field_type, pa.LargeListType):\n            new_field = pa.field(\n                name=field.name,\n                type=pa.list_(\n                    type_mapping[field_type.value_type]\n                    if field_type.value_type in type_mapping\n                    else field_type.value_type\n                ),\n                nullable=field.nullable,\n                metadata=field.metadata,\n            )\n            new_fields.append(new_field)\n        # Handle dictionary with large_string, large_utf8, or large_binary values\n        elif isinstance(field_type, pa.DictionaryType):\n            new_field = pa.field(\n                name=field.name,\n                type=pa.dictionary(\n                    field_type.index_type,\n                    type_mapping[field_type.value_type]\n                    if field_type.value_type in type_mapping\n                    else field_type.value_type,\n                    field_type.ordered,\n                ),\n                # nullable=field.nullable,\n                metadata=field.metadata,\n            )\n            new_fields.append(new_field)\n        else:\n            new_fields.append(field)\n\n    return pa.schema(new_fields)\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.opt_dtype_pa","title":"fsspeckit.datasets.opt_dtype_pa","text":"<pre><code>opt_dtype_pa(\n    table: Table,\n    include: str | list[str] | None = None,\n    exclude: str | list[str] | None = None,\n    time_zone: str | None = None,\n    shrink_numerics: bool = False,\n    allow_unsigned: bool = True,\n    use_large_dtypes: bool = False,\n    strict: bool = False,\n    allow_null: bool = True,\n    sample_size: int | None = 1024,\n    sample_method: SampleMethod = \"first\",\n    *,\n    force_timezone: str | None = None,\n) -&gt; Table\n</code></pre> <p>Optimize data types of a PyArrow Table for performance and memory efficiency. Returns a new table casted to the optimal schema.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>Table</code> <p>The PyArrow table to optimize.</p> required <code>include</code> <code>str | list[str] | None</code> <p>Column(s) to include in optimization (default: all columns).</p> <code>None</code> <code>exclude</code> <code>str | list[str] | None</code> <p>Column(s) to exclude from optimization.</p> <code>None</code> <code>time_zone</code> <code>str | None</code> <p>Optional time zone hint during datetime parsing.</p> <code>None</code> <code>shrink_numerics</code> <code>bool</code> <p>Whether to downcast numeric types when possible.</p> <code>False</code> <code>allow_unsigned</code> <code>bool</code> <p>Whether to allow unsigned integer types.</p> <code>True</code> <code>use_large_dtypes</code> <code>bool</code> <p>If True, keep large types like large_string.</p> <code>False</code> <code>strict</code> <code>bool</code> <p>If True, will raise an error if any column cannot be optimized.</p> <code>False</code> <code>allow_null</code> <code>bool</code> <p>If False, columns that only hold null-like values will not be converted to pyarrow.null().</p> <code>True</code> <code>sample_size</code> <code>int | None</code> <p>Maximum number of cleaned values to inspect during regex inference (None to inspect all).</p> <code>1024</code> <code>sample_method</code> <code>SampleMethod</code> <p>Sampling strategy (<code>\"first\"</code> or <code>\"random\"</code>) for the inference subset.</p> <code>'first'</code> <code>force_timezone</code> <code>str | None</code> <p>If set, ensure all parsed datetime columns end up with this timezone.</p> <code>None</code> Source code in <code>src/fsspeckit/datasets/pyarrow.py</code> <pre><code>def opt_dtype(\n    table: pa.Table,\n    include: str | list[str] | None = None,\n    exclude: str | list[str] | None = None,\n    time_zone: str | None = None,\n    shrink_numerics: bool = False,\n    allow_unsigned: bool = True,\n    use_large_dtypes: bool = False,\n    strict: bool = False,\n    allow_null: bool = True,\n    sample_size: int | None = 1024,\n    sample_method: SampleMethod = \"first\",\n    *,\n    force_timezone: str | None = None,\n) -&gt; pa.Table:\n    \"\"\"\n    Optimize data types of a PyArrow Table for performance and memory efficiency.\n    Returns a new table casted to the optimal schema.\n\n    Args:\n        table: The PyArrow table to optimize.\n        include: Column(s) to include in optimization (default: all columns).\n        exclude: Column(s) to exclude from optimization.\n        time_zone: Optional time zone hint during datetime parsing.\n        shrink_numerics: Whether to downcast numeric types when possible.\n        allow_unsigned: Whether to allow unsigned integer types.\n        use_large_dtypes: If True, keep large types like large_string.\n        strict: If True, will raise an error if any column cannot be optimized.\n        allow_null: If False, columns that only hold null-like values will not be converted to pyarrow.null().\n        sample_size: Maximum number of cleaned values to inspect during regex inference (None to inspect all).\n        sample_method: Sampling strategy (`\"first\"` or `\"random\"`) for the inference subset.\n        force_timezone: If set, ensure all parsed datetime columns end up with this timezone.\n    \"\"\"\n    if sample_method not in (\"first\", \"random\"):\n        raise ValueError(\"sample_method must be 'first' or 'random'\")\n\n    if isinstance(include, str):\n        include = [include]\n    if isinstance(exclude, str):\n        exclude = [exclude]\n\n    cols_to_process = table.column_names\n    if include:\n        cols_to_process = [col for col in include if col in table.column_names]\n    if exclude:\n        cols_to_process = [col for col in cols_to_process if col not in exclude]\n\n    # Prepare arguments for parallel processing\n    args_list = [\n        (\n            table[col_name],\n            col_name,\n            cols_to_process,\n            shrink_numerics,\n            allow_unsigned,\n            time_zone,\n            strict,\n            allow_null,\n            force_timezone,\n            sample_size,\n            sample_method,\n        )\n        for col_name in table.column_names\n    ]\n\n    # Parallelize column processing\n    with concurrent.futures.ThreadPoolExecutor() as executor:\n        results = list(executor.map(_process_column_for_opt_dtype, args_list))\n\n    # Sort results to preserve column order\n    results.sort(key=lambda x: table.column_names.index(x[0]))\n    fields = [field for _, field, _ in results]\n    arrays = [array for _, _, array in results]\n\n    schema = pa.schema(fields)\n    if not use_large_dtypes:\n        schema = convert_large_types_to_normal(schema)\n    return pa.Table.from_arrays(arrays, schema=schema)\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.optimize_parquet_dataset_pyarrow","title":"fsspeckit.datasets.optimize_parquet_dataset_pyarrow","text":"<pre><code>optimize_parquet_dataset_pyarrow(\n    path: str,\n    zorder_columns: list[str],\n    target_mb_per_file: int | None = None,\n    target_rows_per_file: int | None = None,\n    partition_filter: list[str] | None = None,\n    compression: str | None = None,\n    dry_run: bool = False,\n    filesystem: AbstractFileSystem | None = None,\n) -&gt; dict[str, Any]\n</code></pre> <p>Cluster parquet files by <code>zorder_columns</code> while rewriting groups on disk.</p> <p>This function uses the shared core planning algorithm for consistent optimization behavior across backends. It processes data in a streaming, per-group fashion that avoids materializing the entire dataset.</p> <p>The helper enumerates individual parquet files under <code>path</code>, optionally filtering them by <code>partition_filter</code> prefixes so we never materialize the entire dataset with <code>dataset.to_table()</code>. Each optimization group is streamed file-by-file, sorted by <code>zorder_columns</code> in memory, and then rewritten to an <code>optimized-*.parquet</code> file while the original inputs are deleted only after a successful write. Use dry-run mode to inspect the planned groups/metrics before any data is touched.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Dataset root directory (local path or fsspec URL).</p> required <code>zorder_columns</code> <code>list[str]</code> <p>Ordered columns that determine clustering/ordering.</p> required <code>target_mb_per_file</code> <code>int | None</code> <p>Optional max output size per file; must be &gt; 0.</p> <code>None</code> <code>target_rows_per_file</code> <code>int | None</code> <p>Optional max rows per output file; must be &gt; 0.</p> <code>None</code> <code>partition_filter</code> <code>list[str] | None</code> <p>Optional list of partition prefixes (e.g. <code>[\"date=2025-11-15\"]</code>) used to limit both stats collection and rewrites to matching paths.</p> <code>None</code> <code>compression</code> <code>str | None</code> <p>Optional parquet compression codec; defaults to <code>\"snappy\"</code>.</p> <code>None</code> <code>dry_run</code> <code>bool</code> <p>When <code>True</code> the function returns a plan + before/after stats without reading or writing any parquet data.</p> <code>False</code> <code>filesystem</code> <code>AbstractFileSystem | None</code> <p>Optional <code>fsspec.AbstractFileSystem</code> to reuse existing FS clients.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A stats dictionary describing before/after file counts, total bytes,</p> <code>dict[str, Any]</code> <p>rewritten bytes, <code>zorder_columns</code>, and optional <code>planned_groups</code> when</p> <code>dict[str, Any]</code> <p><code>dry_run</code> is enabled. The structure follows the canonical <code>MaintenanceStats</code></p> <code>dict[str, Any]</code> <p>format from the shared core.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If thresholds are invalid or if any <code>zorder_columns</code> are missing.</p> <code>FileNotFoundError</code> <p>If the path does not exist or no parquet files are found.</p> Note <p>This function delegates optimization planning and validation to the shared <code>fsspeckit.core.maintenance.plan_optimize_groups</code> function, ensuring consistent behavior with DuckDB backend.</p> Source code in <code>src/fsspeckit/datasets/pyarrow.py</code> <pre><code>def optimize_parquet_dataset_pyarrow(\n    path: str,\n    zorder_columns: list[str],\n    target_mb_per_file: int | None = None,\n    target_rows_per_file: int | None = None,\n    partition_filter: list[str] | None = None,\n    compression: str | None = None,\n    dry_run: bool = False,\n    filesystem: AbstractFileSystem | None = None,\n) -&gt; dict[str, Any]:\n    \"\"\"Cluster parquet files by ``zorder_columns`` while rewriting groups on disk.\n\n    This function uses the shared core planning algorithm for consistent optimization\n    behavior across backends. It processes data in a streaming, per-group fashion\n    that avoids materializing the entire dataset.\n\n    The helper enumerates individual parquet files under ``path``, optionally\n    filtering them by ``partition_filter`` prefixes so we never materialize the\n    entire dataset with ``dataset.to_table()``. Each optimization group is streamed\n    file-by-file, sorted by ``zorder_columns`` in memory, and then rewritten to an\n    ``optimized-*.parquet`` file while the original inputs are deleted only after\n    a successful write. Use dry-run mode to inspect the planned groups/metrics\n    before any data is touched.\n\n    Args:\n        path: Dataset root directory (local path or fsspec URL).\n        zorder_columns: Ordered columns that determine clustering/ordering.\n        target_mb_per_file: Optional max output size per file; must be &gt; 0.\n        target_rows_per_file: Optional max rows per output file; must be &gt; 0.\n        partition_filter: Optional list of partition prefixes (e.g. ``[\"date=2025-11-15\"]``)\n            used to limit both stats collection and rewrites to matching paths.\n        compression: Optional parquet compression codec; defaults to ``\"snappy\"``.\n        dry_run: When ``True`` the function returns a plan + before/after stats\n            without reading or writing any parquet data.\n        filesystem: Optional ``fsspec.AbstractFileSystem`` to reuse existing FS clients.\n\n    Returns:\n        A stats dictionary describing before/after file counts, total bytes,\n        rewritten bytes, ``zorder_columns``, and optional ``planned_groups`` when\n        ``dry_run`` is enabled. The structure follows the canonical ``MaintenanceStats``\n        format from the shared core.\n\n    Raises:\n        ValueError: If thresholds are invalid or if any ``zorder_columns`` are missing.\n        FileNotFoundError: If the path does not exist or no parquet files are found.\n\n    Note:\n        This function delegates optimization planning and validation to the shared\n        ``fsspeckit.core.maintenance.plan_optimize_groups`` function, ensuring\n        consistent behavior with DuckDB backend.\n    \"\"\"\n    from fsspeckit.core.maintenance import (\n        MaintenanceStats,\n        plan_optimize_groups,\n    )\n\n    # Use shared core validation\n    if not zorder_columns:\n        raise ValueError(\"zorder_columns must be a non-empty list\")\n    if target_mb_per_file is not None and target_mb_per_file &lt;= 0:\n        raise ValueError(\"target_mb_per_file must be &gt; 0\")\n    if target_rows_per_file is not None and target_rows_per_file &lt;= 0:\n        raise ValueError(\"target_rows_per_file must be &gt; 0\")\n\n    # Collect dataset stats using shared core\n    stats = collect_dataset_stats_pyarrow(\n        path=path, filesystem=filesystem, partition_filter=partition_filter\n    )\n    files = stats[\"files\"]\n\n    if not files:\n        return {\n            \"before_file_count\": 0,\n            \"after_file_count\": 0,\n            \"before_total_bytes\": 0,\n            \"after_total_bytes\": 0,\n            \"compacted_file_count\": 0,\n            \"rewritten_bytes\": 0,\n            \"compression_codec\": compression,\n            \"dry_run\": dry_run,\n            \"zorder_columns\": list(zorder_columns),\n        }\n\n    fs = filesystem or fsspec_filesystem(\"file\")\n\n    # Get schema for validation (sample first file)\n    sample_path = str(files[0][\"path\"])\n    with fs.open(sample_path, \"rb\") as fh:\n        sample_table = pq.read_table(fh)\n\n    # Use shared core planning with z-order validation\n    result = plan_optimize_groups(\n        files,\n        zorder_columns=zorder_columns,\n        target_mb_per_file=target_mb_per_file,\n        target_rows_per_file=target_rows_per_file,\n        sample_schema=sample_table,\n    )\n\n    groups = result[\"groups\"]\n    planned_stats = result[\"planned_stats\"]\n\n    # Handle dry run\n    if dry_run:\n        final_stats = MaintenanceStats(\n            before_file_count=planned_stats.before_file_count,\n            after_file_count=planned_stats.after_file_count,\n            before_total_bytes=planned_stats.before_total_bytes,\n            after_total_bytes=planned_stats.after_total_bytes,\n            compacted_file_count=planned_stats.compacted_file_count,\n            rewritten_bytes=planned_stats.rewritten_bytes,\n            compression_codec=compression,\n            dry_run=True,\n            zorder_columns=list(zorder_columns),\n            planned_groups=planned_stats.planned_groups,\n        )\n        return final_stats.to_dict()\n\n    codec = compression or \"snappy\"\n    written_files = []\n\n    # Process each optimization group in streaming fashion\n    for group_idx, group in enumerate(groups):\n        if len(group.files) == 0:\n            continue\n\n        # Read tables for this group only (streaming per-group)\n        tables: list[pa.Table] = []\n        for file_info in group.files:\n            file_path = str(file_info.path)\n            with fs.open(file_path, \"rb\") as fh:\n                tables.append(pq.read_table(fh))\n\n        if not tables:\n            continue\n\n        # Sort the combined group data by z-order columns\n        combined = pa.concat_tables(tables, promote=True)\n        sort_keys = [(col, \"ascending\") for col in zorder_columns]\n        combined_sorted = combined.sort_by(sort_keys)\n\n        # Write sorted group to output file\n        out_name = f\"optimized-{group_idx:05d}.parquet\"\n        out_path = str(Path(path) / out_name)\n        with fs.open(out_path, \"wb\") as fh:\n            pq.write_table(combined_sorted, fh, compression=codec)\n\n        written_files.append(out_path)\n\n        # Delete original files in this group\n        for file_info in group.files:\n            file_path = str(file_info.path)\n            try:\n                fs.rm(file_path)\n            except Exception:\n                print(f\"Warning: failed to delete '{file_path}' during optimize\")\n\n    # Recompute stats after optimization\n    stats_after = collect_dataset_stats_pyarrow(\n        path=path, filesystem=fs, partition_filter=partition_filter\n    )\n\n    # Create final stats\n    final_stats = MaintenanceStats(\n        before_file_count=planned_stats.before_file_count,\n        after_file_count=len(stats_after[\"files\"]),\n        before_total_bytes=planned_stats.before_total_bytes,\n        after_total_bytes=stats_after[\"total_bytes\"],\n        compacted_file_count=planned_stats.compacted_file_count,\n        rewritten_bytes=stats_after[\"total_bytes\"],\n        compression_codec=codec,\n        dry_run=False,\n        zorder_columns=list(zorder_columns),\n    )\n\n    return final_stats.to_dict()\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.unify_schemas_pa","title":"fsspeckit.datasets.unify_schemas_pa","text":"<pre><code>unify_schemas_pa(\n    schemas: list[Schema],\n    use_large_dtypes: bool = False,\n    timezone: str | None = None,\n    standardize_timezones: bool = True,\n    verbose: bool = False,\n    remove_conflicting_columns: bool = False,\n) -&gt; Schema\n</code></pre> <p>Unify a list of PyArrow schemas into a single schema using intelligent conflict resolution.</p> <p>Parameters:</p> Name Type Description Default <code>schemas</code> <code>list[Schema]</code> <p>List of PyArrow schemas to unify.</p> required <code>use_large_dtypes</code> <code>bool</code> <p>If True, keep large types like large_string.</p> <code>False</code> <code>timezone</code> <code>str | None</code> <p>If specified, standardize all timestamp columns to this timezone. If \"auto\", use the most frequent timezone across schemas. If None, remove timezone from all timestamp columns.</p> <code>None</code> <code>standardize_timezones</code> <code>bool</code> <p>If True, standardize all timestamp columns to the most frequent timezone.</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>If True, print conflict resolution details for debugging.</p> <code>False</code> <code>remove_conflicting_columns</code> <code>bool</code> <p>If True, allows removal of columns with type conflicts as a fallback strategy instead of converting them. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Schema</code> <p>pa.Schema: A unified PyArrow schema.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no schemas are provided.</p> Source code in <code>src/fsspeckit/datasets/pyarrow.py</code> <pre><code>def unify_schemas(\n    schemas: list[pa.Schema],\n    use_large_dtypes: bool = False,\n    timezone: str | None = None,\n    standardize_timezones: bool = True,\n    verbose: bool = False,\n    remove_conflicting_columns: bool = False,\n) -&gt; pa.Schema:\n    \"\"\"\n    Unify a list of PyArrow schemas into a single schema using intelligent conflict resolution.\n\n    Args:\n        schemas (list[pa.Schema]): List of PyArrow schemas to unify.\n        use_large_dtypes (bool): If True, keep large types like large_string.\n        timezone (str | None): If specified, standardize all timestamp columns to this timezone.\n            If \"auto\", use the most frequent timezone across schemas.\n            If None, remove timezone from all timestamp columns.\n        standardize_timezones (bool): If True, standardize all timestamp columns to the most frequent timezone.\n        verbose (bool): If True, print conflict resolution details for debugging.\n        remove_conflicting_columns (bool): If True, allows removal of columns with type conflicts as a fallback\n            strategy instead of converting them. Defaults to False.\n\n    Returns:\n        pa.Schema: A unified PyArrow schema.\n\n    Raises:\n        ValueError: If no schemas are provided.\n    \"\"\"\n    if not schemas:\n        raise ValueError(\"At least one schema must be provided for unification\")\n\n    # Early exit for single schema\n    unique_schemas = _unique_schemas(schemas)\n    if len(unique_schemas) == 1:\n        result_schema = unique_schemas[0]\n        if standardize_timezones:\n            result_schema = standardize_schema_timezones([result_schema], timezone)[0]\n        return (\n            result_schema\n            if use_large_dtypes\n            else convert_large_types_to_normal(result_schema)\n        )\n\n    # Step 1: Find and resolve conflicts first\n    conflicts = _find_conflicting_fields(unique_schemas)\n    if conflicts and verbose:\n        _log_conflict_summary(conflicts, verbose)\n\n    if conflicts:\n        # Normalize schemas using intelligent promotion rules\n        unique_schemas = _normalize_schema_types(unique_schemas, conflicts)\n\n    # Step 2: Attempt unification with conflict-resolved schemas\n    try:\n        unified_schema = pa.unify_schemas(unique_schemas, promote_options=\"permissive\")\n\n        # Step 3: Apply timezone standardization to the unified result\n        if standardize_timezones:\n            unified_schema = standardize_schema_timezones([unified_schema], timezone)[0]\n\n        return (\n            unified_schema\n            if use_large_dtypes\n            else convert_large_types_to_normal(unified_schema)\n        )\n\n    except (pa.ArrowInvalid, pa.ArrowTypeError) as e:\n        # Step 4: Intelligent fallback strategies\n        if verbose:\n            print(f\"Primary unification failed: {e}\")\n            print(\"Attempting fallback strategies...\")\n\n        # Fallback 1: Try aggressive string conversion for remaining conflicts\n        try:\n            fallback_schema = _aggressive_fallback_unification(unique_schemas)\n            if standardize_timezones:\n                fallback_schema = standardize_schema_timezones(\n                    [fallback_schema], timezone\n                )[0]\n            if verbose:\n                print(\"\u2713 Aggressive fallback succeeded\")\n            return (\n                fallback_schema\n                if use_large_dtypes\n                else convert_large_types_to_normal(fallback_schema)\n            )\n\n        except Exception:\n            if verbose:\n                print(\"\u2717 Aggressive fallback failed\")\n\n        # Fallback 2: Remove conflicting fields (if enabled)\n        if remove_conflicting_columns:\n            try:\n                non_conflicting_schema = _remove_conflicting_fields(unique_schemas)\n                if standardize_timezones:\n                    non_conflicting_schema = standardize_schema_timezones(\n                        [non_conflicting_schema], timezone\n                    )[0]\n                if verbose:\n                    print(\"\u2713 Remove conflicting fields fallback succeeded\")\n                return (\n                    non_conflicting_schema\n                    if use_large_dtypes\n                    else convert_large_types_to_normal(non_conflicting_schema)\n                )\n\n            except Exception:\n                if verbose:\n                    print(\"\u2717 Remove conflicting fields fallback failed\")\n\n        # Fallback 3: Remove problematic fields that can't be unified\n        try:\n            minimal_schema = _remove_problematic_fields(unique_schemas)\n            if standardize_timezones:\n                minimal_schema = standardize_schema_timezones(\n                    [minimal_schema], timezone\n                )[0]\n            if verbose:\n                print(\"\u2713 Minimal schema (removed problematic fields) succeeded\")\n            return (\n                minimal_schema\n                if use_large_dtypes\n                else convert_large_types_to_normal(minimal_schema)\n            )\n\n        except Exception:\n            if verbose:\n                print(\"\u2717 Minimal schema fallback failed\")\n\n        # Fallback 4: Return first schema as last resort\n        if verbose:\n            print(\"\u2717 All fallback strategies failed, returning first schema\")\n\n        first_schema = unique_schemas[0]\n        if standardize_timezones:\n            first_schema = standardize_schema_timezones([first_schema], timezone)[0]\n        return (\n            first_schema\n            if use_large_dtypes\n            else convert_large_types_to_normal(first_schema)\n        )\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets-modules","title":"Modules","text":""},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.duckdb","title":"fsspeckit.datasets.duckdb","text":"<p>DuckDB-based parquet dataset handler with fsspec integration.</p> <p>This module provides a high-performance interface for reading and writing parquet datasets using DuckDB with support for various filesystems through fsspec. DuckDB provides excellent parquet support with SQL analytics capabilities.</p>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.duckdb-classes","title":"Classes","text":""},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.duckdb.DuckDBParquetHandler","title":"fsspeckit.datasets.duckdb.DuckDBParquetHandler","text":"<pre><code>DuckDBParquetHandler(\n    storage_options: BaseStorageOptions | None = None,\n    filesystem: AbstractFileSystem | None = None,\n)\n</code></pre> <p>Handler for parquet operations using DuckDB with fsspec integration.</p> <p>This class provides methods for reading and writing parquet files and datasets using DuckDB's high-performance parquet engine. It integrates with fsspec filesystems to support local and remote storage (S3, GCS, Azure, etc.).</p> <p>The handler can be initialized with either storage options or an existing filesystem instance. For remote filesystems, the fsspec filesystem is registered in DuckDB using <code>.register_filesystem(fs)</code> to enable direct access to remote paths.</p> <p>Parameters:</p> Name Type Description Default <code>storage_options</code> <code>BaseStorageOptions | None</code> <p>Storage configuration options (e.g., AwsStorageOptions). If provided, a filesystem is created from these options.</p> <code>None</code> <code>filesystem</code> <code>AbstractFileSystem | None</code> <p>An existing fsspec filesystem instance. Takes precedence over storage_options if both are provided.</p> <code>None</code> <p>Examples:</p> <p>Basic usage with local filesystem:</p> <pre><code>&gt;&gt;&gt; from fsspeckit.utils import DuckDBParquetHandler\n&gt;&gt;&gt; import pyarrow as pa\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create sample data\n&gt;&gt;&gt; table = pa.table({'a': [1, 2, 3], 'b': ['x', 'y', 'z']})\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Write and read parquet file\n&gt;&gt;&gt; with DuckDBParquetHandler() as handler:\n...     handler.write_parquet(table, \"/tmp/data.parquet\")\n...     result = handler.read_parquet(\"/tmp/data.parquet\")\n...     print(result)\n</code></pre> <p>Using with AWS S3:</p> <pre><code>&gt;&gt;&gt; from fsspeckit.storage_options import AwsStorageOptions\n&gt;&gt;&gt; from fsspeckit.utils import DuckDBParquetHandler\n&gt;&gt;&gt;\n&gt;&gt;&gt; options = AwsStorageOptions(\n...     access_key_id=\"YOUR_KEY\",\n...     secret_access_key=\"YOUR_SECRET\",\n...     region=\"us-east-1\"\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; with DuckDBParquetHandler(storage_options=options) as handler:\n...     # Read from S3\n...     table = handler.read_parquet(\"s3://bucket/data.parquet\")\n...\n...     # Execute SQL query on S3 data\n...     result = handler.execute_sql(\n...         \"SELECT * FROM parquet_scan('s3://bucket/data.parquet') WHERE col &gt; 10\"\n...     )\n</code></pre> <p>Using with existing filesystem:</p> <pre><code>&gt;&gt;&gt; from fsspeckit import filesystem\n&gt;&gt;&gt; from fsspeckit.utils import DuckDBParquetHandler\n&gt;&gt;&gt;\n&gt;&gt;&gt; fs = filesystem(\"file\")\n&gt;&gt;&gt; with DuckDBParquetHandler(filesystem=fs) as handler:\n...     result = handler.read_parquet(\"/path/to/data.parquet\")\n</code></pre> <p>SQL query execution:</p> <pre><code>&gt;&gt;&gt; with DuckDBParquetHandler() as handler:\n...     handler.write_parquet(table, \"/tmp/data.parquet\")\n...\n...     # Simple query\n...     result = handler.execute_sql(\n...         \"SELECT a, b FROM parquet_scan('/tmp/data.parquet') WHERE a &gt; 1\"\n...     )\n...\n...     # Parameterized query\n...     result = handler.execute_sql(\n...         \"SELECT * FROM parquet_scan('/tmp/data.parquet') WHERE a BETWEEN ? AND ?\",\n...         parameters=[1, 3]\n...     )\n...\n...     # Aggregation query\n...     result = handler.execute_sql(\n...         '''\n...         SELECT b, COUNT(*) as count, AVG(a) as avg_a\n...         FROM parquet_scan('/tmp/data.parquet')\n...         GROUP BY b\n...         '''\n...     )\n</code></pre> <p>Reading specific columns:</p> <pre><code>&gt;&gt;&gt; with DuckDBParquetHandler() as handler:\n...     # Only read columns 'a' and 'b'\n...     result = handler.read_parquet(\"/tmp/data.parquet\", columns=[\"a\", \"b\"])\n</code></pre> <p>Writing with compression:</p> <pre><code>&gt;&gt;&gt; with DuckDBParquetHandler() as handler:\n...     handler.write_parquet(table, \"/tmp/data.parquet\", compression=\"gzip\")\n...     handler.write_parquet(table, \"/tmp/data2.parquet\", compression=\"zstd\")\n</code></pre> <p>Initialize the DuckDB parquet handler.</p> <p>Parameters:</p> Name Type Description Default <code>storage_options</code> <code>BaseStorageOptions | None</code> <p>Storage configuration options. If provided, a filesystem is created from these options.</p> <code>None</code> <code>filesystem</code> <code>AbstractFileSystem | None</code> <p>An existing fsspec filesystem instance. Takes precedence over storage_options if both are provided.</p> <code>None</code> Source code in <code>src/fsspeckit/datasets/duckdb.py</code> <pre><code>def __init__(\n    self,\n    storage_options: \"BaseStorageOptions | None\" = None,\n    filesystem: AbstractFileSystem | None = None,\n) -&gt; None:\n    \"\"\"Initialize the DuckDB parquet handler.\n\n    Args:\n        storage_options: Storage configuration options. If provided, a filesystem\n            is created from these options.\n        filesystem: An existing fsspec filesystem instance. Takes precedence over\n            storage_options if both are provided.\n    \"\"\"\n    self._connection: duckdb.DuckDBPyConnection | None = None\n    self._filesystem: AbstractFileSystem | None = None\n    self._storage_options = storage_options\n\n    # Determine which filesystem to use\n    if filesystem is not None:\n        self._filesystem = filesystem\n    elif storage_options is not None:\n        self._filesystem = storage_options.to_filesystem()\n    else:\n        # Default to local filesystem\n        self._filesystem = fsspec_filesystem(\"file\")\n</code></pre> Functions\u00b6 <code></code> fsspeckit.datasets.duckdb.DuckDBParquetHandler.__del__ \u00b6 <pre><code>__del__() -&gt; None\n</code></pre> <p>Cleanup on deletion.</p> Source code in <code>src/fsspeckit/datasets/duckdb.py</code> <pre><code>def __del__(self) -&gt; None:\n    \"\"\"Cleanup on deletion.\"\"\"\n    self.close()\n</code></pre> <code></code> fsspeckit.datasets.duckdb.DuckDBParquetHandler.__enter__ \u00b6 <pre><code>__enter__() -&gt; DuckDBParquetHandler\n</code></pre> <p>Enter context manager.</p> <p>Returns:</p> Type Description <code>DuckDBParquetHandler</code> <p>Self for use in with statement.</p> Source code in <code>src/fsspeckit/datasets/duckdb.py</code> <pre><code>def __enter__(self) -&gt; \"DuckDBParquetHandler\":\n    \"\"\"Enter context manager.\n\n    Returns:\n        Self for use in with statement.\n    \"\"\"\n    self._ensure_connection()\n    return self\n</code></pre> <code></code> fsspeckit.datasets.duckdb.DuckDBParquetHandler.__exit__ \u00b6 <pre><code>__exit__(exc_type: Any, exc_val: Any, exc_tb: Any) -&gt; None\n</code></pre> <p>Exit context manager and close connection.</p> <p>Parameters:</p> Name Type Description Default <code>exc_type</code> <code>Any</code> <p>Exception type if an exception occurred.</p> required <code>exc_val</code> <code>Any</code> <p>Exception value if an exception occurred.</p> required <code>exc_tb</code> <code>Any</code> <p>Exception traceback if an exception occurred.</p> required Source code in <code>src/fsspeckit/datasets/duckdb.py</code> <pre><code>def __exit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -&gt; None:\n    \"\"\"Exit context manager and close connection.\n\n    Args:\n        exc_type: Exception type if an exception occurred.\n        exc_val: Exception value if an exception occurred.\n        exc_tb: Exception traceback if an exception occurred.\n    \"\"\"\n    self.close()\n</code></pre> <code></code> fsspeckit.datasets.duckdb.DuckDBParquetHandler.close \u00b6 <pre><code>close() -&gt; None\n</code></pre> <p>Close the DuckDB connection.</p> <p>This method is called automatically when using the context manager. Manual calls are only needed when not using the context manager pattern.</p> Source code in <code>src/fsspeckit/datasets/duckdb.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Close the DuckDB connection.\n\n    This method is called automatically when using the context manager.\n    Manual calls are only needed when not using the context manager pattern.\n    \"\"\"\n    if self._connection is not None:\n        self._connection.close()\n        self._connection = None\n</code></pre> <code></code> fsspeckit.datasets.duckdb.DuckDBParquetHandler.compact_parquet_dataset \u00b6 <pre><code>compact_parquet_dataset(\n    path: str,\n    target_mb_per_file: int | None = None,\n    target_rows_per_file: int | None = None,\n    partition_filter: list[str] | None = None,\n    compression: str | None = None,\n    dry_run: bool = False,\n) -&gt; dict[str, Any]\n</code></pre> <p>Compact a parquet dataset directory into fewer larger files using shared planning.</p> <p>This function delegates compaction planning to the shared core module, ensuring consistent behavior across DuckDB and PyArrow backends.</p> <p>Groups small files based on size (MB) and/or row thresholds, rewrites grouped files into new parquet files, optionally changing compression. Supports dry-run mode returning planned groups without writing.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Dataset directory path.</p> required <code>target_mb_per_file</code> <code>int | None</code> <p>Desired approximate size per output file (MB).</p> <code>None</code> <code>target_rows_per_file</code> <code>int | None</code> <p>Desired maximum rows per output file.</p> <code>None</code> <code>partition_filter</code> <code>list[str] | None</code> <p>Optional list of partition prefixes to restrict scope.</p> <code>None</code> <code>compression</code> <code>str | None</code> <p>Optional compression codec; defaults to existing or 'snappy'.</p> <code>None</code> <code>dry_run</code> <code>bool</code> <p>If True, plan only without modifying files.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Statistics dict following canonical MaintenanceStats format, including</p> <code>dict[str, Any]</code> <p>before/after counts and optional plan when dry_run=True.</p> Note <p>This function delegates dataset discovery and compaction planning to the shared <code>fsspeckit.core.maintenance</code> module for consistent behavior with the PyArrow backend.</p> Source code in <code>src/fsspeckit/datasets/duckdb.py</code> <pre><code>def compact_parquet_dataset(\n    self,\n    path: str,\n    target_mb_per_file: int | None = None,\n    target_rows_per_file: int | None = None,\n    partition_filter: list[str] | None = None,\n    compression: str | None = None,\n    dry_run: bool = False,\n) -&gt; dict[str, Any]:\n    \"\"\"Compact a parquet dataset directory into fewer larger files using shared planning.\n\n    This function delegates compaction planning to the shared core module,\n    ensuring consistent behavior across DuckDB and PyArrow backends.\n\n    Groups small files based on size (MB) and/or row thresholds, rewrites grouped\n    files into new parquet files, optionally changing compression. Supports\n    dry-run mode returning planned groups without writing.\n\n    Args:\n        path: Dataset directory path.\n        target_mb_per_file: Desired approximate size per output file (MB).\n        target_rows_per_file: Desired maximum rows per output file.\n        partition_filter: Optional list of partition prefixes to restrict scope.\n        compression: Optional compression codec; defaults to existing or 'snappy'.\n        dry_run: If True, plan only without modifying files.\n\n    Returns:\n        Statistics dict following canonical MaintenanceStats format, including\n        before/after counts and optional plan when dry_run=True.\n\n    Note:\n        This function delegates dataset discovery and compaction planning to the\n        shared ``fsspeckit.core.maintenance`` module for consistent behavior\n        with the PyArrow backend.\n    \"\"\"\n    from fsspeckit.core.maintenance import plan_compaction_groups, MaintenanceStats\n\n    if self._filesystem is None:\n        raise FileNotFoundError(\"Filesystem not initialized for compaction\")\n    filesystem = self._filesystem\n\n    # Get dataset stats using shared logic\n    stats_before = self._collect_dataset_stats(path, partition_filter)\n    files = stats_before[\"files\"]\n\n    # Use shared compaction planning\n    plan_result = plan_compaction_groups(\n        file_infos=files,\n        target_mb_per_file=target_mb_per_file,\n        target_rows_per_file=target_rows_per_file,\n    )\n\n    groups = plan_result[\"groups\"]\n    planned_stats = plan_result[\"planned_stats\"]\n\n    # Update planned stats with compression info\n    planned_stats.compression_codec = compression\n    planned_stats.dry_run = dry_run\n\n    if dry_run or not groups:\n        return planned_stats.to_dict()\n\n    # Execute compaction using DuckDB\n    conn = self._ensure_connection()\n    rewritten_bytes = 0\n\n    for group in groups:\n        # Read group into Arrow table using DuckDB for efficiency\n        paths = [file_info.path for file_info in group.files]\n        try:\n            scan_list = \",\".join([f\"'{p}'\" for p in paths])\n            table = conn.execute(\n                f\"SELECT * FROM parquet_scan([{scan_list}])\"\n            ).arrow()\n            if hasattr(table, \"read_all\"):\n                table = table.read_all()\n        except Exception:\n            # Fallback to pyarrow\n            import pyarrow.parquet as pq\n\n            tables = []\n            for p in paths:\n                with filesystem.open(p, \"rb\") as fh:\n                    tables.append(pq.read_table(fh))\n            table = pa.concat_tables(tables)\n\n        # Write compacted file\n        out_name = self._generate_unique_filename(\"compact-{}.parquet\")\n        out_path = str(Path(path) / out_name)\n        self.write_parquet(table, out_path, compression=compression or \"snappy\")\n\n        rewritten_bytes += group.total_size_bytes\n\n        # Remove original files\n        for file_info in group.files:\n            try:\n                filesystem.rm(file_info.path)\n            except Exception as e:\n                print(f\"Warning: failed to delete '{file_info.path}': {e}\")\n\n    # Recompute stats after compaction\n    stats_after = self._collect_dataset_stats(path, partition_filter=None)\n\n    # Create final stats\n    final_stats = MaintenanceStats(\n        before_file_count=planned_stats.before_file_count,\n        after_file_count=len(stats_after[\"files\"]),\n        before_total_bytes=planned_stats.before_total_bytes,\n        after_total_bytes=stats_after[\"total_bytes\"],\n        compacted_file_count=planned_stats.compacted_file_count,\n        rewritten_bytes=rewritten_bytes,\n        compression_codec=compression or \"snappy\",\n        dry_run=False,\n    )\n\n    return final_stats.to_dict()\n</code></pre> <code></code> fsspeckit.datasets.duckdb.DuckDBParquetHandler.execute_sql \u00b6 <pre><code>execute_sql(\n    query: str, parameters: list[Any] | None = None\n) -&gt; Table\n</code></pre> <p>Execute SQL query on parquet data and return results.</p> <p>Executes a SQL query using DuckDB and returns the results as a PyArrow table. The query can reference parquet files using the <code>parquet_scan()</code> function. Supports parameterized queries for safe value substitution.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>SQL query string. Use <code>parquet_scan('path')</code> to reference parquet files.</p> required <code>parameters</code> <code>list[Any] | None</code> <p>Optional list of parameter values for parameterized queries. Use <code>?</code> placeholders in the query string.</p> <code>None</code> <p>Returns:</p> Type Description <code>Table</code> <p>PyArrow table containing the query results.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If DuckDB encounters a SQL syntax error or query execution error.</p> <p>Examples:</p> <p>Simple query:</p> <pre><code>&gt;&gt;&gt; handler = DuckDBParquetHandler()\n&gt;&gt;&gt; result = handler.execute_sql(\n...     \"SELECT * FROM parquet_scan('/tmp/data.parquet') WHERE age &gt; 30\"\n... )\n</code></pre> <p>Parameterized query:</p> <pre><code>&gt;&gt;&gt; result = handler.execute_sql(\n...     \"SELECT * FROM parquet_scan('/tmp/data.parquet') WHERE age BETWEEN ? AND ?\",\n...     parameters=[25, 40]\n... )\n</code></pre> <p>Aggregation query:</p> <pre><code>&gt;&gt;&gt; result = handler.execute_sql(\n...     '''\n...     SELECT category, COUNT(*) as count, AVG(price) as avg_price\n...     FROM parquet_scan('/tmp/data.parquet')\n...     GROUP BY category\n...     ORDER BY count DESC\n...     '''\n... )\n</code></pre> <p>Join multiple parquet files:</p> <pre><code>&gt;&gt;&gt; result = handler.execute_sql(\n...     '''\n...     SELECT a.*, b.name\n...     FROM parquet_scan('/tmp/data1.parquet') a\n...     JOIN parquet_scan('/tmp/data2.parquet') b\n...     ON a.id = b.id\n...     '''\n... )\n</code></pre> <p>Window functions:</p> <pre><code>&gt;&gt;&gt; result = handler.execute_sql(\n...     '''\n...     SELECT\n...         date,\n...         revenue,\n...         AVG(revenue) OVER (ORDER BY date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) as moving_avg\n...     FROM parquet_scan('/tmp/sales.parquet')\n...     '''\n... )\n</code></pre> Source code in <code>src/fsspeckit/datasets/duckdb.py</code> <pre><code>def execute_sql(\n    self,\n    query: str,\n    parameters: list[Any] | None = None,\n) -&gt; pa.Table:\n    \"\"\"Execute SQL query on parquet data and return results.\n\n    Executes a SQL query using DuckDB and returns the results as a PyArrow table.\n    The query can reference parquet files using the `parquet_scan()` function.\n    Supports parameterized queries for safe value substitution.\n\n    Args:\n        query: SQL query string. Use `parquet_scan('path')` to reference parquet files.\n        parameters: Optional list of parameter values for parameterized queries.\n            Use `?` placeholders in the query string.\n\n    Returns:\n        PyArrow table containing the query results.\n\n    Raises:\n        Exception: If DuckDB encounters a SQL syntax error or query execution error.\n\n    Examples:\n        Simple query:\n        &gt;&gt;&gt; handler = DuckDBParquetHandler()\n        &gt;&gt;&gt; result = handler.execute_sql(\n        ...     \"SELECT * FROM parquet_scan('/tmp/data.parquet') WHERE age &gt; 30\"\n        ... )\n\n        Parameterized query:\n        &gt;&gt;&gt; result = handler.execute_sql(\n        ...     \"SELECT * FROM parquet_scan('/tmp/data.parquet') WHERE age BETWEEN ? AND ?\",\n        ...     parameters=[25, 40]\n        ... )\n\n        Aggregation query:\n        &gt;&gt;&gt; result = handler.execute_sql(\n        ...     '''\n        ...     SELECT category, COUNT(*) as count, AVG(price) as avg_price\n        ...     FROM parquet_scan('/tmp/data.parquet')\n        ...     GROUP BY category\n        ...     ORDER BY count DESC\n        ...     '''\n        ... )\n\n        Join multiple parquet files:\n        &gt;&gt;&gt; result = handler.execute_sql(\n        ...     '''\n        ...     SELECT a.*, b.name\n        ...     FROM parquet_scan('/tmp/data1.parquet') a\n        ...     JOIN parquet_scan('/tmp/data2.parquet') b\n        ...     ON a.id = b.id\n        ...     '''\n        ... )\n\n        Window functions:\n        &gt;&gt;&gt; result = handler.execute_sql(\n        ...     '''\n        ...     SELECT\n        ...         date,\n        ...         revenue,\n        ...         AVG(revenue) OVER (ORDER BY date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) as moving_avg\n        ...     FROM parquet_scan('/tmp/sales.parquet')\n        ...     '''\n        ... )\n    \"\"\"\n    conn = self._ensure_connection()\n\n    try:\n        if parameters is not None:\n            # Execute parameterized query\n            result = conn.execute(query, parameters).arrow()\n        else:\n            # Execute regular query\n            result = conn.execute(query).arrow()\n        # Convert RecordBatchReader to Table\n        if hasattr(result, \"read_all\"):\n            result = result.read_all()\n        return result\n    except Exception as e:\n        raise Exception(f\"Failed to execute SQL query: {e}\\nQuery: {query}\") from e\n</code></pre> <code></code> fsspeckit.datasets.duckdb.DuckDBParquetHandler.merge_parquet_dataset \u00b6 <pre><code>merge_parquet_dataset(\n    source: Table | str,\n    target_path: str,\n    key_columns: list[str] | str,\n    strategy: MergeStrategy = \"upsert\",\n    dedup_order_by: list[str] | None = None,\n    compression: str = \"snappy\",\n    progress_callback: Callable[[str, int, int], None]\n    | None = None,\n) -&gt; dict[str, int]\n</code></pre> <p>Merge source data into target parquet dataset using specified strategy.</p> <p>Performs intelligent merge operations on parquet datasets with support for UPSERT, INSERT-only, UPDATE-only, FULL_MERGE (sync), and DEDUPLICATE strategies. Uses DuckDB's SQL engine for efficient merging with QUALIFY for deduplication.</p> <p>This implementation uses shared merge validation and semantics from fsspeckit.core.merge to ensure consistent behavior across all backends. All merge strategies share the same semantics, validation rules, and statistical calculations as the PyArrow implementation.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Table | str</code> <p>Source data as PyArrow table or path to parquet dataset.</p> required <code>target_path</code> <code>str</code> <p>Path to target parquet dataset directory.</p> required <code>key_columns</code> <code>list[str] | str</code> <p>Column(s) to use for matching records. Can be single column name (string) or list of column names for composite keys.</p> required <code>strategy</code> <code>MergeStrategy</code> <p>Merge strategy to use: - \"upsert\": Insert new records, update existing (default) - \"insert\": Insert only new records, ignore existing - \"update\": Update only existing records, ignore new - \"full_merge\": Insert, update, and delete (full sync with source) - \"deduplicate\": Remove duplicates from source, then upsert</p> <code>'upsert'</code> <code>dedup_order_by</code> <code>list[str] | None</code> <p>Columns to use for ordering when deduplicating (for \"deduplicate\" strategy). Keeps record with highest value. If None, uses first occurrence.</p> <code>None</code> <code>compression</code> <code>str</code> <p>Compression codec for output. Default is \"snappy\".</p> <code>'snappy'</code> <code>progress_callback</code> <code>Callable[[str, int, int], None] | None</code> <p>Optional callback function for progress tracking. Called with (stage, current, total) where stage is a string description.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, int]</code> <p>Dictionary with merge statistics: - \"inserted\": Number of records inserted - \"updated\": Number of records updated - \"deleted\": Number of records deleted - \"total\": Total records in merged dataset</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If strategy invalid, key columns missing, or NULL keys present.</p> <code>TypeError</code> <p>If source/target schemas incompatible.</p> <code>Exception</code> <p>If merge operation fails.</p> <p>Examples:</p> <p>UPSERT - insert new and update existing:</p> <pre><code>&gt;&gt;&gt; with DuckDBParquetHandler() as handler:\n...     stats = handler.merge_parquet_dataset(\n...         source=new_data_table,\n...         target_path=\"/data/customers/\",\n...         key_columns=[\"customer_id\"],\n...         strategy=\"upsert\"\n...     )\n...     print(f\"Inserted: {stats['inserted']}, Updated: {stats['updated']}\")\n</code></pre> <p>INSERT - add only new records:</p> <pre><code>&gt;&gt;&gt; stats = handler.merge_parquet_dataset(\n...     source=\"/staging/new_orders/\",\n...     target_path=\"/data/orders/\",\n...     key_columns=[\"order_id\"],\n...     strategy=\"insert\"\n... )\n... print(f\"Added {stats['inserted']} new orders\")\n</code></pre> <p>UPDATE - update existing only:</p> <pre><code>&gt;&gt;&gt; stats = handler.merge_parquet_dataset(\n...     source=product_updates,\n...     target_path=\"/data/products/\",\n...     key_columns=[\"product_id\"],\n...     strategy=\"update\"\n... )\n</code></pre> <p>FULL_MERGE - complete synchronization:</p> <pre><code>&gt;&gt;&gt; stats = handler.merge_parquet_dataset(\n...     source=authoritative_data,\n...     target_path=\"/data/inventory/\",\n...     key_columns=[\"item_id\"],\n...     strategy=\"full_merge\"\n... )\n... print(f\"Synced: +{stats['inserted']} -{stats['deleted']}\")\n</code></pre> <p>DEDUPLICATE - remove duplicates first:</p> <pre><code>&gt;&gt;&gt; stats = handler.merge_parquet_dataset(\n...     source=raw_data_with_dups,\n...     target_path=\"/data/transactions/\",\n...     key_columns=[\"transaction_id\"],\n...     strategy=\"deduplicate\",\n...     dedup_order_by=[\"timestamp\"]  # Keep latest\n... )\n</code></pre> <p>Composite key:</p> <pre><code>&gt;&gt;&gt; stats = handler.merge_parquet_dataset(\n...     source=updates,\n...     target_path=\"/data/sales/\",\n...     key_columns=[\"customer_id\", \"order_date\"],\n...     strategy=\"upsert\"\n... )\n</code></pre> Note <p>This implementation uses shared merge validation and semantics from fsspeckit.core.merge to ensure consistent behavior across all backends. Atomic operations are performed using temporary directories with backup-and-restore for error recovery.</p> Source code in <code>src/fsspeckit/datasets/duckdb.py</code> <pre><code>def merge_parquet_dataset(\n    self,\n    source: pa.Table | str,\n    target_path: str,\n    key_columns: list[str] | str,\n    strategy: MergeStrategy = \"upsert\",\n    dedup_order_by: list[str] | None = None,\n    compression: str = \"snappy\",\n    progress_callback: Callable[[str, int, int], None] | None = None,\n) -&gt; dict[str, int]:\n    \"\"\"Merge source data into target parquet dataset using specified strategy.\n\n    Performs intelligent merge operations on parquet datasets with support for\n    UPSERT, INSERT-only, UPDATE-only, FULL_MERGE (sync), and DEDUPLICATE strategies.\n    Uses DuckDB's SQL engine for efficient merging with QUALIFY for deduplication.\n\n    This implementation uses shared merge validation and semantics from fsspeckit.core.merge\n    to ensure consistent behavior across all backends. All merge strategies share the same\n    semantics, validation rules, and statistical calculations as the PyArrow implementation.\n\n    Args:\n        source: Source data as PyArrow table or path to parquet dataset.\n        target_path: Path to target parquet dataset directory.\n        key_columns: Column(s) to use for matching records. Can be single column\n            name (string) or list of column names for composite keys.\n        strategy: Merge strategy to use:\n            - \"upsert\": Insert new records, update existing (default)\n            - \"insert\": Insert only new records, ignore existing\n            - \"update\": Update only existing records, ignore new\n            - \"full_merge\": Insert, update, and delete (full sync with source)\n            - \"deduplicate\": Remove duplicates from source, then upsert\n        dedup_order_by: Columns to use for ordering when deduplicating (for\n            \"deduplicate\" strategy). Keeps record with highest value. If None,\n            uses first occurrence.\n        compression: Compression codec for output. Default is \"snappy\".\n        progress_callback: Optional callback function for progress tracking.\n            Called with (stage, current, total) where stage is a string description.\n\n    Returns:\n        Dictionary with merge statistics:\n            - \"inserted\": Number of records inserted\n            - \"updated\": Number of records updated\n            - \"deleted\": Number of records deleted\n            - \"total\": Total records in merged dataset\n\n    Raises:\n        ValueError: If strategy invalid, key columns missing, or NULL keys present.\n        TypeError: If source/target schemas incompatible.\n        Exception: If merge operation fails.\n\n    Examples:\n        UPSERT - insert new and update existing:\n        &gt;&gt;&gt; with DuckDBParquetHandler() as handler:\n        ...     stats = handler.merge_parquet_dataset(\n        ...         source=new_data_table,\n        ...         target_path=\"/data/customers/\",\n        ...         key_columns=[\"customer_id\"],\n        ...         strategy=\"upsert\"\n        ...     )\n        ...     print(f\"Inserted: {stats['inserted']}, Updated: {stats['updated']}\")\n\n        INSERT - add only new records:\n        &gt;&gt;&gt; stats = handler.merge_parquet_dataset(\n        ...     source=\"/staging/new_orders/\",\n        ...     target_path=\"/data/orders/\",\n        ...     key_columns=[\"order_id\"],\n        ...     strategy=\"insert\"\n        ... )\n        ... print(f\"Added {stats['inserted']} new orders\")\n\n        UPDATE - update existing only:\n        &gt;&gt;&gt; stats = handler.merge_parquet_dataset(\n        ...     source=product_updates,\n        ...     target_path=\"/data/products/\",\n        ...     key_columns=[\"product_id\"],\n        ...     strategy=\"update\"\n        ... )\n\n        FULL_MERGE - complete synchronization:\n        &gt;&gt;&gt; stats = handler.merge_parquet_dataset(\n        ...     source=authoritative_data,\n        ...     target_path=\"/data/inventory/\",\n        ...     key_columns=[\"item_id\"],\n        ...     strategy=\"full_merge\"\n        ... )\n        ... print(f\"Synced: +{stats['inserted']} -{stats['deleted']}\")\n\n        DEDUPLICATE - remove duplicates first:\n        &gt;&gt;&gt; stats = handler.merge_parquet_dataset(\n        ...     source=raw_data_with_dups,\n        ...     target_path=\"/data/transactions/\",\n        ...     key_columns=[\"transaction_id\"],\n        ...     strategy=\"deduplicate\",\n        ...     dedup_order_by=[\"timestamp\"]  # Keep latest\n        ... )\n\n        Composite key:\n        &gt;&gt;&gt; stats = handler.merge_parquet_dataset(\n        ...     source=updates,\n        ...     target_path=\"/data/sales/\",\n        ...     key_columns=[\"customer_id\", \"order_date\"],\n        ...     strategy=\"upsert\"\n        ... )\n\n    Note:\n        This implementation uses shared merge validation and semantics from fsspeckit.core.merge\n        to ensure consistent behavior across all backends. Atomic operations are performed using\n        temporary directories with backup-and-restore for error recovery.\n    \"\"\"\n    # Convert string strategy to core enum and validate\n    try:\n        core_strategy = CoreMergeStrategy(strategy)\n    except ValueError:\n        valid_strategies = {s.value for s in CoreMergeStrategy}\n        raise ValueError(\n            f\"Invalid strategy: '{strategy}'. Must be one of: {', '.join(sorted(valid_strategies))}\"\n        )\n\n    # Normalize key_columns using shared helper\n    normalized_keys = normalize_key_columns(key_columns)\n\n    conn = self._ensure_connection()\n\n    # Report progress start\n    if progress_callback:\n        progress_callback(\"Loading source data\", 0, 1)\n\n    # Load source data\n    if isinstance(source, str):\n        # Source is path to parquet dataset\n        source_table = self.read_parquet(source)\n    else:\n        # Source is PyArrow table\n        source_table = source\n\n    # Report progress for source loading\n    if progress_callback:\n        progress_callback(\"Loading target data\", 1, 4)\n\n    # Load target data (create empty if doesn't exist)\n    target_schema = None\n    target_table = None\n    if self._filesystem is not None and self._filesystem.exists(target_path):\n        target_table = self.read_parquet(target_path)\n        target_schema = target_table.schema\n\n    # Report progress for validation\n    if progress_callback:\n        progress_callback(\"Validating inputs\", 2, 4)\n\n    # Validate inputs using shared helpers\n    merge_plan = validate_merge_inputs(\n        source_table.schema, target_schema, normalized_keys, core_strategy\n    )\n    merge_plan.source_count = source_table.num_rows\n\n    # Validate strategy compatibility\n    validate_strategy_compatibility(\n        core_strategy, source_table.num_rows, target_table is not None\n    )\n\n    # Check for NULL keys using shared helper\n    check_null_keys(source_table, target_table, normalized_keys)\n\n    # Calculate pre-merge counts for statistics\n    target_count_before = target_table.num_rows if target_table else 0\n    source_count = source_table.num_rows\n\n    # Create empty target table if needed\n    if target_table is None:\n        target_table = pa.table(\n            {\n                col: pa.array([], type=source_table.schema.field(col).type)\n                for col in source_table.schema.names\n            }\n        )\n\n    # Register tables in DuckDB\n    conn.register(\"source_data\", source_table)\n    conn.register(\"target_dataset\", target_table)\n\n    # Report progress for merge execution\n    if progress_callback:\n        progress_callback(\"Executing merge strategy\", 3, 4)\n\n    # Configure DuckDB for optimal merge performance\n    conn.execute(\"SET memory_limit='1GB'\")\n    conn.execute(\"SET threads=4\")\n\n    # Enable DuckDB's parallel processing for large datasets\n    if source_table.num_rows &gt; 100000:\n        conn.execute(\"SET enable_progress_bar=true\")\n        conn.execute(\"SET preserve_insertion_order=false\")\n\n    # Execute merge based on strategy\n    merged_table = self._execute_merge_strategy(\n        conn, core_strategy, normalized_keys, dedup_order_by\n    )\n\n    # Calculate statistics using shared helper\n    merge_stats = calculate_merge_stats(\n        core_strategy, source_count, target_count_before, merged_table.num_rows\n    )\n    stats = merge_stats.to_dict()\n\n    # Report progress for writing results\n    if progress_callback:\n        progress_callback(\"Writing merged results\", 4, 4)\n\n    # Write merged result to temporary directory first, then move for atomicity\n    import tempfile\n    import shutil\n\n    with tempfile.TemporaryDirectory(prefix=f\"merge_{uuid.uuid4().hex[:8]}_\") as temp_dir:\n        temp_path = Path(temp_dir) / \"merged_dataset\"\n        temp_path_str = str(temp_path)\n\n        # Write merged result to temporary location\n        self.write_parquet_dataset(\n            merged_table, temp_path_str, mode=\"overwrite\", compression=compression\n        )\n\n        # Atomic move: if target exists, remove it first, then move temp data\n        if self._filesystem is not None and self._filesystem.exists(target_path):\n            # Make a backup in case something goes wrong\n            backup_path = f\"{target_path}.backup.{uuid.uuid4().hex[:8]}\"\n            try:\n                # Rename existing target to backup\n                self._filesystem.mv(target_path, backup_path)\n                # Move temp data to final location\n                self._filesystem.mv(temp_path_str, target_path)\n                # Remove backup after successful move\n                self._filesystem.rm(backup_path, recursive=True)\n            except Exception as e:\n                # Try to restore backup if move failed\n                if self._filesystem.exists(backup_path):\n                    self._filesystem.mv(backup_path, target_path)\n                raise Exception(f\"Atomic merge failed: {e}\")\n        else:\n            # No existing target, just move temp data\n            self._filesystem.mv(temp_path_str, target_path)\n\n    # Cleanup\n    try:\n        conn.unregister(\"source_data\")\n        conn.unregister(\"target_dataset\")\n        conn.unregister(\"merged_result\")\n    except Exception:\n        pass\n\n    return stats\n</code></pre> <code></code> fsspeckit.datasets.duckdb.DuckDBParquetHandler.optimize_parquet_dataset \u00b6 <pre><code>optimize_parquet_dataset(\n    path: str,\n    zorder_columns: list[str],\n    target_mb_per_file: int | None = None,\n    target_rows_per_file: int | None = None,\n    partition_filter: list[str] | None = None,\n    compression: str | None = None,\n    dry_run: bool = False,\n) -&gt; dict[str, Any]\n</code></pre> <p>Optimize a parquet dataset by clustering (approximate z-order) using shared planning.</p> <p>This function delegates optimization planning to the shared core module, ensuring consistent behavior across DuckDB and PyArrow backends.</p> <p>Reads dataset, orders rows by given columns, optionally groups into sized chunks similar to compaction, rewrites dataset (overwrite semantics). Supports dry-run.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Dataset directory path.</p> required <code>zorder_columns</code> <code>list[str]</code> <p>Columns to cluster by (must exist).</p> required <code>target_mb_per_file</code> <code>int | None</code> <p>Optional desired size per output file.</p> <code>None</code> <code>target_rows_per_file</code> <code>int | None</code> <p>Optional desired row cap per output file.</p> <code>None</code> <code>partition_filter</code> <code>list[str] | None</code> <p>Optional list of partition prefixes.</p> <code>None</code> <code>compression</code> <code>str | None</code> <p>Optional compression codec for output files.</p> <code>None</code> <code>dry_run</code> <code>bool</code> <p>If True, plan only.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Statistics dict following canonical MaintenanceStats format; may include</p> <code>dict[str, Any]</code> <p>planned grouping if dry-run=True.</p> Note <p>This function delegates optimization planning and validation to the shared <code>fsspeckit.core.maintenance.plan_optimize_groups</code> function, ensuring consistent behavior with the PyArrow backend.</p> Source code in <code>src/fsspeckit/datasets/duckdb.py</code> <pre><code>def optimize_parquet_dataset(\n    self,\n    path: str,\n    zorder_columns: list[str],\n    target_mb_per_file: int | None = None,\n    target_rows_per_file: int | None = None,\n    partition_filter: list[str] | None = None,\n    compression: str | None = None,\n    dry_run: bool = False,\n) -&gt; dict[str, Any]:\n    \"\"\"Optimize a parquet dataset by clustering (approximate z-order) using shared planning.\n\n    This function delegates optimization planning to the shared core module,\n    ensuring consistent behavior across DuckDB and PyArrow backends.\n\n    Reads dataset, orders rows by given columns, optionally groups into sized chunks\n    similar to compaction, rewrites dataset (overwrite semantics). Supports dry-run.\n\n    Args:\n        path: Dataset directory path.\n        zorder_columns: Columns to cluster by (must exist).\n        target_mb_per_file: Optional desired size per output file.\n        target_rows_per_file: Optional desired row cap per output file.\n        partition_filter: Optional list of partition prefixes.\n        compression: Optional compression codec for output files.\n        dry_run: If True, plan only.\n\n    Returns:\n        Statistics dict following canonical MaintenanceStats format; may include\n        planned grouping if dry-run=True.\n\n    Note:\n        This function delegates optimization planning and validation to the\n        shared ``fsspeckit.core.maintenance.plan_optimize_groups`` function,\n        ensuring consistent behavior with the PyArrow backend.\n    \"\"\"\n    from fsspeckit.core.maintenance import plan_optimize_groups, MaintenanceStats\n\n    if not zorder_columns:\n        raise ValueError(\"zorder_columns must be a non-empty list\")\n    if self._filesystem is None:\n        raise FileNotFoundError(\"Filesystem not initialized for optimization\")\n    filesystem = self._filesystem\n\n    # Get dataset stats using shared logic\n    stats_before = self._collect_dataset_stats(path, partition_filter)\n    files = stats_before[\"files\"]\n\n    # Load a sample table to inspect schema for z-order validation\n    sample_table = self.read_parquet(files[0][\"path\"])  # first file\n\n    # Use shared optimization planning with schema validation\n    plan_result = plan_optimize_groups(\n        file_infos=files,\n        zorder_columns=zorder_columns,\n        target_mb_per_file=target_mb_per_file,\n        target_rows_per_file=target_rows_per_file,\n        sample_schema=sample_table.schema,\n    )\n\n    groups = plan_result[\"groups\"]\n    planned_stats = plan_result[\"planned_stats\"]\n\n    # Update planned stats with compression info\n    planned_stats.compression_codec = compression\n    planned_stats.dry_run = dry_run\n\n    if dry_run or not groups:\n        return planned_stats.to_dict()\n\n    # Execute optimization using DuckDB\n    conn = self._ensure_connection()\n    compression_codec = compression or \"snappy\"\n    rewritten_bytes = 0\n\n    def _quote_identifier(identifier: str) -&gt; str:\n        escaped = identifier.replace('\"', '\"\"')\n        return f'\"{escaped}\"'\n\n    # Helper function to build ORDER BY clause with NULL handling\n    def _build_order_clause(columns: list[str]) -&gt; str:\n        order_parts = []\n        for col in columns:\n            quoted = _quote_identifier(col)\n            # Put NULLs last\n            order_parts.append(f\"({quoted} IS NULL) ASC\")\n            order_parts.append(f\"{quoted} ASC\")\n        return \", \".join(order_parts)\n\n    order_clause = _build_order_clause(zorder_columns)\n\n    # Process each group separately for more memory-efficient operation\n    written_paths: list[str] = []\n    for group_idx, group in enumerate(groups):\n        # Read group files and sort by z-order columns\n        paths = [file_info.path for file_info in group.files]\n        all_paths_sql = \",\".join([f\"'{p}'\" for p in paths])\n\n        query = f\"SELECT * FROM parquet_scan([{all_paths_sql}]) ORDER BY {order_clause}\"\n        ordered_table = conn.execute(query).arrow()\n        if hasattr(ordered_table, \"read_all\"):\n            ordered_table = ordered_table.read_all()\n\n        # Apply chunking within the group if needed\n        if target_rows_per_file and target_rows_per_file &gt; 0:\n            # Row-based splitting\n            num_rows = ordered_table.num_rows\n            chunks = []\n            for start in range(0, num_rows, target_rows_per_file):\n                end = min(start + target_rows_per_file, num_rows)\n                chunk = ordered_table.slice(start, end - start)\n                if chunk.num_rows &gt; 0:\n                    chunks.append(chunk)\n        else:\n            chunks = [ordered_table]\n\n        # Write optimized chunks\n        for chunk_idx, chunk in enumerate(chunks):\n            if len(groups) == 1:\n                # Single group case - use simple naming\n                filename = f\"optimized-{chunk_idx:05d}.parquet\"\n            else:\n                # Multiple groups - include group index\n                filename = f\"optimized-{group_idx:02d}-{chunk_idx:05d}.parquet\"\n\n            out_path = str(Path(path) / filename)\n            self.write_parquet(chunk, out_path, compression=compression_codec)\n            written_paths.append(out_path)\n\n        rewritten_bytes += group.total_size_bytes\n\n        # Remove original files in this group\n        for file_info in group.files:\n            try:\n                filesystem.rm(file_info.path)\n            except Exception:\n                pass\n\n    # Recompute stats after optimization\n    stats_after = self._collect_dataset_stats(path, partition_filter=partition_filter)\n\n    # Create final stats\n    final_stats = MaintenanceStats(\n        before_file_count=planned_stats.before_file_count,\n        after_file_count=len(stats_after[\"files\"]),\n        before_total_bytes=planned_stats.before_total_bytes,\n        after_total_bytes=stats_after[\"total_bytes\"],\n        compacted_file_count=planned_stats.compacted_file_count,\n        rewritten_bytes=rewritten_bytes,\n        compression_codec=compression_codec,\n        dry_run=False,\n        zorder_columns=zorder_columns,\n    )\n\n    return final_stats.to_dict()\n</code></pre> <code></code> fsspeckit.datasets.duckdb.DuckDBParquetHandler.read_parquet \u00b6 <pre><code>read_parquet(\n    path: str, columns: list[str] | None = None\n) -&gt; Table\n</code></pre> <p>Read parquet file or dataset directory.</p> <p>Reads a single parquet file or all parquet files in a directory and returns the data as a PyArrow table. Supports column projection for efficient reading of large datasets.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to parquet file or directory containing parquet files. Can be local path or remote URI (s3://, gs://, etc.).</p> required <code>columns</code> <code>list[str] | None</code> <p>Optional list of column names to read. If None, reads all columns. Specifying columns improves performance for large datasets.</p> <code>None</code> <p>Returns:</p> Type Description <code>Table</code> <p>PyArrow table containing the parquet data.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the specified path does not exist.</p> <code>Exception</code> <p>If DuckDB encounters an error reading the parquet file.</p> <p>Examples:</p> <p>Read entire parquet file:</p> <pre><code>&gt;&gt;&gt; handler = DuckDBParquetHandler()\n&gt;&gt;&gt; table = handler.read_parquet(\"/path/to/data.parquet\")\n</code></pre> <p>Read with column selection:</p> <pre><code>&gt;&gt;&gt; table = handler.read_parquet(\"/path/to/data.parquet\", columns=[\"col1\", \"col2\"])\n</code></pre> <p>Read parquet dataset directory:</p> <pre><code>&gt;&gt;&gt; table = handler.read_parquet(\"/path/to/dataset/\")\n</code></pre> <p>Read from S3:</p> <pre><code>&gt;&gt;&gt; from fsspeckit.storage_options import AwsStorageOptions\n&gt;&gt;&gt; handler = DuckDBParquetHandler(storage_options=AwsStorageOptions(...))\n&gt;&gt;&gt; table = handler.read_parquet(\"s3://bucket/data.parquet\")\n</code></pre> Source code in <code>src/fsspeckit/datasets/duckdb.py</code> <pre><code>def read_parquet(\n    self,\n    path: str,\n    columns: list[str] | None = None,\n) -&gt; pa.Table:\n    \"\"\"Read parquet file or dataset directory.\n\n    Reads a single parquet file or all parquet files in a directory and\n    returns the data as a PyArrow table. Supports column projection for\n    efficient reading of large datasets.\n\n    Args:\n        path: Path to parquet file or directory containing parquet files.\n            Can be local path or remote URI (s3://, gs://, etc.).\n        columns: Optional list of column names to read. If None, reads all columns.\n            Specifying columns improves performance for large datasets.\n\n    Returns:\n        PyArrow table containing the parquet data.\n\n    Raises:\n        FileNotFoundError: If the specified path does not exist.\n        Exception: If DuckDB encounters an error reading the parquet file.\n\n    Examples:\n        Read entire parquet file:\n        &gt;&gt;&gt; handler = DuckDBParquetHandler()\n        &gt;&gt;&gt; table = handler.read_parquet(\"/path/to/data.parquet\")\n\n        Read with column selection:\n        &gt;&gt;&gt; table = handler.read_parquet(\"/path/to/data.parquet\", columns=[\"col1\", \"col2\"])\n\n        Read parquet dataset directory:\n        &gt;&gt;&gt; table = handler.read_parquet(\"/path/to/dataset/\")\n\n        Read from S3:\n        &gt;&gt;&gt; from fsspeckit.storage_options import AwsStorageOptions\n        &gt;&gt;&gt; handler = DuckDBParquetHandler(storage_options=AwsStorageOptions(...))\n        &gt;&gt;&gt; table = handler.read_parquet(\"s3://bucket/data.parquet\")\n    \"\"\"\n    conn = self._ensure_connection()\n\n    # Check if path exists before executing DuckDB query\n    if self._filesystem is not None:\n        if not self._filesystem.exists(path):\n            raise FileNotFoundError(f\"Parquet path '{path}' does not exist\")\n\n    # Build column selection clause\n    columns_clause = \"*\" if columns is None else \", \".join(columns)\n\n    # Build query to read parquet\n    query = f\"SELECT {columns_clause} FROM parquet_scan('{path}')\"\n\n    try:\n        # Execute query and return as PyArrow table\n        result = conn.execute(query).arrow()\n        # Convert RecordBatchReader to Table\n        if hasattr(result, \"read_all\"):\n            result = result.read_all()\n        return result\n    except Exception as e:\n        # Preserve original error type and message when possible\n        error_msg = str(e)\n        if (\n            \"does not exist\" in error_msg.lower()\n            or \"not found\" in error_msg.lower()\n        ):\n            raise FileNotFoundError(\n                f\"Parquet path '{path}' does not exist: {error_msg}\"\n            ) from e\n        else:\n            # Re-raise with original exception type preserved\n            raise type(e)(\n                f\"Failed to read parquet from '{path}': {error_msg}\"\n            ) from e\n</code></pre> <code></code> fsspeckit.datasets.duckdb.DuckDBParquetHandler.write_parquet \u00b6 <pre><code>write_parquet(\n    table: Table, path: str, compression: str = \"snappy\"\n) -&gt; None\n</code></pre> <p>Write PyArrow table to parquet file.</p> <p>Writes a PyArrow table to a parquet file with configurable compression. Automatically creates parent directories if they don't exist.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>Table</code> <p>PyArrow table to write.</p> required <code>path</code> <code>str</code> <p>Output path for parquet file. Can be local path or remote URI.</p> required <code>compression</code> <code>str</code> <p>Compression codec to use. Supported values: \"snappy\", \"gzip\", \"lz4\", \"zstd\", \"brotli\", \"uncompressed\". Default is \"snappy\".</p> <code>'snappy'</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If DuckDB encounters an error writing the parquet file.</p> <p>Examples:</p> <p>Write with default compression:</p> <pre><code>&gt;&gt;&gt; import pyarrow as pa\n&gt;&gt;&gt; table = pa.table({'a': [1, 2, 3], 'b': ['x', 'y', 'z']})\n&gt;&gt;&gt; handler = DuckDBParquetHandler()\n&gt;&gt;&gt; handler.write_parquet(table, \"/tmp/output.parquet\")\n</code></pre> <p>Write with gzip compression:</p> <pre><code>&gt;&gt;&gt; handler.write_parquet(table, \"/tmp/output.parquet\", compression=\"gzip\")\n</code></pre> <p>Write to nested directory:</p> <pre><code>&gt;&gt;&gt; handler.write_parquet(table, \"/tmp/2024/01/15/data.parquet\")\n</code></pre> <p>Write to S3:</p> <pre><code>&gt;&gt;&gt; from fsspeckit.storage_options import AwsStorageOptions\n&gt;&gt;&gt; handler = DuckDBParquetHandler(storage_options=AwsStorageOptions(...))\n&gt;&gt;&gt; handler.write_parquet(table, \"s3://bucket/output.parquet\")\n</code></pre> Source code in <code>src/fsspeckit/datasets/duckdb.py</code> <pre><code>def write_parquet(\n    self,\n    table: pa.Table,\n    path: str,\n    compression: str = \"snappy\",\n) -&gt; None:\n    \"\"\"Write PyArrow table to parquet file.\n\n    Writes a PyArrow table to a parquet file with configurable compression.\n    Automatically creates parent directories if they don't exist.\n\n    Args:\n        table: PyArrow table to write.\n        path: Output path for parquet file. Can be local path or remote URI.\n        compression: Compression codec to use. Supported values: \"snappy\", \"gzip\",\n            \"lz4\", \"zstd\", \"brotli\", \"uncompressed\". Default is \"snappy\".\n\n    Raises:\n        Exception: If DuckDB encounters an error writing the parquet file.\n\n    Examples:\n        Write with default compression:\n        &gt;&gt;&gt; import pyarrow as pa\n        &gt;&gt;&gt; table = pa.table({'a': [1, 2, 3], 'b': ['x', 'y', 'z']})\n        &gt;&gt;&gt; handler = DuckDBParquetHandler()\n        &gt;&gt;&gt; handler.write_parquet(table, \"/tmp/output.parquet\")\n\n        Write with gzip compression:\n        &gt;&gt;&gt; handler.write_parquet(table, \"/tmp/output.parquet\", compression=\"gzip\")\n\n        Write to nested directory:\n        &gt;&gt;&gt; handler.write_parquet(table, \"/tmp/2024/01/15/data.parquet\")\n\n        Write to S3:\n        &gt;&gt;&gt; from fsspeckit.storage_options import AwsStorageOptions\n        &gt;&gt;&gt; handler = DuckDBParquetHandler(storage_options=AwsStorageOptions(...))\n        &gt;&gt;&gt; handler.write_parquet(table, \"s3://bucket/output.parquet\")\n    \"\"\"\n    conn = self._ensure_connection()\n\n    # Ensure parent directory exists and is not a file\n    parent_path = str(Path(path).parent)\n    if self._filesystem is not None:\n        # Check if parent path exists and is a file (not directory)\n        if self._filesystem.exists(parent_path):\n            if not self._filesystem.isdir(parent_path):\n                raise NotADirectoryError(\n                    f\"Parent directory '{parent_path}' exists but is a file. Cannot create file '{path}'.\"\n                )\n\n        try:\n            if not self._filesystem.exists(parent_path):\n                self._filesystem.makedirs(parent_path, exist_ok=True)\n        except Exception:\n            # Some filesystems may not support exists/makedirs on remote paths\n            # DuckDB will handle the path directly\n            pass\n\n    try:\n        # Register the table in DuckDB\n        conn.register(\"temp_table\", table)\n\n        # Use COPY command to write parquet\n        query = f\"COPY temp_table TO '{path}' (FORMAT PARQUET, COMPRESSION '{compression}')\"\n        conn.execute(query)\n\n        # Unregister the temporary table\n        conn.unregister(\"temp_table\")\n\n    except Exception as e:\n        # Clean up on error\n        try:\n            conn.unregister(\"temp_table\")\n        except Exception:\n            pass\n        raise Exception(f\"Failed to write parquet to '{path}': {e}\") from e\n</code></pre> <code></code> fsspeckit.datasets.duckdb.DuckDBParquetHandler.write_parquet_dataset \u00b6 <pre><code>write_parquet_dataset(\n    table: Table,\n    path: str,\n    mode: Literal[\"overwrite\", \"append\"] = \"append\",\n    max_rows_per_file: int | None = None,\n    compression: str = \"snappy\",\n    basename_template: str = \"part-{}.parquet\",\n) -&gt; None\n</code></pre> <p>Write PyArrow table to parquet dataset directory with unique filenames.</p> <p>Writes a PyArrow table to a directory as one or more parquet files with automatically generated unique filenames. Supports overwrite and append modes for managing existing datasets, and can split large tables across multiple files.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>Table</code> <p>PyArrow table to write.</p> required <code>path</code> <code>str</code> <p>Output directory path for the dataset. Can be local or remote URI.</p> required <code>mode</code> <code>Literal['overwrite', 'append']</code> <p>Write mode. \"append\" (default) adds files without deleting existing ones. \"overwrite\" deletes existing parquet files before writing.</p> <code>'append'</code> <code>max_rows_per_file</code> <code>int | None</code> <p>Optional maximum rows per file. If specified and table has more rows, splits into multiple files. If None, writes single file.</p> <code>None</code> <code>compression</code> <code>str</code> <p>Compression codec. Supported: \"snappy\", \"gzip\", \"lz4\", \"zstd\", \"brotli\", \"uncompressed\". Default is \"snappy\".</p> <code>'snappy'</code> <code>basename_template</code> <code>str</code> <p>Template for filenames with {} placeholder for unique ID. Default is \"part-{}.parquet\". The {} will be replaced with a short UUID.</p> <code>'part-{}.parquet'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If mode is invalid or max_rows_per_file &lt;= 0.</p> <code>Exception</code> <p>If filesystem operations or writing fails.</p> <p>Examples:</p> <p>Basic dataset write with unique filename:</p> <pre><code>&gt;&gt;&gt; import pyarrow as pa\n&gt;&gt;&gt; table = pa.table({'a': [1, 2, 3], 'b': ['x', 'y', 'z']})\n&gt;&gt;&gt; with DuckDBParquetHandler() as handler:\n...     handler.write_parquet_dataset(table, \"/tmp/dataset/\")\n...     # Creates: /tmp/dataset/part-a1b2c3d4.parquet\n</code></pre> <p>Append mode (incremental updates):</p> <pre><code>&gt;&gt;&gt; # First write\n&gt;&gt;&gt; handler.write_parquet_dataset(table1, \"/data/sales/\", mode=\"append\")\n&gt;&gt;&gt; # Second write (adds new file)\n&gt;&gt;&gt; handler.write_parquet_dataset(table2, \"/data/sales/\", mode=\"append\")\n&gt;&gt;&gt; # Read combined dataset\n&gt;&gt;&gt; result = handler.read_parquet(\"/data/sales/\")\n</code></pre> <p>Overwrite mode (replace dataset):</p> <pre><code>&gt;&gt;&gt; handler.write_parquet_dataset(\n...     new_table,\n...     \"/data/output/\",\n...     mode=\"overwrite\"  # Deletes existing parquet files\n... )\n</code></pre> <p>Split large table across multiple files:</p> <pre><code>&gt;&gt;&gt; large_table = pa.table({'id': range(10000), 'value': range(10000)})\n&gt;&gt;&gt; handler.write_parquet_dataset(\n...     large_table,\n...     \"/data/output/\",\n...     max_rows_per_file=2500  # Creates 4 files\n... )\n</code></pre> <p>Custom filename template:</p> <pre><code>&gt;&gt;&gt; handler.write_parquet_dataset(\n...     table,\n...     \"/data/output/\",\n...     basename_template=\"data_{}.parquet\"\n... )\n... # Creates: data_a1b2c3d4.parquet\n</code></pre> <p>With compression:</p> <pre><code>&gt;&gt;&gt; handler.write_parquet_dataset(\n...     table,\n...     \"/data/output/\",\n...     compression=\"gzip\"\n... )\n</code></pre> <p>Remote storage (S3):</p> <pre><code>&gt;&gt;&gt; from fsspeckit.storage_options import AwsStorageOptions\n&gt;&gt;&gt; handler = DuckDBParquetHandler(storage_options=AwsStorageOptions(...))\n&gt;&gt;&gt; handler.write_parquet_dataset(table, \"s3://bucket/dataset/\")\n</code></pre> Source code in <code>src/fsspeckit/datasets/duckdb.py</code> <pre><code>def write_parquet_dataset(\n    self,\n    table: pa.Table,\n    path: str,\n    mode: Literal[\"overwrite\", \"append\"] = \"append\",\n    max_rows_per_file: int | None = None,\n    compression: str = \"snappy\",\n    basename_template: str = \"part-{}.parquet\",\n) -&gt; None:\n    \"\"\"Write PyArrow table to parquet dataset directory with unique filenames.\n\n    Writes a PyArrow table to a directory as one or more parquet files with\n    automatically generated unique filenames. Supports overwrite and append modes\n    for managing existing datasets, and can split large tables across multiple files.\n\n    Args:\n        table: PyArrow table to write.\n        path: Output directory path for the dataset. Can be local or remote URI.\n        mode: Write mode. \"append\" (default) adds files without deleting existing ones.\n            \"overwrite\" deletes existing parquet files before writing.\n        max_rows_per_file: Optional maximum rows per file. If specified and table\n            has more rows, splits into multiple files. If None, writes single file.\n        compression: Compression codec. Supported: \"snappy\", \"gzip\", \"lz4\", \"zstd\",\n            \"brotli\", \"uncompressed\". Default is \"snappy\".\n        basename_template: Template for filenames with {} placeholder for unique ID.\n            Default is \"part-{}.parquet\". The {} will be replaced with a short UUID.\n\n    Raises:\n        ValueError: If mode is invalid or max_rows_per_file &lt;= 0.\n        Exception: If filesystem operations or writing fails.\n\n    Examples:\n        Basic dataset write with unique filename:\n        &gt;&gt;&gt; import pyarrow as pa\n        &gt;&gt;&gt; table = pa.table({'a': [1, 2, 3], 'b': ['x', 'y', 'z']})\n        &gt;&gt;&gt; with DuckDBParquetHandler() as handler:\n        ...     handler.write_parquet_dataset(table, \"/tmp/dataset/\")\n        ...     # Creates: /tmp/dataset/part-a1b2c3d4.parquet\n\n        Append mode (incremental updates):\n        &gt;&gt;&gt; # First write\n        &gt;&gt;&gt; handler.write_parquet_dataset(table1, \"/data/sales/\", mode=\"append\")\n        &gt;&gt;&gt; # Second write (adds new file)\n        &gt;&gt;&gt; handler.write_parquet_dataset(table2, \"/data/sales/\", mode=\"append\")\n        &gt;&gt;&gt; # Read combined dataset\n        &gt;&gt;&gt; result = handler.read_parquet(\"/data/sales/\")\n\n        Overwrite mode (replace dataset):\n        &gt;&gt;&gt; handler.write_parquet_dataset(\n        ...     new_table,\n        ...     \"/data/output/\",\n        ...     mode=\"overwrite\"  # Deletes existing parquet files\n        ... )\n\n        Split large table across multiple files:\n        &gt;&gt;&gt; large_table = pa.table({'id': range(10000), 'value': range(10000)})\n        &gt;&gt;&gt; handler.write_parquet_dataset(\n        ...     large_table,\n        ...     \"/data/output/\",\n        ...     max_rows_per_file=2500  # Creates 4 files\n        ... )\n\n        Custom filename template:\n        &gt;&gt;&gt; handler.write_parquet_dataset(\n        ...     table,\n        ...     \"/data/output/\",\n        ...     basename_template=\"data_{}.parquet\"\n        ... )\n        ... # Creates: data_a1b2c3d4.parquet\n\n        With compression:\n        &gt;&gt;&gt; handler.write_parquet_dataset(\n        ...     table,\n        ...     \"/data/output/\",\n        ...     compression=\"gzip\"\n        ... )\n\n        Remote storage (S3):\n        &gt;&gt;&gt; from fsspeckit.storage_options import AwsStorageOptions\n        &gt;&gt;&gt; handler = DuckDBParquetHandler(storage_options=AwsStorageOptions(...))\n        &gt;&gt;&gt; handler.write_parquet_dataset(table, \"s3://bucket/dataset/\")\n    \"\"\"\n    # Validate inputs\n    if mode not in (\"overwrite\", \"append\"):\n        raise ValueError(\n            f\"Invalid mode: '{mode}'. Must be 'overwrite' or 'append'.\"\n        )\n\n    if max_rows_per_file is not None and max_rows_per_file &lt;= 0:\n        raise ValueError(f\"max_rows_per_file must be &gt; 0, got {max_rows_per_file}\")\n\n    conn = self._ensure_connection()\n\n    # Ensure directory exists\n    if self._filesystem is not None:\n        # Check if path exists and is a file (not directory)\n        if self._filesystem.exists(path):\n            if not self._filesystem.isdir(path):\n                raise NotADirectoryError(\n                    f\"Dataset path '{path}' exists but is a file. Dataset paths must be directories.\"\n                )\n\n        try:\n            if not self._filesystem.exists(path):\n                self._filesystem.makedirs(path, exist_ok=True)\n        except Exception as e:\n            raise Exception(\n                f\"Failed to create dataset directory '{path}': {e}\"\n            ) from e\n\n    # Handle overwrite mode - clear existing parquet files\n    if mode == \"overwrite\":\n        self._clear_dataset(path)\n\n    # Determine how many files to write\n    if max_rows_per_file is not None and table.num_rows &gt; max_rows_per_file:\n        # Split table into multiple files\n        num_files = (table.num_rows + max_rows_per_file - 1) // max_rows_per_file\n\n        for i in range(num_files):\n            start_idx = i * max_rows_per_file\n            end_idx = min((i + 1) * max_rows_per_file, table.num_rows)\n            slice_table = table.slice(start_idx, end_idx - start_idx)\n\n            # Generate unique filename\n            filename = self._generate_unique_filename(basename_template)\n            file_path = str(Path(path) / filename)\n\n            # Write slice to file\n            try:\n                conn.register(\"temp_table\", slice_table)\n                query = f\"COPY temp_table TO '{file_path}' (FORMAT PARQUET, COMPRESSION '{compression}')\"\n                conn.execute(query)\n                conn.unregister(\"temp_table\")\n            except Exception as e:\n                try:\n                    conn.unregister(\"temp_table\")\n                except Exception:\n                    pass\n                raise Exception(\n                    f\"Failed to write parquet file '{file_path}': {e}\"\n                ) from e\n    else:\n        # Write single file\n        filename = self._generate_unique_filename(basename_template)\n        file_path = str(Path(path) / filename)\n\n        try:\n            conn.register(\"temp_table\", table)\n            query = f\"COPY temp_table TO '{file_path}' (FORMAT PARQUET, COMPRESSION '{compression}')\"\n            conn.execute(query)\n            conn.unregister(\"temp_table\")\n        except Exception as e:\n            try:\n                conn.unregister(\"temp_table\")\n            except Exception:\n                pass\n            raise Exception(\n                f\"Failed to write parquet file '{file_path}': {e}\"\n            ) from e\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.duckdb-functions","title":"Functions","text":""},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.pyarrow","title":"fsspeckit.datasets.pyarrow","text":""},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.pyarrow-classes","title":"Classes","text":""},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.pyarrow-functions","title":"Functions","text":""},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.pyarrow.cast_schema","title":"fsspeckit.datasets.pyarrow.cast_schema","text":"<pre><code>cast_schema(table: Table, schema: Schema) -&gt; Table\n</code></pre> <p>Cast a PyArrow table to a given schema, updating the schema to match the table's columns.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>Table</code> <p>The PyArrow table to cast.</p> required <code>schema</code> <code>Schema</code> <p>The target schema to cast the table to.</p> required <p>Returns:</p> Type Description <code>Table</code> <p>pa.Table: A new PyArrow table with the specified schema.</p> Source code in <code>src/fsspeckit/datasets/pyarrow.py</code> <pre><code>def cast_schema(table: pa.Table, schema: pa.Schema) -&gt; pa.Table:\n    \"\"\"\n    Cast a PyArrow table to a given schema, updating the schema to match the table's columns.\n\n    Args:\n        table (pa.Table): The PyArrow table to cast.\n        schema (pa.Schema): The target schema to cast the table to.\n\n    Returns:\n        pa.Table: A new PyArrow table with the specified schema.\n    \"\"\"\n    table_columns = set(table.schema.names)\n    for field in schema:\n        if field.name not in table_columns:\n            table = table.append_column(\n                field.name, pa.nulls(table.num_rows, type=field.type)\n            )\n    return table.cast(schema)\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.pyarrow.collect_dataset_stats_pyarrow","title":"fsspeckit.datasets.pyarrow.collect_dataset_stats_pyarrow","text":"<pre><code>collect_dataset_stats_pyarrow(\n    path: str,\n    filesystem: AbstractFileSystem | None = None,\n    partition_filter: list[str] | None = None,\n) -&gt; dict[str, Any]\n</code></pre> <p>Collect file-level statistics for a parquet dataset using shared core logic.</p> <p>This function delegates to the shared <code>fsspeckit.core.maintenance.collect_dataset_stats</code> function, ensuring consistent dataset discovery and statistics across both DuckDB and PyArrow backends.</p> <p>The helper walks the given dataset directory on the provided filesystem, discovers parquet files (recursively), and returns basic statistics:</p> <ul> <li>Per-file path, size in bytes, and number of rows</li> <li>Aggregated total bytes and total rows</li> </ul> <p>The function is intentionally streaming/metadata-driven and never materializes the full dataset as a single :class:<code>pyarrow.Table</code>.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Root directory of the parquet dataset.</p> required <code>filesystem</code> <code>AbstractFileSystem | None</code> <p>Optional fsspec filesystem. If omitted, a local \"file\" filesystem is used.</p> <code>None</code> <code>partition_filter</code> <code>list[str] | None</code> <p>Optional list of partition prefix filters (e.g. [\"date=2025-11-04\"]). Only files whose path relative to <code>path</code> starts with one of these prefixes are included.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict with keys:</p> <code>dict[str, Any]</code> <ul> <li><code>files</code>: list of <code>{\"path\", \"size_bytes\", \"num_rows\"}</code> dicts</li> </ul> <code>dict[str, Any]</code> <ul> <li><code>total_bytes</code>: sum of file sizes</li> </ul> <code>dict[str, Any]</code> <ul> <li><code>total_rows</code>: sum of row counts</li> </ul> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the path does not exist or no parquet files match the optional partition filter.</p> Note <p>This is a thin wrapper around the shared core function. See :func:<code>fsspeckit.core.maintenance.collect_dataset_stats</code> for the authoritative implementation.</p> Source code in <code>src/fsspeckit/datasets/pyarrow.py</code> <pre><code>def collect_dataset_stats_pyarrow(\n    path: str,\n    filesystem: AbstractFileSystem | None = None,\n    partition_filter: list[str] | None = None,\n) -&gt; dict[str, Any]:\n    \"\"\"Collect file-level statistics for a parquet dataset using shared core logic.\n\n    This function delegates to the shared ``fsspeckit.core.maintenance.collect_dataset_stats``\n    function, ensuring consistent dataset discovery and statistics across both DuckDB\n    and PyArrow backends.\n\n    The helper walks the given dataset directory on the provided filesystem,\n    discovers parquet files (recursively), and returns basic statistics:\n\n    - Per-file path, size in bytes, and number of rows\n    - Aggregated total bytes and total rows\n\n    The function is intentionally streaming/metadata-driven and never\n    materializes the full dataset as a single :class:`pyarrow.Table`.\n\n    Args:\n        path: Root directory of the parquet dataset.\n        filesystem: Optional fsspec filesystem. If omitted, a local \"file\"\n            filesystem is used.\n        partition_filter: Optional list of partition prefix filters\n            (e.g. [\"date=2025-11-04\"]). Only files whose path relative to\n            ``path`` starts with one of these prefixes are included.\n\n    Returns:\n        Dict with keys:\n\n        - ``files``: list of ``{\"path\", \"size_bytes\", \"num_rows\"}`` dicts\n        - ``total_bytes``: sum of file sizes\n        - ``total_rows``: sum of row counts\n\n    Raises:\n        FileNotFoundError: If the path does not exist or no parquet files\n            match the optional partition filter.\n\n    Note:\n        This is a thin wrapper around the shared core function. See\n        :func:`fsspeckit.core.maintenance.collect_dataset_stats` for the\n        authoritative implementation.\n    \"\"\"\n    from fsspeckit.core.maintenance import collect_dataset_stats\n\n    return collect_dataset_stats(\n        path=path,\n        filesystem=filesystem,\n        partition_filter=partition_filter,\n    )\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.pyarrow.compact_parquet_dataset_pyarrow","title":"fsspeckit.datasets.pyarrow.compact_parquet_dataset_pyarrow","text":"<pre><code>compact_parquet_dataset_pyarrow(\n    path: str,\n    target_mb_per_file: int | None = None,\n    target_rows_per_file: int | None = None,\n    partition_filter: list[str] | None = None,\n    compression: str | None = None,\n    dry_run: bool = False,\n    filesystem: AbstractFileSystem | None = None,\n) -&gt; dict[str, Any]\n</code></pre> <p>Compact a parquet dataset directory into fewer larger files using PyArrow and shared planning.</p> <p>Groups small files based on size (MB) and/or row thresholds, rewrites grouped files into new parquet files, and optionally changes compression. Supports a dry-run mode that returns the compaction plan without modifying files.</p> <p>The implementation uses the shared core planning algorithm for consistent behavior across backends. It processes data in a group-based, streaming fashion: it reads only the files in a given group into memory when processing that group and never materializes the entire dataset as a single table.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Dataset root directory (local path or fsspec URL).</p> required <code>target_mb_per_file</code> <code>int | None</code> <p>Optional max output size per file; must be &gt; 0.</p> <code>None</code> <code>target_rows_per_file</code> <code>int | None</code> <p>Optional max rows per output file; must be &gt; 0.</p> <code>None</code> <code>partition_filter</code> <code>list[str] | None</code> <p>Optional list of partition prefixes (e.g. <code>[\"date=2025-11-15\"]</code>) used to limit both stats collection and rewrites to matching paths.</p> <code>None</code> <code>compression</code> <code>str | None</code> <p>Optional parquet compression codec; defaults to <code>\"snappy\"</code>.</p> <code>None</code> <code>dry_run</code> <code>bool</code> <p>When <code>True</code> the function returns a plan + before/after stats without reading or writing any parquet data.</p> <code>False</code> <code>filesystem</code> <code>AbstractFileSystem | None</code> <p>Optional <code>fsspec.AbstractFileSystem</code> to reuse existing FS clients.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A stats dictionary describing before/after file counts, total bytes,</p> <code>dict[str, Any]</code> <p>rewritten bytes, and optional <code>planned_groups</code> when <code>dry_run</code> is enabled.</p> <code>dict[str, Any]</code> <p>The structure follows the canonical <code>MaintenanceStats</code> format from the shared core.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If thresholds are invalid or no files match partition filter.</p> <code>FileNotFoundError</code> <p>If the path does not exist.</p> Example <p>result = compact_parquet_dataset_pyarrow( ...     \"/path/to/dataset\", ...     target_mb_per_file=64, ...     dry_run=True ... ) print(f\"Files before: {result['before_file_count']}\") print(f\"Files after: {result['after_file_count']}\")</p> Note <p>This function delegates dataset discovery and compaction planning to the shared <code>fsspeckit.core.maintenance</code> module, ensuring consistent behavior across DuckDB and PyArrow backends.</p> Source code in <code>src/fsspeckit/datasets/pyarrow.py</code> <pre><code>def compact_parquet_dataset_pyarrow(\n    path: str,\n    target_mb_per_file: int | None = None,\n    target_rows_per_file: int | None = None,\n    partition_filter: list[str] | None = None,\n    compression: str | None = None,\n    dry_run: bool = False,\n    filesystem: AbstractFileSystem | None = None,\n) -&gt; dict[str, Any]:\n    \"\"\"Compact a parquet dataset directory into fewer larger files using PyArrow and shared planning.\n\n    Groups small files based on size (MB) and/or row thresholds, rewrites grouped\n    files into new parquet files, and optionally changes compression. Supports a\n    dry-run mode that returns the compaction plan without modifying files.\n\n    The implementation uses the shared core planning algorithm for consistent\n    behavior across backends. It processes data in a group-based, streaming fashion:\n    it reads only the files in a given group into memory when processing that group\n    and never materializes the entire dataset as a single table.\n\n    Args:\n        path: Dataset root directory (local path or fsspec URL).\n        target_mb_per_file: Optional max output size per file; must be &gt; 0.\n        target_rows_per_file: Optional max rows per output file; must be &gt; 0.\n        partition_filter: Optional list of partition prefixes (e.g. ``[\"date=2025-11-15\"]``)\n            used to limit both stats collection and rewrites to matching paths.\n        compression: Optional parquet compression codec; defaults to ``\"snappy\"``.\n        dry_run: When ``True`` the function returns a plan + before/after stats\n            without reading or writing any parquet data.\n        filesystem: Optional ``fsspec.AbstractFileSystem`` to reuse existing FS clients.\n\n    Returns:\n        A stats dictionary describing before/after file counts, total bytes,\n        rewritten bytes, and optional ``planned_groups`` when ``dry_run`` is enabled.\n        The structure follows the canonical ``MaintenanceStats`` format from the shared core.\n\n    Raises:\n        ValueError: If thresholds are invalid or no files match partition filter.\n        FileNotFoundError: If the path does not exist.\n\n    Example:\n        &gt;&gt;&gt; result = compact_parquet_dataset_pyarrow(\n        ...     \"/path/to/dataset\",\n        ...     target_mb_per_file=64,\n        ...     dry_run=True\n        ... )\n        &gt;&gt;&gt; print(f\"Files before: {result['before_file_count']}\")\n        &gt;&gt;&gt; print(f\"Files after: {result['after_file_count']}\")\n\n    Note:\n        This function delegates dataset discovery and compaction planning to the\n        shared ``fsspeckit.core.maintenance`` module, ensuring consistent behavior\n        across DuckDB and PyArrow backends.\n    \"\"\"\n    from fsspeckit.core.maintenance import plan_compaction_groups, MaintenanceStats\n\n    # Get dataset stats using shared logic\n    stats = collect_dataset_stats_pyarrow(\n        path=path, filesystem=filesystem, partition_filter=partition_filter\n    )\n    files = stats[\"files\"]\n\n    # Use shared compaction planning\n    plan_result = plan_compaction_groups(\n        file_infos=files,\n        target_mb_per_file=target_mb_per_file,\n        target_rows_per_file=target_rows_per_file,\n    )\n\n    groups = plan_result[\"groups\"]\n    planned_stats = plan_result[\"planned_stats\"]\n\n    # Update planned stats with compression info\n    planned_stats.compression_codec = compression\n    planned_stats.dry_run = dry_run\n\n    if dry_run or not groups:\n        return planned_stats.to_dict()\n\n    # Execute compaction using PyArrow\n    fs = filesystem or fsspec_filesystem(\"file\")\n    codec = compression or \"snappy\"\n    rewritten_bytes_live = 0\n\n    # Process each group: read, concatenate, write a new file, then delete\n    # originals for that group. This ensures peak memory is bounded by the\n    # group size.\n    for group_idx, group in enumerate(groups):\n        paths = [file_info.path for file_info in group.files]\n        tables: list[pa.Table] = []\n        for filename in paths:\n            with fs.open(filename, \"rb\") as fh:\n                tables.append(pq.read_table(fh))\n        combined = pa.concat_tables(tables, promote=True)\n\n        out_name = f\"compact-{group_idx:05d}.parquet\"\n        out_path = str(Path(path) / out_name)\n        with fs.open(out_path, \"wb\") as fh:\n            pq.write_table(combined, fh, compression=codec)\n\n        rewritten_bytes_live += group.total_size_bytes\n\n        # Remove original files in this group\n        for file_info in group.files:\n            try:\n                fs.rm(file_info.path)\n            except Exception:\n                # Best-effort cleanup; leave a warning to the caller.\n                print(f\"Warning: failed to delete '{file_info.path}' after compaction\")\n\n    # Recompute stats after compaction for the affected subset.\n    stats_after = collect_dataset_stats_pyarrow(\n        path=path, filesystem=fs, partition_filter=partition_filter\n    )\n\n    # Create final stats\n    final_stats = MaintenanceStats(\n        before_file_count=planned_stats.before_file_count,\n        after_file_count=len(stats_after[\"files\"]),\n        before_total_bytes=planned_stats.before_total_bytes,\n        after_total_bytes=stats_after[\"total_bytes\"],\n        compacted_file_count=planned_stats.compacted_file_count,\n        rewritten_bytes=rewritten_bytes_live,\n        compression_codec=codec,\n        dry_run=False,\n    )\n\n    return final_stats.to_dict()\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.pyarrow.convert_large_types_to_normal","title":"fsspeckit.datasets.pyarrow.convert_large_types_to_normal","text":"<pre><code>convert_large_types_to_normal(schema: Schema) -&gt; Schema\n</code></pre> <p>Convert large types in a PyArrow schema to their standard types.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>Schema</code> <p>The PyArrow schema to convert.</p> required <p>Returns:</p> Type Description <code>Schema</code> <p>pa.Schema: A new PyArrow schema with large types converted to standard types.</p> Source code in <code>src/fsspeckit/datasets/pyarrow.py</code> <pre><code>def convert_large_types_to_normal(schema: pa.Schema) -&gt; pa.Schema:\n    \"\"\"\n    Convert large types in a PyArrow schema to their standard types.\n\n    Args:\n        schema (pa.Schema): The PyArrow schema to convert.\n\n    Returns:\n        pa.Schema: A new PyArrow schema with large types converted to standard types.\n    \"\"\"\n    # Define mapping of large types to standard types\n    type_mapping = {\n        pa.large_string(): pa.string(),\n        pa.large_binary(): pa.binary(),\n        pa.large_utf8(): pa.utf8(),\n        pa.large_list(pa.null()): pa.list_(pa.null()),\n        pa.large_list_view(pa.null()): pa.list_view(pa.null()),\n    }\n    # Convert fields\n    new_fields = []\n    for field in schema:\n        field_type = field.type\n        # Check if type exists in mapping\n        if field_type in type_mapping:\n            new_field = pa.field(\n                name=field.name,\n                type=type_mapping[field_type],\n                nullable=field.nullable,\n                metadata=field.metadata,\n            )\n            new_fields.append(new_field)\n        # Handle large lists with nested types\n        elif isinstance(field_type, pa.LargeListType):\n            new_field = pa.field(\n                name=field.name,\n                type=pa.list_(\n                    type_mapping[field_type.value_type]\n                    if field_type.value_type in type_mapping\n                    else field_type.value_type\n                ),\n                nullable=field.nullable,\n                metadata=field.metadata,\n            )\n            new_fields.append(new_field)\n        # Handle dictionary with large_string, large_utf8, or large_binary values\n        elif isinstance(field_type, pa.DictionaryType):\n            new_field = pa.field(\n                name=field.name,\n                type=pa.dictionary(\n                    field_type.index_type,\n                    type_mapping[field_type.value_type]\n                    if field_type.value_type in type_mapping\n                    else field_type.value_type,\n                    field_type.ordered,\n                ),\n                # nullable=field.nullable,\n                metadata=field.metadata,\n            )\n            new_fields.append(new_field)\n        else:\n            new_fields.append(field)\n\n    return pa.schema(new_fields)\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.pyarrow.dominant_timezone_per_column","title":"fsspeckit.datasets.pyarrow.dominant_timezone_per_column","text":"<pre><code>dominant_timezone_per_column(\n    schemas: list[Schema],\n) -&gt; dict[str, tuple[str | None, str | None]]\n</code></pre> <p>For each timestamp column (by name) across all schemas, detect the most frequent timezone (including None). If None and a timezone are tied, prefer the timezone. Returns a dict: {column_name: dominant_timezone}</p> Source code in <code>src/fsspeckit/datasets/pyarrow.py</code> <pre><code>def dominant_timezone_per_column(\n    schemas: list[pa.Schema],\n) -&gt; dict[str, tuple[str | None, str | None]]:\n    \"\"\"\n    For each timestamp column (by name) across all schemas, detect the most frequent timezone (including None).\n    If None and a timezone are tied, prefer the timezone.\n    Returns a dict: {column_name: dominant_timezone}\n    \"\"\"\n    from collections import Counter, defaultdict\n\n    tz_counts: defaultdict[str, Counter[str | None]] = defaultdict(Counter)\n    units: dict[str, str | None] = {}\n\n    for schema in schemas:\n        for field in schema:\n            if pa.types.is_timestamp(field.type):\n                tz = field.type.tz\n                name = field.name\n                tz_counts[name][tz] += 1\n                # Track unit for each column (assume consistent)\n                if name not in units:\n                    units[name] = field.type.unit\n\n    dominant = {}\n    for name, counter in tz_counts.items():\n        most_common = counter.most_common()\n        if not most_common:\n            continue\n        top_count = most_common[0][1]\n        # Find all with top_count\n        top_tzs = [tz for tz, cnt in most_common if cnt == top_count]\n        # If tie and one is not None, prefer not-None\n        if len(top_tzs) &gt; 1 and any(tz is not None for tz in top_tzs):\n            tz = next(tz for tz in top_tzs if tz is not None)\n        else:\n            tz = most_common[0][0]\n        dominant[name] = (units[name], tz)\n    return dominant\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.pyarrow.merge_parquet_dataset_pyarrow","title":"fsspeckit.datasets.pyarrow.merge_parquet_dataset_pyarrow","text":"<pre><code>merge_parquet_dataset_pyarrow(\n    source: Table | str,\n    target_path: str,\n    key_columns: list[str] | str,\n    strategy: Literal[\n        \"upsert\",\n        \"insert\",\n        \"update\",\n        \"full_merge\",\n        \"deduplicate\",\n    ] = \"upsert\",\n    dedup_order_by: list[str] | None = None,\n    compression: str | None = None,\n    filesystem: AbstractFileSystem | None = None,\n    batch_rows: int = 10000,\n    progress_callback: Callable[[str, int, int], None]\n    | None = None,\n) -&gt; dict[str, int]\n</code></pre> <p>Merge a source table/dataset into a parquet dataset using PyArrow only.</p> <p>This function provides the same merge semantics as DuckDBParquetHandler.merge_parquet_dataset but executes entirely with PyArrow datasets, scanners, and compute filters. It uses shared merge semantics and validation for consistent behavior across backends.</p> <p>The function streams both the source and target datasets in manageable batches and never calls <code>dataset.to_table()</code> on the entire target without a filter, ensuring memory efficiency for large datasets.</p> <p>All merge strategies (UPSERT, INSERT, UPDATE, FULL_MERGE, DEDUPLICATE) share the same semantics and validation rules across both DuckDB and PyArrow backends.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Table | str</code> <p>Source data as PyArrow table or path to parquet dataset.</p> required <code>target_path</code> <code>str</code> <p>Path to target parquet dataset directory.</p> required <code>key_columns</code> <code>list[str] | str</code> <p>Column(s) to use for matching records. Can be single column name (string) or list of column names for composite keys.</p> required <code>strategy</code> <code>Literal['upsert', 'insert', 'update', 'full_merge', 'deduplicate']</code> <p>Merge strategy to use: - \"upsert\": Insert new records, update existing (default) - \"insert\": Insert only new records, ignore existing - \"update\": Update only existing records, ignore new - \"full_merge\": Insert, update, and delete (full sync with source) - \"deduplicate\": Remove duplicates from source, then upsert</p> <code>'upsert'</code> <code>dedup_order_by</code> <code>list[str] | None</code> <p>Columns to use for ordering when deduplicating (for \"deduplicate\" strategy). Keeps record with highest value. If None, uses key columns.</p> <code>None</code> <code>compression</code> <code>str | None</code> <p>Compression codec for output files.</p> <code>None</code> <code>filesystem</code> <code>AbstractFileSystem | None</code> <p>Filesystem to use for operations. If None, uses local filesystem.</p> <code>None</code> <code>batch_rows</code> <code>int</code> <p>Number of rows to process in each batch for streaming operations.</p> <code>10000</code> <code>progress_callback</code> <code>Callable[[str, int, int], None] | None</code> <p>Optional callback function for progress tracking. Called with (stage, current, total) where stage is a string description.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, int]</code> <p>Dictionary with merge statistics: - \"inserted\": Number of records inserted - \"updated\": Number of records updated - \"deleted\": Number of records deleted - \"total\": Total records in merged dataset</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If strategy invalid, key columns missing, or NULL keys present.</p> <code>TypeError</code> <p>If source/target schemas incompatible.</p> <code>Exception</code> <p>If merge operation fails.</p> Note <p>This implementation uses shared merge validation and semantics from fsspeckit.core.merge to ensure consistent behavior across all backends.</p> Source code in <code>src/fsspeckit/datasets/pyarrow.py</code> <pre><code>def merge_parquet_dataset_pyarrow(\n    source: pa.Table | str,\n    target_path: str,\n    key_columns: list[str] | str,\n    strategy: Literal[\"upsert\", \"insert\", \"update\", \"full_merge\", \"deduplicate\"] = \"upsert\",\n    dedup_order_by: list[str] | None = None,\n    compression: str | None = None,\n    filesystem: AbstractFileSystem | None = None,\n    batch_rows: int = 10_000,\n    progress_callback: Callable[[str, int, int], None] | None = None,\n) -&gt; dict[str, int]:\n    \"\"\"Merge a source table/dataset into a parquet dataset using PyArrow only.\n\n    This function provides the same merge semantics as DuckDBParquetHandler.merge_parquet_dataset\n    but executes entirely with PyArrow datasets, scanners, and compute filters. It uses\n    shared merge semantics and validation for consistent behavior across backends.\n\n    The function streams both the source and target datasets in manageable batches and never\n    calls ``dataset.to_table()`` on the entire target without a filter, ensuring memory efficiency\n    for large datasets.\n\n    All merge strategies (UPSERT, INSERT, UPDATE, FULL_MERGE, DEDUPLICATE) share the same\n    semantics and validation rules across both DuckDB and PyArrow backends.\n\n    Args:\n        source: Source data as PyArrow table or path to parquet dataset.\n        target_path: Path to target parquet dataset directory.\n        key_columns: Column(s) to use for matching records. Can be single column\n            name (string) or list of column names for composite keys.\n        strategy: Merge strategy to use:\n            - \"upsert\": Insert new records, update existing (default)\n            - \"insert\": Insert only new records, ignore existing\n            - \"update\": Update only existing records, ignore new\n            - \"full_merge\": Insert, update, and delete (full sync with source)\n            - \"deduplicate\": Remove duplicates from source, then upsert\n        dedup_order_by: Columns to use for ordering when deduplicating (for\n            \"deduplicate\" strategy). Keeps record with highest value. If None,\n            uses key columns.\n        compression: Compression codec for output files.\n        filesystem: Filesystem to use for operations. If None, uses local filesystem.\n        batch_rows: Number of rows to process in each batch for streaming operations.\n        progress_callback: Optional callback function for progress tracking.\n            Called with (stage, current, total) where stage is a string description.\n\n    Returns:\n        Dictionary with merge statistics:\n            - \"inserted\": Number of records inserted\n            - \"updated\": Number of records updated\n            - \"deleted\": Number of records deleted\n            - \"total\": Total records in merged dataset\n\n    Raises:\n        ValueError: If strategy invalid, key columns missing, or NULL keys present.\n        TypeError: If source/target schemas incompatible.\n        Exception: If merge operation fails.\n\n    Note:\n        This implementation uses shared merge validation and semantics from fsspeckit.core.merge\n        to ensure consistent behavior across all backends.\n    \"\"\"\n\n    # Convert strategy and validate using shared helpers\n    try:\n        core_strategy = CoreMergeStrategy(strategy)\n    except ValueError:\n        valid_strategies = {s.value for s in CoreMergeStrategy}\n        raise ValueError(\n            f\"Invalid strategy '{strategy}'. Supported: {', '.join(sorted(valid_strategies))}\"\n        )\n\n    # Normalize key columns using shared helper\n    normalized_keys = normalize_key_columns(key_columns)\n\n    fs = filesystem or fsspec_filesystem(\"file\")\n    arrow_fs = _ensure_pyarrow_filesystem(filesystem)\n\n    # Report progress start\n    if progress_callback:\n        progress_callback(\"Loading source data\", 0, 5)\n\n    source_table = _load_source_table_pyarrow(source, arrow_fs)\n\n    # Report progress for target loading\n    if progress_callback:\n        progress_callback(\"Loading target data\", 1, 5)\n\n    target_dataset: ds.Dataset | None\n    try:\n        target_dataset = ds.dataset(target_path, filesystem=arrow_fs)\n    except (FileNotFoundError, pa.ArrowInvalid, ValueError):\n        target_dataset = None\n\n    if source_table.num_rows == 0:\n        total_rows = 0\n        if target_dataset is not None:\n            try:\n                total_rows = target_dataset.count_rows()\n            except AttributeError:\n                total_rows = sum(batch.num_rows for batch in target_dataset.scanner().to_batches())\n        if strategy == \"full_merge\" and fs.exists(target_path):\n            fs.rm(target_path, recursive=True)\n            fs.makedirs(target_path, exist_ok=True)\n        return {\"inserted\": 0, \"updated\": 0, \"deleted\": 0, \"total\": total_rows}\n\n    # Report progress for validation\n    if progress_callback:\n        progress_callback(\"Validating inputs\", 2, 5)\n\n    # Validate using shared helpers\n    target_schema = target_dataset.schema if target_dataset else None\n    merge_plan = validate_merge_inputs(\n        source_table.schema, target_schema, normalized_keys, core_strategy\n    )\n    merge_plan.source_count = source_table.num_rows\n\n    # Validate strategy compatibility\n    validate_strategy_compatibility(\n        core_strategy, source_table.num_rows, target_dataset is not None\n    )\n\n    # Check for NULL keys using shared helper\n    check_null_keys(source_table, None, normalized_keys)  # We'll check target dataset during streaming\n\n    # Unify schemas for compatibility\n    schemas = [source_table.schema]\n    if target_dataset is not None:\n        schemas.append(target_dataset.schema)\n    unified_schema = unify_schemas(schemas)\n    source_table = cast_schema(source_table, unified_schema)\n\n    if core_strategy == CoreMergeStrategy.DEDUPLICATE:\n        if dedup_order_by:\n            sort_keys = [(column, \"descending\") for column in dedup_order_by]\n            source_table = source_table.sort_by(sort_keys)\n        seen_keys: set[tuple[Any, ...]] = set()\n        key_arrays = [source_table.column(col).to_pylist() for col in normalized_keys]\n        dedup_keep_indices: list[int] = []\n        for row_idx in range(source_table.num_rows):\n            key = tuple(arr[row_idx] for arr in key_arrays)\n            if key in seen_keys:\n                continue\n            seen_keys.add(key)\n            dedup_keep_indices.append(row_idx)\n        if dedup_keep_indices:\n            source_table = source_table.take(pa.array(dedup_keep_indices))\n\n    # Report progress for merge execution\n    if progress_callback:\n        progress_callback(\"Executing merge strategy\", 3, 5)\n\n    # Touch the target via filtered scanners to comply with the streaming spec.\n    if target_dataset is not None:\n        for batch in _iter_table_slices(source_table, batch_rows):\n            filter_expression = _build_filter_expression(batch, normalized_keys)\n            if filter_expression is None:\n                continue\n            scanner = target_dataset.scanner(filter=filter_expression, columns=normalized_keys)\n            for _ in scanner.to_batches():\n                # Consume batches to ensure the filtered scan executes.\n                pass\n\n    source_key_arrays = [source_table.column(col).to_pylist() for col in normalized_keys]\n    source_index: dict[tuple[Any, ...], int] = {}\n    for row_idx in range(source_table.num_rows):\n        key = tuple(arr[row_idx] for arr in source_key_arrays)\n        source_index[key] = row_idx\n\n    processed_source_indices: set[int] = set()\n    output_tables: list[pa.Table] = []\n    stats = {\"inserted\": 0, \"updated\": 0, \"deleted\": 0, \"total\": 0}\n\n    if target_dataset is None:\n        if core_strategy == CoreMergeStrategy.UPDATE:\n            raise ValueError(\"Target dataset is empty; nothing to update\")\n        if core_strategy == CoreMergeStrategy.INSERT:\n            stats[\"inserted\"] = source_table.num_rows\n        elif core_strategy in {CoreMergeStrategy.UPSERT, CoreMergeStrategy.DEDUPLICATE, CoreMergeStrategy.FULL_MERGE}:\n            stats[\"inserted\"] = source_table.num_rows\n        output_tables.append(source_table)\n        processed_source_indices.update(range(source_table.num_rows))\n    else:\n        scanner = target_dataset.scanner(columns=unified_schema.names)\n        for batch in scanner.to_batches():\n            table = pa.Table.from_batches([batch])\n            keep_indices: list[int] = []\n            replacement_indices: list[int] = []\n            key_lists = [table.column(col).to_pylist() for col in normalized_keys]\n            for idx in range(table.num_rows):\n                key = tuple(values[idx] for values in key_lists)\n                source_row_idx = source_index.get(key)\n                if source_row_idx is not None:\n                    processed_source_indices.add(source_row_idx)\n                    if core_strategy == CoreMergeStrategy.INSERT:\n                        keep_indices.append(idx)\n                    else:\n                        replacement_indices.append(source_row_idx)\n                        if core_strategy in {CoreMergeStrategy.UPSERT, CoreMergeStrategy.DEDUPLICATE, CoreMergeStrategy.UPDATE, CoreMergeStrategy.FULL_MERGE}:\n                            stats[\"updated\"] += 1\n                else:\n                    if core_strategy == CoreMergeStrategy.FULL_MERGE:\n                        stats[\"deleted\"] += 1\n                        continue\n                    keep_indices.append(idx)\n\n            if keep_indices:\n                kept = table.take(pa.array(keep_indices))\n                output_tables.append(cast_schema(kept, unified_schema))\n            if replacement_indices and core_strategy != CoreMergeStrategy.INSERT:\n                replacements = source_table.take(pa.array(replacement_indices))\n                output_tables.append(cast_schema(replacements, unified_schema))\n\n    remaining = [\n        idx\n        for idx in range(source_table.num_rows)\n        if idx not in processed_source_indices\n    ]\n    if remaining:\n        if core_strategy in {CoreMergeStrategy.INSERT, CoreMergeStrategy.UPSERT, CoreMergeStrategy.DEDUPLICATE, CoreMergeStrategy.FULL_MERGE}:\n            inserted_rows = source_table.take(pa.array(remaining))\n            output_tables.append(cast_schema(inserted_rows, unified_schema))\n            stats[\"inserted\"] += len(remaining)\n        # update strategy ignores non-matching rows\n\n    # Report progress for writing results\n    if progress_callback:\n        progress_callback(\"Writing merged results\", 4, 5)\n\n    _write_tables_to_dataset(output_tables, unified_schema, target_path, fs, compression)\n    stats[\"total\"] = sum(table.num_rows for table in output_tables)\n    return stats\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.pyarrow.opt_dtype","title":"fsspeckit.datasets.pyarrow.opt_dtype","text":"<pre><code>opt_dtype(\n    table: Table,\n    include: str | list[str] | None = None,\n    exclude: str | list[str] | None = None,\n    time_zone: str | None = None,\n    shrink_numerics: bool = False,\n    allow_unsigned: bool = True,\n    use_large_dtypes: bool = False,\n    strict: bool = False,\n    allow_null: bool = True,\n    sample_size: int | None = 1024,\n    sample_method: SampleMethod = \"first\",\n    *,\n    force_timezone: str | None = None,\n) -&gt; Table\n</code></pre> <p>Optimize data types of a PyArrow Table for performance and memory efficiency. Returns a new table casted to the optimal schema.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>Table</code> <p>The PyArrow table to optimize.</p> required <code>include</code> <code>str | list[str] | None</code> <p>Column(s) to include in optimization (default: all columns).</p> <code>None</code> <code>exclude</code> <code>str | list[str] | None</code> <p>Column(s) to exclude from optimization.</p> <code>None</code> <code>time_zone</code> <code>str | None</code> <p>Optional time zone hint during datetime parsing.</p> <code>None</code> <code>shrink_numerics</code> <code>bool</code> <p>Whether to downcast numeric types when possible.</p> <code>False</code> <code>allow_unsigned</code> <code>bool</code> <p>Whether to allow unsigned integer types.</p> <code>True</code> <code>use_large_dtypes</code> <code>bool</code> <p>If True, keep large types like large_string.</p> <code>False</code> <code>strict</code> <code>bool</code> <p>If True, will raise an error if any column cannot be optimized.</p> <code>False</code> <code>allow_null</code> <code>bool</code> <p>If False, columns that only hold null-like values will not be converted to pyarrow.null().</p> <code>True</code> <code>sample_size</code> <code>int | None</code> <p>Maximum number of cleaned values to inspect during regex inference (None to inspect all).</p> <code>1024</code> <code>sample_method</code> <code>SampleMethod</code> <p>Sampling strategy (<code>\"first\"</code> or <code>\"random\"</code>) for the inference subset.</p> <code>'first'</code> <code>force_timezone</code> <code>str | None</code> <p>If set, ensure all parsed datetime columns end up with this timezone.</p> <code>None</code> Source code in <code>src/fsspeckit/datasets/pyarrow.py</code> <pre><code>def opt_dtype(\n    table: pa.Table,\n    include: str | list[str] | None = None,\n    exclude: str | list[str] | None = None,\n    time_zone: str | None = None,\n    shrink_numerics: bool = False,\n    allow_unsigned: bool = True,\n    use_large_dtypes: bool = False,\n    strict: bool = False,\n    allow_null: bool = True,\n    sample_size: int | None = 1024,\n    sample_method: SampleMethod = \"first\",\n    *,\n    force_timezone: str | None = None,\n) -&gt; pa.Table:\n    \"\"\"\n    Optimize data types of a PyArrow Table for performance and memory efficiency.\n    Returns a new table casted to the optimal schema.\n\n    Args:\n        table: The PyArrow table to optimize.\n        include: Column(s) to include in optimization (default: all columns).\n        exclude: Column(s) to exclude from optimization.\n        time_zone: Optional time zone hint during datetime parsing.\n        shrink_numerics: Whether to downcast numeric types when possible.\n        allow_unsigned: Whether to allow unsigned integer types.\n        use_large_dtypes: If True, keep large types like large_string.\n        strict: If True, will raise an error if any column cannot be optimized.\n        allow_null: If False, columns that only hold null-like values will not be converted to pyarrow.null().\n        sample_size: Maximum number of cleaned values to inspect during regex inference (None to inspect all).\n        sample_method: Sampling strategy (`\"first\"` or `\"random\"`) for the inference subset.\n        force_timezone: If set, ensure all parsed datetime columns end up with this timezone.\n    \"\"\"\n    if sample_method not in (\"first\", \"random\"):\n        raise ValueError(\"sample_method must be 'first' or 'random'\")\n\n    if isinstance(include, str):\n        include = [include]\n    if isinstance(exclude, str):\n        exclude = [exclude]\n\n    cols_to_process = table.column_names\n    if include:\n        cols_to_process = [col for col in include if col in table.column_names]\n    if exclude:\n        cols_to_process = [col for col in cols_to_process if col not in exclude]\n\n    # Prepare arguments for parallel processing\n    args_list = [\n        (\n            table[col_name],\n            col_name,\n            cols_to_process,\n            shrink_numerics,\n            allow_unsigned,\n            time_zone,\n            strict,\n            allow_null,\n            force_timezone,\n            sample_size,\n            sample_method,\n        )\n        for col_name in table.column_names\n    ]\n\n    # Parallelize column processing\n    with concurrent.futures.ThreadPoolExecutor() as executor:\n        results = list(executor.map(_process_column_for_opt_dtype, args_list))\n\n    # Sort results to preserve column order\n    results.sort(key=lambda x: table.column_names.index(x[0]))\n    fields = [field for _, field, _ in results]\n    arrays = [array for _, _, array in results]\n\n    schema = pa.schema(fields)\n    if not use_large_dtypes:\n        schema = convert_large_types_to_normal(schema)\n    return pa.Table.from_arrays(arrays, schema=schema)\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.pyarrow.optimize_parquet_dataset_pyarrow","title":"fsspeckit.datasets.pyarrow.optimize_parquet_dataset_pyarrow","text":"<pre><code>optimize_parquet_dataset_pyarrow(\n    path: str,\n    zorder_columns: list[str],\n    target_mb_per_file: int | None = None,\n    target_rows_per_file: int | None = None,\n    partition_filter: list[str] | None = None,\n    compression: str | None = None,\n    dry_run: bool = False,\n    filesystem: AbstractFileSystem | None = None,\n) -&gt; dict[str, Any]\n</code></pre> <p>Cluster parquet files by <code>zorder_columns</code> while rewriting groups on disk.</p> <p>This function uses the shared core planning algorithm for consistent optimization behavior across backends. It processes data in a streaming, per-group fashion that avoids materializing the entire dataset.</p> <p>The helper enumerates individual parquet files under <code>path</code>, optionally filtering them by <code>partition_filter</code> prefixes so we never materialize the entire dataset with <code>dataset.to_table()</code>. Each optimization group is streamed file-by-file, sorted by <code>zorder_columns</code> in memory, and then rewritten to an <code>optimized-*.parquet</code> file while the original inputs are deleted only after a successful write. Use dry-run mode to inspect the planned groups/metrics before any data is touched.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Dataset root directory (local path or fsspec URL).</p> required <code>zorder_columns</code> <code>list[str]</code> <p>Ordered columns that determine clustering/ordering.</p> required <code>target_mb_per_file</code> <code>int | None</code> <p>Optional max output size per file; must be &gt; 0.</p> <code>None</code> <code>target_rows_per_file</code> <code>int | None</code> <p>Optional max rows per output file; must be &gt; 0.</p> <code>None</code> <code>partition_filter</code> <code>list[str] | None</code> <p>Optional list of partition prefixes (e.g. <code>[\"date=2025-11-15\"]</code>) used to limit both stats collection and rewrites to matching paths.</p> <code>None</code> <code>compression</code> <code>str | None</code> <p>Optional parquet compression codec; defaults to <code>\"snappy\"</code>.</p> <code>None</code> <code>dry_run</code> <code>bool</code> <p>When <code>True</code> the function returns a plan + before/after stats without reading or writing any parquet data.</p> <code>False</code> <code>filesystem</code> <code>AbstractFileSystem | None</code> <p>Optional <code>fsspec.AbstractFileSystem</code> to reuse existing FS clients.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A stats dictionary describing before/after file counts, total bytes,</p> <code>dict[str, Any]</code> <p>rewritten bytes, <code>zorder_columns</code>, and optional <code>planned_groups</code> when</p> <code>dict[str, Any]</code> <p><code>dry_run</code> is enabled. The structure follows the canonical <code>MaintenanceStats</code></p> <code>dict[str, Any]</code> <p>format from the shared core.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If thresholds are invalid or if any <code>zorder_columns</code> are missing.</p> <code>FileNotFoundError</code> <p>If the path does not exist or no parquet files are found.</p> Note <p>This function delegates optimization planning and validation to the shared <code>fsspeckit.core.maintenance.plan_optimize_groups</code> function, ensuring consistent behavior with DuckDB backend.</p> Source code in <code>src/fsspeckit/datasets/pyarrow.py</code> <pre><code>def optimize_parquet_dataset_pyarrow(\n    path: str,\n    zorder_columns: list[str],\n    target_mb_per_file: int | None = None,\n    target_rows_per_file: int | None = None,\n    partition_filter: list[str] | None = None,\n    compression: str | None = None,\n    dry_run: bool = False,\n    filesystem: AbstractFileSystem | None = None,\n) -&gt; dict[str, Any]:\n    \"\"\"Cluster parquet files by ``zorder_columns`` while rewriting groups on disk.\n\n    This function uses the shared core planning algorithm for consistent optimization\n    behavior across backends. It processes data in a streaming, per-group fashion\n    that avoids materializing the entire dataset.\n\n    The helper enumerates individual parquet files under ``path``, optionally\n    filtering them by ``partition_filter`` prefixes so we never materialize the\n    entire dataset with ``dataset.to_table()``. Each optimization group is streamed\n    file-by-file, sorted by ``zorder_columns`` in memory, and then rewritten to an\n    ``optimized-*.parquet`` file while the original inputs are deleted only after\n    a successful write. Use dry-run mode to inspect the planned groups/metrics\n    before any data is touched.\n\n    Args:\n        path: Dataset root directory (local path or fsspec URL).\n        zorder_columns: Ordered columns that determine clustering/ordering.\n        target_mb_per_file: Optional max output size per file; must be &gt; 0.\n        target_rows_per_file: Optional max rows per output file; must be &gt; 0.\n        partition_filter: Optional list of partition prefixes (e.g. ``[\"date=2025-11-15\"]``)\n            used to limit both stats collection and rewrites to matching paths.\n        compression: Optional parquet compression codec; defaults to ``\"snappy\"``.\n        dry_run: When ``True`` the function returns a plan + before/after stats\n            without reading or writing any parquet data.\n        filesystem: Optional ``fsspec.AbstractFileSystem`` to reuse existing FS clients.\n\n    Returns:\n        A stats dictionary describing before/after file counts, total bytes,\n        rewritten bytes, ``zorder_columns``, and optional ``planned_groups`` when\n        ``dry_run`` is enabled. The structure follows the canonical ``MaintenanceStats``\n        format from the shared core.\n\n    Raises:\n        ValueError: If thresholds are invalid or if any ``zorder_columns`` are missing.\n        FileNotFoundError: If the path does not exist or no parquet files are found.\n\n    Note:\n        This function delegates optimization planning and validation to the shared\n        ``fsspeckit.core.maintenance.plan_optimize_groups`` function, ensuring\n        consistent behavior with DuckDB backend.\n    \"\"\"\n    from fsspeckit.core.maintenance import (\n        MaintenanceStats,\n        plan_optimize_groups,\n    )\n\n    # Use shared core validation\n    if not zorder_columns:\n        raise ValueError(\"zorder_columns must be a non-empty list\")\n    if target_mb_per_file is not None and target_mb_per_file &lt;= 0:\n        raise ValueError(\"target_mb_per_file must be &gt; 0\")\n    if target_rows_per_file is not None and target_rows_per_file &lt;= 0:\n        raise ValueError(\"target_rows_per_file must be &gt; 0\")\n\n    # Collect dataset stats using shared core\n    stats = collect_dataset_stats_pyarrow(\n        path=path, filesystem=filesystem, partition_filter=partition_filter\n    )\n    files = stats[\"files\"]\n\n    if not files:\n        return {\n            \"before_file_count\": 0,\n            \"after_file_count\": 0,\n            \"before_total_bytes\": 0,\n            \"after_total_bytes\": 0,\n            \"compacted_file_count\": 0,\n            \"rewritten_bytes\": 0,\n            \"compression_codec\": compression,\n            \"dry_run\": dry_run,\n            \"zorder_columns\": list(zorder_columns),\n        }\n\n    fs = filesystem or fsspec_filesystem(\"file\")\n\n    # Get schema for validation (sample first file)\n    sample_path = str(files[0][\"path\"])\n    with fs.open(sample_path, \"rb\") as fh:\n        sample_table = pq.read_table(fh)\n\n    # Use shared core planning with z-order validation\n    result = plan_optimize_groups(\n        files,\n        zorder_columns=zorder_columns,\n        target_mb_per_file=target_mb_per_file,\n        target_rows_per_file=target_rows_per_file,\n        sample_schema=sample_table,\n    )\n\n    groups = result[\"groups\"]\n    planned_stats = result[\"planned_stats\"]\n\n    # Handle dry run\n    if dry_run:\n        final_stats = MaintenanceStats(\n            before_file_count=planned_stats.before_file_count,\n            after_file_count=planned_stats.after_file_count,\n            before_total_bytes=planned_stats.before_total_bytes,\n            after_total_bytes=planned_stats.after_total_bytes,\n            compacted_file_count=planned_stats.compacted_file_count,\n            rewritten_bytes=planned_stats.rewritten_bytes,\n            compression_codec=compression,\n            dry_run=True,\n            zorder_columns=list(zorder_columns),\n            planned_groups=planned_stats.planned_groups,\n        )\n        return final_stats.to_dict()\n\n    codec = compression or \"snappy\"\n    written_files = []\n\n    # Process each optimization group in streaming fashion\n    for group_idx, group in enumerate(groups):\n        if len(group.files) == 0:\n            continue\n\n        # Read tables for this group only (streaming per-group)\n        tables: list[pa.Table] = []\n        for file_info in group.files:\n            file_path = str(file_info.path)\n            with fs.open(file_path, \"rb\") as fh:\n                tables.append(pq.read_table(fh))\n\n        if not tables:\n            continue\n\n        # Sort the combined group data by z-order columns\n        combined = pa.concat_tables(tables, promote=True)\n        sort_keys = [(col, \"ascending\") for col in zorder_columns]\n        combined_sorted = combined.sort_by(sort_keys)\n\n        # Write sorted group to output file\n        out_name = f\"optimized-{group_idx:05d}.parquet\"\n        out_path = str(Path(path) / out_name)\n        with fs.open(out_path, \"wb\") as fh:\n            pq.write_table(combined_sorted, fh, compression=codec)\n\n        written_files.append(out_path)\n\n        # Delete original files in this group\n        for file_info in group.files:\n            file_path = str(file_info.path)\n            try:\n                fs.rm(file_path)\n            except Exception:\n                print(f\"Warning: failed to delete '{file_path}' during optimize\")\n\n    # Recompute stats after optimization\n    stats_after = collect_dataset_stats_pyarrow(\n        path=path, filesystem=fs, partition_filter=partition_filter\n    )\n\n    # Create final stats\n    final_stats = MaintenanceStats(\n        before_file_count=planned_stats.before_file_count,\n        after_file_count=len(stats_after[\"files\"]),\n        before_total_bytes=planned_stats.before_total_bytes,\n        after_total_bytes=stats_after[\"total_bytes\"],\n        compacted_file_count=planned_stats.compacted_file_count,\n        rewritten_bytes=stats_after[\"total_bytes\"],\n        compression_codec=codec,\n        dry_run=False,\n        zorder_columns=list(zorder_columns),\n    )\n\n    return final_stats.to_dict()\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.pyarrow.remove_empty_columns","title":"fsspeckit.datasets.pyarrow.remove_empty_columns","text":"<pre><code>remove_empty_columns(table: Table) -&gt; Table\n</code></pre> <p>Remove columns that are entirely empty from a PyArrow table.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>Table</code> <p>The PyArrow table to process.</p> required <p>Returns:</p> Type Description <code>Table</code> <p>pa.Table: A new PyArrow table with empty columns removed.</p> Source code in <code>src/fsspeckit/datasets/pyarrow.py</code> <pre><code>def remove_empty_columns(table: pa.Table) -&gt; pa.Table:\n    \"\"\"Remove columns that are entirely empty from a PyArrow table.\n\n    Args:\n        table (pa.Table): The PyArrow table to process.\n\n    Returns:\n        pa.Table: A new PyArrow table with empty columns removed.\n    \"\"\"\n    empty_cols = _identify_empty_columns(table)\n    if not empty_cols:\n        return table\n    return table.drop(empty_cols)\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.pyarrow.standardize_schema_timezones","title":"fsspeckit.datasets.pyarrow.standardize_schema_timezones","text":"<pre><code>standardize_schema_timezones(\n    schemas: Schema | list[Schema],\n    timezone: str | None = None,\n) -&gt; Schema | list[Schema]\n</code></pre> <p>Standardize timezone info for all timestamp columns in a list of PyArrow schemas.</p> <p>Parameters:</p> Name Type Description Default <code>schemas</code> <code>list of pa.Schema</code> <p>List of PyArrow schemas.</p> required <code>timezone</code> <code>str or None</code> <p>If None, remove timezone from all timestamp columns.                     If str, set this timezone for all timestamp columns.                     If \"auto\", use the most frequent timezone across schemas.</p> <code>None</code> <p>Returns:</p> Type Description <code>Schema | list[Schema]</code> <p>list of pa.Schema: New schemas with standardized timezone info.</p> Source code in <code>src/fsspeckit/datasets/pyarrow.py</code> <pre><code>def standardize_schema_timezones(\n    schemas: pa.Schema | list[pa.Schema], timezone: str | None = None\n) -&gt; pa.Schema | list[pa.Schema]:\n    \"\"\"\n    Standardize timezone info for all timestamp columns in a list of PyArrow schemas.\n\n    Args:\n        schemas (list of pa.Schema): List of PyArrow schemas.\n        timezone (str or None): If None, remove timezone from all timestamp columns.\n                                If str, set this timezone for all timestamp columns.\n                                If \"auto\", use the most frequent timezone across schemas.\n\n    Returns:\n        list of pa.Schema: New schemas with standardized timezone info.\n    \"\"\"\n    if isinstance(schemas, pa.Schema):\n        single_input = True\n        schema_list: list[pa.Schema] = [schemas]\n    else:\n        single_input = False\n        schema_list = list(schemas)\n    if timezone == \"auto\":\n        majority_schema = standardize_schema_timezones_by_majority(schema_list)\n        result_list = [majority_schema for _ in schema_list]\n        return majority_schema if single_input else result_list\n    new_schemas = []\n    for schema in schema_list:\n        fields = []\n        for field in schema:\n            if pa.types.is_timestamp(field.type):\n                fields.append(\n                    pa.field(\n                        field.name,\n                        pa.timestamp(field.type.unit, timezone),\n                        field.nullable,\n                        field.metadata,\n                    )\n                )\n            else:\n                fields.append(field)\n        new_schemas.append(pa.schema(fields, schema.metadata))\n    return new_schemas[0] if single_input else new_schemas\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.pyarrow.standardize_schema_timezones_by_majority","title":"fsspeckit.datasets.pyarrow.standardize_schema_timezones_by_majority","text":"<pre><code>standardize_schema_timezones_by_majority(\n    schemas: list[Schema],\n) -&gt; Schema\n</code></pre> <p>For each timestamp column (by name) across all schemas, set the timezone to the most frequent (with tie-breaking). Returns a new list of schemas with updated timestamp timezones.</p> Source code in <code>src/fsspeckit/datasets/pyarrow.py</code> <pre><code>def standardize_schema_timezones_by_majority(\n    schemas: list[pa.Schema],\n) -&gt; pa.Schema:\n    \"\"\"\n    For each timestamp column (by name) across all schemas, set the timezone to the most frequent (with tie-breaking).\n    Returns a new list of schemas with updated timestamp timezones.\n    \"\"\"\n    dom = dominant_timezone_per_column(schemas)\n    if not schemas:\n        return pa.schema([])\n\n    seen: set[str] = set()\n    fields: list[pa.Field] = []\n    for schema in schemas:\n        for field in schema:\n            if field.name in seen:\n                continue\n            seen.add(field.name)\n            if pa.types.is_timestamp(field.type) and field.name in dom:\n                unit, tz = dom[field.name]\n                fields.append(\n                    pa.field(\n                        field.name,\n                        pa.timestamp(unit, tz),\n                        field.nullable,\n                        field.metadata,\n                    )\n                )\n            else:\n                fields.append(field)\n    return pa.schema(fields, schemas[0].metadata)\n</code></pre>"},{"location":"api/fsspeckit.datasets/#fsspeckit.datasets.pyarrow.unify_schemas","title":"fsspeckit.datasets.pyarrow.unify_schemas","text":"<pre><code>unify_schemas(\n    schemas: list[Schema],\n    use_large_dtypes: bool = False,\n    timezone: str | None = None,\n    standardize_timezones: bool = True,\n    verbose: bool = False,\n    remove_conflicting_columns: bool = False,\n) -&gt; Schema\n</code></pre> <p>Unify a list of PyArrow schemas into a single schema using intelligent conflict resolution.</p> <p>Parameters:</p> Name Type Description Default <code>schemas</code> <code>list[Schema]</code> <p>List of PyArrow schemas to unify.</p> required <code>use_large_dtypes</code> <code>bool</code> <p>If True, keep large types like large_string.</p> <code>False</code> <code>timezone</code> <code>str | None</code> <p>If specified, standardize all timestamp columns to this timezone. If \"auto\", use the most frequent timezone across schemas. If None, remove timezone from all timestamp columns.</p> <code>None</code> <code>standardize_timezones</code> <code>bool</code> <p>If True, standardize all timestamp columns to the most frequent timezone.</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>If True, print conflict resolution details for debugging.</p> <code>False</code> <code>remove_conflicting_columns</code> <code>bool</code> <p>If True, allows removal of columns with type conflicts as a fallback strategy instead of converting them. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Schema</code> <p>pa.Schema: A unified PyArrow schema.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no schemas are provided.</p> Source code in <code>src/fsspeckit/datasets/pyarrow.py</code> <pre><code>def unify_schemas(\n    schemas: list[pa.Schema],\n    use_large_dtypes: bool = False,\n    timezone: str | None = None,\n    standardize_timezones: bool = True,\n    verbose: bool = False,\n    remove_conflicting_columns: bool = False,\n) -&gt; pa.Schema:\n    \"\"\"\n    Unify a list of PyArrow schemas into a single schema using intelligent conflict resolution.\n\n    Args:\n        schemas (list[pa.Schema]): List of PyArrow schemas to unify.\n        use_large_dtypes (bool): If True, keep large types like large_string.\n        timezone (str | None): If specified, standardize all timestamp columns to this timezone.\n            If \"auto\", use the most frequent timezone across schemas.\n            If None, remove timezone from all timestamp columns.\n        standardize_timezones (bool): If True, standardize all timestamp columns to the most frequent timezone.\n        verbose (bool): If True, print conflict resolution details for debugging.\n        remove_conflicting_columns (bool): If True, allows removal of columns with type conflicts as a fallback\n            strategy instead of converting them. Defaults to False.\n\n    Returns:\n        pa.Schema: A unified PyArrow schema.\n\n    Raises:\n        ValueError: If no schemas are provided.\n    \"\"\"\n    if not schemas:\n        raise ValueError(\"At least one schema must be provided for unification\")\n\n    # Early exit for single schema\n    unique_schemas = _unique_schemas(schemas)\n    if len(unique_schemas) == 1:\n        result_schema = unique_schemas[0]\n        if standardize_timezones:\n            result_schema = standardize_schema_timezones([result_schema], timezone)[0]\n        return (\n            result_schema\n            if use_large_dtypes\n            else convert_large_types_to_normal(result_schema)\n        )\n\n    # Step 1: Find and resolve conflicts first\n    conflicts = _find_conflicting_fields(unique_schemas)\n    if conflicts and verbose:\n        _log_conflict_summary(conflicts, verbose)\n\n    if conflicts:\n        # Normalize schemas using intelligent promotion rules\n        unique_schemas = _normalize_schema_types(unique_schemas, conflicts)\n\n    # Step 2: Attempt unification with conflict-resolved schemas\n    try:\n        unified_schema = pa.unify_schemas(unique_schemas, promote_options=\"permissive\")\n\n        # Step 3: Apply timezone standardization to the unified result\n        if standardize_timezones:\n            unified_schema = standardize_schema_timezones([unified_schema], timezone)[0]\n\n        return (\n            unified_schema\n            if use_large_dtypes\n            else convert_large_types_to_normal(unified_schema)\n        )\n\n    except (pa.ArrowInvalid, pa.ArrowTypeError) as e:\n        # Step 4: Intelligent fallback strategies\n        if verbose:\n            print(f\"Primary unification failed: {e}\")\n            print(\"Attempting fallback strategies...\")\n\n        # Fallback 1: Try aggressive string conversion for remaining conflicts\n        try:\n            fallback_schema = _aggressive_fallback_unification(unique_schemas)\n            if standardize_timezones:\n                fallback_schema = standardize_schema_timezones(\n                    [fallback_schema], timezone\n                )[0]\n            if verbose:\n                print(\"\u2713 Aggressive fallback succeeded\")\n            return (\n                fallback_schema\n                if use_large_dtypes\n                else convert_large_types_to_normal(fallback_schema)\n            )\n\n        except Exception:\n            if verbose:\n                print(\"\u2717 Aggressive fallback failed\")\n\n        # Fallback 2: Remove conflicting fields (if enabled)\n        if remove_conflicting_columns:\n            try:\n                non_conflicting_schema = _remove_conflicting_fields(unique_schemas)\n                if standardize_timezones:\n                    non_conflicting_schema = standardize_schema_timezones(\n                        [non_conflicting_schema], timezone\n                    )[0]\n                if verbose:\n                    print(\"\u2713 Remove conflicting fields fallback succeeded\")\n                return (\n                    non_conflicting_schema\n                    if use_large_dtypes\n                    else convert_large_types_to_normal(non_conflicting_schema)\n                )\n\n            except Exception:\n                if verbose:\n                    print(\"\u2717 Remove conflicting fields fallback failed\")\n\n        # Fallback 3: Remove problematic fields that can't be unified\n        try:\n            minimal_schema = _remove_problematic_fields(unique_schemas)\n            if standardize_timezones:\n                minimal_schema = standardize_schema_timezones(\n                    [minimal_schema], timezone\n                )[0]\n            if verbose:\n                print(\"\u2713 Minimal schema (removed problematic fields) succeeded\")\n            return (\n                minimal_schema\n                if use_large_dtypes\n                else convert_large_types_to_normal(minimal_schema)\n            )\n\n        except Exception:\n            if verbose:\n                print(\"\u2717 Minimal schema fallback failed\")\n\n        # Fallback 4: Return first schema as last resort\n        if verbose:\n            print(\"\u2717 All fallback strategies failed, returning first schema\")\n\n        first_schema = unique_schemas[0]\n        if standardize_timezones:\n            first_schema = standardize_schema_timezones([first_schema], timezone)[0]\n        return (\n            first_schema\n            if use_large_dtypes\n            else convert_large_types_to_normal(first_schema)\n        )\n</code></pre>"},{"location":"api/fsspeckit.sql.filters/","title":"fsspeckit.sql.filters","text":""},{"location":"api/fsspeckit.sql.filters/#fsspeckit.sql.filters","title":"filters","text":""},{"location":"api/fsspeckit.sql.filters/#fsspeckit.sql.filters-functions","title":"Functions","text":""},{"location":"api/fsspeckit.sql.filters/#fsspeckit.sql.filters.sql2polars_filter","title":"fsspeckit.sql.filters.sql2polars_filter","text":"<pre><code>sql2polars_filter(string: str, schema: Schema) -&gt; Expr\n</code></pre> <p>Generates a filter expression for Polars based on a given string and schema.</p> <p>Parameters:</p> Name Type Description Default <code>string</code> <code>str</code> <p>The string containing the filter expression.</p> required <code>schema</code> <code>Schema</code> <p>The Polars schema used to validate the filter expression.</p> required <p>Returns:</p> Type Description <code>Expr</code> <p>pl.Expr: The generated filter expression.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input string is invalid or contains unsupported operations.</p> Source code in <code>src/fsspeckit/sql/filters/__init__.py</code> <pre><code>def sql2polars_filter(string: str, schema: pl.Schema) -&gt; pl.Expr:\n    \"\"\"\n    Generates a filter expression for Polars based on a given string and schema.\n\n    Parameters:\n        string (str): The string containing the filter expression.\n        schema (pl.Schema): The Polars schema used to validate the filter expression.\n\n    Returns:\n        pl.Expr: The generated filter expression.\n\n    Raises:\n        ValueError: If the input string is invalid or contains unsupported operations.\n    \"\"\"\n\n    def parse_value(val: str, dtype: pl.DataType) -&gt; Any:\n        \"\"\"Parse and convert value based on the field type.\"\"\"\n        if isinstance(val, (tuple, list)):\n            return type(val)(parse_value(v, dtype) for v in val)\n\n        # Remove quotes from the value if present\n        val = val.strip().strip(\"'\\\"\")\n\n        if dtype == pl.Datetime:\n            return timestamp_from_string(val, tz=dtype.time_zone)\n        elif dtype == pl.Date:\n            return timestamp_from_string(val).date()\n        elif dtype == pl.Time:\n            return timestamp_from_string(val).time()\n        elif dtype in (pl.Int8, pl.Int16, pl.Int32, pl.Int64):\n            return int(float(val.replace(\",\", \".\")))\n        elif dtype in (pl.Float32, pl.Float64):\n            return float(val.replace(\",\", \".\"))\n        elif dtype == pl.Boolean:\n            return val.lower() in (\"true\", \"1\", \"yes\")\n        else:\n            return val\n\n    def _get_field_type_from_context(expr):\n        \"\"\"Try to determine the field type from a comparison expression.\"\"\"\n        if isinstance(expr.this, exp.Column):\n            field_name = expr.this.name\n            if field_name in schema.names():\n                return schema[field_name]\n        elif isinstance(expr.expression, exp.Column):\n            field_name = expr.expression.name\n            if field_name in schema.names():\n                return schema[field_name]\n        return None\n\n    def _convert_expression(expr, field_type=None) -&gt; pl.Expr:\n        \"\"\"Convert a sqlglot expression to a Polars expression.\"\"\"\n        if isinstance(expr, exp.Column):\n            field_name = expr.name\n            if field_name not in schema.names():\n                raise ValueError(f\"Unknown field: {field_name}\")\n            return pl.col(field_name)\n\n        elif isinstance(expr, exp.Literal):\n            # Convert literal value based on field type if available\n            if field_type:\n                val = str(expr.this)\n                # Remove quotes if present\n                val = val.strip().strip(\"'\\\"\")\n                return parse_value(val, field_type)\n            return expr.this\n\n        elif isinstance(expr, exp.Null):\n            return None\n\n        elif isinstance(expr, (exp.EQ, exp.NEQ, exp.GT, exp.GTE, exp.LT, exp.LTE)):\n            # Binary comparison operations\n            # Try to determine field type from context\n            context_type = _get_field_type_from_context(expr)\n\n            left = _convert_expression(expr.this, context_type)\n            right = _convert_expression(expr.expression, context_type)\n\n            if isinstance(expr, exp.EQ):\n                return left == right\n            elif isinstance(expr, exp.NEQ):\n                return left != right\n            elif isinstance(expr, exp.GT):\n                return left &gt; right\n            elif isinstance(expr, exp.GTE):\n                return left &gt;= right\n            elif isinstance(expr, exp.LT):\n                return left &lt; right\n            elif isinstance(expr, exp.LTE):\n                return left &lt;= right\n\n        elif isinstance(expr, exp.In):\n            # IN operation\n            context_type = _get_field_type_from_context(expr)\n            left = _convert_expression(expr.this, context_type)\n            # Convert the IN list\n            if hasattr(expr, \"expression\") and hasattr(expr.expression, \"expressions\"):\n                right = [\n                    _convert_expression(e, context_type)\n                    for e in expr.expression.expressions\n                ]\n            else:\n                right = _convert_expression(expr.expression, context_type)\n            return left.is_in(right)\n\n        elif isinstance(expr, exp.Not):\n            # NOT operation - check if it's NOT IN or IS NOT NULL\n            inner = expr.this\n            if isinstance(inner, exp.In):\n                # NOT IN case\n                context_type = _get_field_type_from_context(inner)\n                left = _convert_expression(inner.this, context_type)\n                if hasattr(inner, \"expression\") and hasattr(\n                    inner.expression, \"expressions\"\n                ):\n                    right = [\n                        _convert_expression(e, context_type)\n                        for e in inner.expression.expressions\n                    ]\n                else:\n                    right = _convert_expression(inner.expression, context_type)\n                return ~left.is_in(right)\n            elif isinstance(inner, exp.Is):\n                # IS NOT NULL case\n                left = _convert_expression(inner.this)\n                return left.is_not_null()\n            else:\n                # Generic NOT\n                return ~_convert_expression(inner)\n\n        elif isinstance(expr, exp.Is):\n            left = _convert_expression(expr.this)\n            return left.is_null()\n\n        elif isinstance(expr, exp.And):\n            return _convert_expression(expr.this) &amp; _convert_expression(expr.expression)\n\n        elif isinstance(expr, exp.Or):\n            return _convert_expression(expr.this) | _convert_expression(expr.expression)\n\n        elif isinstance(expr, exp.Not):\n            return ~_convert_expression(expr.this)\n\n        elif isinstance(expr, exp.Paren):\n            return _convert_expression(expr.this)\n\n        else:\n            raise ValueError(f\"Unsupported expression type: {type(expr)}\")\n\n    try:\n        # Parse the SQL expression using sqlglot\n        parsed = parse_one(string)\n\n        # Convert to Polars expression\n        return _convert_expression(parsed)\n\n    except Exception as e:\n        raise ValueError(f\"Failed to parse SQL expression: {e}\")\n</code></pre>"},{"location":"api/fsspeckit.sql.filters/#fsspeckit.sql.filters.sql2pyarrow_filter","title":"fsspeckit.sql.filters.sql2pyarrow_filter","text":"<pre><code>sql2pyarrow_filter(\n    string: str, schema: Schema\n) -&gt; Expression\n</code></pre> <p>Generates a filter expression for PyArrow based on a given string and schema.</p> <p>Parameters:</p> Name Type Description Default <code>string</code> <code>str</code> <p>The string containing the filter expression.</p> required <code>schema</code> <code>Schema</code> <p>The PyArrow schema used to validate the filter expression.</p> required <p>Returns:</p> Type Description <code>Expression</code> <p>pc.Expression: The generated filter expression.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input string is invalid or contains unsupported operations.</p> Source code in <code>src/fsspeckit/sql/filters/__init__.py</code> <pre><code>def sql2pyarrow_filter(string: str, schema: pa.Schema) -&gt; pc.Expression:\n    \"\"\"\n    Generates a filter expression for PyArrow based on a given string and schema.\n\n    Parameters:\n        string (str): The string containing the filter expression.\n        schema (pa.Schema): The PyArrow schema used to validate the filter expression.\n\n    Returns:\n        pc.Expression: The generated filter expression.\n\n    Raises:\n        ValueError: If the input string is invalid or contains unsupported operations.\n    \"\"\"\n\n    def parse_value(val: str, type_: pa.DataType) -&gt; Any:\n        \"\"\"Parse and convert value based on the field type.\"\"\"\n        if isinstance(val, (tuple, list)):\n            return type(val)(parse_value(v, type_) for v in val)\n\n        # Remove quotes from the value if present\n        val = val.strip().strip(\"'\\\"\")\n\n        if pa.types.is_timestamp(type_):\n            return timestamp_from_string(val, tz=type_.tz)\n        elif pa.types.is_date(type_):\n            parsed = timestamp_from_string(val)\n            return parsed.date() if hasattr(parsed, \"date\") else parsed\n        elif pa.types.is_time(type_):\n            parsed = timestamp_from_string(val)\n            return parsed.time() if hasattr(parsed, \"time\") else parsed\n\n        elif pa.types.is_integer(type_):\n            return int(float(val.replace(\",\", \".\")))\n        elif pa.types.is_floating(type_):\n            return float(val.replace(\",\", \".\"))\n        elif pa.types.is_boolean(type_):\n            return val.lower() in (\"true\", \"1\", \"yes\")\n        else:\n            return val\n\n    def _get_field_type_from_context(expr):\n        \"\"\"Try to determine the field type from a comparison expression.\"\"\"\n        if isinstance(expr.this, exp.Column):\n            field_name = expr.this.name\n            if field_name in schema.names:\n                return schema.field(field_name).type\n        elif isinstance(expr.expression, exp.Column):\n            field_name = expr.expression.name\n            if field_name in schema.names:\n                return schema.field(field_name).type\n        return None\n\n    def _convert_expression(expr, field_type=None) -&gt; pc.Expression:\n        \"\"\"Convert a sqlglot expression to a PyArrow compute expression.\"\"\"\n        if isinstance(expr, exp.Column):\n            field_name = expr.name\n            if field_name not in schema.names:\n                raise ValueError(f\"Unknown field: {field_name}\")\n            return pc.field(field_name)\n\n        elif isinstance(expr, exp.Literal):\n            # Convert literal value based on field type if available\n            if field_type:\n                val = str(expr.this)\n                # Remove quotes if present\n                val = val.strip().strip(\"'\\\"\")\n                return parse_value(val, field_type)\n            return expr.this\n\n        elif isinstance(expr, exp.Boolean):\n            return bool(expr.this)\n\n        elif isinstance(expr, exp.Null):\n            return None\n\n        elif isinstance(expr, (exp.EQ, exp.NEQ, exp.GT, exp.GTE, exp.LT, exp.LTE)):\n            # Binary comparison operations\n            # Try to determine field type from context\n            context_type = _get_field_type_from_context(expr)\n\n            left = _convert_expression(expr.this, context_type)\n            right = _convert_expression(expr.expression, context_type)\n\n            if isinstance(expr, exp.EQ):\n                return left == right\n            elif isinstance(expr, exp.NEQ):\n                return left != right\n            elif isinstance(expr, exp.GT):\n                return left &gt; right\n            elif isinstance(expr, exp.GTE):\n                return left &gt;= right\n            elif isinstance(expr, exp.LT):\n                return left &lt; right\n            elif isinstance(expr, exp.LTE):\n                return left &lt;= right\n\n        elif isinstance(expr, exp.In):\n            # IN operation\n            context_type = _get_field_type_from_context(expr)\n            left = _convert_expression(expr.this, context_type)\n            expressions = expr.args.get(\"expressions\")\n            if expressions is None and getattr(expr, \"expression\", None) is not None:\n                expressions = getattr(expr.expression, \"expressions\", None)\n\n            if expressions is None:\n                right = _convert_expression(expr.expression, context_type)\n            else:\n                right = [\n                    _convert_expression(e, context_type) for e in expressions  # type: ignore[arg-type]\n                ]\n            return left.isin(right)\n\n        elif isinstance(expr, exp.Not):\n            # NOT operation - check if it's NOT IN or IS NOT NULL\n            inner = expr.this\n            if isinstance(inner, exp.In):\n                # NOT IN case\n                context_type = _get_field_type_from_context(inner)\n                left = _convert_expression(inner.this, context_type)\n                expressions = inner.args.get(\"expressions\")\n                if expressions is None and getattr(inner, \"expression\", None) is not None:\n                    expressions = getattr(inner.expression, \"expressions\", None)\n\n                if expressions is None:\n                    right = _convert_expression(inner.expression, context_type)\n                else:\n                    right = [\n                        _convert_expression(e, context_type)\n                        for e in expressions  # type: ignore[arg-type]\n                    ]\n                return ~left.isin(right)\n            elif isinstance(inner, exp.Is):\n                # IS NOT NULL case\n                left = _convert_expression(inner.this)\n                return ~left.is_null(nan_is_null=True)\n            else:\n                # Generic NOT\n                return ~_convert_expression(inner)\n\n        elif isinstance(expr, exp.Is):\n            left = _convert_expression(expr.this)\n            return left.is_null(nan_is_null=True)\n\n        elif isinstance(expr, exp.And):\n            return _convert_expression(expr.this) &amp; _convert_expression(expr.expression)\n\n        elif isinstance(expr, exp.Or):\n            return _convert_expression(expr.this) | _convert_expression(expr.expression)\n\n        elif isinstance(expr, exp.Not):\n            return ~_convert_expression(expr.this)\n\n        elif isinstance(expr, exp.Paren):\n            return _convert_expression(expr.this)\n\n        else:\n            raise ValueError(f\"Unsupported expression type: {type(expr)}\")\n\n    try:\n        # Parse the SQL expression using sqlglot\n        parsed = parse_one(string)\n\n        # Convert to PyArrow expression\n        return _convert_expression(parsed)\n\n    except Exception as e:\n        raise ValueError(f\"Failed to parse SQL expression: {e}\")\n</code></pre>"},{"location":"api/fsspeckit.storage_options.base/","title":"<code>fsspeckit.storage_options.base</code> API Documentation","text":"<p>This module defines the base class for filesystem storage configuration options.</p>"},{"location":"api/fsspeckit.storage_options.base/#basestorageoptions","title":"<code>BaseStorageOptions</code>","text":"<p>Base class for filesystem storage configuration options.</p> <p>Provides common functionality for all storage option classes including:</p> <ul> <li>YAML serialization/deserialization</li> <li>Dictionary conversion</li> <li>Filesystem instance creation</li> <li>Configuration updates</li> </ul> <p>Attributes:</p> <ul> <li><code>protocol</code> (<code>str</code>): Storage protocol identifier (e.g., \"s3\", \"gs\", \"file\")</li> </ul> <p>Example:</p> <pre><code>from fsspeckit.storage_options.base import BaseStorageOptions\n\n# Create and save options\noptions = BaseStorageOptions(protocol=\"s3\")\noptions.to_yaml(\"config.yml\")\n\n# Load from YAML\nloaded = BaseStorageOptions.from_yaml(\"config.yml\")\nprint(loaded.protocol)\n# 's3'\n</code></pre>"},{"location":"api/fsspeckit.storage_options.base/#to_dict","title":"<code>to_dict()</code>","text":"<p>Convert storage options to dictionary.</p> Parameter Type Description <code>with_protocol</code> <code>bool</code> Whether to include protocol in output dictionary Returns Type Description <code>dict</code> <code>dict</code> Dictionary of storage options with non-None values <p>Example:</p> <pre><code>from fsspeckit.storage_options.base import BaseStorageOptions\n\noptions = BaseStorageOptions(protocol=\"s3\")\nprint(options.to_dict())\n# {}\nprint(options.to_dict(with_protocol=True))\n# {'protocol': 's3'}\n</code></pre>"},{"location":"api/fsspeckit.storage_options.base/#from_yaml","title":"<code>from_yaml()</code>","text":"<p>Load storage options from YAML file.</p> Parameter Type Description <code>path</code> <code>str</code> Path to YAML configuration file <code>fs</code> <code>AbstractFileSystem</code> Filesystem to use for reading file Returns Type Description <code>BaseStorageOptions</code> <code>BaseStorageOptions</code> Loaded storage options instance <p>Example:</p> <pre><code># Load from local file\nfrom fsspeckit.storage_options.base import BaseStorageOptions\nfrom fsspec.implementations.local import LocalFileSystem\n\n# Assuming 'config.yml' exists and contains valid YAML for BaseStorageOptions\n# For example, a file named config.yml with content:\n# protocol: s3\n#\n# To make this example runnable, we'll create a dummy config.yml\nfs_local = LocalFileSystem()\nfs_local.write_text(\"config.yml\", \"protocol: s3\")\n\noptions = BaseStorageOptions.from_yaml(\"config.yml\")\nprint(options.protocol)\n# 's3'\n\n# Clean up the dummy file\nfs_local.rm(\"config.yml\")\n</code></pre>"},{"location":"api/fsspeckit.storage_options.base/#to_yaml","title":"<code>to_yaml()</code>","text":"<p>Save storage options to YAML file.</p> Parameter Type Description <code>path</code> <code>str</code> Path where to save configuration <code>fs</code> <code>AbstractFileSystem</code> Filesystem to use for writing <p>Example:</p> <pre><code>from fsspeckit.storage_options.base import BaseStorageOptions\nfrom fsspec.implementations.local import LocalFileSystem\n\noptions = BaseStorageOptions(protocol=\"s3\")\nfs_local = LocalFileSystem()\noptions.to_yaml(\"config.yml\", fs=fs_local) # Specify filesystem for writing\n</code></pre>"},{"location":"api/fsspeckit.storage_options.base/#to_filesystem","title":"<code>to_filesystem()</code>","text":"<p>Create fsspec filesystem instance from options.</p> Returns Type Description <code>AbstractFileSystem</code> <code>AbstractFileSystem</code> Configured filesystem instance <p>Example:</p> <pre><code>from fsspeckit.storage_options.base import BaseStorageOptions\n\noptions = BaseStorageOptions(protocol=\"file\")\nfs = options.to_filesystem()\n# Example usage: list files in a dummy directory\nimport os\nimport tempfile\n\nwith tempfile.TemporaryDirectory() as tmpdir:\n    dummy_file_path = os.path.join(tmpdir, \"test.txt\")\n    with open(dummy_file_path, \"w\") as f:\n        f.write(\"dummy content\")\n    fs_temp = options.to_filesystem()\n    files = fs_temp.ls(tmpdir)\n    print(files)\n</code></pre>"},{"location":"api/fsspeckit.storage_options.base/#update","title":"<code>update()</code>","text":"<p>Update storage options with new values.</p> Parameter Type Description <code>**kwargs</code> <code>Any</code> New option values to set Returns Type Description <code>BaseStorageOptions</code> <code>BaseStorageOptions</code> Updated instance <p>Example:</p> <pre><code>from fsspeckit.storage_options.base import BaseStorageOptions\n\noptions = BaseStorageOptions(protocol=\"s3\")\noptions = options.update(region=\"us-east-1\")\nprint(options.region)\n# 'us-east-1'\n</code></pre>"},{"location":"api/fsspeckit.storage_options.cloud/","title":"<code>fsspeckit.storage_options.cloud</code> API Documentation","text":"<p>This module defines storage option classes for various cloud providers, including Azure, Google Cloud Storage (GCS), and Amazon Web Services (AWS) S3. These classes provide structured ways to configure access to cloud storage, supporting different authentication methods and specific cloud service parameters.</p>"},{"location":"api/fsspeckit.storage_options.cloud/#azurestorageoptions","title":"<code>AzureStorageOptions</code>","text":"<p>Azure Storage configuration options.</p> <p>Provides configuration for Azure storage services:</p> <ul> <li>Azure Blob Storage (<code>az://</code>)</li> <li>Azure Data Lake Storage Gen2 (<code>abfs://</code>)</li> <li>Azure Data Lake Storage Gen1 (<code>adl://</code>)</li> </ul> <p>Supports multiple authentication methods:</p> <ul> <li>Connection string</li> <li>Account key</li> <li>Service principal</li> <li>Managed identity</li> <li>SAS token</li> </ul> <p>Attributes:</p> <ul> <li><code>protocol</code> (<code>str</code>): Storage protocol (\"az\", \"abfs\", or \"adl\")</li> <li><code>account_name</code> (<code>str</code>): Storage account name</li> <li><code>account_key</code> (<code>str</code>): Storage account access key</li> <li><code>connection_string</code> (<code>str</code>): Full connection string</li> <li><code>tenant_id</code> (<code>str</code>): Azure AD tenant ID</li> <li><code>client_id</code> (<code>str</code>): Service principal client ID</li> <li><code>client_secret</code> (<code>str</code>): Service principal client secret</li> <li><code>sas_token</code> (<code>str</code>): SAS token for limited access</li> </ul> <p>Example:</p> <pre><code>from fsspeckit.storage_options.cloud import AzureStorageOptions\n\n# Blob Storage with account key\noptions = AzureStorageOptions(\n    protocol=\"az\",\n    account_name=\"mystorageacct\",\n    account_key=\"key123...\"\n)\n\n# Data Lake with service principal\noptions = AzureStorageOptions(\n    protocol=\"abfs\",\n    account_name=\"mydatalake\",\n    tenant_id=\"tenant123\",\n    client_id=\"client123\",\n    client_secret=\"secret123\"\n)\n\n# Simple connection string auth\noptions = AzureStorageOptions(\n    protocol=\"az\",\n    connection_string=\"DefaultEndpoints...\"\n)\n</code></pre>"},{"location":"api/fsspeckit.storage_options.cloud/#from_env","title":"<code>from_env()</code>","text":"<p>Create storage options from environment variables.</p> <p>Reads standard Azure environment variables:</p> <ul> <li><code>AZURE_STORAGE_PROTOCOL</code></li> <li><code>AZURE_STORAGE_ACCOUNT_NAME</code></li> <li><code>AZURE_STORAGE_ACCOUNT_KEY</code></li> <li><code>AZURE_STORAGE_CONNECTION_STRING</code></li> <li><code>AZURE_TENANT_ID</code></li> <li><code>AZURE_CLIENT_ID</code></li> <li><code>AZURE_CLIENT_SECRET</code></li> <li><code>AZURE_STORAGE_SAS_TOKEN</code></li> </ul> Returns Type Description <code>AzureStorageOptions</code> <code>AzureStorageOptions</code> Configured storage options <p>Example:</p> <pre><code># With environment variables set:\nfrom fsspeckit.storage_options.cloud import AzureStorageOptions\nimport os\n\n# Set environment variables for testing (replace with actual values if needed)\nos.environ[\"AZURE_STORAGE_ACCOUNT_NAME\"] = \"mystorageacct\"\nos.environ[\"AZURE_STORAGE_ACCOUNT_KEY\"] = \"dummy_key\" # Dummy key for example\n\noptions = AzureStorageOptions.from_env()\nprint(options.account_name)  # From AZURE_STORAGE_ACCOUNT_NAME\n# 'mystorageacct'\n\n# Clean up environment variables\ndel os.environ[\"AZURE_STORAGE_ACCOUNT_NAME\"]\ndel os.environ[\"AZURE_STORAGE_ACCOUNT_KEY\"]\n</code></pre>"},{"location":"api/fsspeckit.storage_options.cloud/#to_env","title":"<code>to_env()</code>","text":"<p>Export options to environment variables.</p> <p>Sets standard Azure environment variables.</p> <p>Example:</p> <pre><code>from fsspeckit.storage_options.cloud import AzureStorageOptions\nimport os\n\noptions = AzureStorageOptions(\n    protocol=\"az\",\n    account_name=\"mystorageacct\",\n    account_key=\"key123\"\n)\noptions.to_env()\nprint(os.getenv(\"AZURE_STORAGE_ACCOUNT_NAME\"))\n# 'mystorageacct'\n\n# Clean up environment variables\ndel os.environ[\"AZURE_STORAGE_ACCOUNT_NAME\"]\ndel os.environ[\"AZURE_STORAGE_ACCOUNT_KEY\"]\n</code></pre>"},{"location":"api/fsspeckit.storage_options.cloud/#gcsstorageoptions","title":"<code>GcsStorageOptions</code>","text":"<p>Google Cloud Storage configuration options.</p> <p>Provides configuration for GCS access with support for:</p> <ul> <li>Service account authentication</li> <li>Default application credentials</li> <li>Token-based authentication</li> <li>Project configuration</li> <li>Custom endpoints</li> </ul> <p>Attributes:</p> <ul> <li><code>protocol</code> (<code>str</code>): Storage protocol (\"gs\" or \"gcs\")</li> <li><code>token</code> (<code>str</code>): Path to service account JSON file</li> <li><code>project</code> (<code>str</code>): Google Cloud project ID</li> <li><code>access_token</code> (<code>str</code>): OAuth2 access token</li> <li><code>endpoint_url</code> (<code>str</code>): Custom storage endpoint</li> <li><code>timeout</code> (<code>int</code>): Request timeout in seconds</li> </ul> <p>Example:</p> <pre><code>from fsspeckit.storage_options.cloud import GcsStorageOptions\n\n# Service account auth\noptions = GcsStorageOptions(\n    protocol=\"gs\",\n    token=\"path/to/service-account.json\",\n    project=\"my-project-123\"\n)\n\n# Application default credentials\noptions = GcsStorageOptions(\n    protocol=\"gcs\",\n    project=\"my-project-123\"\n)\n\n# Custom endpoint (e.g., test server)\noptions = GcsStorageOptions(\n    protocol=\"gs\",\n    endpoint_url=\"http://localhost:4443\",\n    token=\"test-token.json\"\n)\n</code></pre>"},{"location":"api/fsspeckit.storage_options.cloud/#from_env_1","title":"<code>from_env()</code>","text":"<p>Create storage options from environment variables.</p> <p>Reads standard GCP environment variables:</p> <ul> <li><code>GOOGLE_CLOUD_PROJECT</code>: Project</li> <li><code>GOOGLE_APPLICATION_CREDENTIALS</code>: Service account file path</li> <li><code>STORAGE_EMULATOR_HOST</code>: Custom endpoint (for testing)</li> <li><code>GCS_OAUTH_TOKEN</code>: OAuth2 access token</li> </ul> Returns Type Description <code>GcsStorageOptions</code> <code>GcsStorageOptions</code> Configured storage options <p>Example:</p> <pre><code># With environment variables set:\nfrom fsspeckit.storage_options.cloud import GcsStorageOptions\nimport os\n\n# Set environment variables for testing (replace with actual values if needed)\nos.environ[\"GOOGLE_CLOUD_PROJECT\"] = \"my-project-123\"\n\noptions = GcsStorageOptions.from_env()\nprint(options.project)  # From GOOGLE_CLOUD_PROJECT\n# 'my-project-123'\n\n# Clean up environment variables\ndel os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n</code></pre>"},{"location":"api/fsspeckit.storage_options.cloud/#to_env_1","title":"<code>to_env()</code>","text":"<p>Export options to environment variables.</p> <p>Sets standard GCP environment variables.</p> <p>Example:</p> <pre><code>from fsspeckit.storage_options.cloud import GcsStorageOptions\nimport os\n\noptions = GcsStorageOptions(\n    protocol=\"gs\",\n    project=\"my-project\",\n    token=\"service-account.json\"\n)\noptions.to_env()\nprint(os.getenv(\"GOOGLE_CLOUD_PROJECT\"))\n# 'my-project'\n\n# Clean up environment variables\ndel os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n</code></pre>"},{"location":"api/fsspeckit.storage_options.cloud/#to_fsspec_kwargs","title":"<code>to_fsspec_kwargs()</code>","text":"<p>Convert options to fsspec filesystem arguments.</p> Returns Type Description <code>dict</code> <code>dict</code> Arguments suitable for GCSFileSystem <p>Example:</p> <pre><code>from fsspeckit.storage_options.cloud import GcsStorageOptions\nfrom fsspeckit.core.base import filesystem\n\noptions = GcsStorageOptions(\n    protocol=\"gs\",\n    token=\"service-account.json\",\n    project=\"my-project\"\n)\nkwargs = options.to_fsspec_kwargs()\nfs = filesystem(\"gcs\", **kwargs)\n</code></pre>"},{"location":"api/fsspeckit.storage_options.cloud/#awsstorageoptions","title":"<code>AwsStorageOptions</code>","text":"<p>AWS S3 storage configuration options.</p> <p>Provides comprehensive configuration for S3 access with support for:</p> <ul> <li>Multiple authentication methods (keys, profiles, environment)</li> <li>Custom endpoints for S3-compatible services</li> <li>Region configuration</li> <li>SSL/TLS settings</li> <li>Anonymous access for public buckets</li> </ul> <p>Attributes:</p> <ul> <li><code>protocol</code> (<code>str</code>): Always \"s3\" for S3 storage</li> <li><code>access_key_id</code> (<code>str</code>): AWS access key ID</li> <li><code>secret_access_key</code> (<code>str</code>): AWS secret access key</li> <li><code>session_token</code> (<code>str</code>): AWS session token</li> <li><code>endpoint_url</code> (<code>str</code>): Custom S3 endpoint URL</li> <li><code>region</code> (<code>str</code>): AWS region name</li> <li><code>allow_invalid_certificates</code> (<code>bool</code>): Skip SSL certificate validation</li> <li><code>allow_http</code> (<code>bool</code>): Allow unencrypted HTTP connections</li> <li><code>anonymous</code> (<code>bool</code>): Use anonymous (unsigned) S3 access</li> </ul> <p>Example:</p> <pre><code># Basic credentials\noptions = AwsStorageOptions(\n    access_key_id=\"AKIAXXXXXXXX\",\n    secret_access_key=\"SECRETKEY\",\n    region=\"us-east-1\"\n)\n\n# Profile-based auth\noptions = AwsStorageOptions.create(profile=\"dev\")\n\n# S3-compatible service (MinIO)\noptions = AwsStorageOptions(\n    endpoint_url=\"http://localhost:9000\",\n    access_key_id=\"minioadmin\",\n    secret_access_key=\"minioadmin\",\n    allow_http=True\n)\n\n# Anonymous access for public buckets\noptions = AwsStorageOptions(anonymous=True)\n</code></pre>"},{"location":"api/fsspeckit.storage_options.cloud/#create","title":"<code>create()</code>","text":"<p>Creates an <code>AwsStorageOptions</code> instance, handling aliases and profile loading.</p> Parameter Type Description <code>protocol</code> <code>str</code> Storage protocol, defaults to \"s3\". <code>access_key_id</code> <code>str | None</code> AWS access key ID. <code>secret_access_key</code> <code>str | None</code> AWS secret access key. <code>session_token</code> <code>str | None</code> AWS session token. <code>endpoint_url</code> <code>str | None</code> Custom S3 endpoint URL. <code>region</code> <code>str | None</code> AWS region name. <code>allow_invalid_certificates</code> <code>bool | None</code> Skip SSL certificate validation. <code>allow_http</code> <code>bool | None</code> Allow unencrypted HTTP connections. <code>anonymous</code> <code>bool | None</code> Use anonymous (unsigned) S3 access. <code>key</code> <code>str | None</code> Alias for <code>access_key_id</code>. <code>secret</code> <code>str | None</code> Alias for <code>secret_access_key</code>. <code>token</code> <code>str | None</code> Alias for <code>session_token</code>. <code>profile</code> <code>str | None</code> AWS credentials profile name to load credentials from. Returns Type Description <code>AwsStorageOptions</code> <code>AwsStorageOptions</code> An initialized <code>AwsStorageOptions</code> instance."},{"location":"api/fsspeckit.storage_options.cloud/#from_aws_credentials","title":"<code>from_aws_credentials()</code>","text":"<p>Create storage options from AWS credentials file.</p> <p>Loads credentials from <code>~/.aws/credentials</code> and <code>~/.aws/config</code> files.</p> Parameter Type Description <code>profile</code> <code>str</code> AWS credentials profile name <code>allow_invalid_certificates</code> <code>bool</code> Skip SSL certificate validation <code>allow_http</code> <code>bool</code> Allow unencrypted HTTP connections <code>anonymous</code> <code>bool</code> Use anonymous (unsigned) S3 access Returns Type Description <code>AwsStorageOptions</code> <code>AwsStorageOptions</code> Configured storage options Raises Type Description <code>ValueError</code> <code>ValueError</code> If profile not found <code>FileNotFoundError</code> <code>FileNotFoundError</code> If credentials files missing <p>Example:</p> <pre><code># Load developer profile\noptions = AwsStorageOptions.from_aws_credentials(\n    profile=\"dev\",\n    allow_http=True  # For local testing\n)\n</code></pre>"},{"location":"api/fsspeckit.storage_options.cloud/#from_env_2","title":"<code>from_env()</code>","text":"<p>Create storage options from environment variables.</p> <p>Reads standard AWS environment variables:</p> <ul> <li><code>AWS_ACCESS_KEY_ID</code></li> <li><code>AWS_SECRET_ACCESS_KEY</code></li> <li><code>AWS_SESSION_TOKEN</code></li> <li><code>AWS_ENDPOINT_URL</code></li> <li><code>AWS_DEFAULT_REGION</code></li> <li><code>ALLOW_INVALID_CERTIFICATE</code></li> <li><code>AWS_ALLOW_HTTP</code></li> <li><code>AWS_S3_ANONYMOUS</code></li> </ul> Returns Type Description <code>AwsStorageOptions</code> <code>AwsStorageOptions</code> Configured storage options <p>Example:</p> <pre><code># Load from environment\nfrom fsspeckit.storage_options.cloud import AwsStorageOptions\nimport os\n\n# Set environment variables for testing (replace with actual values if needed)\nos.environ[\"AWS_DEFAULT_REGION\"] = \"us-east-1\"\n\noptions = AwsStorageOptions.from_env()\nprint(options.region)\n# 'us-east-1'  # From AWS_DEFAULT_REGION\n\n# Clean up environment variables\ndel os.environ[\"AWS_DEFAULT_REGION\"]\n</code></pre>"},{"location":"api/fsspeckit.storage_options.cloud/#to_fsspec_kwargs_1","title":"<code>to_fsspec_kwargs()</code>","text":"<p>Convert options to fsspec filesystem arguments.</p> Returns Type Description <code>dict</code> <code>dict</code> Arguments suitable for fsspec S3FileSystem <p>Example:</p> <pre><code>options = AwsStorageOptions(\n    access_key_id=\"KEY\",\n    secret_access_key=\"SECRET\",\n    region=\"us-west-2\"\n)\nkwargs = options.to_fsspec_kwargs()\nfs = filesystem(\"s3\", **kwargs)\n</code></pre>"},{"location":"api/fsspeckit.storage_options.cloud/#to_object_store_kwargs","title":"<code>to_object_store_kwargs()</code>","text":"<p>Convert options to object store arguments.</p> Parameter Type Description <code>with_conditional_put</code> <code>bool</code> Add etag-based conditional put support Returns Type Description <code>dict</code> <code>dict</code> Arguments suitable for object store clients <p>Example:</p> <pre><code>from fsspeckit.storage_options.cloud import AwsStorageOptions\n# Assuming ObjectStore is a hypothetical client for demonstration\n# from some_object_store_library import ObjectStore\n\noptions = AwsStorageOptions(\n    access_key_id=\"KEY\",\n    secret_access_key=\"SECRET\"\n)\nkwargs = options.to_object_store_kwargs()\n# client = ObjectStore(**kwargs)\n</code></pre>"},{"location":"api/fsspeckit.storage_options.cloud/#to_env_2","title":"<code>to_env()</code>","text":"<p>Export options to environment variables.</p> <p>Sets standard AWS environment variables.</p> <p>Example:</p> <pre><code>from fsspeckit.storage_options.cloud import AwsStorageOptions\nimport os\n\noptions = AwsStorageOptions(\n    access_key_id=\"KEY\",\n    secret_access_key=\"SECRET\",\n    region=\"us-east-1\"\n)\noptions.to_env()\nprint(os.getenv(\"AWS_ACCESS_KEY_ID\"))\n# 'KEY'\n\n# Clean up environment variables\ndel os.environ[\"AWS_ACCESS_KEY_ID\"]\ndel os.environ[\"AWS_SECRET_ACCESS_KEY\"]\nif \"AWS_DEFAULT_REGION\" in os.environ: # Only delete if it was set\n    del os.environ[\"AWS_DEFAULT_REGION\"]\n</code></pre>"},{"location":"api/fsspeckit.storage_options.cloud/#to_filesystem","title":"<code>to_filesystem()</code>","text":"<p>Create fsspec filesystem instance from options.</p> Returns Type Description <code>AbstractFileSystem</code> <code>AbstractFileSystem</code> Configured filesystem instance"},{"location":"api/fsspeckit.storage_options.core/","title":"<code>fsspeckit.storage_options.core</code> API Reference","text":""},{"location":"api/fsspeckit.storage_options.core/#localstorageoptions","title":"<code>LocalStorageOptions</code>","text":"<p>Local filesystem configuration options.</p> <p>Provides basic configuration for local file access. While this class is simple, it maintains consistency with other storage options and enables transparent switching between local and remote storage.</p> <p>Attributes:</p> <ul> <li><code>protocol</code> (<code>str</code>): Always \"file\" for local filesystem</li> <li><code>auto_mkdir</code> (<code>bool</code>): Create directories automatically</li> <li><code>mode</code> (<code>int</code>): Default file creation mode (unix-style)</li> </ul> <p>Example: <pre><code># Basic local access\noptions = LocalStorageOptions()\nfs = options.to_filesystem()\nfiles = fs.ls(\"/path/to/data\")\n\n# With auto directory creation\noptions = LocalStorageOptions(auto_mkdir=True)\nfs = options.to_filesystem()\nwith fs.open(\"/new/path/file.txt\", \"w\") as f:\n    f.write(\"test\")  # Creates /new/path/ automatically\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.core/#to_fsspec_kwargs","title":"<code>to_fsspec_kwargs()</code>","text":"<p>Convert options to fsspec filesystem arguments.</p> <p>Returns:</p> <ul> <li><code>dict</code>: Arguments suitable for LocalFileSystem</li> </ul> <p>Example: <pre><code>options = LocalStorageOptions(auto_mkdir=True)\nkwargs = options.to_fsspec_kwargs()\nfs = filesystem(\"file\", **kwargs)\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.core/#from_dict","title":"<code>from_dict()</code>","text":"<p>Create appropriate storage options instance from dictionary.</p> <p>Factory function that creates the correct storage options class based on protocol.</p> <p>Parameters:</p> Name Type Description <code>protocol</code> <code>str</code> Storage protocol identifier (e.g., \"s3\", \"gs\", \"file\") <code>storage_options</code> <code>dict</code> Dictionary of configuration options <p>Returns:</p> <ul> <li><code>BaseStorageOptions</code>: Appropriate storage options instance</li> </ul> <p>Raises:</p> <ul> <li><code>ValueError</code>: If protocol is not supported</li> </ul> <p>Example: <pre><code># Create S3 options\noptions = from_dict(\"s3\", {\n    \"access_key_id\": \"KEY\",\n    \"secret_access_key\": \"SECRET\"\n})\nprint(type(options).__name__)\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.core/#from_env","title":"<code>from_env()</code>","text":"<p>Create storage options from environment variables.</p> <p>Factory function that creates and configures storage options from protocol-specific environment variables.</p> <p>Parameters:</p> Name Type Description <code>protocol</code> <code>str</code> Storage protocol identifier (e.g., \"s3\", \"github\") <p>Returns:</p> <ul> <li><code>BaseStorageOptions</code>: Configured storage options instance</li> </ul> <p>Raises:</p> <ul> <li><code>ValueError</code>: If protocol is not supported</li> </ul> <p>Example: <pre><code># With AWS credentials in environment\noptions = from_env(\"s3\")\nprint(options.access_key_id)\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.core/#infer_protocol_from_uri","title":"<code>infer_protocol_from_uri()</code>","text":"<p>Infer the storage protocol from a URI string.</p> <p>Analyzes the URI to determine the appropriate storage protocol based on the scheme or path format.</p> <p>Parameters:</p> Name Type Description <code>uri</code> <code>str</code> URI or path string to analyze. Examples: - \"s3://bucket/path\" - \"gs://bucket/path\" - \"github://org/repo\" - \"/local/path\" <p>Returns:</p> <ul> <li><code>str</code>: Inferred protocol identifier</li> </ul> <p>Example: <pre><code># S3 protocol\ninfer_protocol_from_uri(\"s3://my-bucket/data\")\n\n# Local file\ninfer_protocol_from_uri(\"/home/user/data\")\n\n# GitHub repository\ninfer_protocol_from_uri(\"github://microsoft/vscode\")\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.core/#storage_options_from_uri","title":"<code>storage_options_from_uri()</code>","text":"<p>Create storage options instance from a URI string.</p> <p>Infers the protocol and extracts relevant configuration from the URI to create appropriate storage options.</p> <p>Parameters:</p> Name Type Description <code>uri</code> <code>str</code> URI string containing protocol and optional configuration. Examples: - \"s3://bucket/path\" - \"gs://project/bucket/path\" - \"github://org/repo\" <p>Returns:</p> <ul> <li><code>BaseStorageOptions</code>: Configured storage options instance</li> </ul> <p>Example: <pre><code># S3 options\nopts = storage_options_from_uri(\"s3://my-bucket/data\")\nprint(opts.protocol)\n\n# GitHub options\nopts = storage_options_from_uri(\"github://microsoft/vscode\")\nprint(opts.org)\nprint(opts.repo)\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.core/#merge_storage_options","title":"<code>merge_storage_options()</code>","text":"<p>Merge multiple storage options into a single configuration.</p> <p>Combines options from multiple sources with control over precedence.</p> <p>Parameters:</p> Name Type Description <code>*options</code> <code>BaseStorageOptions</code> or <code>dict</code> Storage options to merge. Can be: - BaseStorageOptions instances - Dictionaries of options - None values (ignored) <code>overwrite</code> <code>bool</code> Whether later options override earlier ones <p>Returns:</p> <ul> <li><code>BaseStorageOptions</code>: Combined storage options</li> </ul> <p>Example: <pre><code># Merge with overwrite\nbase = AwsStorageOptions(\n    region=\"us-east-1\",\n    access_key_id=\"OLD_KEY\"\n)\noverride = {\"access_key_id\": \"NEW_KEY\"}\nmerged = merge_storage_options(base, override)\nprint(merged.access_key_id)\n\n# Preserve existing values\nmerged = merge_storage_options(\n    base,\n    override,\n    overwrite=False\n)\nprint(merged.access_key_id)\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.core/#storageoptions","title":"<code>StorageOptions</code>","text":"<p>High-level storage options container and factory.</p> <p>Provides a unified interface for creating and managing storage options for different protocols.</p> <p>Attributes:</p> <ul> <li><code>storage_options</code> (<code>BaseStorageOptions</code>): Underlying storage options instance</li> </ul> <p>Example: <pre><code># Create from protocol\noptions = StorageOptions.create(\n    protocol=\"s3\",\n    access_key_id=\"KEY\",\n    secret_access_key=\"SECRET\"\n)\n\n# Create from existing options\ns3_opts = AwsStorageOptions(access_key_id=\"KEY\")\noptions = StorageOptions(storage_options=s3_opts)\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.core/#create","title":"<code>create()</code>","text":"<p>Create storage options from arguments.</p> <p>Parameters:</p> Name Type Description <code>**data</code> <code>dict</code> Either: - protocol and configuration options - storage_options=pre-configured instance <p>Returns:</p> <ul> <li><code>StorageOptions</code>: Configured storage options instance</li> </ul> <p>Raises:</p> <ul> <li><code>ValueError</code>: If protocol missing or invalid</li> </ul> <p>Example: <pre><code># Direct protocol config\noptions = StorageOptions.create(\n    protocol=\"s3\",\n    region=\"us-east-1\"\n)\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.core/#from_yaml","title":"<code>from_yaml()</code>","text":"<p>Create storage options from YAML configuration.</p> <p>Parameters:</p> Name Type Description <code>path</code> <code>str</code> Path to YAML configuration file <code>fs</code> <code>fsspec.AbstractFileSystem</code>, optional Filesystem for reading configuration <p>Returns:</p> <ul> <li><code>StorageOptions</code>: Configured storage options</li> </ul> <p>Example: <pre><code># Load from config file\noptions = StorageOptions.from_yaml(\"storage.yml\")\nprint(options.storage_options.protocol)\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.core/#from_env_1","title":"<code>from_env()</code>","text":"<p>Create storage options from environment variables.</p> <p>Parameters:</p> Name Type Description <code>protocol</code> <code>str</code> Storage protocol to configure <p>Returns:</p> <ul> <li><code>StorageOptions</code>: Environment-configured options</li> </ul> <p>Example: <pre><code># Load AWS config from environment\noptions = StorageOptions.from_env(\"s3\")\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.core/#to_filesystem","title":"<code>to_filesystem()</code>","text":"<p>Create fsspec filesystem instance.</p> <p>Returns:</p> <ul> <li><code>AbstractFileSystem</code>: Configured filesystem instance</li> </ul> <p>Example: <pre><code>options = StorageOptions.create(protocol=\"file\")\nfs = options.to_filesystem()\nfiles = fs.ls(\"/data\")\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.core/#to_dict","title":"<code>to_dict()</code>","text":"<p>Convert storage options to dictionary.</p> <p>Parameters:</p> Name Type Description <code>with_protocol</code> <code>bool</code> Whether to include protocol in output <p>Returns:</p> <ul> <li><code>dict</code>: Storage options as dictionary</li> </ul> <p>Example: <pre><code>options = StorageOptions.create(\n    protocol=\"s3\",\n    region=\"us-east-1\"\n)\nprint(options.to_dict())\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.core/#to_object_store_kwargs","title":"<code>to_object_store_kwargs()</code>","text":"<p>Get options formatted for object store clients.</p> <p>Parameters:</p> Name Type Description <code>with_conditional_put</code> <code>bool</code> Add etag-based conditional put support <p>Returns:</p> <ul> <li><code>dict</code>: Object store configuration dictionary</li> </ul> <p>Example: <pre><code>options = StorageOptions.create(protocol=\"s3\")\nkwargs = options.to_object_store_kwargs()\n# store = ObjectStore(**kwargs)\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.core/#basestorageoptions","title":"<code>BaseStorageOptions</code>","text":"<p>Base class for filesystem storage configuration options.</p> <p>Provides common functionality for all storage option classes including: - YAML serialization/deserialization - Dictionary conversion - Filesystem instance creation - Configuration updates</p> <p>Attributes:</p> <ul> <li><code>protocol</code> (<code>str</code>): Storage protocol identifier (e.g., \"s3\", \"gs\", \"file\")</li> </ul> <p>Example: <pre><code># Create and save options\noptions = BaseStorageOptions(protocol=\"s3\")\noptions.to_yaml(\"config.yml\")\n\n# Load from YAML\nloaded = BaseStorageOptions.from_yaml(\"config.yml\")\nprint(loaded.protocol)\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.core/#to_dict_1","title":"<code>to_dict()</code>","text":"<p>Convert storage options to dictionary.</p> <p>Parameters:</p> Name Type Description <code>with_protocol</code> <code>bool</code> Whether to include protocol in output dictionary <p>Returns:</p> <ul> <li><code>dict</code>: Dictionary of storage options with non-None values</li> </ul> <p>Example: <pre><code>options = BaseStorageOptions(protocol=\"s3\")\nprint(options.to_dict())\nprint(options.to_dict(with_protocol=True))\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.core/#from_yaml_1","title":"<code>from_yaml()</code>","text":"<p>Load storage options from YAML file.</p> <p>Parameters:</p> Name Type Description <code>path</code> <code>str</code> Path to YAML configuration file <code>fs</code> <code>fsspec.AbstractFileSystem</code>, optional Filesystem to use for reading file <p>Returns:</p> <ul> <li><code>BaseStorageOptions</code>: Loaded storage options instance</li> </ul> <p>Example: <pre><code># Load from local file\noptions = BaseStorageOptions.from_yaml(\"config.yml\")\nprint(options.protocol)\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.core/#to_yaml","title":"<code>to_yaml()</code>","text":"<p>Save storage options to YAML file.</p> <p>Parameters:</p> Name Type Description <code>path</code> <code>str</code> Path where to save configuration <code>fs</code> <code>fsspec.AbstractFileSystem</code>, optional Filesystem to use for writing <p>Example: <pre><code>options = BaseStorageOptions(protocol=\"s3\")\noptions.to_yaml(\"config.yml\")\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.core/#to_filesystem_1","title":"<code>to_filesystem()</code>","text":"<p>Create fsspec filesystem instance from options.</p> <p>Returns:</p> <ul> <li><code>AbstractFileSystem</code>: Configured filesystem instance</li> </ul> <p>Example: <pre><code>options = BaseStorageOptions(protocol=\"file\")\nfs = options.to_filesystem()\nfiles = fs.ls(\"/path/to/data\")\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.core/#update","title":"<code>update()</code>","text":"<p>Update storage options with new values.</p> <p>Parameters:</p> Name Type Description <code>**kwargs</code> <code>dict</code> New option values to set <p>Returns:</p> <ul> <li><code>BaseStorageOptions</code>: Updated instance</li> </ul> <p>Example: <pre><code>options = BaseStorageOptions(protocol=\"s3\")\noptions = options.update(region=\"us-east-1\")\nprint(options.region)\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.git/","title":"<code>fsspeckit.storage_options.git</code> API Reference","text":""},{"location":"api/fsspeckit.storage_options.git/#githubstorageoptions","title":"<code>GitHubStorageOptions</code>","text":"<p>GitHub repository storage configuration options.</p> <p>Provides access to files in GitHub repositories with support for: - Public and private repositories - Branch/tag/commit selection - Token-based authentication - Custom GitHub Enterprise instances</p> <p>Attributes:</p> <ul> <li><code>protocol</code> (<code>str</code>): Always \"github\" for GitHub storage</li> <li><code>org</code> (<code>str</code>): Organization or user name</li> <li><code>repo</code> (<code>str</code>): Repository name</li> <li><code>ref</code> (<code>str</code>): Git reference (branch, tag, or commit SHA</li> <li><code>token</code> (<code>str</code>): GitHub personal access token</li> <li><code>api_url</code> (<code>str</code>): Custom GitHub API URL for enterprise instances</li> </ul> <p>Example: <pre><code># Public repository\noptions = GitHubStorageOptions(\n    org=\"microsoft\",\n    repo=\"vscode\",\n    ref=\"main\"\n)\n\n# Private repository\noptions = GitHubStorageOptions(\n    org=\"myorg\",\n    repo=\"private-repo\",\n    token=\"ghp_xxxx\",\n    ref=\"develop\"\n)\n\n# Enterprise instance\noptions = GitHubStorageOptions(\n    org=\"company\",\n    repo=\"internal\",\n    api_url=\"https://github.company.com/api/v3\",\n    token=\"ghp_xxxx\"\n)\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.git/#from_env","title":"<code>from_env()</code>","text":"<p>Create storage options from environment variables.</p> <p>Reads standard GitHub environment variables: - GITHUB_ORG: Organization or user name - GITHUB_REPO: Repository name - GITHUB_REF: Git reference - GITHUB_TOKEN: Personal access token - GITHUB_API_URL: Custom API URL</p> <p>Returns:</p> <ul> <li><code>GitHubStorageOptions</code>: Configured storage options</li> </ul> <p>Example: <pre><code># With environment variables set:\noptions = GitHubStorageOptions.from_env()\nprint(options.org)  # From GITHUB_ORG 'microsoft'\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.git/#to_env","title":"<code>to_env()</code>","text":"<p>Export options to environment variables.</p> <p>Sets standard GitHub environment variables.</p> <p>Example: <pre><code>options = GitHubStorageOptions(\n    org=\"microsoft\",\n    repo=\"vscode\",\n    token=\"ghp_xxxx\"\n)\noptions.to_env()\nprint(os.getenv(\"GITHUB_ORG\"))  # 'microsoft'\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.git/#to_fsspec_kwargs","title":"<code>to_fsspec_kwargs()</code>","text":"<p>Convert options to fsspec filesystem arguments.</p> <p>Returns:</p> <ul> <li><code>dict</code>: Arguments suitable for GitHubFileSystem</li> </ul> <p>Example: <pre><code>options = GitHubStorageOptions(\n    org=\"microsoft\",\n    repo=\"vscode\",\n    token=\"ghp_xxxx\"\n)\nkwargs = options.to_fsspec_kwargs()\nfs = filesystem(\"github\", **kwargs)\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.git/#gitlabstorageoptions","title":"<code>GitLabStorageOptions</code>","text":"<p>GitLab repository storage configuration options.</p> <p>Provides access to files in GitLab repositories with support for: - Public and private repositories - Self-hosted GitLab instances - Project ID or name-based access - Branch/tag/commit selection - Token-based authentication</p> <p>Attributes:</p> <ul> <li><code>protocol</code> (<code>str</code>): Always \"gitlab\" for GitLab storage</li> <li><code>base_url</code> (<code>str</code>): GitLab instance URL, defaults to gitlab.com</li> <li><code>project_id</code> (<code>str</code> | <code>int</code>): Project ID number</li> <li><code>project_name</code> (<code>str</code>): Project name/path</li> <li><code>ref</code> (<code>str</code>): Git reference (branch, tag, or commit SHA)</li> <li><code>token</code> (<code>str</code>): GitLab personal access token</li> <li><code>api_version</code> (<code>str</code>): API version to use</li> </ul> <p>Example: <pre><code># Public project on gitlab.com\noptions = GitLabStorageOptions(\n    project_name=\"group/project\",\n    ref=\"main\"\n)\n\n# Private project with token\noptions = GitLabStorageOptions(\n    project_id=12345,\n    token=\"glpat_xxxx\",\n    ref=\"develop\"\n)\n\n# Self-hosted instance\noptions = GitLabStorageOptions(\n    base_url=\"https://gitlab.company.com\",\n    project_name=\"internal/project\",\n    token=\"glpat_xxxx\"\n)\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.git/#from_env_1","title":"<code>from_env()</code>","text":"<p>Create storage options from environment variables.</p> <p>Reads standard GitLab environment variables: - GITLAB_URL: Instance URL - GITLAB_PROJECT_ID: Project ID - GITLAB_PROJECT_NAME: Project name/path - GITLAB_REF: Git reference - GITLAB_TOKEN: Personal access token - GITLAB_API_VERSION: API version</p> <p>Returns:</p> <ul> <li><code>GitLabStorageOptions</code>: Configured storage options</li> </ul> <p>Example: <pre><code># With environment variables set:\noptions = GitLabStorageOptions.from_env()\nprint(options.project_id)  # From GITLAB_PROJECT_ID '12345'\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.git/#to_env_1","title":"<code>to_env()</code>","text":"<p>Export options to environment variables.</p> <p>Sets standard GitLab environment variables.</p> <p>Example: <pre><code>options = GitLabStorageOptions(\n    project_id=12345,\n    token=\"glpat_xxxx\"\n)\noptions.to_env()\nprint(os.getenv(\"GITLAB_PROJECT_ID\"))  # '12345'\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.git/#to_fsspec_kwargs_1","title":"<code>to_fsspec_kwargs()</code>","text":"<p>Convert options to fsspec filesystem arguments.</p> <p>Returns:</p> <ul> <li><code>dict</code>: Arguments suitable for GitLabFileSystem</li> </ul> <p>Example: <pre><code>options = GitLabStorageOptions(\n    project_id=12345,\n    token=\"glpat_xxxx\"\n)\nkwargs = options.to_fsspec_kwargs()\nfs = filesystem(\"gitlab\", **kwargs)\n</code></pre></p>"},{"location":"api/fsspeckit.utils.datetime/","title":"<code>fsspeckit.utils.datetime</code> API Reference","text":""},{"location":"api/fsspeckit.utils.datetime/#get_timestamp_column","title":"<code>get_timestamp_column()</code>","text":"<p>Parameters:</p> Name Type Description <code>df</code> <code>polars.DataFrame</code> Input DataFrame. <p>Example:</p> <pre><code>import polars as pl\nfrom fsspeckit.utils.datetime import get_timestamp_column\n\ndf = pl.DataFrame({\n    \"timestamp_col\": [1678886400, 1678972800],\n    \"value\": [10, 20]\n})\ncol_name = get_timestamp_column(df)\nprint(col_name)\n# \"timestamp_col\"\n</code></pre> <p>Returns:</p> <ul> <li><code>None</code></li> </ul>"},{"location":"api/fsspeckit.utils.datetime/#get_timedelta_str","title":"<code>get_timedelta_str()</code>","text":"<p>Parameters:</p> Name Type Description <code>timedelta_string</code> <code>str</code> Timedelta string (e.g., \"1h\", \"2d\", \"3w\"). <p>Example:</p> <pre><code>from fsspeckit.utils.datetime import get_timedelta_str\n\n# Convert to Polars duration string\npolars_duration = get_timedelta_str(\"1h\")\nprint(polars_duration)\n# \"1h\"\n\n# Convert to Pandas timedelta string\npandas_timedelta = get_timedelta_str(\"2d\", to=\"pandas\")\nprint(pandas_timedelta)\n# \"2 days\"\n</code></pre> <p>| <code>to</code> | <code>str</code> | Defaults to 'polars' |</p> <p>Returns:</p> <ul> <li><code>None</code></li> </ul>"},{"location":"api/fsspeckit.utils.datetime/#timestamp_from_string","title":"<code>timestamp_from_string()</code>","text":"<p>Converts a timestamp string (ISO 8601 format) into a datetime, date, or time object</p> <p>using only standard Python libraries. Handles strings with or without timezone information (e.g., '2023-01-01T10:00:00+02:00', '2023-01-01', '10:00:00'). Supports timezone offsets like '+HH:MM' or '+HHMM'. For named timezones (e.g., 'Europe/Paris'), requires Python 3.9+ and the 'tzdata' package to be installed.</p> <p>Parameters:</p> Name Type Description <code>timestamp_str</code> <code>str</code> The string representation of the timestamp (ISO 8601 format). <code>tz</code> <code>str</code>, optional Target timezone identifier (e.g., 'UTC', '+02:00', 'Europe/Paris'). If provided, the output datetime/time will be localized or converted to this timezone. Defaults to None. <code>naive</code> <code>bool</code>, optional If True, return a naive datetime/time (no timezone info), even if the input string or <code>tz</code> parameter specifies one. Defaults to False. <p>Returns:</p> <ul> <li><code>Union[dt.datetime, dt.date, dt.time]</code>: The parsed datetime, date, or time object.</li> </ul> <p>Example:</p> <pre><code>from fsspeckit.utils.datetime import timestamp_from_string\n\n# Parse a timestamp string with timezone\ndt_obj = timestamp_from_string(\"2023-01-01T10:00:00+02:00\")\nprint(dt_obj)\n# 2023-01-01 10:00:00+02:00\n\n# Parse a date string\ndate_obj = timestamp_from_string(\"2023-01-01\")\nprint(date_obj)\n# 2023-01-01\n\n# Parse a time string and localize to UTC\ntime_obj = timestamp_from_string(\"15:30:00\", tz=\"UTC\")\nprint(time_obj)\n# 15:30:00+00:00\n\n# Parse a timestamp and return as naive datetime\nnaive_dt_obj = timestamp_from_string(\"2023-01-01T10:00:00+02:00\", naive=True)\nprint(naive_dt_obj)\n# 2023-01-01 10:00:00\n</code></pre> <p>Raises:</p> <ul> <li><code>ValueError</code>: If the timestamp string format is invalid or the timezone is invalid/unsupported.</li> </ul>"},{"location":"api/fsspeckit.utils.logging/","title":"<code>fsspeckit.utils.logging</code> API Reference","text":""},{"location":"api/fsspeckit.utils.logging/#setup_logging","title":"<code>setup_logging()</code>","text":"<p>Configure the Loguru logger for fsspec-utils.</p> <p>Removes the default handler and adds a new one targeting stderr with customizable level and format.</p> <p>Parameters:</p> Name Type Description <code>level</code> <code>str</code>, optional Log level (DEBUG, INFO, WARNING, ERROR, CRITICAL). If None, uses fsspeckit_LOG_LEVEL environment variable or defaults to \"INFO\". <code>disable</code> <code>bool</code> Whether to disable logging for fsspec-utils package. <code>format_string</code> <code>str</code>, optional Custom format string for log messages. If None, uses a default comprehensive format. <p>Returns:</p> <ul> <li><code>None</code></li> </ul> <p>Example: <pre><code># Basic setup\nsetup_logging()\n\n# Custom level and format\nsetup_logging(level=\"DEBUG\", format_string=\"{time} | {level} | {message}\")\n\n# Disable logging\nsetup_logging(disable=True)\n</code></pre></p>"},{"location":"api/fsspeckit.utils.logging/#get_logger","title":"<code>get_logger()</code>","text":"<p>Get a logger instance for the given name.</p> <p>Parameters:</p> Name Type Description <code>name</code> <code>str</code> Logger name, typically the module name. <p>Returns:</p> <ul> <li><code>Logger</code>: Configured logger instance.</li> </ul> <p>Example: <pre><code>logger = get_logger(__name__)\nlogger.info(\"This is a log message\")\n</code></pre></p>"},{"location":"api/fsspeckit.utils.misc/","title":"<code>fsspeckit.utils.misc</code> API Reference","text":""},{"location":"api/fsspeckit.utils.misc/#run_parallel","title":"<code>run_parallel()</code>","text":"<p>Run a function for a list of parameters in parallel.</p> <p>Provides parallel execution with progress tracking and flexible argument handling.</p> <p>Parameters:</p> Name Type Description <code>func</code> <code>Callable</code> The function to be executed in parallel. <code>*args</code> <code>Any</code> Positional arguments to pass to <code>func</code>. If an iterable, <code>func</code> will be called for each item. <code>n_jobs</code> <code>int</code> The number of CPU cores to use. -1 means all available cores. <code>backend</code> <code>str</code> The backend to use for parallel processing. Options include 'loky', 'threading', 'multiprocessing', and 'sequential'. <code>verbose</code> <code>bool</code> If True, a progress bar will be displayed during execution. <code>**kwargs</code> <code>Any</code> Keyword arguments to pass to <code>func</code>. If an iterable, <code>func</code> will be called for each item. <p>Returns:</p> <ul> <li><code>list</code>: List of function outputs in the same order as inputs.</li> </ul> <p>Raises:</p> <ul> <li><code>ValueError</code>: If no iterable arguments provided or length mismatch.</li> </ul> <p>Examples: <pre><code># Single iterable argument\nrun_parallel(str.upper, [\"hello\", \"world\"])\n\n# Multiple iterables in args and kwargs\ndef add(x, y, offset=0):\n    return x + y + offset\nrun_parallel(add, [1, 2, 3], y=[4, 5, 6], offset=10)\n\n# Fixed and iterable arguments\nrun_parallel(pow, [2, 3, 4], exp=2)\n</code></pre></p>"},{"location":"api/fsspeckit.utils.misc/#get_partitions_from_path","title":"<code>get_partitions_from_path()</code>","text":"<p>Extract dataset partitions from a file path.</p> <p>Parses file paths to extract partition information based on different partitioning schemes.</p> <p>Parameters:</p> Name Type Description <code>path</code> <code>str</code> The file path from which to extract partition information. <code>partitioning</code> <code>str</code> or <code>list[str]</code> or <code>None</code> The partitioning scheme to use. Can be \"hive\" for Hive-style, a string for a single partition column, a list of strings for multiple partition columns, or None for no specific partitioning. <p>Returns:</p> <ul> <li><code>list[tuple[str, str]]</code>: List of tuples containing (column, value) pairs.</li> </ul> <p>Examples: <pre><code># Hive-style partitioning\nget_partitions_from_path(\"data/year=2023/month=01/file.parquet\", \"hive\")\n\n# Single partition column\nget_partitions_from_path(\"data/2023/01/file.parquet\", \"year\")\n\n# Multiple partition columns\nget_partitions_from_path(\"data/2023/01/file.parquet\", [\"year\", \"month\"])\n</code></pre></p>"},{"location":"api/fsspeckit.utils.misc/#path_to_glob","title":"<code>path_to_glob()</code>","text":"<p>Convert a path to a glob pattern for file matching.</p> <p>Intelligently converts paths to glob patterns that match files of the specified format, handling various directory and wildcard patterns.</p> <p>Parameters:</p> Name Type Description <code>path</code> <code>str</code> The file or directory path to convert into a glob pattern. <code>format</code> <code>str</code> or <code>None</code> The desired file format or extension to match (e.g., \"parquet\", \"csv\", \"json\"). If None, the format is inferred from the path. <p>Returns:</p> <ul> <li><code>str</code>: Glob pattern for matching files</li> </ul> <p>Example: <pre><code># Directory to parquet files glob\npath_to_glob(\"data/\", \"parquet\")\n\n# Already a glob pattern\npath_to_glob(\"data/*.csv\")\n\n# Specific file\npath_to_glob(\"data/file.json\")\n</code></pre></p>"},{"location":"api/fsspeckit.utils.misc/#check_optional_dependency","title":"<code>check_optional_dependency()</code>","text":"<p>Check if an optional dependency is available.</p> <p>Parameters:</p> Name Type Description <code>package_name</code> <code>str</code> The name of the optional package to check for availability. <code>feature_name</code> <code>str</code> A descriptive name of the feature that requires this package. <p>Raises:</p> <ul> <li><code>ImportError</code>: If the package is not available</li> </ul>"},{"location":"api/fsspeckit.utils.polars/","title":"<code>fsspeckit.utils.polars</code> API Reference","text":""},{"location":"api/fsspeckit.utils.polars/#opt_dtype","title":"<code>opt_dtype()</code>","text":"<p>Optimize data types of a Polars DataFrame for performance and memory efficiency.</p> <p>This function analyzes each column and converts it to the most appropriate data type based on content, handling string-to-type conversions and numeric type downcasting.</p> <p>Parameters:</p> Name Type Description <code>df</code> <code>polars.DataFrame</code> The input Polars DataFrame to optimize. <code>include</code> <code>list[str]</code> or <code>None</code> Optional list of column names to include in the optimization process. If None, all columns are considered. <code>exclude</code> <code>list[str]</code> or <code>None</code> Optional list of column names to exclude from the optimization process. <code>time_zone</code> <code>str</code> or <code>None</code> Optional time zone string for datetime parsing. <code>shrink_numerics</code> <code>bool</code> If True, numeric columns will be downcasted to smaller data types if possible without losing precision. <code>allow_unsigned</code> <code>bool</code> If True, unsigned integer types will be considered for numeric column optimization. <code>allow_null</code> <code>bool</code> If True, columns containing only null values will be cast to the Null type. <code>sample_size</code> <code>int</code> or <code>None</code> Maximum number of cleaned values inspected during regex-based inference (<code>1024</code> by default). The inferred schema is based solely on this sample before casting the full column. <code>sample_method</code> <code>str</code> Which subset to inspect (<code>\"first\"</code> or <code>\"random\"</code>) when sampling values for inference. <code>strict</code> <code>bool</code> If True, an error will be raised if any column cannot be optimized (e.g., due to type inference issues). <p>Example:</p> <pre><code>import polars as pl\nfrom fsspeckit.utils.polars import opt_dtype\n\ndf = pl.DataFrame({\n    \"col_int\": [\"1\", \"2\", \"3\"],\n    \"col_float\": [\"1.1\", \"2.2\", \"3.3\"],\n    \"col_bool\": [\"True\", \"False\", \"True\"],\n    \"col_date\": [\"2023-01-01\", \"2023-01-02\", \"2023-01-03\"],\n    \"col_str\": [\"a\", \"b\", \"c\"],\n    \"col_null\": [None, None, None]\n})\noptimized_df = opt_dtype(df, shrink_numerics=True)\nprint(optimized_df.schema)\n# Expected output similar to:\n# Schema({\n#     'col_int': Int8,\n#     'col_float': Float32,\n#     'col_bool': Boolean,\n#     'col_date': Date,\n#     'col_str': Utf8,\n#     'col_null': Null\n# })\n</code></pre> <p>Returns:</p> <ul> <li><code>polars.DataFrame</code>: DataFrame with optimized data types</li> </ul>"},{"location":"api/fsspeckit.utils.polars/#unnest_all","title":"<code>unnest_all()</code>","text":"<p>Parameters:</p> Name Type Description <code>df</code> <code>polars.DataFrame</code> The input Polars DataFrame. <code>seperator</code> <code>str</code> The separator used to flatten nested column names. Defaults to '_'. <code>fields</code> <code>list[str]</code> or <code>None</code> Optional list of specific fields (structs) to unnest. If None, all struct columns will be unnested. <p>Example:</p> <pre><code>import polars as pl\nfrom fsspeckit.utils.polars import explode_all\n\ndf = pl.DataFrame({\n    \"id\": [1, 2],\n    \"values\": [[10, 20], [30]]\n})\nexploded_df = explode_all(df)\nprint(exploded_df)\n# shape: (3, 2)\n# \u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n# \u2502 id  \u2506 values \u2502\n# \u2502 --- \u2506 ---    \u2502\n# \u2502 i64 \u2506 i64    \u2502\n# \u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n# \u2502 1   \u2506 10     \u2502\n# \u2502 1   \u2506 20     \u2502\n# \u2502 2   \u2506 30     \u2502\n# \u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <pre><code>import polars as pl\nfrom fsspeckit.utils.polars import unnest_all\n\ndf = pl.DataFrame({\n    \"id\": [1, 2],\n    \"data\": [\n        {\"a\": 1, \"b\": {\"c\": 3}},\n        {\"a\": 4, \"b\": {\"c\": 6}}\n    ]\n})\nunnested_df = unnest_all(df, seperator='__')\nprint(unnested_df)\n# shape: (2, 3)\n# \u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n# \u2502 id  \u2506 data__a \u2506 data__b__c \u2502\n# \u2502 --- \u2506 ---  \u2506 ---     \u2502\n# \u2502 i64 \u2506 i64  \u2506 i64     \u2502\n# \u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n# \u2502 1   \u2506 1    \u2506 3       \u2502\n# \u2502 2   \u2506 4    \u2506 6       \u2502\n# \u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Returns:</p> <ul> <li><code>None</code></li> </ul>"},{"location":"api/fsspeckit.utils.polars/#explode_all","title":"<code>explode_all()</code>","text":"<p>Parameters:</p> Name Type Description <code>df</code> <code>polars.DataFrame</code> The input Polars DataFrame. <p>Example:</p> <pre><code>import polars as pl\nfrom fsspeckit.utils.polars import drop_null_columns\n\ndf = pl.DataFrame({\n    \"col1\": [1, 2, 3],\n    \"col2\": [None, None, None],\n    \"col3\": [\"a\", None, \"c\"]\n})\ndf_cleaned = drop_null_columns(df)\nprint(df_cleaned)\n# shape: (3, 2)\n# \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n# \u2502 col1 \u2506 col3  \u2502\n# \u2502 ---  \u2506 ---   \u2502\n# \u2502 i64  \u2506 str   \u2502\n# \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n# \u2502 1    \u2506 a     \u2502\n# \u2502 2    \u2506 null  \u2502\n# \u2502 3    \u2506 c     \u2502\n# \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Returns:</p> <ul> <li><code>None</code></li> </ul>"},{"location":"api/fsspeckit.utils.polars/#with_strftime_columns","title":"<code>with_strftime_columns()</code>","text":"<p>Parameters:</p> Name Type Description <code>df</code> <code>polars.DataFrame</code> The input Polars DataFrame. <code>strftime</code> <code>str</code> The <code>strftime</code> format string (e.g., \"%Y-%m-%d\" for date, \"%H\" for hour). <code>timestamp_column</code> <code>str</code> The name of the timestamp column to use. Defaults to 'auto' (attempts to infer). <code>column_names</code> <code>list[str]</code> or <code>None</code> Optional list of new column names to use for the generated columns. If None, names are derived from the <code>strftime</code> format. <p>Returns:</p> <ul> <li><code>None</code></li> </ul>"},{"location":"api/fsspeckit.utils.polars/#with_truncated_columns","title":"<code>with_truncated_columns()</code>","text":"<p>Parameters:</p> Name Type Description <code>df</code> <code>polars.DataFrame</code> The input Polars DataFrame. <code>truncate_by</code> <code>str</code> The duration string to truncate by (e.g., \"1h\", \"1d\", \"1mo\"). <code>timestamp_column</code> <code>str</code> The name of the timestamp column to truncate. Defaults to 'auto' (attempts to infer). <code>column_names</code> <code>list[str]</code> or <code>None</code> Optional list of new column names for the truncated columns. If None, names are derived automatically. <p>Returns:</p> <ul> <li><code>None</code></li> </ul>"},{"location":"api/fsspeckit.utils.polars/#with_datepart_columns","title":"<code>with_datepart_columns()</code>","text":"<p>Parameters:</p> Name Type Description <code>df</code> <code>polars.DataFrame</code> The input Polars DataFrame. <code>timestamp_column</code> <code>str</code> The name of the timestamp column to extract date parts from. Defaults to 'auto' (attempts to infer). <code>year</code> <code>bool</code> If True, extract the year as a new column. <code>month</code> <code>bool</code> If True, extract the month as a new column. <code>week</code> <code>bool</code> If True, extract the week of the year as a new column. <code>yearday</code> <code>bool</code> If True, extract the day of the year as a new column. <code>monthday</code> <code>bool</code> If True, extract the day of the month as a new column. <code>day</code> <code>bool</code> If True, extract the day of the week (1-7, Monday=1) as a new column. <code>weekday</code> <code>bool</code> If True, extract the weekday (0-6, Monday=0) as a new column. <code>hour</code> <code>bool</code> If True, extract the hour as a new column. <code>minute</code> <code>bool</code> If True, extract the minute as a new column. <code>strftime</code> <code>str</code> or <code>None</code> Optional <code>strftime</code> format string to apply to the timestamp column before extracting parts. <p>Returns:</p> <ul> <li><code>None</code></li> </ul>"},{"location":"api/fsspeckit.utils.polars/#with_row_count","title":"<code>with_row_count()</code>","text":"<p>Parameters:</p> Name Type Description <code>df</code> <code>polars.DataFrame</code> The input Polars DataFrame. <code>over</code> <code>list[str]</code> or <code>None</code> Optional list of column names to partition the data by before adding row counts. If None, a global row count is added. <p>Returns:</p> <ul> <li><code>None</code></li> </ul>"},{"location":"api/fsspeckit.utils.polars/#drop_null_columns","title":"<code>drop_null_columns()</code>","text":"<p>Remove columns with all null values from the DataFrame.</p> <p>Parameters:</p> Name Type Description <code>df</code> <code>polars.DataFrame</code> The input Polars DataFrame. <p>Returns:</p> <ul> <li><code>None</code></li> </ul>"},{"location":"api/fsspeckit.utils.polars/#unify_schemas","title":"<code>unify_schemas()</code>","text":"<p>Parameters:</p> Name Type Description <code>dfs</code> <code>list[polars.DataFrame]</code> A list of Polars DataFrames to unify their schemas. <p>Returns:</p> <ul> <li><code>None</code></li> </ul>"},{"location":"api/fsspeckit.utils.polars/#cast_relaxed","title":"<code>cast_relaxed()</code>","text":"<p>Parameters:</p> Name Type Description <code>df</code> <code>polars.DataFrame</code> The input Polars DataFrame to cast. <code>schema</code> <code>dict</code> or <code>polars.Schema</code> The target schema to cast the DataFrame to. Can be a dictionary mapping column names to data types or a Polars Schema object. <p>Returns:</p> <ul> <li><code>None</code></li> </ul>"},{"location":"api/fsspeckit.utils.polars/#delta","title":"<code>delta()</code>","text":"<p>Parameters:</p> Name Type Description <code>df1</code> <code>polars.DataFrame</code> The first Polars DataFrame. <code>df2</code> <code>polars.DataFrame</code> The second Polars DataFrame. <code>subset</code> <code>list[str]</code> or <code>None</code> Optional list of column names to consider when calculating the delta. If None, all columns are used. <code>eager</code> <code>bool</code> If True, the delta calculation is performed eagerly. Defaults to False (lazy). <p>Returns:</p> <ul> <li><code>None</code></li> </ul>"},{"location":"api/fsspeckit.utils.polars/#partition_by","title":"<code>partition_by()</code>","text":"<p>Parameters:</p> Name Type Description <code>df</code> <code>polars.DataFrame</code> The input Polars DataFrame to partition. <code>timestamp_column</code> <code>str</code> or <code>None</code> The name of the timestamp column to use for time-based partitioning. Defaults to None. <code>columns</code> <code>list[str]</code> or <code>None</code> Optional list of column names to partition by. Defaults to None. <code>strftime</code> <code>str</code> or <code>None</code> Optional <code>strftime</code> format string for time-based partitioning. Defaults to None. <code>timedelta</code> <code>str</code> or <code>None</code> Optional timedelta string (e.g., \"1h\", \"1d\") for time-based partitioning. Defaults to None. <code>num_rows</code> <code>int</code> or <code>None</code> Optional number of rows per partition for row-based partitioning. Defaults to None. <p>Returns:</p> <ul> <li><code>None</code></li> </ul>"},{"location":"api/fsspeckit.utils.pyarrow/","title":"<code>fsspeckit.utils.pyarrow</code> API Reference","text":""},{"location":"api/fsspeckit.utils.pyarrow/#dominant_timezone_per_column","title":"<code>dominant_timezone_per_column()</code>","text":"<p>For each timestamp column (by name) across all schemas, detect the most frequent timezone (including None).</p> <p>If None and a timezone are tied, prefer the timezone. Returns a dict: {column_name: dominant_timezone}</p> <p>Parameters:</p> Name Type Description <code>schemas</code> <code>list[pyarrow.Schema]</code> A list of PyArrow schemas to analyze. <p>Example:</p> <pre><code>import pyarrow as pa\nfrom fsspeckit.utils.pyarrow import dominant_timezone_per_column\n\nschema1 = pa.schema([(\"ts\", pa.timestamp(\"ns\", tz=\"UTC\"))])\nschema2 = pa.schema([(\"ts\", pa.timestamp(\"ns\", tz=\"Europe/Berlin\"))])\nschema3 = pa.schema([(\"ts\", pa.timestamp(\"ns\"))]) # naive\nschemas = [schema1, schema2, schema3]\n\ndominant_tz = dominant_timezone_per_column(schemas)\nprint(dominant_tz)\n# Expected: {'ts': 'UTC'} (or 'Europe/Berlin' depending on logic)\n</code></pre> <p>Returns:</p> <ul> <li><code>dict</code>: {column_name: dominant_timezone}</li> </ul>"},{"location":"api/fsspeckit.utils.pyarrow/#standardize_schema_timezones_by_majority","title":"<code>standardize_schema_timezones_by_majority()</code>","text":"<p>For each timestamp column (by name) across all schemas, set the timezone to the most frequent (with tie-breaking).</p> <p>Parameters:</p> Name Type Description <code>schemas</code> <code>list[pyarrow.Schema]</code> A list of PyArrow schemas to standardize. <p>Example:</p> <pre><code>import pyarrow as pa\nfrom fsspeckit.utils.pyarrow import standardize_schema_timezones_by_majority\n\nschema1 = pa.schema([(\"ts\", pa.timestamp(\"ns\", tz=\"UTC\"))])\nschema2 = pa.schema([(\"ts\", pa.timestamp(\"ns\", tz=\"Europe/Berlin\"))])\nschemas = [schema1, schema2]\n\nstandardized_schemas = standardize_schema_timezones_by_majority(schemas)\nprint(standardized_schemas[0].field(\"ts\").type)\nprint(standardized_schemas[1].field(\"ts\").type)\n# Expected: timestamp[ns, tz=Europe/Berlin] (or UTC, depending on tie-breaking)\n</code></pre> <p>Returns:</p> <ul> <li><code>list[pyarrow.Schema]</code>: A new list of schemas with updated timestamp timezones.</li> </ul>"},{"location":"api/fsspeckit.utils.pyarrow/#standardize_schema_timezones","title":"<code>standardize_schema_timezones()</code>","text":"<p>Standardize timezone info for all timestamp columns in a list of PyArrow schemas.</p> <p>Parameters:</p> Name Type Description <code>schemas</code> <code>list[pyarrow.Schema]</code> The list of PyArrow schemas to process. <code>timezone</code> <code>str</code> or <code>None</code> The target timezone to apply to timestamp columns. If None, timezones are removed. If \"auto\", the most frequent timezone across schemas is used. <p>Example:</p> <pre><code>import pyarrow as pa\nfrom fsspeckit.utils.pyarrow import standardize_schema_timezones\n\nschema1 = pa.schema([(\"ts\", pa.timestamp(\"ns\", tz=\"UTC\"))])\nschema2 = pa.schema([(\"ts\", pa.timestamp(\"ns\"))]) # naive\nschemas = [schema1, schema2]\n\n# Remove timezones\nnew_schemas_naive = standardize_schema_timezones(schemas, timezone=None)\nprint(new_schemas_naive[0].field(\"ts\").type)\n# Expected: timestamp[ns]\n\n# Set a specific timezone\nnew_schemas_berlin = standardize_schema_timezones(schemas, timezone=\"Europe/Berlin\")\nprint(new_schemas_berlin[0].field(\"ts\").type)\n# Expected: timestamp[ns, tz=Europe/Berlin]\n</code></pre> <p>Returns:</p> <ul> <li><code>list[pyarrow.Schema]</code>: New schemas with standardized timezone info.</li> </ul>"},{"location":"api/fsspeckit.utils.pyarrow/#unify_schemas","title":"<code>unify_schemas()</code>","text":"<p>Unify a list of PyArrow schemas into a single schema.</p> <p>Parameters:</p> Name Type Description <code>schemas</code> <code>list[pyarrow.Schema]</code> List of PyArrow schemas to unify. <code>use_large_dtypes</code> <code>bool</code> If True, keep large types like large_string. <code>timezone</code> <code>str</code> or <code>None</code> If specified, standardize all timestamp columns to this timezone. If \"auto\", use the most frequent timezone across schemas. If None, remove timezone from all timestamp columns. <code>standardize_timezones</code> <code>bool</code> If True, standardize all timestamp columns to the most frequent timezone. <p>Returns:</p> <ul> <li><code>pyarrow.Schema</code>: A unified PyArrow schema.</li> </ul>"},{"location":"api/fsspeckit.utils.pyarrow/#cast_schema","title":"<code>cast_schema()</code>","text":"<p>Cast a PyArrow table to a given schema, updating the schema to match the table's columns.</p> <p>Parameters:</p> Name Type Description <code>table</code> <code>pyarrow.Table</code> The PyArrow table to cast. <code>schema</code> <code>pyarrow.Schema</code> The target schema to cast the table to. <p>Returns:</p> <ul> <li><code>pyarrow.Table</code>: A new PyArrow table with the specified schema.</li> </ul>"},{"location":"api/fsspeckit.utils.pyarrow/#convert_large_types_to_normal","title":"<code>convert_large_types_to_normal()</code>","text":"<p>Convert large types in a PyArrow schema to their standard types.</p> <p>Parameters:</p> Name Type Description <code>schema</code> <code>pyarrow.Schema</code> The PyArrow schema to convert. <p>Returns:</p> <ul> <li><code>pyarrow.Schema</code>: A new PyArrow schema with large types converted to standard types.</li> </ul>"},{"location":"api/fsspeckit.utils.pyarrow/#merge_parquet_dataset_pyarrow","title":"<code>merge_parquet_dataset_pyarrow()</code>","text":"<p>Merge a source PyArrow table or parquet dataset into a target parquet dataset directory using PyArrow-only primitives.</p> <p>Parameters:</p> Name Type Description <code>source</code> <code>pyarrow.Table</code> or <code>str</code> In-memory table or path to a parquet dataset containing new data. <code>target_path</code> <code>str</code> Directory containing the parquet dataset to update. <code>key_columns</code> <code>list[str]</code> or <code>str</code> Column(s) that uniquely identify rows. <code>strategy</code> <code>Literal[\"upsert\", \"insert\", \"update\", \"full_merge\", \"deduplicate\"]</code> Merge strategy mirroring the DuckDB helper semantics. <code>dedup_order_by</code> <code>list[str]</code>, optional Columns to sort by (descending) before deduplicating the source when <code>strategy=\"deduplicate\"</code>. <code>compression</code> <code>str</code>, optional Compression codec for rewritten parquet files (<code>\"snappy\"</code> by default). <code>filesystem</code> <code>fsspec.AbstractFileSystem</code>, optional Filesystem implementation for both source (if path) and target datasets. <code>batch_rows</code> <code>int</code>, optional Number of source rows per batch when building filtered target scanners (default: <code>10_000</code>). <p>Example:</p> <pre><code>from fsspeckit.utils.pyarrow import merge_parquet_dataset_pyarrow\nimport pyarrow as pa\n\nsource = pa.table({\n    \"user_id\": [101, 102],\n    \"value\": [\"gold\", \"silver\"],\n})\n\nstats = merge_parquet_dataset_pyarrow(\n    source,\n    target_path=\"/data/customers/\",\n    key_columns=\"user_id\",\n    strategy=\"upsert\",\n)\n\nprint(stats)\n# {'inserted': 1, 'updated': 1, 'deleted': 0, 'total': 42}\n</code></pre> <p>Returns:</p> <ul> <li><code>dict[str, int]</code>: Merge statistics with keys <code>inserted</code>, <code>updated</code>, <code>deleted</code>, and final <code>total</code> rows.</li> </ul>"},{"location":"api/fsspeckit.utils.pyarrow/#opt_dtype","title":"<code>opt_dtype()</code>","text":"<p>Optimize data types of a PyArrow Table for performance and memory efficiency.</p> <p>Parameters:</p> Name Type Description <code>table</code> <code>pyarrow.Table</code> <code>include</code> <code>list[str]</code>, optional <code>exclude</code> <code>list[str]</code>, optional <code>time_zone</code> <code>str</code>, optional <code>shrink_numerics</code> <code>bool</code> <code>allow_unsigned</code> <code>bool</code> <code>use_large_dtypes</code> <code>bool</code> <code>strict</code> <code>bool</code> <code>allow_null</code> <code>bool</code> If False, columns that only hold null-like values will not be converted to pyarrow.null(). <code>sample_size</code> <code>int</code> or <code>None</code> Maximum number of cleaned values inspected during regex-based inference (<code>1024</code> by default). The inferred schema is derived solely from the samples before casting the complete column. <code>sample_method</code> <code>str</code> Sampling strategy (<code>\"first\"</code> or <code>\"random\"</code>) for the inference subset. <p>Returns:</p> <ul> <li><code>pyarrow.Table</code>: A new table casted to the optimal schema.</li> </ul>"},{"location":"api/fsspeckit.utils.sql/","title":"<code>fsspeckit.utils.sql</code> API Reference","text":""},{"location":"api/fsspeckit.utils.sql/#sql2pyarrow_filter","title":"<code>sql2pyarrow_filter()</code>","text":"<p>Generates a filter expression for PyArrow based on a given string and schema.</p> <p>Parameters:</p> Name Type Description <code>string</code> <code>str</code> The string containing the filter expression. <code>schema</code> <code>pyarrow.Schema</code> The PyArrow schema used to validate the filter expression. <p>Returns:</p> <ul> <li><code>pyarrow.compute.Expression</code>: The generated filter expression.</li> </ul> <p>Raises:</p> <ul> <li><code>ValueError</code>: If the input string is invalid or contains unsupported operations.</li> </ul>"},{"location":"api/fsspeckit.utils.sql/#sql2polars_filter","title":"<code>sql2polars_filter()</code>","text":"<p>Generates a filter expression for Polars based on a given string and schema.</p> <p>Parameters:</p> Name Type Description <p>| <code>string</code> | <code>str</code> | The string containing the filter expression. | | <code>schema</code> | <code>polars.Schema</code> | The Polars schema used to validate the filter expression. |</p> <p>Returns:</p> <ul> <li><code>polars.Expr</code>: The generated filter expression.</li> </ul> <p>Raises:</p> <ul> <li><code>ValueError</code>: If the input string is invalid or contains unsupported operations.</li> </ul>"},{"location":"api/fsspeckit.utils.sql/#get_table_names","title":"<code>get_table_names()</code>","text":"<p>Parameters:</p> Name Type Description <code>sql_query</code> <code>str</code> The SQL query string to parse. <p>Example:</p> <pre><code>from fsspeckit.utils.sql import get_table_names\n\nquery = \"SELECT a FROM my_table WHERE b &gt; 10\"\ntables = get_table_names(query)\nprint(tables)\n# Expected: ['my_table']\n\nquery_join = \"SELECT t1.a, t2.b FROM table1 AS t1 JOIN table2 AS t2 ON t1.id = t2.id\"\ntables_join = get_table_names(query_join)\nprint(tables_join)\n# Expected: ['table1', 'table2']\n</code></pre> <p>Returns:</p> <ul> <li><code>None</code></li> </ul>"},{"location":"api/fsspeckit.utils.types/","title":"<code>fsspeckit.utils.types</code> API Reference","text":""},{"location":"api/fsspeckit.utils.types/#dict_to_dataframe","title":"<code>dict_to_dataframe()</code>","text":"<p>Convert a dictionary or list of dictionaries to a Polars DataFrame.</p> <p>Handles various input formats: - Single dict with list values -&gt; DataFrame rows - Single dict with scalar values -&gt; Single row DataFrame - List of dicts with scalar values -&gt; Multi-row DataFrame - List of dicts with list values -&gt; DataFrame with list columns</p> <p>Parameters:</p> Name Type Description <code>data</code> <code>dict</code> or <code>list[dict]</code> The input data, either a dictionary or a list of dictionaries. <code>unique</code> <code>bool</code> If True, duplicate rows will be removed from the resulting DataFrame. <p>Returns:</p> <ul> <li><code>polars.DataFrame</code>: Polars DataFrame containing the converted data.</li> </ul> <p>Examples: <pre><code># Single dict with list values\ndata = {'a': [1, 2, 3], 'b': [4, 5, 6]}\ndict_to_dataframe(data)\n\n# Single dict with scalar values\ndata = {'a': 1, 'b': 2}\ndict_to_dataframe(data)\n\n# List of dicts with scalar values\ndata = [{'a': 1, 'b': 2}, {'a': 3, 'b': 4}]\ndict_to_dataframe(data)\n</code></pre></p>"},{"location":"api/fsspeckit.utils.types/#to_pyarrow_table","title":"<code>to_pyarrow_table()</code>","text":"<p>Convert various data formats to PyArrow Table.</p> <p>Handles conversion from Polars DataFrames, Pandas DataFrames, dictionaries, and lists of these types to PyArrow Tables.</p> <p>Parameters:</p> Name Type Description <code>data</code> <code>Any</code> Input data to convert. <code>concat</code> <code>bool</code> Whether to concatenate multiple inputs into single table. <code>unique</code> <code>bool</code> If True, duplicate rows will be removed from the resulting Table. <p>Example:</p> <pre><code>import polars as pl\nimport pyarrow as pa\nfrom fsspeckit.utils.types import to_pyarrow_table\n\n# Convert Polars DataFrame to PyArrow Table\ndf = pl.DataFrame({\"a\": [1, 2], \"b\": [3, 4]})\ntable = to_pyarrow_table(df)\nprint(table.schema)\n\n# Convert list of dicts to PyArrow Table\ndata = [{\"a\": 1, \"b\": 10}, {\"a\": 2, \"b\": 20}]\ntable_from_dict = to_pyarrow_table(data)\nprint(table_from_dict.to_pydf())\n</code></pre> <p>Returns:</p> <ul> <li><code>pyarrow.Table</code>: PyArrow Table containing the converted data.</li> </ul> <p>Example: <pre><code>df = pl.DataFrame({\"a\": [1, 2], \"b\": [3, 4]})\ntable = to_pyarrow_table(df)\nprint(table.schema)\n</code></pre></p>"}]}