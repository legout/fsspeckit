{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to fsspeckit!","text":"<p><code>fsspeckit</code> is a powerful library designed to enhance <code>fsspec</code> (Filesystem Spec) with advanced utilities and extensions, making multi-format I/O, cloud storage configuration, caching, monitoring, and batch processing more streamlined and efficient.</p>"},{"location":"#purpose","title":"Purpose","text":"<p>This library aims to simplify complex data operations across various file systems, providing a unified and extended interface for handling diverse data formats and storage solutions. Whether you're working with local files, cloud storage like AWS S3, Azure Blob Storage, or Google Cloud Storage, <code>fsspeckit</code> provides the tools to manage your data effectively.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Multi-format Data I/O: Seamlessly read and write data in various formats, including JSON, CSV, and Parquet.</li> <li>Cloud Storage Configuration: Simplified utilities for configuring and interacting with different cloud storage providers.</li> <li>Enhanced Caching and Monitoring: Improve performance and gain insights into your data operations with built-in caching mechanisms and monitoring capabilities.</li> <li>Batch Processing and Parallel Operations: Efficiently handle large datasets and execute operations in parallel for improved throughput.</li> <li>Directory-like Filesystem: Interact with nested data structures as if they were traditional directories, even on object stores.</li> <li>Domain-Specific Packages: Organized into logical packages (<code>core</code>, <code>datasets</code>, <code>sql</code>, <code>common</code>) for better discoverability and cleaner APIs.</li> </ul>"},{"location":"#get-started","title":"Get Started","text":"<p>Ready to dive in? Check out our Quickstart Guide to begin using <code>fsspeckit</code> in your projects.</p>"},{"location":"#badges","title":"Badges","text":""},{"location":"advanced/","title":"Advanced Usage","text":"<p><code>fsspeckit</code> extends the capabilities of <code>fsspec</code> to provide a more robust and feature-rich experience for handling diverse file systems and data formats. This section delves into advanced features, configurations, and performance tips to help you get the most out of the library.</p>"},{"location":"advanced/#domain-package-organization","title":"Domain Package Organization","text":"<p>The refactored <code>fsspeckit</code> architecture organizes functionality into domain-specific packages, making it easier to discover and use the right tools for each task. This design improves maintainability and provides clearer separation of concerns.</p>"},{"location":"advanced/#choosing-the-right-package","title":"Choosing the Right Package","text":"<p>For optimal experience, import from the appropriate domain package:</p> <ul> <li>Dataset Operations: Use <code>fsspeckit.datasets</code> for DuckDB and PyArrow dataset operations</li> <li>SQL Filtering: Use <code>fsspeckit.sql</code> for SQL-to-filter translation</li> <li>General Utilities: Use <code>fsspeckit.common</code> for logging, parallel processing, type conversion</li> <li>Backwards Compatibility: <code>fsspeckit.utils</code> continues to work for existing code</li> </ul>"},{"location":"advanced/#import-examples-by-use-case","title":"Import Examples by Use Case","text":"<pre><code># Dataset operations (recommended)\nfrom fsspeckit.datasets import DuckDBParquetHandler\nfrom fsspeckit.datasets.pyarrow import merge_parquet_dataset_pyarrow\n\n# SQL filtering\nfrom fsspeckit.sql.filters import sql2pyarrow_filter\n\n# Common utilities\nfrom fsspeckit.common.logging import setup_logging\nfrom fsspeckit.common.misc import run_parallel\n\n# Backwards compatible (legacy)\nfrom fsspeckit.utils import DuckDBParquetHandler  # Still works\n</code></pre>"},{"location":"advanced/#unified-filesystem-creation-with-filesystem","title":"Unified Filesystem Creation with <code>filesystem</code>","text":"<p>The <code>fsspeckit.core.filesystem</code> function offers a centralized and enhanced way to instantiate <code>fsspec</code> filesystem objects. It supports:</p> <ul> <li>Intelligent Caching: Automatically wraps filesystems with <code>MonitoredSimpleCacheFileSystem</code> for improved performance and verbose logging of cache operations.</li> <li>Structured Storage Options: Integrates seamlessly with <code>fsspeckit.storage_options</code> classes, allowing for type-safe and organized configuration of cloud and Git-based storage.</li> <li>Protocol Inference: Can infer the filesystem protocol directly from a URI or path, reducing boilerplate.</li> </ul> <p>Example: Cached S3 Filesystem with Structured Options</p> <pre><code>from fsspeckit import filesystem  # Top-level import for convenience\nfrom fsspeckit.storage_options import AwsStorageOptions\n\n# Configure S3 options using the structured class\ns3_opts = AwsStorageOptions(\n    region=\"us-east-1\",\n    access_key_id=\"YOUR_ACCESS_KEY\",\n    secret_access_key=\"YOUR_SECRET_KEY\"\n)\n\n# Create a cached S3 filesystem using the 'filesystem' helper\nfs = filesystem(\n    \"s3\",\n    storage_options=s3_opts,\n    cached=True,\n    cache_storage=\"/tmp/s3_cache\", # Optional: specify cache directory\n    verbose=True # Enable verbose cache logging\n)\n\n# Use the filesystem as usual\nprint(fs.ls(\"s3://your-bucket/\"))\n</code></pre>"},{"location":"advanced/#detailed-caching-for-improved-performance","title":"Detailed Caching for Improved Performance","text":"<p><code>fsspeckit</code> provides an enhanced caching mechanism that improves performance for repeated file operations, especially useful for remote filesystems.</p> <p>This example demonstrates how caching improves read performance. The first read populates the cache, while subsequent reads retrieve data directly from the cache, significantly reducing access time. It also shows that data can still be retrieved from the cache even if the original source becomes unavailable.</p> <p>Caching in fsspeckit is an enhanced mechanism that improves performance for repeated file operations, especially useful for remote filesystems where network latency can significantly impact performance.</p> <p>The <code>filesystem()</code> function provides several parameters for configuring caching:</p> <ul> <li><code>cached</code>: When set to <code>True</code>, enables caching for all read operations</li> <li><code>cache_storage</code>: Specifies the directory where cached files will be stored</li> <li><code>verbose</code>: When set to <code>True</code>, provides detailed logging about cache operations</li> </ul> <p>Step-by-step walkthrough:</p> <ol> <li> <p>First read (populating cache): When reading a file for the first time, the data is retrieved from the source (disk, network, etc.) and stored in the cache directory. This takes longer than subsequent reads because it involves both reading from the source and writing to the cache.</p> </li> <li> <p>Second read (using cache): When the same file is read again, the data is retrieved directly from the cache instead of the source. This is significantly faster because it avoids network latency or disk I/O.</p> </li> <li> <p>Demonstrating cache effectiveness: Even after the original file is removed, the cached version can still be accessed. This demonstrates that the cache acts as a persistent copy of the data, independent of the source file.</p> </li> <li> <p>Performance comparison: The timing results clearly show the performance benefits of caching, with subsequent reads being orders of magnitude faster than the initial read.</p> </li> </ol> <p>This caching mechanism is particularly valuable when working with:</p> <ul> <li>Remote filesystems (S3, GCS, Azure) where network latency is a bottleneck</li> <li>Frequently accessed files that don't change often</li> <li>Applications that read the same data multiple times</li> <li>Environments with unreliable network connections</li> </ul>"},{"location":"advanced/#setup-and-first-read-populating-cache","title":"Setup and First Read (Populating Cache)","text":"<p>In this step, we create a sample JSON file and initialize the <code>fsspeckit</code> filesystem with caching enabled. The first read operation retrieves data from the source and populates the cache.</p> <p>Setup steps:</p> <ol> <li>Create a temporary directory for our example</li> <li>Create sample data file</li> <li>Configure filesystem with caching</li> </ol> <pre><code>import tempfile\nimport time\nimport os\nimport shutil\nfrom fsspeckit import filesystem\nfrom examples.caching.setup_data import create_sample_data_file\n\ntmpdir = tempfile.mkdtemp()\nprint(f\"Created temporary directory: {tmpdir}\")\n\nsample_file = create_sample_data_file(tmpdir)\n\ncache_dir = os.path.join(tmpdir, \"cache\")\nfs = filesystem(\n    protocol_or_path=\"file\",\n    cached=True,\n    cache_storage=cache_dir,\n    verbose=True\n)\n\nprint(\"\\n=== First read (populating cache) ===\")\nstart_time = time.time()\ndata1 = fs.read_json(sample_file)\nfirst_read_time = time.time() - start_time\nprint(f\"First read completed in {first_read_time:.4f} seconds\")\n</code></pre>"},{"location":"advanced/#second-read-using-cache","title":"Second Read (Using Cache)","text":"<p>Now, let's read the same file again to see the performance improvement from using the cache.</p> <pre><code>print(\"\\n=== Second read (using cache) ===\")\nstart_time = time.time()\ndata2 = fs.read_json(sample_file)\nsecond_read_time = time.time() - start_time\nprint(f\"Second read completed in {second_read_time:.4f} seconds\")\n</code></pre> <p>The second read retrieves data directly from the cache, which is significantly faster than reading from the source again.</p>"},{"location":"advanced/#reading-from-cache-after-deletion","title":"Reading from Cache after Deletion","text":"<p>To demonstrate that the cache is persistent, we'll remove the original file and try to read it again.</p> <pre><code>print(\"\\n=== Demonstrating cache effectiveness ===\")\nprint(\"Removing original file...\")\nos.remove(sample_file)\nprint(f\"Original file exists: {os.path.exists(sample_file)}\")\n\nprint(\"\\n=== Third read (from cache only) ===\")\nstart_time = time.time()\ndata3 = fs.read_json(sample_file)\nthird_read_time = time.time() - start_time\nprint(f\"Third read completed in {third_read_time:.4f} seconds\")\nprint(\"\u2713 Successfully read from cache even after original file was removed\")\n\nprint(\"\\n=== Performance Comparison ===\")\nprint(f\"First read (from disk): {first_read_time:.4f} seconds\")\nprint(f\"Second read (from cache): {second_read_time:.4f} seconds\")\nprint(f\"Third read (from cache): {third_read_time:.4f} seconds\")\n\nshutil.rmtree(tmpdir)\nprint(f\"Cleaned up temporary directory: {tmpdir}\")\n</code></pre> <p>This step proves that the cache acts as a persistent copy of the data, allowing access even if the original source is unavailable.</p>"},{"location":"advanced/#custom-filesystem-implementations","title":"Custom Filesystem Implementations","text":"<p><code>fsspeckit</code> provides specialized filesystem implementations for unique use cases:</p>"},{"location":"advanced/#gitlab-filesystem-gitlabfilesystem","title":"GitLab Filesystem (<code>GitLabFileSystem</code>)","text":"<p>Access files directly from GitLab repositories. This is particularly useful for configuration files, datasets, or code stored in private or public GitLab instances.</p> <p>Example: Reading from a GitLab Repository</p> <pre><code>from fsspeckit.core import filesystem\n\n# Instantiate a GitLab filesystem\ngitlab_fs = filesystem(\n    \"gitlab\",\n    storage_options={\n        \"project_name\": \"your-group/your-project\", # Or \"project_id\": 12345\n        \"ref\": \"main\", # Branch, tag, or commit SHA\n        \"token\": \"glpat_YOUR_PRIVATE_TOKEN\" # Required for private repos\n    }\n)\n\n# List files in the repository root\nprint(gitlab_fs.ls(\"/\"))\n\n# Read a specific file\ncontent = gitlab_fs.cat(\"README.md\").decode(\"utf-8\")\nprint(content[:200]) # Print first 200 characters\n</code></pre>"},{"location":"advanced/#advanced-data-reading-and-writing-read_files-write_files","title":"Advanced Data Reading and Writing (<code>read_files</code>, <code>write_files</code>)","text":"<p>The <code>fsspeckit.core.ext</code> module (exposed via <code>AbstractFileSystem</code> extensions) provides powerful functions for reading and writing various data formats (JSON, CSV, Parquet) with advanced features like:</p> <ul> <li>Batch Processing: Efficiently handle large datasets by processing files in configurable batches.</li> <li>Parallel Processing: Leverage multi-threading to speed up file I/O operations.</li> <li>Schema Unification &amp; Optimization: Automatically unifies schemas when concatenating multiple files and optimizes data types for memory efficiency (e.g., using Polars' <code>opt_dtypes</code> or PyArrow's schema casting).</li> <li>File Path Tracking: Optionally include the source file path as a column in the resulting DataFrame/Table.</li> </ul>"},{"location":"advanced/#universal-read_files","title":"Universal <code>read_files</code>","text":"<p>The <code>read_files</code> function acts as a universal reader, delegating to format-specific readers (JSON, CSV, Parquet) while maintaining consistent options.</p> <p>Example: Reading CSVs in Batches with Parallelism</p> <pre><code>from fsspeckit.core import filesystem\n\n# Assuming you have multiple CSV files like 'data/part_0.csv', 'data/part_1.csv', etc.\n# on your local filesystem\nfs = filesystem(\"file\")\n\n# Read CSV files in batches of 10, using multiple threads, and including file path\nfor batch_df in fs.read_files(\n    \"data/*.csv\",\n    format=\"csv\",\n    batch_size=10,\n    include_file_path=True,\n    use_threads=True,\n    verbose=True\n):\n    print(f\"Processed batch with {len(batch_df)} rows. Columns: {batch_df.columns}\")\n    print(batch_df.head(2))\n</code></pre>"},{"location":"advanced/#reading-and-processing-multiple-files-pyarrow-tables-batch-processing","title":"Reading and Processing Multiple Files (PyArrow Tables, Batch Processing)","text":"<p><code>fsspeckit</code> simplifies reading multiple files of various formats (Parquet, CSV, JSON) from a folder into a single PyArrow Table or Polars DataFrame.</p> <p>Reading multiple files into a single table is a powerful feature that allows you to efficiently process data distributed across multiple files. This is particularly useful when dealing with large datasets that are split into smaller files for better organization or parallel processing.</p> <p>Key concepts demonstrated:</p> <ol> <li> <p>Glob patterns: The <code>**/*.parquet</code>, <code>**/*.csv</code>, and <code>**/*.json</code> patterns are used to select files recursively from the directory and its subdirectories. The <code>**</code> pattern matches any directories, allowing the function to find files in nested directories.</p> </li> <li> <p>Concat parameter: The <code>concat=True</code> parameter tells the function to combine data from multiple files into a single table or DataFrame. When set to <code>False</code>, the function would return a list of individual tables/DataFrames.</p> </li> <li> <p>Format flexibility: The same interface can be used to read different file formats (Parquet, CSV, JSON), making it easy to work with heterogeneous data sources.</p> </li> </ol> <p>Step-by-step explanation:</p> <ol> <li> <p>Creating sample data: We create two subdirectories and populate them with sample data in three different formats (Parquet, CSV, JSON). Each format contains the same structured data but in different serialization formats.</p> </li> <li> <p>Reading Parquet files: Using <code>fs.read_parquet(\"**/*.parquet\", concat=True)</code>, we read all Parquet files recursively and combine them into a single PyArrow Table. Parquet is a columnar storage format that is highly efficient for analytical workloads.</p> </li> <li> <p>Reading CSV files: Using <code>fs.read_csv(\"**/*.csv\", concat=True)</code>, we read all CSV files and combine them into a Polars DataFrame, which we then convert to a PyArrow Table for consistency.</p> </li> <li> <p>Reading JSON files: Using <code>fs.read_json(\"**/*.json\", as_dataframe=True, concat=True)</code>, we read all JSON files and combine them into a Polars DataFrame, then convert it to a PyArrow Table.</p> </li> <li> <p>Verification: Finally, we verify that all three tables have the same number of rows, confirming that the data was correctly read and combined across all files and formats.</p> </li> </ol> <p>The flexibility of <code>fsspeckit</code> allows you to use the same approach with different data sources, including remote filesystems like S3, GCS, or Azure Blob Storage, simply by changing the filesystem path.</p>"},{"location":"advanced/#setup","title":"Setup","text":"<p>First, we'll create a temporary directory with sample data in different formats.</p> <p>Setup steps:</p> <ol> <li>Create a temporary directory for our example</li> <li>Create sample data in subdirectories</li> </ol> <pre><code>import tempfile\nimport shutil\nimport os\nfrom examples.read_folder.create_dataset import create_sample_dataset\n\ntemp_dir = tempfile.mkdtemp()\nprint(f\"Created temporary directory: {temp_dir}\")\n\ncreate_sample_dataset(temp_dir)\n</code></pre> <p>This step sets up the environment by creating a temporary directory and populating it with sample data files.</p>"},{"location":"advanced/#reading-parquet-files","title":"Reading Parquet Files","text":"<p>Now, let's read all the Parquet files from the directory and its subdirectories into a single PyArrow Table.</p> <p>Reading Parquet files:</p> <ol> <li>Read Parquet files using glob pattern</li> <li>Display table information and sample data</li> </ol> <pre><code>print(\"\\n=== Reading Parquet Files ===\")\nfrom fsspeckit import filesystem\nfs = filesystem(temp_dir)\nparquet_table = fs.read_parquet(\"**/*.parquet\", concat=True)\nprint(f\"Successfully read Parquet files into PyArrow Table\")\nprint(f\"Table shape: {parquet_table.num_rows} rows x {parquet_table.num_columns} columns\")\nprint(\"First 3 rows:\")\nprint(parquet_table.slice(0, 3).to_pandas())\n</code></pre> <p>We use the <code>read_parquet</code> method with a glob pattern <code>**/*.parquet</code> to find all Parquet files recursively. The <code>concat=True</code> parameter combines them into a single table.</p>"},{"location":"advanced/#reading-csv-files","title":"Reading CSV Files","text":"<p>Next, we'll read all the CSV files into a Polars DataFrame and then convert it to a PyArrow Table.</p> <p>Reading CSV files:</p> <ol> <li>Read CSV files using glob pattern</li> <li>Display DataFrame information and sample data</li> <li>Convert to PyArrow Table for consistency</li> </ol> <pre><code>print(\"\\n=== Reading CSV Files ===\")\ncsv_df = fs.read_csv(\"**/*.csv\", concat=True)\nprint(f\"Successfully read CSV files into Polars DataFrame\")\nprint(f\"DataFrame shape: {csv_df.shape}\")\nprint(\"First 3 rows:\")\nprint(csv_df.head(3))\ncsv_table = csv_df.to_arrow()\n</code></pre> <p>Similarly, we use <code>read_csv</code> with the same glob pattern to read all CSV files.</p>"},{"location":"advanced/#reading-json-files","title":"Reading JSON Files","text":"<p>Finally, let's read all the JSON files.</p> <p>Reading JSON files:</p> <ol> <li>Read JSON files using glob pattern</li> <li>Display DataFrame information and sample data</li> <li>Convert to PyArrow Table for consistency</li> </ol> <pre><code>print(\"\\n=== Reading JSON Files ===\")\njson_df = fs.read_json(\"**/*.json\", as_dataframe=True, concat=True)\nprint(f\"Successfully read JSON files into Polars DataFrame\")\nprint(f\"DataFrame shape: {json_df.shape}\")\nprint(\"First 3 rows:\")\nprint(json_df.head(3))\njson_table = json_df.to_arrow()\n</code></pre> <p>The <code>read_json</code> method is used to read all JSON files. We set <code>as_dataframe=True</code> to get a Polars DataFrame.</p>"},{"location":"advanced/#verification","title":"Verification","text":"<p>Let's verify that all the tables have the same number of rows.</p> <pre><code>print(\"\\n=== Verification ===\")\nprint(f\"All tables have the same number of rows: {parquet_table.num_rows == csv_table.num_rows == json_table.num_rows}\")\n\nshutil.rmtree(temp_dir)\nprint(f\"\\nCleaned up temporary directory: {temp_dir}\")\n</code></pre> <p>This final step confirms that our data reading and concatenation were successful.</p> <p>This example shows how to read various file formats from a directory, including subdirectories, into a unified PyArrow Table or Polars DataFrame. It highlights the flexibility of <code>fsspeckit</code> in handling different data sources and formats.</p> <p><code>fsspeckit</code> enables efficient batch processing of large datasets by reading files in smaller, manageable chunks. This is particularly useful for memory-constrained environments or when processing streaming data.</p> <p>Batch processing is a technique for handling large datasets by dividing them into smaller, manageable chunks. This is particularly important for:</p> <ol> <li>Memory-constrained environments: When working with datasets that are too large to fit in memory, batch processing allows you to process the data incrementally.</li> <li>Streaming data: When data is continuously generated (e.g., from IoT devices or real-time applications), batch processing enables you to process data as it arrives.</li> <li>Distributed processing: In distributed computing environments, batch processing allows different nodes to work on different chunks of data simultaneously.</li> </ol> <p>The <code>batch_size</code> parameter controls how many files or records are processed together in each batch. A smaller batch size reduces memory usage but may increase processing overhead, while a larger batch size improves throughput but requires more memory.</p> <p>Step-by-step walkthrough:</p> <ol> <li> <p>Creating sample batched data: We generate sample data and distribute it across multiple files in each format (Parquet, CSV, JSON). Each file contains a subset of the total data, simulating a real-world scenario where data is split across multiple files.</p> </li> <li> <p>Reading Parquet files in batches: Using <code>fs.read_parquet(parquet_path, batch_size=2)</code>, we read all Parquet files in batches of 2 files at a time. Each iteration of the loop processes a batch of files, and the <code>batch</code> variable contains the combined data from those files.</p> </li> <li> <p>Reading CSV files in batches: Similarly, we use <code>fs.read_csv(csv_path, batch_size=2)</code> to read CSV files in batches. The result is a Polars DataFrame for each batch, which we can process individually.</p> </li> <li> <p>Reading JSON files in batches: Finally, we use <code>fs.read_json(json_path, batch_size=2)</code> to read JSON files in batches. The JSON data is automatically converted to Polars DataFrames for easy processing. <pre><code>```python\nprint(\"\\n=== JSON Batch Reading ===\")\njson_path = os.path.join(temp_dir, \"*.json\")\nprint(\"\\nReading JSON files in batches (batch_size=2):\")\nfor i, batch in enumerate(fs.read_json(json_path, batch_size=2)):\n    print(f\"   Batch {i+1}: shape={batch.shape}\")\n    print(f\"   - Data preview: {batch.head(1).to_dicts()}\")\n\nshutil.rmtree(temp_dir)\nprint(f\"\\nCleaned up temporary directory: {temp_dir}\")\n</code></pre></p> </li> </ol> <p>The <code>read_json</code> method is also used with <code>batch_size=2</code> to process JSON files in batches.</p> <p>This example illustrates how to read Parquet, CSV, and JSON files in batches using the <code>batch_size</code> parameter. This approach allows for processing of large datasets without loading the entire dataset into memory at once.</p>"},{"location":"advanced/#advanced-parquet-handling-and-delta-lake-integration","title":"Advanced Parquet Handling and Delta Lake Integration","text":"<p><code>fsspeckit</code> enhances Parquet operations with deep integration with PyArrow, enabling efficient dataset management, partitioning, and delta lake capabilities.</p> <ul> <li><code>pyarrow_dataset</code>: Create PyArrow datasets for optimized querying, partitioning, and predicate pushdown.</li> <li><code>pyarrow_parquet_dataset</code>: Specialized for Parquet, handling <code>_metadata</code> files for overall dataset schemas.</li> </ul> <p>Example: Writing to a PyArrow Dataset with Partitioning</p> <pre><code>import polars as pl\nfrom fsspeckit.core import filesystem\n\nfs = filesystem(\"file\")\nbase_path = \"output/my_partitioned_data\"\n\n# Sample data\ndata = pl.DataFrame({\n    \"id\": [1, 2, 3, 4],\n    \"value\": [\"A\", \"B\", \"C\", \"D\"],\n    \"year\": [2023, 2023, 2024, 2024],\n    \"month\": [10, 11, 1, 2]\n})\n\n# Write data as a partitioned PyArrow dataset\nfs.write_pyarrow_dataset(\n    data=data,\n    path=base_path,\n    partition_by=[\"year\", \"month\"], # Partition by year and month\n    format=\"parquet\",\n    compression=\"zstd\",\n    mode=\"overwrite\" # Overwrite if path exists\n)\n\nprint(f\"Data written to {base_path} partitioned by year/month.\")\n# Expected structure: output/my_partitioned_data/year=2023/month=10/data-*.parquet\n</code></pre> <p>&lt;!--Example: Delta Lake Operations with Pydala Dataset</p> <pre><code>import polars as pl\nfrom fsspeckit.core import filesystem\n\nfs = filesystem(\"file\")\ndelta_path = \"output/my_delta_table\"\n\n# Initial data\ninitial_data = pl.DataFrame({\n    \"id\": [1, 2],\n    \"name\": [\"Alice\", \"Bob\"],\n    \"version\": [1, 1]\n})\n\n# Write initial data to a Pydala dataset\nfs.write_pydala_dataset(\n    data=initial_data,\n    path=delta_path,\n    mode=\"overwrite\"\n)\nprint(\"Initial Delta table created.\")\n\n# New data for an upsert: update Alice, add Charlie\nnew_data = pl.DataFrame({\n    \"id\": [1, 3],\n    \"name\": [\"Alicia\", \"Charlie\"],\n    \"version\": [2, 1]\n})\n\n# Perform a delta merge (upsert)\nfs.write_pydala_dataset(\n    data=new_data,\n    path=delta_path,\n    mode=\"delta\",\n    delta_subset=[\"id\"] # Column(s) to use for merging\n)\nprint(\"Delta merge completed.\")\n\n# Read the updated table\nupdated_df = fs.pydala_dataset(delta_path).to_polars()\nprint(\"Updated Delta table:\")\nprint(updated_df)\n# Expected: id=1 Alicia version=2, id=2 Bob version=1, id=3 Charlie version=1\n```--&gt;\n\n`fsspeckit` facilitates integration with Delta Lake by providing `StorageOptions` that can be used to configure `deltalake`'s `DeltaTable` for various storage backends.\n\nThis example demonstrates how to use `LocalStorageOptions` with `deltalake`'s `DeltaTable`. It shows how to initialize a `DeltaTable` instance by passing the `fsspeckit` storage options, enabling seamless interaction with Delta Lake tables across different storage types.\n\n**Step-by-step walkthrough:**\n\n1. Create a temporary directory for our example\n2. Create a simple Polars DataFrame\n3. Write initial data to create the Delta table\n4. Create a LocalStorageOptions object for the temporary directory\n5. Create a DeltaTable instance, passing storage options\n   - Note: deltalake expects storage_options as a dict, which to_object_store_kwargs provides\n6. Read data from the DeltaTable\n7. Clean up the temporary directory\n\n**Delta Lake** is an open-source storage layer that brings ACID transactions to Apache Spark and big data workloads. It provides a reliable, scalable, and performant way to work with data lakes, combining the benefits of data lakes (low cost, flexibility) with data warehouses (reliability, performance).\n\n```python\nfrom deltalake import DeltaTable\nfrom fsspeckit.storage_options import LocalStorageOptions\nimport tempfile\nimport shutil\nimport os\nimport polars as pl\n\ntemp_dir = tempfile.mkdtemp()\nprint(f\"Created temporary directory: {temp_dir}\")\n\ndelta_table_path = os.path.join(temp_dir, \"my_delta_table\")\nprint(f\"Creating a dummy Delta table at: {delta_table_path}\")\n\ndata = pl.DataFrame({\n    \"id\": [1, 2, 3],\n    \"value\": [\"A\", \"B\", \"C\"]\n})\n\ndata.write_delta(delta_table_path, mode=\"overwrite\")\nprint(\"Initial data written to Delta table.\")\n\nlocal_options = LocalStorageOptions(path=temp_dir)\n\ndt = DeltaTable(delta_table_path, storage_options=local_options.to_object_store_kwargs())\nprint(f\"\\nSuccessfully created DeltaTable instance from: {delta_table_path}\")\nprint(f\"DeltaTable version: {dt.version()}\")\nprint(f\"DeltaTable files: {dt.files()}\")\n\ntable_data = dt.to_pyarrow_table()\nprint(\"\\nData read from DeltaTable:\")\nprint(table_data.to_pandas())\n\nshutil.rmtree(temp_dir)\nprint(f\"Cleaned up temporary directory: {temp_dir}\")\n</code></pre> <p>Key features of Delta Lake:</p> <ul> <li>ACID transactions: Ensures data integrity even with concurrent operations</li> <li>Time travel: Allows querying data as it existed at any point in time</li> <li>Schema enforcement: Maintains data consistency with schema validation</li> <li>Scalable metadata: Handles billions of files efficiently</li> <li>Unified analytics: Supports both batch and streaming workloads</li> </ul> <p>Integrating fsspeckit with Delta Lake:</p> <p>The <code>fsspeckit</code> <code>StorageOptions</code> classes can be used to configure <code>deltalake</code>'s <code>DeltaTable</code> for various storage backends. This integration allows you to:</p> <ol> <li>Use consistent configuration patterns across different storage systems</li> <li>Leverage the benefits of fsspec's unified filesystem interface</li> <li>Seamlessly switch between local and cloud storage without changing your Delta Lake code</li> </ol> <p>The <code>to_object_store_kwargs()</code> method converts <code>fsspeckit</code> storage options into a dictionary format that <code>deltalake</code> expects for its <code>storage_options</code> parameter. This is necessary because <code>deltalake</code> requires storage options as a dictionary, while <code>fsspeckit</code> provides them as structured objects.</p> <p>Step-by-step walkthrough:</p> <ol> <li> <p>Creating a temporary directory: We create a temporary directory to store our Delta table, ensuring the example is self-contained and doesn't leave artifacts on your system.</p> </li> <li> <p>Creating sample data: We create a simple Polars DataFrame with sample data that will be written to our Delta table.</p> </li> <li> <p>Writing to Delta table: Using the <code>write_delta</code> method, we convert our DataFrame into a Delta table. This creates the necessary Delta Lake metadata alongside the data files.</p> </li> <li> <p>Configuring storage options: We create a <code>LocalStorageOptions</code> object that points to our temporary directory. This object contains all the information needed to access the Delta table.</p> </li> <li> <p>Initializing DeltaTable: We create a <code>DeltaTable</code> instance by passing the table path and the storage options converted to a dictionary via <code>to_object_store_kwargs()</code>. This allows <code>deltalake</code> to locate and access the Delta table files.</p> </li> <li> <p>Verifying the DeltaTable: We check the version and files of our Delta table to confirm it was created correctly. Delta tables maintain version history, allowing you to track changes over time.</p> </li> <li> <p>Reading data: Finally, we read the data from our Delta table back into a PyArrow Table, demonstrating that we can successfully interact with the Delta Lake table using the fsspeckit configuration.</p> </li> </ol> <p>This integration is particularly valuable when working with Delta Lake in cloud environments, as it allows you to use the same configuration approach for local development and production deployments across different cloud providers.</p>"},{"location":"advanced/#storage-options-management","title":"Storage Options Management","text":"<p><code>fsspeckit</code> provides a robust system for managing storage configurations, simplifying credential handling and environment setup.</p>"},{"location":"advanced/#loading-from-environment-variables","title":"Loading from Environment Variables","text":"<p>Instead of hardcoding credentials, you can load storage options directly from environment variables.</p> <p>Example: Loading AWS S3 Configuration from Environment</p> <p>Set these environment variables before running your script: <pre><code>export AWS_ACCESS_KEY_ID=\"YOUR_ACCESS_KEY\"\nexport AWS_SECRET_ACCESS_KEY=\"YOUR_SECRET_KEY\"\nexport AWS_DEFAULT_REGION=\"us-west-2\"\n</code></pre></p> <p>Then in Python: <pre><code>from fsspeckit.storage_options import AwsStorageOptions\n\n# Load AWS options directly from environment variables\naws_opts = AwsStorageOptions.from_env()\nprint(f\"Loaded AWS region: {aws_opts.region}\")\n\n# Use it to create a filesystem\n# fs = aws_opts.to_filesystem()\n</code></pre></p>"},{"location":"advanced/#merging-storage-options","title":"Merging Storage Options","text":"<p>Combine multiple storage option configurations, useful for layering default settings with user-specific overrides.</p> <p>Example: Merging S3 Options</p> <pre><code>from fsspeckit.storage_options import AwsStorageOptions, merge_storage_options\n\n# Base configuration\nbase_opts = AwsStorageOptions(\n    protocol=\"s3\",\n    region=\"us-east-1\",\n    access_key_id=\"DEFAULT_KEY\"\n)\n\n# User-provided overrides\nuser_overrides = {\n    \"access_key_id\": \"USER_KEY\",\n    \"allow_http\": True # New setting\n}\n\n# Merge, with user_overrides taking precedence\nmerged_opts = merge_storage_options(base_opts, user_overrides, overwrite=True)\n\nprint(f\"Merged Access Key ID: {merged_opts.access_key_id}\") # USER_KEY\nprint(f\"Merged Region: {merged_opts.region}\") # us-east-1\nprint(f\"Allow HTTP: {merged_opts.allow_http}\") # True\n</code></pre>"},{"location":"advanced/#note-on-github-examples","title":"Note on GitHub Examples","text":"<p>For a comprehensive collection of executable examples demonstrating various functionalities and advanced patterns of <code>fsspeckit</code>, including those discussed in this document, please refer to the examples directory on GitHub. Each example is designed to be runnable and provides detailed insights into practical usage.</p>"},{"location":"advanced/#performance-tips","title":"Performance Tips","text":"<ul> <li>Caching: Always consider using <code>cached=True</code> with the <code>filesystem</code> function, especially for remote filesystems, to minimize repeated downloads.</li> <li>Parallel Reading: For multiple files, set <code>use_threads=True</code> in <code>read_json</code>, <code>read_csv</code>, and <code>read_parquet</code> to leverage concurrent I/O.</li> <li>Batch Processing: When dealing with a very large number of files or extremely large individual files, use the <code>batch_size</code> parameter in reading functions to process data in chunks, reducing memory footprint.</li> <li><code>opt_dtypes</code>: Utilize <code>opt_dtypes=True</code> in reading functions when converting to Polars or PyArrow to automatically optimize column data types, leading to more efficient memory usage and faster subsequent operations.</li> <li>Parquet Datasets: For large, partitioned Parquet datasets, use <code>pyarrow_dataset</code> or <code>pyarrow_parquet_dataset</code>. These leverage PyArrow's dataset API for efficient metadata handling, partition pruning, and predicate pushdown, reading only the necessary data.</li> <li>Compression: When writing Parquet files, choose an appropriate compression codec (e.g., <code>zstd</code>, <code>snappy</code>) to reduce file size and improve I/O performance. <code>zstd</code> often provides a good balance of compression ratio and speed.</li> </ul>"},{"location":"advanced/#flexible-storage-configuration","title":"Flexible Storage Configuration","text":"<p><code>fsspeckit</code> simplifies configuring connections to various storage systems, including local filesystems, AWS S3, Azure Storage, and Google Cloud Storage, using <code>StorageOptions</code> classes. These options can then be converted into <code>fsspec</code> filesystems.</p>"},{"location":"advanced/#local-storage-example","title":"Local Storage Example","text":"<p>This example demonstrates how to initialize <code>LocalStorageOptions</code> and use it to interact with the local filesystem.</p> <p>Step-by-step walkthrough:</p> <ol> <li>Create a temporary directory for our test</li> <li>Create a test file and write content to it</li> <li>List files in the directory to verify our file was created</li> <li>Read the content back to verify it was written correctly</li> <li>Clean up the temporary directory</li> </ol> <p>StorageOptions classes simplify configuration for different storage systems and provide a consistent interface for creating fsspec filesystem objects.</p> <pre><code>import os\nimport tempfile\nimport shutil\nfrom fsspeckit.storage_options import LocalStorageOptions\n\nprint(\"=== LocalStorageOptions Example ===\\n\")\n\nlocal_options = LocalStorageOptions(auto_mkdir=True)\nlocal_fs = local_options.to_filesystem()\n\ntemp_dir = tempfile.mkdtemp()\nprint(f\"Working in temporary directory: {temp_dir}\")\n\ntemp_file = os.path.join(temp_dir, \"test_file.txt\")\nwith local_fs.open(temp_file, \"w\") as f:\n    f.write(\"Hello, LocalStorageOptions!\")\nprint(f\"Created test file: {temp_file}\")\n\nfiles = local_fs.ls(temp_dir)\nprint(f\"Files in {temp_dir}: {[os.path.basename(f) for f in files]}\")\n\nwith local_fs.open(temp_file, \"r\") as f:\n    content = f.read()\nprint(f\"File content: '{content}'\")\n\nshutil.rmtree(temp_dir)\nprint(f\"Cleaned up temporary directory: {temp_dir}\")\nprint(\"Local storage example completed.\\n\")\n</code></pre>"},{"location":"advanced/#conceptual-aws-s3-configuration","title":"Conceptual AWS S3 Configuration","text":"<p>This example demonstrates the configuration pattern for <code>AwsStorageOptions</code>. It is expected to fail when attempting to connect to actual cloud services because it uses dummy credentials.</p> <p>Note: The <code>to_filesystem()</code> method converts StorageOptions into fsspec-compatible objects, allowing seamless integration with any fsspec-compatible library. <pre><code>from fsspeckit.storage_options import AwsStorageOptions\n\nprint(\"=== Conceptual AwsStorageOptions Example (using a dummy endpoint) ===\\n\")\n\naws_options = AwsStorageOptions(\n    endpoint_url=\"http://s3.dummy-endpoint.com\",\n    access_key_id=\"DUMMY_KEY\",\n    secret_access_key=\"DUMMY_SECRET\",\n    allow_http=True,\n    region=\"us-east-1\"\n)\n\naws_fs = aws_options.to_filesystem()\nprint(f\"Created fsspec filesystem for S3: {type(aws_fs).__name__}\")\nprint(\"AWS storage example completed.\\n\")\n</code></pre></p>"},{"location":"advanced/#conceptual-azure-configuration","title":"Conceptual Azure Configuration","text":"<p>This example shows how to configure <code>AzureStorageOptions</code>.  It is expected to fail when attempting to connect to actual cloud services because it uses dummy credentials.</p> <pre><code>from fsspeckit.storage_options import AzureStorageOptions\n\nprint(\"=== Conceptual AzureStorageOptions Example (using a dummy connection string) ===\\n\")\nazure_options = AzureStorageOptions(\n    protocol=\"az\",\n    account_name=\"demoaccount\",\n    connection_string=\"DefaultEndpointsProtocol=https;AccountName=demoaccount;AccountKey=demokey==;EndpointSuffix=core.windows.net\"\n)\n\nazure_fs = azure_options.to_filesystem()\nprint(f\"Created fsspec filesystem for Azure: {type(azure_fs).__name__}\")\nprint(\"Azure storage example completed.\\n\")\n</code></pre>"},{"location":"advanced/#conceptual-gcs-configuration","title":"Conceptual GCS Configuration","text":"<p>This example shows how to configure <code>GcsStorageOptions</code>.  It is expected to fail when attempting to connect to actual cloud services because it uses dummy credentials.</p> <p>StorageOptions classes provide a simplified, consistent interface for configuring connections to various storage systems. They abstract away the complexity of different storage backends and provide a unified way to create fsspec filesystem objects.</p> <p>The <code>to_filesystem()</code> method converts these options into <code>fsspec</code> compatible objects, enabling seamless integration with any fsspec-compatible library or tool.</p> <p>Important Note: The AWS, Azure, and GCS examples use dummy credentials and are for illustrative purposes only. These examples are expected to fail when attempting to connect to actual cloud services because:</p> <ol> <li>The endpoint URLs are not real service endpoints</li> <li>The credentials are placeholder values that don't correspond to actual accounts</li> <li>The connection strings and tokens are examples, not valid credentials</li> </ol> <p>This approach allows you to understand the configuration pattern without needing actual cloud credentials. When using these examples in production, you would replace the dummy values with your real credentials and service endpoints.</p> <p>```python from fsspeckit.storage_options import GcsStorageOptions</p> <p>print(\"=== Conceptual GcsStorageOptions Example (using a dummy token path) ===\\n\") gcs_options = GcsStorageOptions(     protocol=\"gs\",     project=\"demo-project\",     token=\"path/to/dummy-service-account.json\" )</p> <p>gcs_fs = gcs_options.to_filesystem() print(f\"Created fsspec filesystem for GCS: {type(gcs_fs).name}\") print(\"GCS storage example completed.\\n\")</p>"},{"location":"api-guide/","title":"API Guide","text":"<p>This guide provides a comprehensive overview of the fsspeckit public API and how to use its main components.</p>"},{"location":"api-guide/#core-filesystem-factory","title":"Core Filesystem Factory","text":""},{"location":"api-guide/#filesystem","title":"<code>filesystem()</code>","text":"<p>The main entry point for creating configured filesystems:</p> <pre><code>from fsspeckit import filesystem\n\n# Basic local filesystem\nfs = filesystem(\"file\")\n\n# S3 with caching\nfs = filesystem(\"s3://my-bucket/\", cached=True)\n\n# With storage options\nfs = filesystem(\"s3\", storage_options={\"region\": \"us-west-2\"})\n\n# With base filesystem (DirFileSystem hierarchy)\nfs = filesystem(\"/data\", dirfs=True, base_fs=parent_fs)\n</code></pre> <p>Parameters: - <code>protocol_or_path</code> - Protocol (e.g., 's3', 'gs') or path - <code>storage_options</code> - Dict or StorageOptions object - <code>cached</code> - Enable caching (default: False) - <code>cache_storage</code> - Cache directory location - <code>verbose</code> - Log cache operations (default: False) - <code>dirfs</code> - Wrap in DirFileSystem (default: True) - <code>base_fs</code> - Parent DirFileSystem for hierarchy - <code>use_listings_cache</code> - Use listings cache (default: True) - <code>skip_instance_cache</code> - Skip instance cache (default: False) - <code>**kwargs</code> - Protocol-specific options</p>"},{"location":"api-guide/#storage-options-classes","title":"Storage Options Classes","text":"<p>Storage options provide structured configuration for different providers:</p> <pre><code>from fsspeckit.storage_options import (\n    AwsStorageOptions,\n    GcsStorageOptions,\n    AzureStorageOptions,\n    GitHubStorageOptions,\n    GitLabStorageOptions,\n    LocalStorageOptions\n)\n\n# AWS S3\naws = AwsStorageOptions(\n    region=\"us-east-1\",\n    access_key_id=\"key\",\n    secret_access_key=\"secret\"\n)\nfs = aws.to_filesystem()\n\n# Google Cloud Storage\ngcs = GcsStorageOptions(\n    project=\"my-project\",\n    token=\"path/to/service-account.json\"\n)\nfs = gcs.to_filesystem()\n\n# Azure\nazure = AzureStorageOptions(\n    account_name=\"myaccount\",\n    account_key=\"key...\"\n)\nfs = azure.to_filesystem()\n\n# GitHub Repository\ngithub = GitHubStorageOptions(\n    org=\"microsoft\",\n    repo=\"vscode\",\n    token=\"ghp_xxx\"\n)\nfs = github.to_filesystem()\n\n# GitLab Repository\ngitlab = GitLabStorageOptions(\n    project_id=12345,\n    token=\"glpat_xxx\"\n)\nfs = gitlab.to_filesystem()\n\n# Local\nlocal = LocalStorageOptions(auto_mkdir=True)\nfs = local.to_filesystem()\n</code></pre>"},{"location":"api-guide/#storageoptions-factories","title":"StorageOptions Factories","text":"<p>Create storage options from various sources:</p> <pre><code>from fsspeckit.storage_options import (\n    from_dict,\n    from_env,\n    merge_storage_options,\n    infer_protocol_from_uri,\n    storage_options_from_uri\n)\n\n# From dictionary\nopts = from_dict(\"s3\", {\"region\": \"us-west-2\"})\n\n# From environment variables\nopts = AwsStorageOptions.from_env()\n\n# Merge multiple options\nmerged = merge_storage_options(opts1, opts2, overwrite=True)\n\n# Infer protocol from URI\nprotocol = infer_protocol_from_uri(\"s3://bucket/path\")\n\n# Create from URI\nopts = storage_options_from_uri(\"s3://my-bucket/data\")\n</code></pre>"},{"location":"api-guide/#extended-io-operations","title":"Extended I/O Operations","text":"<p>fsspeckit adds rich I/O methods to all fsspec filesystems through monkey-patching.</p>"},{"location":"api-guide/#reading-operations","title":"Reading Operations","text":""},{"location":"api-guide/#json-operations","title":"JSON Operations","text":"<pre><code>from fsspeckit import filesystem\n\nfs = filesystem(\".\")\n\n# Read single JSON file\ndata = fs.read_json_file(\"data.json\")  # Returns dict\ndf = fs.read_json_file(\"data.json\", as_dataframe=True)  # Returns Polars DF\n\n# Read multiple JSON files with batching\nfor batch in fs.read_json(\"data/*.json\", batch_size=5):\n    # Process batch\n    pass\n\n# Read JSON Lines format\ndf = fs.read_json(\"data/lines.jsonl\", as_dataframe=True)\n\n# With threading\ndf = fs.read_json(\"data/*.json\", use_threads=True, num_threads=4)\n\n# Include source file path\ndf = fs.read_json(\"data/*.json\", include_file_path=True)\n</code></pre>"},{"location":"api-guide/#csv-operations","title":"CSV Operations","text":"<pre><code># Read single CSV\ndf = fs.read_csv_file(\"data.csv\")\n\n# Read multiple CSV files\ndf = fs.read_csv(\"data/*.csv\", concat=True)\n\n# Batch reading\nfor batch in fs.read_csv(\"data/*.csv\", batch_size=10):\n    pass\n\n# Optimize data types\ndf = fs.read_csv(\"data/*.csv\", opt_dtypes=True)\n\n# With parallelism\ndf = fs.read_csv(\"data/*.csv\", use_threads=True)\n</code></pre>"},{"location":"api-guide/#parquet-operations","title":"Parquet Operations","text":"<pre><code># Read single Parquet file\ntable = fs.read_parquet_file(\"data.parquet\")\n\n# Read multiple with schema unification\ntable = fs.read_parquet(\"data/*.parquet\", concat=True)\n\n# Batch reading\nfor batch in fs.read_parquet(\"data/*.parquet\", batch_size=20):\n    pass\n\n# With partitioning support\ntable = fs.read_parquet(\"partitioned_data/**/*.parquet\", concat=True)\n\n# Include file path column\ntable = fs.read_parquet(\"data/*.parquet\", include_file_path=True)\n</code></pre>"},{"location":"api-guide/#universal-reader","title":"Universal Reader","text":"<pre><code># Auto-detect format from file extension\ndf = fs.read_files(\"data/mixed/*\", format=\"auto\")\n\n# Explicit format\ndf = fs.read_files(\"data/*.csv\", format=\"csv\")\n\n# Control result type\ndf_polars = fs.read_files(\"data/*.parquet\", as_dataframe=True)\ntable_arrow = fs.read_files(\"data/*.parquet\", as_dataframe=False)\n</code></pre>"},{"location":"api-guide/#dataset-operations","title":"Dataset Operations","text":"<pre><code># Create PyArrow dataset\ndataset = fs.pyarrow_dataset(\"data/\")\n\n# Optimized for Parquet with metadata\ndataset = fs.pyarrow_parquet_dataset(\"partitioned_data/\")\n\n# Query dataset with filtering\nfiltered = dataset.to_table(filter=pyarrow.compute.greater(dataset.column(\"age\"), 25))\n\n# Schema inspection\nprint(dataset.schema)\n\n# Statistics and metadata\nprint(dataset.count_rows())\n</code></pre>"},{"location":"api-guide/#writing-operations","title":"Writing Operations","text":""},{"location":"api-guide/#parquet-writing","title":"Parquet Writing","text":"<pre><code>import polars as pl\n\n# From Polars DataFrame\ndf = pl.DataFrame({\"col1\": [1, 2, 3], \"col2\": [4, 5, 6]})\nfs.write_parquet(df, \"output.parquet\")\n\n# From Pandas\nimport pandas as pd\ndf = pd.DataFrame({\"a\": [1, 2], \"b\": [3, 4]})\nfs.write_parquet(df, \"output.parquet\")\n\n# Compression\nfs.write_parquet(df, \"output.parquet\", compression=\"zstd\")\n\n# Append mode\nfs.write_parquet(df, \"output.parquet\", mode=\"append\")\n</code></pre>"},{"location":"api-guide/#csv-writing","title":"CSV Writing","text":"<pre><code># Write DataFrame\ndf.write_csv(\"output.csv\")\n\n# Append to existing file\nfs.write_csv(new_df, \"output.csv\", mode=\"append\")\n</code></pre>"},{"location":"api-guide/#json-writing","title":"JSON Writing","text":"<pre><code># Write DataFrame to JSON\ndf.write_json(\"output.json\")\n\n# JSON Lines format\nfs.write_json(df, \"output.jsonl\", format=\"json_lines\")\n\n# Include file path metadata\nfs.write_json(df, \"output.json\", include_file_path=True)\n</code></pre>"},{"location":"api-guide/#pyarrow-dataset-writing","title":"PyArrow Dataset Writing","text":"<pre><code>import pyarrow as pa\n\n# Write partitioned dataset\ntable = pa.table({\"year\": [2023, 2023, 2024], \"value\": [10, 20, 30]})\nfs.write_pyarrow_dataset(\n    data=table,\n    path=\"partitioned_data\",\n    partition_by=[\"year\"],\n    format=\"parquet\",\n    compression=\"zstd\"\n)\n\n# Result structure: partitioned_data/year=2023/...parquet files\n</code></pre>"},{"location":"api-guide/#cache-management","title":"Cache Management","text":"<pre><code># Clear all caches\nfs.clear_cache()\n\n# Check cache size\nsize = fs.get_cache_size()\n\n# Sync cache (ensure data is written)\nfs.sync_cache()\n</code></pre>"},{"location":"api-guide/#helper-utilities","title":"Helper Utilities","text":"<p>Note: fsspeckit utilities are organized into domain packages. See Architecture for details.</p>"},{"location":"api-guide/#parallel-processing","title":"Parallel Processing","text":"<pre><code>from fsspeckit.common.misc import run_parallel\n\ndef process(item):\n    return len(item)\n\nresults = run_parallel(\n    process,\n    [\"item1\", \"item2\", \"item3\"],\n    n_jobs=4,\n    verbose=True\n)\n</code></pre>"},{"location":"api-guide/#type-conversions","title":"Type Conversions","text":"<pre><code>from fsspeckit.common.types import (\n    dict_to_dataframe,\n    to_pyarrow_table\n)\n\n# Dict to DataFrame\ndf = dict_to_dataframe({\"col1\": [1, 2], \"col2\": [3, 4]})\n\n# Any to PyArrow\ntable = to_pyarrow_table(df)\n</code></pre>"},{"location":"api-guide/#data-type-optimization","title":"Data Type Optimization","text":"<pre><code>import polars as pl\nfrom fsspeckit.common import opt_dtype_pl\n\n# Optimize DataFrame\ndf = pl.read_csv(\"data.csv\")\ndf_opt = opt_dtype_pl(df)\n\n# Or use extension\ndf_opt = df.opt_dtype\n</code></pre>"},{"location":"api-guide/#sql-filtering","title":"SQL Filtering","text":"<pre><code>from fsspeckit.sql.filters import sql2pyarrow_filter\nimport pyarrow as pa\n\nschema = pa.schema([(\"age\", pa.int32()), (\"name\", pa.string())])\nexpr = sql2pyarrow_filter(\"age &gt; 25 AND name = 'Alice'\", schema)\nfiltered_table = dataset.to_table(filter=expr)\n</code></pre>"},{"location":"api-guide/#file-synchronization","title":"File Synchronization","text":"<pre><code>from fsspeckit.common.misc import sync_dir\n\n# Sync directories\nsync_dir(\n    fs_source, \"/source/\",\n    fs_target, \"/target/\",\n    overwrite=False\n)\n</code></pre>"},{"location":"api-guide/#filesystem-classes","title":"Filesystem Classes","text":""},{"location":"api-guide/#custom-implementations","title":"Custom Implementations","text":""},{"location":"api-guide/#gitlabfilesystem","title":"<code>GitLabFileSystem</code>","text":"<p>Read-only filesystem for GitLab repositories:</p> <pre><code>from fsspeckit.core import filesystem\n\nfs = filesystem(\n    \"gitlab\",\n    storage_options={\n        \"project_name\": \"group/project\",\n        \"token\": \"glpat_xxx\",\n        \"ref\": \"main\"\n    }\n)\n\n# List files\nfiles = fs.ls(\"/\")\n\n# Read file\ncontent = fs.cat(\"README.md\")\n</code></pre>"},{"location":"api-guide/#monitoredsimplecachefilesystem","title":"<code>MonitoredSimpleCacheFileSystem</code>","text":"<p>Enhanced cache with logging and monitoring:</p> <pre><code># Automatically used when cached=True\nfs = filesystem(\"s3\", cached=True, verbose=True)\n\n# Monitor cache operations\nfs.sync_cache()\nsize = fs.get_cache_size()\n</code></pre>"},{"location":"api-guide/#working-with-dirfilesystem","title":"Working with DirFileSystem","text":"<pre><code>from fsspeckit import filesystem\n\n# Create DirFileSystem wrapper\nfs = filesystem(\"/data\", dirfs=True)\n\n# Access files within the base directory\nfs.ls(\"/subdir\")\n\n# Create hierarchical filesystem (base_fs parameter)\nparent_fs = filesystem(\"/datasets\", dirfs=True)\nchild_fs = filesystem(\"/datasets/project1\", dirfs=True, base_fs=parent_fs)\n\n# Files are accessible only within the base directory\n# Attempting to escape raises an error\n</code></pre>"},{"location":"api-guide/#configuration-methods","title":"Configuration Methods","text":"<p>All storage option classes provide conversion methods:</p> <pre><code>opts = AwsStorageOptions(...)\n\n# Convert to fsspec kwargs\nkwargs = opts.to_fsspec_kwargs()\n\n# Convert to filesystem\nfs = opts.to_filesystem()\n\n# Convert to object store kwargs (for deltalake, etc.)\nobj_store_kwargs = opts.to_object_store_kwargs()\n\n# Convert to YAML\nyaml_str = opts.to_yaml()\n\n# Load from YAML\nopts = AwsStorageOptions.from_yaml(yaml_str)\n\n# Convert to environment variables\nenv = opts.to_env()\n\n# Load from environment\nopts = AwsStorageOptions.from_env()\n</code></pre>"},{"location":"api-guide/#error-handling","title":"Error Handling","text":"<pre><code>from fsspeckit import filesystem\nfrom fsspeckit.storage_options import AwsStorageOptions\n\ntry:\n    fs = filesystem(\"s3\", storage_options={\"region\": \"invalid\"})\n    data = fs.cat(\"s3://bucket/file.txt\")\nexcept Exception as e:\n    print(f\"Error: {e}\")\n\n# Check optional dependencies\nfrom fsspeckit.common.misc import check_optional_dependency\ntry:\n    check_optional_dependency(\"deltalake\")\nexcept ImportError:\n    print(\"Install with: pip install deltalake\")\n</code></pre>"},{"location":"api-guide/#best-practices","title":"Best Practices","text":"<ol> <li>Use StorageOptions - Type-safe and consistent configuration</li> <li>Enable Caching - For remote filesystems, always consider caching</li> <li>Batch Processing - Use batch_size for large datasets</li> <li>Parallel I/O - Enable threading for multi-file operations</li> <li>Type Optimization - Use opt_dtypes=True to reduce memory</li> <li>Error Handling - Wrap filesystem operations in try/except</li> </ol>"},{"location":"api-guide/#see-also","title":"See Also","text":"<ul> <li>Advanced Usage - In-depth guides and patterns</li> <li>Utils Module - Utility functions reference</li> <li>Examples - Runnable example scripts</li> <li>Architecture - Design and implementation details</li> </ul>"},{"location":"architecture/","title":"Architecture Overview","text":"<p><code>fsspeckit</code> is designed to extend and enhance the capabilities of <code>fsspec</code>, providing a robust and flexible framework for interacting with various filesystems and data formats. Its architecture is modular, built around core components that abstract away complexities and offer specialized functionalities.</p>"},{"location":"architecture/#extending-fsspec","title":"Extending <code>fsspec</code>","text":"<p>At its core, <code>fsspeckit</code> builds upon the <code>fsspec</code> (Filesystem Spec) library, which provides a unified Pythonic interface to various storage backends. <code>fsspeckit</code> extends this functionality by:</p> <ul> <li>Simplifying Storage Configuration: It offers <code>StorageOptions</code> classes for various cloud providers (AWS S3, Google Cloud Storage, Azure Storage) and Git platforms (GitHub, GitLab), allowing for easier and more consistent configuration of filesystem access.</li> <li>Enhancing I/O Operations: It provides extended read/write capabilities for common data formats like JSON, CSV, and Parquet, with integrations for high-performance libraries like Polars and PyArrow.</li> <li>Improving Caching: The library includes an enhanced caching mechanism that preserves directory structures and offers better monitoring.</li> </ul>"},{"location":"architecture/#domain-package-architecture","title":"Domain Package Architecture","text":"<p>The <code>fsspeckit</code> library is organized into domain-specific packages that provide clear boundaries and improved discoverability:</p>"},{"location":"architecture/#fsspeckitcore","title":"<code>fsspeckit.core</code>","text":"<p>Contains the fundamental filesystem APIs and backend-neutral planning logic: - <code>filesystem()</code> - Central factory for creating fsspec-compatible filesystem objects - <code>DirFileSystem</code> - Specialized handling for directory-based filesystems - <code>MonitoredSimpleCacheFileSystem</code> - Enhanced caching with monitoring - <code>GitLabFileSystem</code> - GitLab repository access - Backend-neutral merge and maintenance planning in <code>core.merge</code> and <code>core.maintenance</code></p>"},{"location":"architecture/#fsspeckitstorage_options","title":"<code>fsspeckit.storage_options</code>","text":"<p>Manages storage configurations for cloud and Git providers: - <code>AwsStorageOptions</code> - AWS S3 configuration - <code>GcsStorageOptions</code> - Google Cloud Storage configuration - <code>AzureStorageOptions</code> - Azure Storage configuration - <code>GitHubStorageOptions</code> - GitHub repository access - <code>GitLabStorageOptions</code> - GitLab repository access - Utility functions for protocol inference and option merging</p>"},{"location":"architecture/#fsspeckitdatasets","title":"<code>fsspeckit.datasets</code>","text":"<p>Dataset-level operations for large-scale data processing: - <code>DuckDBParquetHandler</code> - High-performance DuckDB parquet operations - PyArrow dataset helpers for merge, compaction, and optimization - Schema management and type conversion utilities - Backend-neutral planning delegates to core modules</p>"},{"location":"architecture/#fsspeckitsql","title":"<code>fsspeckit.sql</code>","text":"<p>SQL-to-filter translation helpers for data filtering: - <code>sql2pyarrow_filter</code> - Convert SQL to PyArrow filter expressions - <code>sql2polars_filter</code> - Convert SQL to Polars filter expressions - SQL parsing and table name extraction utilities</p>"},{"location":"architecture/#fsspeckitcommon","title":"<code>fsspeckit.common</code>","text":"<p>Cross-cutting utilities shared across all domains: - Logging: <code>setup_logging()</code>, <code>get_logger()</code> for consistent logging - Parallel Processing: <code>run_parallel()</code> for concurrent operations - Type Conversion: <code>dict_to_dataframe()</code>, <code>to_pyarrow_table()</code> for data manipulation - DateTime Utilities: Timestamp parsing and timezone handling - Polars Helpers: DataFrame optimization and manipulation utilities</p>"},{"location":"architecture/#fsspeckitutils","title":"<code>fsspeckit.utils</code>","text":"<p>Backwards-compatible fa\u00e7ade that re-exports selected helpers from domain packages. This module ensures existing code continues to work while new code should import directly from the appropriate domain packages.</p>"},{"location":"architecture/#diagrams","title":"Diagrams","text":"<pre><code>graph TD\n    A[fsspeckit] --&gt; B[Core Package]\n    A --&gt; C[Storage Options]\n    A --&gt; D[Datasets Package]\n    A --&gt; E[SQL Package]\n    A --&gt; F[Common Package]\n    A --&gt; G[Utils Fa\u00e7ade]\n\n    B --&gt; H[Filesystem APIs]\n    B --&gt; I[Caching &amp; Monitoring]\n    B --&gt; J[Merge/Maintenance Logic]\n\n    C --&gt; K[Cloud Providers]\n    C --&gt; L[Git Platforms]\n\n    D --&gt; M[DuckDB Handlers]\n    D --&gt; N[PyArrow Dataset Ops]\n    D --&gt; J\n\n    E --&gt; O[SQL Filter Translation]\n    E --&gt; F\n\n    F --&gt; P[Parallel Processing]\n    F --&gt; Q[Type Conversion]\n    F --&gt; R[Logging]\n    F --&gt; S[DateTime Utils]\n    F --&gt; T[Polars Helpers]\n\n    G --&gt; U[Backwards Compatibility]\n    G --&gt; V[Re-exports from Domain Packages]\n\n    K --&gt; W(AWS S3)\n    K --&gt; X(Google Cloud)\n    K --&gt; Y(Azure Storage)\n    L --&gt; Z(GitHub)\n    L --&gt; AA(GitLab)</code></pre>"},{"location":"contributing/","title":"Contributing to fsspeckit","text":"<p>We welcome contributions to <code>fsspeckit</code>! Your help makes this project better. This guide outlines how you can contribute, from reporting issues to submitting pull requests.</p>"},{"location":"contributing/#how-to-contribute","title":"How to Contribute","text":""},{"location":"contributing/#reporting-issues","title":"Reporting Issues","text":"<p>If you encounter any bugs, unexpected behavior, or have suggestions for new features, please open an issue on our GitHub Issues page.</p> <p>When reporting an issue, please include: - A clear and concise description of the problem. - Steps to reproduce the behavior. - Expected behavior. - Screenshots or error messages if applicable. - Your <code>fsspeckit</code> version and Python environment details.</p>"},{"location":"contributing/#submitting-pull-requests","title":"Submitting Pull Requests","text":"<p>We gladly accept pull requests for bug fixes, new features, and improvements. To submit a pull request:</p> <ol> <li>Fork the Repository: Start by forking the <code>fsspeckit</code> repository on GitHub.</li> <li>Clone Your Fork: Clone your forked repository to your local machine.     <pre><code>git clone https://github.com/your-username/fsspeckit.git\ncd fsspeckit\n</code></pre></li> <li>Create a New Branch: Create a new branch for your changes.     <pre><code>git checkout -b feature/your-feature-name\n# or\ngit checkout -b bugfix/issue-description\n</code></pre></li> <li>Make Your Changes: Implement your bug fix or feature.</li> <li>Write Tests: Ensure your changes are covered by appropriate unit tests.</li> <li>Run Tests: Verify all tests pass before submitting.     <pre><code>uv run pytest\n</code></pre></li> <li>Format Code: Ensure your code adheres to the project's style guidelines. The project uses <code>ruff</code> for linting and formatting.     <pre><code>uv run ruff check . --fix\nuv run ruff format .\n</code></pre></li> <li>Commit Your Changes: Write clear and concise commit messages.     <pre><code>git commit -m \"feat: Add new awesome feature\"\n</code></pre></li> <li>Push to Your Fork: Push your branch to your forked repository.     <pre><code>git push origin feature/your-feature-name\n</code></pre></li> <li>Open a Pull Request: Go to the original <code>fsspeckit</code> repository on GitHub and open a pull request from your new branch. Provide a detailed description of your changes.</li> </ol>"},{"location":"contributing/#development-setup","title":"Development Setup","text":"<p>To set up your development environment, follow these steps:</p> <ol> <li>Clone the repository:     <pre><code>git clone https://github.com/legout/fsspeckit.git\ncd fsspeckit\n</code></pre></li> <li>Install <code>uv</code>:     <code>fsspeckit</code> uses <code>uv</code> for dependency management and running commands. If you don't have <code>uv</code> installed, you can install it via <code>pip</code>:     <pre><code>pip install uv\n</code></pre></li> <li>Install Development Dependencies:     The project uses <code>uv</code> to manage dependencies. Install the <code>dev</code> dependency group which includes tools for testing, linting, and documentation generation.     <pre><code>uv pip install -e \".[dev]\"\n</code></pre>     This command installs the project in editable mode (<code>-e</code>) and includes all development-related dependencies specified in <code>pyproject.toml</code> under the <code>[project.optional-dependencies] dev</code> section.</li> </ol>"},{"location":"contributing/#best-practices-for-contributions","title":"Best Practices for Contributions","text":"<ul> <li>Code Style: Adhere to the existing code style. We use <code>ruff</code> for linting and formatting.</li> <li>Testing: All new features and bug fixes should be accompanied by relevant unit tests.</li> <li>Documentation: If your changes introduce new features or modify existing behavior, please update the documentation accordingly.</li> <li>Commit Messages: Write descriptive commit messages that explain the purpose of your changes.</li> <li>Atomic Commits: Try to keep your commits focused on a single logical change.</li> <li>Branch Naming: Use clear and concise branch names (e.g., <code>feature/new-feature</code>, <code>bugfix/fix-issue-123</code>).</li> </ul>"},{"location":"examples/","title":"Examples Guide","text":"<p>This page provides an overview of the available examples in the <code>fsspeckit</code> repository. Each example is designed to be runnable and demonstrates real-world usage patterns.</p>"},{"location":"examples/#available-examples","title":"Available Examples","text":""},{"location":"examples/#storage-configuration","title":"Storage Configuration","text":"<p>Location: <code>examples/storage_options/</code></p> <p>Demonstrates how to create and use storage option objects for different cloud providers:</p> <pre><code>from fsspeckit.storage_options import (\n    LocalStorageOptions,\n    AwsStorageOptions,\n    GcsStorageOptions,\n    AzureStorageOptions\n)\n\n# Local filesystem\nlocal = LocalStorageOptions(auto_mkdir=True)\nfs = local.to_filesystem()\n\n# AWS S3\naws = AwsStorageOptions(\n    region=\"us-east-1\",\n    access_key_id=\"YOUR_KEY\",\n    secret_access_key=\"YOUR_SECRET\"\n)\nfs = aws.to_filesystem()\n</code></pre> <p>Topics covered: - Creating storage options for different providers - Converting to fsspec filesystems - Environment variable loading - YAML configuration</p>"},{"location":"examples/#directory-filesystem-dirfilesystem","title":"Directory Filesystem (DirFileSystem)","text":"<p>Location: <code>examples/dir_file_system/</code></p> <p>Shows how to use DirFileSystem for treating directories as files:</p> <pre><code>from fsspeckit import filesystem\n\n# Create DirFileSystem for a directory\nfs = filesystem(\"/data/\", dirfs=True)\n\n# Access files within the directory\nfiles = fs.ls(\"/\")\ndata = fs.cat(\"subdir/file.txt\")\n</code></pre> <p>Topics covered: - DirFileSystem creation and usage - Path handling with directory boundaries - Combining with storage options</p>"},{"location":"examples/#caching","title":"Caching","text":"<p>Location: <code>examples/caching/</code></p> <p>Demonstrates how to improve performance using the enhanced caching mechanism:</p> <pre><code>from fsspeckit import filesystem\n\n# Enable caching for S3 operations\nfs = filesystem(\n    \"s3://my-bucket/\",\n    cached=True,\n    cache_storage=\"/tmp/cache\",\n    verbose=True\n)\n\n# First access populates cache\ndata1 = fs.read_json(\"data.json\")\n\n# Subsequent accesses use cache (much faster!)\ndata2 = fs.read_json(\"data.json\")\n</code></pre> <p>Topics covered: - Cache configuration and parameters - Performance monitoring - Cache persistence - Handling cache invalidation</p>"},{"location":"examples/#batch-processing","title":"Batch Processing","text":"<p>Location: <code>examples/batch_processing/</code></p> <p>Shows how to process large numbers of files in batches:</p> <pre><code>from fsspeckit import filesystem\n\nfs = filesystem(\".\")\n\n# Read files in batches to control memory usage\nfor batch_df in fs.read_csv(\"data/*.csv\", batch_size=10):\n    print(f\"Processing batch with {len(batch_df)} rows\")\n</code></pre> <p>Topics covered: - Batch reading of multiple files - Memory-efficient processing - Batch aggregation - Progress tracking</p>"},{"location":"examples/#reading-folders","title":"Reading Folders","text":"<p>Location: <code>examples/read_folder/</code></p> <p>Demonstrates reading multiple files in various formats from a directory:</p> <pre><code>from fsspeckit import filesystem\n\nfs = filesystem(\"/data/\")\n\n# Read all Parquet files and combine\ntable = fs.read_parquet(\"**/*.parquet\", concat=True)\n\n# Read all CSV files as batches\nfor df in fs.read_csv(\"**/*.csv\", batch_size=5):\n    # Process each batch\n    pass\n</code></pre> <p>Topics covered: - Glob patterns for file discovery - Format-specific readers - Schema unification - Recursive directory traversal</p>"},{"location":"examples/#s3r2minio-with-pyarrow-datasets","title":"S3/R2/MinIO with PyArrow Datasets","text":"<p>Location: <code>examples/s3_pyarrow_dataset/</code></p> <p>Shows how to work with partitioned datasets on object storage:</p> <pre><code>from fsspeckit import filesystem\nfrom fsspeckit.storage_options import AwsStorageOptions\n\n# Configure for S3, Cloudflare R2, or MinIO\noptions = AwsStorageOptions(\n    region=\"us-east-1\",\n    access_key_id=\"YOUR_KEY\",\n    secret_access_key=\"YOUR_SECRET\"\n)\n\nfs = filesystem(\"s3\", storage_options=options)\n\n# Read as PyArrow dataset\ndataset = fs.pyarrow_dataset(\"s3://bucket/data/\")\n\n# Perform efficient filtering\nfiltered = dataset.to_table(filter=...)\n</code></pre> <p>Topics covered: - Cloud object storage configuration - PyArrow dataset operations - Partitioned dataset reading - Metadata handling - Predicate pushdown</p>"},{"location":"examples/#delta-lake-integration","title":"Delta Lake Integration","text":"<p>Location: <code>examples/deltalake_delta_table/</code></p> <p>Demonstrates integration with Delta Lake:</p> <pre><code>from deltalake import DeltaTable\nfrom fsspeckit.storage_options import LocalStorageOptions\nimport polars as pl\n\n# Create sample Delta table\ndata = pl.DataFrame({\"id\": [1, 2, 3], \"value\": [\"A\", \"B\", \"C\"]})\ndata.write_delta(\"/path/to/delta_table\")\n\n# Access with fsspeckit storage options\nlocal_opts = LocalStorageOptions()\ndt = DeltaTable(\n    \"/path/to/delta_table\",\n    storage_options=local_opts.to_object_store_kwargs()\n)\n\n# Read data\ntable = dt.to_pyarrow_table()\n</code></pre> <p>Topics covered: - Creating Delta tables - Storage options integration - Reading Delta metadata - Version tracking</p>"},{"location":"examples/#pydala-dataset","title":"PyDala Dataset","text":"<p>Location: <code>examples/__pydala_dataset/</code></p> <p>Shows how to work with Pydala datasets:</p> <pre><code>from fsspeckit import filesystem\n\nfs = filesystem(\".\")\n\n# Read/write Pydala datasets\ndataset = fs.pydala_dataset(\"/path/to/dataset\")\n\n# Access as different formats\narrow_table = dataset.to_arrow()\npolars_df = dataset.to_polars()\n</code></pre> <p>Topics covered: - Pydala dataset format - Format conversions - Dataset metadata</p>"},{"location":"examples/#running-examples","title":"Running Examples","text":""},{"location":"examples/#prerequisites","title":"Prerequisites","text":"<p>Install fsspeckit with all optional dependencies:</p> <pre><code>pip install \"fsspeckit[aws,gcp,azure]\"\n</code></pre> <p>For Delta Lake examples:</p> <pre><code>pip install deltalake\n</code></pre>"},{"location":"examples/#execution","title":"Execution","text":"<p>Most examples can be run directly:</p> <pre><code># Run a specific example\npython examples/caching/caching_example.py\n\n# Run from Python REPL\nimport sys\nsys.path.insert(0, '.')\nexec(open('examples/dir_file_system/dir_file_system_example.py').read())\n\n# Run with uv (if you have it installed)\nuv run examples/batch_processing/batch_processing_example.py\n</code></pre>"},{"location":"examples/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<p>Examples are available as both <code>.py</code> files and <code>.ipynb</code> Jupyter notebooks for interactive exploration:</p> <pre><code>jupyter notebook examples/s3_pyarrow_dataset/s3_pyarrow_dataset.ipynb\n</code></pre>"},{"location":"examples/#example-naming-conventions","title":"Example Naming Conventions","text":"<ul> <li><code>*_example.py</code> - Standard Python script version</li> <li><code>*_example.ipynb</code> - Jupyter notebook version</li> <li><code>*_example_mamo.py</code> - Alternative implementation (if available)</li> </ul>"},{"location":"examples/#contributing-examples","title":"Contributing Examples","text":"<p>To contribute a new example:</p> <ol> <li>Create a new subdirectory under <code>examples/</code> with a descriptive name</li> <li>Add both <code>.py</code> and <code>.ipynb</code> versions</li> <li>Include sample data generation if needed</li> <li>Add docstrings explaining the example</li> <li>Update this guide with the new example</li> </ol>"},{"location":"examples/#quick-reference","title":"Quick Reference","text":"Use Case Example Key Methods Cloud storage access <code>storage_options/</code> <code>AwsStorageOptions.from_env()</code> Local directory handling <code>dir_file_system/</code> <code>filesystem(..., dirfs=True)</code> Performance optimization <code>caching/</code> <code>filesystem(..., cached=True)</code> Large data processing <code>batch_processing/</code> <code>fs.read_csv(..., batch_size=N)</code> Multi-format reading <code>read_folder/</code> <code>fs.read_files(..., format='auto')</code> Object storage datasets <code>s3_pyarrow_dataset/</code> <code>fs.pyarrow_dataset(...)</code> Data lake integration <code>deltalake_delta_table/</code> <code>DeltaTable(..., storage_options=...)</code>"},{"location":"examples/#troubleshooting-examples","title":"Troubleshooting Examples","text":"<p>Missing dependencies: Install with <code>pip install \"fsspeckit[aws,gcp,azure]\"</code></p> <p>Cloud credentials not found: Set environment variables or update examples with credentials</p> <p>Out of memory: Reduce batch size in batch processing examples</p> <p>Network errors: Check connectivity to cloud services in cloud storage examples</p> <p>For more help, see the Advanced Usage guide or check individual example source code.</p>"},{"location":"installation/","title":"Installation","text":"<p><code>fsspeckit</code> can be installed using <code>pip</code>, the Python package installer.</p>"},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.11 or higher is required.</li> </ul>"},{"location":"installation/#install-with-pip","title":"Install with pip","text":"<p>To install <code>fsspeckit</code> using <code>pip</code>, run the following command:</p> <pre><code>pip install fsspeckit\n</code></pre>"},{"location":"installation/#optional-cloud-provider-support","title":"Optional Cloud Provider Support","text":"<p>Install with support for specific cloud providers:</p> <pre><code># AWS S3 support\npip install \"fsspeckit[aws]\"\n\n# Google Cloud Storage support\npip install \"fsspeckit[gcp]\"\n\n# Azure Storage support\npip install \"fsspeckit[azure]\"\n\n# All cloud providers\npip install \"fsspeckit[aws,gcp,azure]\"\n</code></pre>"},{"location":"installation/#upgrading","title":"Upgrading","text":"<p>To upgrade <code>fsspeckit</code> to the latest version, use:</p> <pre><code>pip install --upgrade fsspeckit\n</code></pre>"},{"location":"installation/#environment-management-with-uv-and-pixi","title":"Environment Management with <code>uv</code> and <code>pixi</code>","text":"<p>For robust dependency management and faster installations, we recommend using <code>uv</code> or <code>pixi</code>.</p>"},{"location":"installation/#using-uv","title":"Using <code>uv</code>","text":"<p><code>uv</code> is a fast Python package installer and resolver. To install <code>fsspeckit</code> with <code>uv</code>:</p> <pre><code>uv pip install fsspeckit\n</code></pre>"},{"location":"installation/#using-pixi","title":"Using <code>pixi</code>","text":"<p><code>pixi</code> is a modern package manager for Python and other languages. To add <code>fsspeckit</code> to your <code>pixi</code> project:</p> <pre><code>pixi add fsspeckit\n</code></pre>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter any issues during installation, consider the following:</p> <ul> <li>Python Version: Ensure you are using Python 3.11 or higher. You can check your Python version with <code>python --version</code>.</li> <li>Virtual Environments: It is highly recommended to use a virtual environment (e.g., <code>venv</code>, <code>conda</code>, <code>uv</code>, <code>pixi</code>) to avoid conflicts with system-wide packages.</li> <li>Permissions: If you encounter permission errors, you might need to run the installation command with <code>sudo</code> (e.g., <code>sudo pip install fsspeckit</code>), but this is generally not recommended in a virtual environment.</li> <li>Network Issues: Check your internet connection if the installation fails to download packages.</li> </ul> <p>For further assistance, please refer to the official fsspeckit GitHub repository or open an issue.</p>"},{"location":"quickstart/","title":"Quickstart","text":"<p>This guide will help you get started with <code>fsspeckit</code> by demonstrating how to create and interact with a directory-based filesystem for local paths.</p>"},{"location":"quickstart/#installation","title":"Installation","text":"<p>First, ensure you have <code>fsspeckit</code> installed.</p> <pre><code>pip install fsspeckit\n</code></pre>"},{"location":"quickstart/#basic-usage-local-directory-filesystem","title":"Basic Usage: Local Directory FileSystem","text":"<p><code>fsspeckit</code> simplifies working with various file systems by providing a unified interface. Here, we'll create a <code>DirFileSystem</code> for a local directory.</p> <p>The <code>filesystem</code> function from <code>fsspeckit</code> allows you to instantiate a file system object. By setting <code>dirfs=True</code>, you indicate that you want a directory-based filesystem, which treats directories as files themselves.</p> <p>Let's create a local directory and then instantiate a <code>DirFileSystem</code> for it:</p> <pre><code>import os\nfrom fsspeckit import filesystem\n\n# Define a local directory path\nlocal_dir_path = \"./my_local_data/\"\n\n# Ensure the directory exists\nos.makedirs(local_dir_path, exist_ok=True)\n\n# Create a DirFileSystem for the local path\nfs_dir_local = filesystem(local_dir_path, dirfs=True)\n\nprint(f\"Local DirFileSystem created: {fs_dir_local}\")\n\n# You can now use the fs_dir_local object to interact with the directory\n# For example, to list its contents (initially empty)\nprint(f\"Contents of {local_dir_path}: {fs_dir_local.ls('/')}\")\n\n# Let's create a dummy file inside the directory\nwith fs_dir_local.open(\"test_file.txt\", \"w\") as f:\n    f.write(\"Hello, fsspeckit!\")\n\nprint(f\"Contents after creating test_file.txt: {fs_dir_local.ls('/')}\")\n\n# Read the content of the dummy file\nwith fs_dir_local.open(\"test_file.txt\", \"r\") as f:\n    content = f.read()\nprint(f\"Content of test_file.txt: {content}\")\n\n# Clean up the created directory and file\nfs_dir_local.rm(\"test_file.txt\")\nos.rmdir(local_dir_path)\nprint(f\"Cleaned up {local_dir_path}\")\n</code></pre>"},{"location":"quickstart/#explanation","title":"Explanation","text":"<ol> <li><code>import os</code> and <code>from fsspeckit import filesystem</code>: We import the necessary modules. <code>os</code> is used here to ensure the local directory exists, and <code>filesystem</code> is the core function from <code>fsspeckit</code>.</li> <li><code>local_dir_path = \"./my_local_data/\"</code>: We define a relative path for our local directory.</li> <li><code>os.makedirs(local_dir_path, exist_ok=True)</code>: This line creates the <code>my_local_data</code> directory if it doesn't already exist.</li> <li><code>fs_dir_local = filesystem(local_dir_path, dirfs=True)</code>: This is where <code>fsspeckit</code> comes into play. We create a <code>DirFileSystem</code> instance pointing to our local directory. The <code>dirfs=True</code> argument is crucial for enabling directory-level operations.</li> <li><code>fs_dir_local.ls('/')</code>: We use the <code>ls</code> method of our <code>fs_dir_local</code> object to list the contents of the root of our <code>my_local_data</code> directory. Initially, it will be empty.</li> <li><code>fs_dir_local.open(\"test_file.txt\", \"w\")</code>: We demonstrate writing a file within our <code>DirFileSystem</code> using the <code>open</code> method, similar to Python's built-in <code>open</code>.</li> <li><code>fs_dir_local.open(\"test_file.txt\", \"r\")</code>: We demonstrate reading the content of the file we just created.</li> <li><code>fs_dir_local.rm(\"test_file.txt\")</code> and <code>os.rmdir(local_dir_path)</code>: Finally, we clean up by removing the created file and the directory.</li> </ol> <p>This example provides a basic overview of how to use <code>fsspeckit</code> to interact with a local directory as a filesystem. The same <code>filesystem</code> function can be used for various other storage backends like S3, GCS, HDFS, etc., by simply changing the path and providing appropriate <code>storage_options</code>.</p>"},{"location":"utils/","title":"Utilities Reference","text":"<p>This page documents utilities available in fsspeckit. The utilities are organized into domain packages for better discoverability and maintainability.</p> <p>Package Layout Overview: fsspeckit is organized into domain packages - see Architecture for details. Backwards Compatibility: All imports from <code>fsspeckit.utils</code> continue to work unchanged.</p>"},{"location":"utils/#cross-cutting-utilities-fsspeckitcommon","title":"Cross-Cutting Utilities (<code>fsspeckit.common</code>)","text":""},{"location":"utils/#logging","title":"Logging","text":""},{"location":"utils/#setup_logging","title":"<code>setup_logging()</code>","text":"<p>Configure logging throughout your application with loguru:</p> <pre><code>from fsspeckit.common.logging import setup_logging\n\n# Basic setup\nsetup_logging()\n\n# With custom level and format\nsetup_logging(level=\"DEBUG\", format_string=\"{time} | {level} | {message}\")\n\n# Control logging via environment variable\n# export fsspeckit_LOG_LEVEL=DEBUG\n</code></pre> <p>Environment Variables: - <code>fsspeckit_LOG_LEVEL</code> - Set the logging level (default: INFO)</p>"},{"location":"utils/#parallel-processing","title":"Parallel Processing","text":""},{"location":"utils/#run_parallel","title":"<code>run_parallel()</code>","text":"<p>Execute a function across multiple inputs using parallel threads with optional progress bar:</p> <pre><code>from fsspeckit.common.misc import run_parallel\n\ndef process_file(path, multiplier=1):\n    return len(path) * multiplier\n\nresults = run_parallel(\n    process_file,\n    [\"/path1\", \"/path2\", \"/path3\"],\n    multiplier=2,\n    n_jobs=4,\n    verbose=True,  # Show progress bar\n    backend=\"threading\"\n)\n</code></pre> <p>Parameters: - <code>func</code> - Function to apply to each item - <code>items</code> - List of inputs to process - <code>n_jobs</code> - Number of parallel threads (default: CPU count) - <code>backend</code> - Parallel backend: \"threading\" or \"process\" (default: \"threading\")</p>"},{"location":"utils/#file-system-operations","title":"File System Operations","text":""},{"location":"utils/#file-synchronization","title":"File Synchronization","text":"<pre><code>from fsspeckit.common.misc import sync_files, sync_dir\n\n# Sync individual files\nsync_files(\n    source_paths=[\"/data/file1.txt\", \"/data/file2.txt\"],\n    fs_target=filesystem(\"s3://bucket/\"),\n    verbose=True\n)\n\n# Sync directories recursively\nsync_dir(\n    source_dir=\"/data/\",\n    fs_target=filesystem(\"s3://bucket/\"),\n    pattern=\"*.parquet\",\n    delete=True\n)\n</code></pre>"},{"location":"utils/#path-utilities","title":"Path Utilities","text":"<pre><code>from fsspeckit.common.misc import get_partitions_from_path\n\n# Extract partition information from paths\npartitions = get_partitions_from_path(\"/data/year=2023/month=01/file.parquet\")\n# Returns: {'year': '2023', 'month': '01'}\n</code></pre>"},{"location":"utils/#type-conversion","title":"Type Conversion","text":""},{"location":"utils/#dataframe-conversion","title":"DataFrame Conversion","text":"<pre><code>from fsspeckit.common.types import dict_to_dataframe, to_pyarrow_table\n\n# Convert dict to DataFrame\ndata = {\"col1\": [1, 2, 3], \"col2\": [4, 5, 6]}\ndf = dict_to_dataframe(data)\n\n# Convert to PyArrow table\ntable = to_pyarrow_table(df)\n</code></pre>"},{"location":"utils/#datetime-utilities","title":"DateTime Utilities","text":"<pre><code>from fsspeckit.common.datetime import timestamp_from_string, get_timestamp_column\n\n# Parse timestamp strings\nts = timestamp_from_string(\"2023-01-15 10:30:00\")\nts_with_tz = timestamp_from_string(\"2023-01-15T10:30:00Z\", tz=\"UTC\")\n\n# Find timestamp column in DataFrame\ntimestamp_cols = get_timestamp_column(df)\n</code></pre>"},{"location":"utils/#polars-optimization","title":"Polars Optimization","text":"<pre><code>from fsspeckit.common.polars import opt_dtype_pl\n\n# Optimize DataFrame data types\ndf_optimized = opt_dtype_pl(df, shrink_numerics=True)\n\n# Optimize specific columns\ndf_optimized = opt_dtype_pl(df, columns=[\"numeric_col\"])\n</code></pre>"},{"location":"utils/#dataset-operations-fsspeckitdatasets","title":"Dataset Operations (<code>fsspeckit.datasets</code>)","text":""},{"location":"utils/#duckdb-dataset-handler","title":"DuckDB Dataset Handler","text":""},{"location":"utils/#duckdbparquethandler","title":"<code>DuckDBParquetHandler</code>","text":"<p>High-performance parquet dataset operations using DuckDB:</p> <pre><code>from fsspeckit.datasets import DuckDBParquetHandler\n\nwith DuckDBParquetHandler() as handler:\n    # Dataset maintenance operations\n    handler.compact_parquet_dataset(\n        path=\"/data/events/\",\n        target_rows_per_file=500_000\n    )\n\n    # Data analytics\n    handler.optimize_parquet_dataset(\n        path=\"/data/events/\",\n        zorder_columns=[\"user_id\", \"timestamp\"]\n    )\n</code></pre>"},{"location":"utils/#pyarrow-dataset-helpers","title":"PyArrow Dataset Helpers","text":""},{"location":"utils/#schema-management","title":"Schema Management","text":"<pre><code>from fsspeckit.datasets import cast_schema, convert_large_types_to_normal\n\n# Convert to standard schema\nstandard_schema = convert_large_types_normal(original_schema)\n\n# Cast table to schema\ntable_casted = cast_schema(table, target_schema)\n</code></pre>"},{"location":"utils/#data-type-optimization","title":"Data Type Optimization","text":"<pre><code>from fsspeckit.datasets import opt_dtype_pa\n\n# Optimize PyArrow table data types\ntable_optimized = opt_dtype_pa(table)\n</code></pre>"},{"location":"utils/#dataset-merging","title":"Dataset Merging","text":"<pre><code>from fsspeckit.datasets.pyarrow import merge_parquet_dataset_pyarrow\n\n# Merge multiple datasets\nmerge_parquet_pyarrow(\n    dataset_paths=[\"/data/part1/\", \"/data/part2/\"],\n    target_path=\"/data/merged/\",\n    key_columns=[\"id\"]\n)\n</code></pre>"},{"location":"utils/#sql-filtering-fsspeckitsql","title":"SQL Filtering (<code>fsspeckit.sql</code>)","text":""},{"location":"utils/#sql-to-expression-translation","title":"SQL to Expression Translation","text":""},{"location":"utils/#sql2pyarrow_filter","title":"<code>sql2pyarrow_filter()</code>","text":"<p>Convert SQL WHERE clauses to PyArrow filter expressions:</p> <pre><code>from fsspeckit.sql.filters import sql2pyarrow_filter\n\nimport pyarrow as pa\n\nschema = pa.schema([\n    (\"id\", pa.int64()),\n    (\"name\", pa.string()),\n    (\"timestamp\", pa.timestamp(\"us\")),\n    (\"value\", pa.float64())\n])\n\n# Convert SQL to PyArrow filter\nfilter_expr = sql2pyarrow_filter(\"name = 'test' AND value &gt; 100\", schema)\n</code></pre>"},{"location":"utils/#sql2polars_filter","title":"<code>sql2polars_filter()</code>","text":"<p>Convert SQL WHERE clauses to Polars expressions:</p> <pre><code>from fsspeckit.sql.filters import sql2polars_filter\n\nimport polars as pl\n\n# Convert SQL to Polars filter\nfilter_expr = sql2polars_filter(\"name == 'test' AND value &gt; 100\")\n</code></pre>"},{"location":"utils/#backwards-compatibility-fsspeckitutils","title":"Backwards Compatibility (<code>fsspeckit.utils</code>)","text":"<p>The <code>fsspeckit.utils</code> module provides a backwards-compatible fa\u00e7ade that re-exports selected helpers from the domain packages.</p>"},{"location":"utils/#legacy-import-examples","title":"Legacy Import Examples","text":"<pre><code># These imports continue to work for backwards compatibility\nfrom fsspeckit.utils import (\n    setup_logging,\n    run_parallel,\n    DuckDBParquetHandler,\n    sql2pyarrow_filter,\n    dict_to_dataframe,\n    to_pyarrow_table,\n    opt_dtype_pl,\n    opt_dtype_pa\n)\n</code></pre>"},{"location":"utils/#migration-recommendation","title":"Migration Recommendation","text":"<p>For new code, prefer importing directly from domain packages:</p> <pre><code># Recommended (new code)\nfrom fsspeckit.datasets import DuckDBParquetHandler\nfrom fsspeckit.common.logging import setup_logging\nfrom fsspeckit.sql.filters import sql2pyarrow_filter\n\n# Legacy (existing code - still works)\nfrom fsspeckit.utils import DuckDBParquetHandler, setup_logging, sql2pyarrow_filter\n</code></pre> <p>For detailed migration instructions, see the Migration Guide.</p>"},{"location":"api/","title":"<code>fsspeckit</code> API Reference","text":"<p>Welcome to the <code>fsspeckit</code> API reference documentation. This section provides detailed information on the various modules, classes, and functions available in the library.</p> <p>Package Structure: fsspeckit is organized into domain packages. See Architecture for details.</p>"},{"location":"api/#core-modules","title":"Core Modules","text":"<ul> <li> <p><code>fsspeckit.core.base</code></p> </li> <li> <p><code>fsspeckit.core.ext</code></p> </li> </ul>"},{"location":"api/#storage-options","title":"Storage Options","text":"<ul> <li> <p><code>fsspeckit.storage_options.base</code></p> </li> <li> <p><code>fsspeckit.storage_options.cloud</code></p> </li> <li> <p><code>fsspeckit.storage_options.core</code></p> </li> <li> <p><code>fsspeckit.storage_options.git</code></p> </li> </ul>"},{"location":"api/#utils-backwards-compatibility","title":"Utils (Backwards Compatibility)","text":"<ul> <li> <p><code>fsspeckit.utils.datetime</code> - Date and time utilities</p> </li> <li> <p><code>fsspeckit.utils.logging</code> - Logging configuration and utilities</p> </li> <li> <p><code>fsspeckit.utils.misc</code> - Miscellaneous utility functions</p> </li> <li> <p><code>fsspeckit.utils.polars</code> - Polars DataFrame utilities</p> </li> <li> <p><code>fsspeckit.utils.pyarrow</code> - PyArrow utilities and integrations</p> </li> <li> <p><code>fsspeckit.utils.sql</code> - SQL query and filter utilities</p> </li> <li> <p><code>fsspeckit.utils.types</code> - Type definitions and utilities</p> </li> </ul> <p>Note: API documentation for the new domain packages (datasets, sql, common) will be added in a future update. The utils modules above provide backwards compatibility for all functionality.</p>"},{"location":"api/fsspeckit.core.base/","title":"<code>fsspeckit.core.base</code> API Documentation","text":"<p>This module provides core filesystem functionalities and utilities, including custom cache mappers, enhanced cached filesystems, and a GitLab filesystem implementation.</p>"},{"location":"api/fsspeckit.core.base/#filenamecachemapper","title":"<code>FileNameCacheMapper</code>","text":"<p>Maps remote file paths to local cache paths while preserving directory structure.</p> <p>This cache mapper maintains the original file path structure in the cache directory, creating necessary subdirectories as needed.</p> <p>Attributes:</p> <ul> <li><code>directory</code> (<code>str</code>): Base directory for cached files</li> </ul> <p>Example:</p> <pre><code>from fsspeckit.core.base import FileNameCacheMapper\n\n# Create cache mapper for S3 files\nmapper = FileNameCacheMapper(\"/tmp/cache\")\n\n# Map remote path to cache path\ncache_path = mapper(\"bucket/data/file.csv\")\nprint(cache_path)  # Preserves structure\n</code></pre>"},{"location":"api/fsspeckit.core.base/#__init__","title":"<code>__init__()</code>","text":"<p>Initialize cache mapper with base directory.</p> Parameter Type Description <code>directory</code> <code>str</code> Base directory where cached files will be stored"},{"location":"api/fsspeckit.core.base/#__call__","title":"<code>__call__()</code>","text":"<p>Map remote file path to cache file path.</p> <p>Creates necessary subdirectories in the cache directory to maintain the original path structure.</p> Parameter Type Description <code>path</code> <code>str</code> Original file path from remote filesystem Returns Type Description <code>str</code> <code>str</code> Cache file path that preserves original structure <p>Example:</p> <pre><code>from fsspeckit.core.base import FileNameCacheMapper\n\nmapper = FileNameCacheMapper(\"/tmp/cache\")\n# Maps maintain directory structure\nprint(mapper(\"data/nested/file.txt\"))\n</code></pre>"},{"location":"api/fsspeckit.core.base/#monitoredsimplecachefilesystem","title":"<code>MonitoredSimpleCacheFileSystem</code>","text":"<p>Enhanced caching filesystem with monitoring and improved path handling.</p> <p>This filesystem extends <code>SimpleCacheFileSystem</code> to provide:</p> <ul> <li>Verbose logging of cache operations</li> <li>Improved path mapping for cache files</li> <li>Enhanced synchronization capabilities</li> <li>Better handling of parallel operations</li> </ul> <p>Attributes:</p> <ul> <li><code>_verbose</code> (<code>bool</code>): Whether to print verbose cache operations</li> <li><code>_mapper</code> (<code>FileNameCacheMapper</code>): Maps remote paths to cache paths</li> <li><code>storage</code> (<code>list[str]</code>): List of cache storage locations</li> <li><code>fs</code> (<code>AbstractFileSystem</code>): Underlying filesystem being cached</li> </ul> <p>Example:</p> <pre><code>from fsspec import filesystem\nfrom fsspeckit.core.base import MonitoredSimpleCacheFileSystem\n\ns3_fs = filesystem(\"s3\")\ncached_fs = MonitoredSimpleCacheFileSystem(\n    fs=s3_fs,\n    cache_storage=\"/tmp/cache\",\n    verbose=True\n)\n# Use cached_fs like any other filesystem\nfiles = cached_fs.ls(\"my-bucket/\")\n</code></pre>"},{"location":"api/fsspeckit.core.base/#__init___1","title":"<code>__init__()</code>","text":"<p>Initialize monitored cache filesystem.</p> Parameter Type Description <code>fs</code> <code>Optional[fsspec.AbstractFileSystem]</code> Underlying filesystem to cache. If None, creates a local filesystem. <code>cache_storage</code> <code>Union[str, list[str]]</code> Cache storage location(s). Can be string path or list of paths. <code>verbose</code> <code>bool</code> Whether to enable verbose logging of cache operations. <code>**kwargs</code> <code>Any</code> Additional arguments passed to <code>SimpleCacheFileSystem</code>. <p>Example:</p> <pre><code># Cache S3 filesystem\ns3_fs = filesystem(\"s3\")\ncached = MonitoredSimpleCacheFileSystem(\n    fs=s3_fs,\n    cache_storage=\"/tmp/s3_cache\",\n    verbose=True\n)\n</code></pre>"},{"location":"api/fsspeckit.core.base/#_check_cache","title":"<code>_check_cache()</code>","text":"<p>Check if file exists in cache and return cache path if found.</p> Parameter Type Description <code>path</code> <code>str</code> Remote file path to check Returns Type Descript <code>Optional[str]</code> <code>str</code> or <code>None</code> Cache file path if found, None otherwise <p>Example:</p> <pre><code>from fsspeckit.core.base import MonitoredSimpleCacheFileSystem\nfrom fsspec import filesystem\n\ncached_fs = MonitoredSimpleCacheFileSystem(fs=filesystem(\"s3\"), cache_storage=\"/tmp/cache\")\n# Check if a file is in cache\ncache_path = cached_fs._check_cache(\"my-bucket/data/file.txt\")\nif cache_path:\n    print(f\"File found in cache at: {cache_path}\")\nelse:\n    print(\"File not in cache.\")\n</code></pre>"},{"location":"api/fsspeckit.core.base/#_check_file","title":"<code>_check_file()</code>","text":"<p>Ensure file is in cache, downloading if necessary.</p> Parameter Type Description <code>path</code> <code>str</code> Remote file path Returns Type Description <code>str</code> <code>str</code> Local cache path for the file <p>Example:</p> <pre><code>from fsspeckit.core.base import MonitoredSimpleCacheFileSystem\nfrom fsspec import filesystem\n\ncached_fs = MonitoredSimpleCacheFileSystem(fs=filesystem(\"s3\"), cache_storage=\"/tmp/cache\")\n# Ensure file is in cache (downloads if not present)\nlocal_path = cached_fs._check_file(\"my-bucket/data/large_file.parquet\")\nprint(f\"File available locally at: {local_path}\")\n</code></pre>"},{"location":"api/fsspeckit.core.base/#gitlabfilesystem","title":"<code>GitLabFileSystem</code>","text":"<p>Filesystem interface for GitLab repositories.</p> <p>Provides read-only access to files in GitLab repositories, including:</p> <ul> <li>Public and private repositories</li> <li>Self-hosted GitLab instances</li> <li>Branch/tag/commit selection</li> <li>Token-based authentication</li> </ul> <p>Attributes:</p> <ul> <li><code>protocol</code> (<code>str</code>): Always \"gitlab\"</li> <li><code>base_url</code> (<code>str</code>): GitLab instance URL</li> <li><code>project_id</code> (<code>str</code>): Project ID</li> <li><code>project_name</code> (<code>str</code>): Project name/path</li> <li><code>ref</code> (<code>str</code>): Git reference (branch, tag, commit)</li> <li><code>token</code> (<code>str</code>): Access token</li> <li><code>api_version</code> (<code>str</code>): API version</li> </ul> <p>Example:</p> <pre><code># Public repository\nfs = GitLabFileSystem(\n    project_name=\"group/project\",\n    ref=\"main\"\n)\nfiles = fs.ls(\"/\")\n\n# Private repository with token\nfs = GitLabFileSystem(\n    project_id=\"12345\",\n    token=\"glpat_xxxx\",\n    ref=\"develop\"\n)\ncontent = fs.cat(\"README.md\")\n</code></pre>"},{"location":"api/fsspeckit.core.base/#__init___2","title":"<code>__init__()</code>","text":"<p>Initialize GitLab filesystem.</p> Parameter Type Description <code>base_url</code> <code>str</code> GitLab instance URL <code>project_id</code> <code>Optional[Union[str, int]]</code> Project ID number <code>project_name</code> <code>Optional[str]</code> Project name/path (alternative to project_id) <code>ref</code> <code>str</code> Git reference (branch, tag, or commit SHA) <code>token</code> <code>Optional[str]</code> GitLab personal access token <code>api_version</code> <code>str</code> API version to use <p>| <code>**kwargs</code> | <code>Any</code> | Additional filesystem arguments |</p> Raises Type Description <code>ValueError</code> <code>ValueError</code> If neither <code>project_id</code> nor <code>project_name</code> is provided <p>Example:</p> <pre><code>from fsspeckit.core.base import GitLabFileSystem\n\n# Access a public repository\nfs_public = GitLabFileSystem(\n    project_name=\"gitlab-org/gitlab\",\n    ref=\"master\"\n)\nprint(fs_public.ls(\"README.md\"))\n\n# Access a private repository (replace with your token and project info)\n# fs_private = GitLabFileSystem(\n#     project_id=\"12345\",\n# #    token=\"your_private_token\",\n#     ref=\"main\"\n# )\n# print(fs_private.ls(\"/\"))\n</code></pre>"},{"location":"api/fsspeckit.core.base/#_get_file_content","title":"<code>_get_file_content()</code>","text":"<p>Get file content from GitLab API.</p> Parameter Type Description <code>path</code> <code>str</code> File path in repository Returns Type Description <code>bytes</code> <code>bytes</code> File content as bytes <p>Example:</p> <pre><code>from fsspeckit.core.base import GitLabFileSystem\n\nfs = GitLabFileSystem(project_name=\"gitlab-org/gitlab\")\ncontent = fs.cat(\"README.md\")\nprint(content[:50])\n</code></pre> Raises Type Description <code>FileNotFoundError</code> <code>FileNotFoundError</code> If file doesn't exist <code>requests.HTTPError</code> <code>requests.HTTPError</code> For other HTTP errors"},{"location":"api/fsspeckit.core.base/#_open","title":"<code>_open()</code>","text":"<p>Open file for reading.</p> Parameter Type Description <code>path</code> <code>str</code> File path to open <code>mode</code> <code>str</code> File mode (only 'rb' and 'r' supported) <code>block_size</code> <code>Optional[int]</code> Block size for reading (unused) <code>cache_options</code> <code>Optional[dict]</code> Cache options (unused) <code>**kwargs</code> <code>Any</code> Additional options Returns Type Description File-like object File-like object File-like object for reading Raises Type Description <code>ValueError</code> <code>ValueError</code> If mode is not supported"},{"location":"api/fsspeckit.core.base/#cat","title":"<code>cat()</code>","text":"<p>Get file contents as bytes.</p> Parameter Type Description <code>path</code> <code>str</code> File path <code>**kwargs</code> <code>Any</code> Additional options Returns Type Description <code>bytes</code> <code>bytes</code> File content as bytes"},{"location":"api/fsspeckit.core.base/#ls","title":"<code>ls()</code>","text":"<p>List directory contents.</p> Parameter Type Description <code>path</code> <code>str</code> Directory path to list <code>detail</code> <code>bool</code> Whether to return detailed information <code>**kwargs</code> <code>Any</code> Additional options Returns Type Description <code>list</code> <code>list</code> List of files/directories or their details"},{"location":"api/fsspeckit.core.base/#exists","title":"<code>exists()</code>","text":"<p>Check if file or directory exists.</p> Parameter Type Description <code>path</code> <code>str</code> Path to check <code>**kwargs</code> <code>Any</code> Additional options Returns Type Description <code>bool</code> <code>bool</code> True if path exists, False otherwise"},{"location":"api/fsspeckit.core.base/#info","title":"<code>info()</code>","text":"<p>Get file information.</p> Parameter Type Description <code>path</code> <code>str</code> File path <code>**kwargs</code> <code>Any</code> Additional options Returns Type Description <code>dict</code> <code>dict</code> Dictionary with file information Raises Type Description <code>FileNotFoundError</code> <code>FileNotFoundError</code> If file not found"},{"location":"api/fsspeckit.core.base/#filesystem","title":"<code>filesystem()</code>","text":"<p>Get filesystem instance with enhanced configuration options.</p> <p>Creates filesystem instances with support for storage options classes, intelligent caching, and protocol inference from paths.</p> Parameter Type Description <code>protocol_or_path</code> <code>str</code> Filesystem protocol (e.g., \"s3\", \"file\") or path with protocol prefix <code>storage_options</code> <code>Optional[Union[BaseStorageOptions, dict]]</code> Storage configuration as <code>BaseStorageOptions</code> instance or dict <code>cached</code> <code>bool</code> Whether to wrap filesystem in caching layer <code>cache_storage</code> <code>Optional[str]</code> Cache directory path (if <code>cached=True</code>) <code>verbose</code> <code>bool</code> Enable verbose logging for cache operations <code>dirfs</code> <code>bool</code> Whether to wrap the filesystem in a <code>DirFileSystem</code>. Defaults to <code>True</code>. <code>base_fs</code> <code>AbstractFileSystem</code> An existing filesystem to wrap. <code>**kwargs</code> <code>Any</code> Additional filesystem arguments <p>| Ret | :------ | :--- | :---------- | | <code>AbstractFileSystem</code> | <code>fsspec.AbstractFileSystem</code> | Configured filesystem instance |</p> <p>Example:</p> <pre><code># Basic local filesystem\nfs = filesystem(\"file\")\n\n# S3 with storage options\nfrom fsspeckit.storage_options.cloud import AwsStorageOptions\nopts = AwsStorageOptions(region=\"us-west-2\")\nfs = filesystem(\"s3\", storage_options=opts, cached=True)\n\n# Infer protocol from path\nfs = filesystem(\"s3://my-bucket/\", cached=True)\n\n# GitLab filesystem\nfs = filesystem(\"gitlab\", storage_options={\n    \"project_name\": \"group/project\",\n    \"token\": \"glpat_xxxx\"\n})\n</code></pre>"},{"location":"api/fsspeckit.core.base/#get_filesystem","title":"<code>get_filesystem()</code>","text":"<p>Get filesystem instance with enhanced configuration options.</p> <p>Deprecated</p> <p>Use <code>filesystem</code> instead. This function will be removed in a future version.</p> <p>Creates filesystem instances with support for storage options classes, intelligent caching, and protocol inference from paths.</p> Parameter Type Description <code>protocol_or_path</code> <code>str</code> Filesystem protocol (e.g., \"s3\", \"file\") or path with protocol prefix <code>storage_options</code> <code>Optional[Union[BaseStorageOptions, dict]]</code> Storage configuration as <code>BaseStorageOptions</code> instance or dict <code>cached</code> <code>bool</code> Whether to wrap filesystem in caching layer <code>cache_storage</code> <code>Optional[str]</code> Cache directory path (if <code>cached=True</code>) <code>verbose</code> <code>bool</code> Enable verbose logging for cache operations <code>**kwargs</code> <code>Any</code> Additional filesystem arguments Returns Type Description <code>fsspec.AbstractFileSystem</code> <code>fsspec.AbstractFileSystem</code> Configured filesystem instance <p>Example:</p> <pre><code># Basic local filesystem\nfs = get_filesystem(\"file\")\n\n# S3 with storage options\nfrom fsspeckit.storage_options.cloud import AwsStorageOptions\nopts = AwsStorageOptions(region=\"us-west-2\")\nfs = get_filesystem(\"s3\", storage_options=opts, cached=True)\n\n# Infer protocol from path\nfs = get_filesystem(\"s3://my-bucket/\", cached=True)\n\n# GitLab filesystem\nfs = get_filesystem(\"gitlab\", storage_options={\n    \"project_name\": \"group/project\",\n    \"token\": \"glpat_xxxx\"\n})\n</code></pre>"},{"location":"api/fsspeckit.core.ext/","title":"<code>fsspeckit.core.ext</code> API Documentation","text":"<p>This module provides extended functionalities for <code>fsspec.AbstractFileSystem</code>, including methods for reading and writing various file formats (JSON, CSV, Parquet) with advanced options like batch processing, parallelization, and data type optimization. It also includes functions for creating PyArrow datasets.</p>"},{"location":"api/fsspeckit.core.ext/#path_to_glob","title":"<code>path_to_glob()</code>","text":"<p>Convert a path to a glob pattern for file matching.</p> <p>Intelligently converts paths to glob patterns that match files of the specified format, handling various directory and wildcard patterns.</p> Parameter Type Description <code>path</code> <code>str</code> Base path to convert. Can include wildcards (<code>*</code> or <code>**</code>). Examples: \"data/\", \"data/.json\", \"data/*\" <code>format</code> <code>str | None</code> File format to match (without dot). If None, inferred from path. Examples: \"json\", \"csv\", \"parquet\" Returns Type Description <code>str</code> <code>str</code> Glob pattern that matches files of specified format. Examples: \"data/**/.json\", \"data/.csv\" <p>Example:</p> <pre><code># Basic directory\npath_to_glob(\"data\", \"json\")\n# 'data/**/*.json'\n\n# With wildcards\npath_to_glob(\"data/**\", \"csv\")\n# 'data/**/*.csv'\n\n# Format inference\npath_to_glob(\"data/file.parquet\")\n# 'data/file.parquet'\n</code></pre>"},{"location":"api/fsspeckit.core.ext/#read_json_file","title":"<code>read_json_file()</code>","text":"<p>Read a single JSON file from any filesystem.</p> <p>A public wrapper around <code>_read_json_file</code> providing a clean interface for reading individual JSON files.</p> Parameter Type Description <code>self</code> <code>AbstractFileSystem</code> Filesystem instance to use for reading <code>path</code> <code>str</code> Path to JSON file to read <code>include_file_path</code> <code>bool</code> Whether to return dict with filepath as key <code>jsonlines</code> <code>bool</code> Whether to read as JSON Lines format Returns Type Description <code>dict</code> or <code>list[dict]</code> <code>dict</code> or <code>list[dict]</code> Parsed JSON data. For regular JSON, returns a dict. For JSON Lines, returns a list of dicts. If <code>include_file_path=True</code>, returns <code>{filepath: data}</code>. <p>Example:</p> <pre><code>from fsspec.implementations.local import LocalFileSystem\n\nfs = LocalFileSystem()\n# Read regular JSON\ndata = fs.read_json_file(\"config.json\")\nprint(data[\"setting\"])\n# 'value'\n\n# Read JSON Lines with filepath\ndata = fs.read_json_file(\n    \"logs.jsonl\",\n    include_file_path=True,\n    jsonlines=True\n)\nprint(list(data.keys())[0])\n# 'logs.jsonl'\n</code></pre>"},{"location":"api/fsspeckit.core.ext/#read_json","title":"<code>read_json()</code>","text":"<p>Read JSON data from one or more files with powerful options.</p> <p>Provides a flexible interface for reading JSON data with support for:</p> <ul> <li>Single file or multiple files</li> <li>Regular JSON or JSON Lines format</li> <li>Batch processing for large datasets</li> <li>Parallel processing</li> <li>DataFrame conversion</li> <li>File path tracking</li> </ul> Parameter Type Description <code>path</code> <code>str</code> or <code>list[str]</code> Path(s) to JSON file(s). Can be: - Single path string (globs supported) - List of path strings <code>batch_size</code> <code>int | None</code> If set, enables batch reading with this many files per batch <code>include_file_path</code> <code>bool</code> Include source filepath in output <code>jsonlines</code> <code>bool</code> Whether to read as JSON Lines format <code>as_dataframe</code> <code>bool</code> Convert output to Polars DataFrame(s) <code>concat</code> <code>bool</code> Combine multiple files/batches into single result <code>use_threads</code> <code>bool</code> Enable parallel file reading <code>verbose</code> <code>bool</code> Print progress information <code>opt_dtypes</code> <code>bool</code> Optimize DataFrame dtypes for performance <code>**kwargs</code> <code>Any</code> Additional arguments passed to DataFrame conversion Returns Type Description <code>dict</code> or <code>list[dict]</code> or <code>pl.DataFrame</code> or <code>list[pl.DataFrame]</code> or <code>Generator</code> Various types depending on arguments: - <code>dict</code>: Single JSON file as dictionary - <code>list[dict]</code>: Multiple JSON files as list of dictionaries - <code>pl.DataFrame</code>: Single or concatenated DataFrame - <code>list[pl.DataFrame]</code>: List of Dataframes (if <code>concat=False</code>) - <code>Generator</code>: If <code>batch_size</code> set, yields batches of above types <p>Example:</p> <pre><code>from fsspec.implementations.local import LocalFileSystem\n\nfs = LocalFileSystem()\n# Read all JSON files in directory\ndf = fs.read_json(\n    \"data/*.json\",\n    as_dataframe=True,\n    concat=True\n)\nprint(df.shape)\n# (1000, 5)  # Combined data from all files\n\n# Batch process large dataset\nfor batch_df in fs.read_json(\n    \"logs/*.jsonl\",\n    batch_size=100,\n    jsonlines=True,\n    include_file_path=True\n):\n    print(f\"Processing {len(batch_df)} records\")\n\n# Parallel read with custom options\ndfs = fs.read_json(\n    [\"file1.json\", \"file2.json\"],\n    use_threads=True,\n    concat=False,\n    verbose=True\n)\nprint(f\"Read {len(dfs)} files\")\n</code></pre>"},{"location":"api/fsspeckit.core.ext/#read_csv_file","title":"<code>read_csv_file()</code>","text":"<p>Read a single CSV file from any filesystem.</p> <p>Internal function that handles reading individual CSV files and optionally adds the source filepath as a column.</p> Parameter Type Description <code>self</code> <code>AbstractFileSystem</code> Filesystem instance to use for reading <code>path</code> <code>str</code> Path to CSV file <code>include_file_path</code> <code>bool</code> Add source filepath as a column <code>opt_dtypes</code> <code>bool</code> Optimize DataFrame dtypes <code>**kwargs</code> <code>Any</code> Additional arguments passed to <code>pl.read_csv()</code> Returns Type Description <code>pl.DataFrame</code> <code>pl.DataFrame</code> DataFrame containing CSV data <p>Example:</p> <pre><code>from fsspec.implementations.local import LocalFileSystem\n\nfs = LocalFileSystem()\n# This example assumes _read_csv_file is an internal method or needs to be called differently.\n# For public use, you would typically use fs.read_csv().\n# df = fs.read_csv_file(\n#     \"data.csv\",\n#     include_file_path=True,\n#     delimiter=\"|\"\n# )\n# print(\"file_path\" in df.columns)\n# True\n</code></pre>"},{"location":"api/fsspeckit.core.ext/#read_csv","title":"<code>read_csv()</code>","text":"<p>Read CSV data from one or more files with powerful options.</p> <p>Provides a flexible interface for reading CSV files with support for:</p> <ul> <li>Single file or multiple files</li> <li>Batch processing for large datasets</li> <li>Parallel</li> <li>File path tracking</li> <li>Polars DataFrame output</li> </ul> Parameter Type Description <code>path</code> <code>str</code> or <code>list[str]</code> Path(s) to CSV file(s). Can be: - Single path string (globs supported) - List of path strings <code>batch_size</code> <code>int | None</code> If set, enables batch reading with this many files per batch <code>include_file_path</code> <code>bool</code> Add source filepath as a column <code>concat</code> <code>bool</code> Combine multiple files/batches into single DataFrame <code>use_threads</code> <code>bool</code> Enable parallel file reading <code>verbose</code> <code>bool</code> Print progress information <code>**kwargs</code> <code>Any</code> Additional arguments passed to <code>pl.read_csv()</code> Returns Type Description <code>pl.DataFrame</code> or <code>list[pl.DataFrame]</code> or <code>Generator</code> Various types depending on arguments: - <code>pl.DataFrame</code>: Single or concatenated DataFrame - <code>list[pl.DataFrame]</code>: List of DataFrames (if <code>concat=False</code>) - <code>Generator</code>: If <code>batch_size</code> set, yields batches of above types <p>Example:</p> <pre><code>from fsspec.implementations.local import LocalFileSystem\n\nfs = LocalFileSystem()\n# Read all CSVs in directory\ndf = fs.read_csv(\n    \"data/*.csv\",\n    include_file_path=True\n)\nprint(df.columns)\n# ['file_path', 'col1', 'col2', ...]\n\n# Batch process large dataset\nfor batch_df in fs.read_csv(\n    \"logs/*.csv\",\n    batch_size=100,\n    use_threads=True,\n    verbose=True\n):\n    print(f\"Processing {len(batch_df)} rows\")\n\n# Multiple files without concatenation\ndfs = fs.read_csv(\n    [\"file1.csv\", \"file2.csv\"],\n    concat=False,\n    use_threads=True\n)\nprint(f\"Read {len(dfs)} files\")\n</code></pre>"},{"location":"api/fsspeckit.core.ext/#read_parquet_file","title":"<code>read_parquet_file()</code>","text":"<p>Read a single Parquet file from any filesystem.</p> <p>Internal function that handles reading individual Parquet files and optionally adds the source filepath as a column.</p> Parameter Type Description <code>self</code> <code>AbstractFileSystem</code> Filesystem instance to use for reading <code>path</code> <code>str</code> Path to Parquet file <code>include_file_path</code> <code>bool</code> Add source filepath as a column <code>opt_dtypes</code> <code>bool</code> Optimize DataFrame dtypes <code>**kwargs</code> <code>Any</code> Additional arguments passed to <code>pq.read_table()</code> Returns Type Description <code>pa.Table</code> <code>pa.Table</code> PyArrow Table containing Parquet data <p>Example:</p> <pre><code>from fsspec.implementations.local import LocalFileSystem\n\nfs = LocalFileSystem()\n# This example assumes _read_parquet_file is an internal method or needs to be called differently.\n# For public use, you would typically use fs.read_parquet().\n# table = fs.read_parquet_file(\n#     \"data.parquet\",\n#     include_file_path=True,\n#     use_threads=True\n# )\n# print(\"file_path\" in table.column_names)\n# True\n</code></pre>"},{"location":"api/fsspeckit.core.ext/#read_parquet","title":"<code>read_parquet()</code>","text":"<p>Read Parquet data with advanced features and optimizations.</p> <p>Provides a high-performance interface for reading Parquet files with support for:</p> <ul> <li>Single file or multiple files</li> <li>Batch processing for large datasets</li> <li>Parallel processing</li> <li>File path tracking</li> <li>Automatic concatenation</li> <li>PyArrow Table output</li> </ul> <p>The function automatically uses optimal reading strategies:</p> <ul> <li>Direct dataset reading for simple cases</li> <li>Parallel processing for multiple files</li> <li>Batched reading for memory efficiency</li> </ul> Parameter Type Description <code>path</code> <code>str</code> or <code>list[str]</code> Path(s) to Parquet file(s). Can be: - Single path string (globs supported) - List of path strings - Directory containing _metadata file <code>batch_size</code> <code>int | None</code> If set, enables batch reading with this many files per batch <code>include_file_path</code> <code>bool</code> Add source filepath as a column <code>concat</code> <code>bool</code> Combine multiple files/batches into single Table <code>use_threads</code> <code>bool</code> Enable parallel file reading <code>verbose</code> <code>bool</code> Print progress information <code>opt_dtypes</code> <code>bool</code> Optimize Table dtypes for performance <code>**kwargs</code> <code>Any</code> Additional arguments passed to <code>pq.read_table()</code> Returns Type Description <code>pa.Table</code> or <code>list[pa.Table]</code> or <code>Generator</code> Various types depending on arguments: - <code>pa.Table</code>: Single or concatenated Table - <code>list[pa.Table]</code>: List of Tables (if <code>concat=False</code>) - <code>Generator</code>: If <code>batch_size</code> set, yields batches of above types <p>Example:</p> <pre><code>from fsspec.implementations.local import LocalFileSystem\n\nfs = LocalFileSystem()\n# Read all Parquet files in directory\ntable = fs.read_parquet(\n    \"data/*.parquet\",\n    include_file_path=True\n)\nprint(table.column_names)\n# ['file_path', 'col1', 'col2', ...]\n\n# Batch process large dataset\nfor batch in fs.read_parquet(\n    \"data/*.parquet\",\n    batch_size=100,\n    use_threads=True\n):\n    print(f\"Processing {batch.num_rows} rows\")\n\n# Read from directory with metadata\ntable = fs.read_parquet(\n    \"data/\",  # Contains _metadata\n    use_threads=True\n)\nprint(f\"Total rows: {table.num_rows}\")\n</code></pre>"},{"location":"api/fsspeckit.core.ext/#read_files","title":"<code>read_files()</code>","text":"<p>Universal interface for reading data files of any supported format.</p> <p>A unified API that automatically delegates to the appropriate reading function based on file format, while preserving all advanced features like:</p> <ul> <li>Batch processing</li> <li>Parallel reading</li> <li>File path tracking</li> <li>Format-specific optimizations</li> </ul> Parameter Type Description <code>path</code> <code>str</code> or <code>list[str]</code> Path(s) to data file(s). Can be: - Single path string (globs supported) - List of path strings <code>format</code> <code>str</code> File format to read. Supported values: - \"json\": Regular JSON or JSON Lines - \"csv\": CSV files - \"parquet\": Parquet files <code>batch_size</code> <code>int | None</code> If set, enables batch reading with this many files per batch <code>include_file_path</code> <code>bool</code> Add source filepath as column/field <code>concat</code> <code>bool</code> Combine multiple files/batches into single result <code>jsonlines</code> <code>bool</code> For JSON format, whether to read as JSON Lines <code>use_threads</code> <code>bool</code> Enable parallel file reading <code>verbose</code> <code>bool</code> Print progress information <code>opt_dtypes</code> <code>bool</code> Optimize DataFrame/Arrow Table dtypes for performance <code>**kwargs</code> <code>Any</code> Additional format-specific arguments Returns Type Description <code>pl.DataFrame</code> or <code>pa.Table</code> or <code>list[pl.DataFrame]</code> or <code>list[pa.Table]</code> or <code>Generator</code> Various types depending on format and arguments: - <code>pl.DataFrame</code>: For CSV and optionally JSON - <code>pa.Table</code>: For Parquet - <code>list[pl.DataFrame</code> or <code>pa.Table]</code>: Without concatenation - <code>Generator</code>: If <code>batch_size</code> set, yields batches <p>Example:</p> <pre><code>from fsspec.implementations.local import LocalFileSystem\n\nfs = LocalFileSystem()\n# Read CSV files\ndf = fs.read_files(\n    \"data/*.csv\",\n    format=\"csv\",\n    include_file_path=True\n)\nprint(type(df))\n# &lt;class 'polars.DataFrame'&gt;\n\n# Batch process Parquet files\nfor batch in fs.read_files(\n    \"data/*.parquet\",\n    format=\"parquet\",\n    batch_size=100,\n    use_threads=True\n):\n    print(f\"Batch type: {type(batch)}\")\n\n# Read JSON Lines\ndf = fs.read_files(\n    \"logs/*.jsonl\",\n    format=\"json\",\n    jsonlines=True,\n    concat=True\n)\nprint(df.columns)\n</code></pre>"},{"location":"api/fsspeckit.core.ext/#pyarrow_dataset","title":"<code>pyarrow_dataset()</code>","text":"<p>Create a PyArrow dataset from files in any supported format.</p> <p>Creates a dataset that provides optimized reading and querying capabilities including:</p> <ul> <li>Schema inference and enforcement</li> <li>Partition discovery and pruning</li> <li>Predicate pushdown</li> <li>Column projection</li> </ul> Parameter Type Description <code>path</code> <code>str</code> Base path to dataset files <code>format</code> <code>str</code> File format. Currently supports: - \"parquet\" (default) - \"csv\" - \"json\" (experimental) <code>schema</code> <code>pa.Schema | None</code> Optional schema to enforce. If None, inferred from data. <code>partitioning</code> <code>str</code> or <code>list[str]</code> or <code>pds.Partitioning</code> How the dataset is partitioned. Can be: - <code>str</code>: Single partition field - <code>list[str]</code>: Multiple partition fields - <code>pds.Partitioning</code>: Custom partitioning scheme <code>**kwargs</code> <code>Any</code> Additional arguments for dataset creation Returns Type Description <code>pds.Dataset</code> <code>pds.Dataset</code> PyArrow dataset instance <p>Example:</p> <pre><code>from fsspec.implementations.local import LocalFileSystem\n\nfs = LocalFileSystem()\n# Simple Parquet dataset\nds = fs.pyarrow_dataset(\"data/\")\nprint(ds.schema)\n\n# Partitioned dataset\nds = fs.pyarrow_dataset(\n    \"events/\",\n    partitioning=[\"year\", \"month\"]\n)\n# Query with partition pruning\ntable = ds.to_table(\n    filter=(ds.field(\"year\") == 2024)\n)\n\n# CSV with schema\nds = fs.pyarrow_dataset(\n    \"logs/\",\n    format=\"csv\",\n    schema=pa.schema([\n        (\"timestamp\", pa.timestamp(\"s\")),\n        (\"level\", pa.string()),\n        (\"message\", pa.string())\n    ])\n)\n</code></pre>"},{"location":"api/fsspeckit.core.ext/#pyarrow_parquet_dataset","title":"<code>pyarrow_parquet_dataset()</code>","text":"<p>Create a PyArrow dataset optimized for Parquet files.</p> <p>Creates a dataset specifically for Parquet data, automatically handling <code>_metadata</code> files for optimized reading.</p> <p>This function is particularly useful for:</p> <ul> <li>Datasets with existing <code>_metadata</code> files</li> <li>Multi-file datasets that should be treated as one</li> <li>Partitioned Parquet datasets</li> </ul> Parameter Type Description <code>path</code> <code>str</code> Path to dataset directory or <code>_metadata</code> file <code>schema</code> <code>pa.Schema | None</code> Optional schema to enforce. If None, inferred from data. <code>partitioning</code> <code>str</code> or <code>list[str]</code> or <code>pds.Partitioning</code> How the dataset is partitioned. Can be: - <code>str</code>: Single partition field - <code>list[str]</code>: Multiple partition fields - <code>pds.Partitioning</code>: Custom partitioning scheme <code>**kwargs</code> <code>Any</code> Additional dataset arguments Returns Type Description <code>pds.Dataset</code> <code>pds.Dataset</code> PyArrow dataset instance <p>Example:</p> <pre><code>from fsspec.implementations.local import LocalFileSystem\n\nfs = LocalFileSystem()\n# Dataset with _metadata\nds = fs.pyarrow_parquet_dataset(\"data/_metadata\")\nprint(ds.files)  # Shows all data files\n\n# Partitioned dataset directory\nds = fs.pyarrow_parquet_dataset(\n    \"sales/\",\n    partitioning=[\"year\", \"region\"]\n)\n# Query with partition pruning\ntable = ds.to_table(\n    filter=(\n        (ds.field(\"year\") == 2024) &amp;\n        (ds.field(\"region\") == \"EMEA\")\n    )\n)\n</code></pre>"},{"location":"api/fsspeckit.storage_options.base/","title":"<code>fsspeckit.storage_options.base</code> API Documentation","text":"<p>This module defines the base class for filesystem storage configuration options.</p>"},{"location":"api/fsspeckit.storage_options.base/#basestorageoptions","title":"<code>BaseStorageOptions</code>","text":"<p>Base class for filesystem storage configuration options.</p> <p>Provides common functionality for all storage option classes including:</p> <ul> <li>YAML serialization/deserialization</li> <li>Dictionary conversion</li> <li>Filesystem instance creation</li> <li>Configuration updates</li> </ul> <p>Attributes:</p> <ul> <li><code>protocol</code> (<code>str</code>): Storage protocol identifier (e.g., \"s3\", \"gs\", \"file\")</li> </ul> <p>Example:</p> <pre><code>from fsspeckit.storage_options.base import BaseStorageOptions\n\n# Create and save options\noptions = BaseStorageOptions(protocol=\"s3\")\noptions.to_yaml(\"config.yml\")\n\n# Load from YAML\nloaded = BaseStorageOptions.from_yaml(\"config.yml\")\nprint(loaded.protocol)\n# 's3'\n</code></pre>"},{"location":"api/fsspeckit.storage_options.base/#to_dict","title":"<code>to_dict()</code>","text":"<p>Convert storage options to dictionary.</p> Parameter Type Description <code>with_protocol</code> <code>bool</code> Whether to include protocol in output dictionary Returns Type Description <code>dict</code> <code>dict</code> Dictionary of storage options with non-None values <p>Example:</p> <pre><code>from fsspeckit.storage_options.base import BaseStorageOptions\n\noptions = BaseStorageOptions(protocol=\"s3\")\nprint(options.to_dict())\n# {}\nprint(options.to_dict(with_protocol=True))\n# {'protocol': 's3'}\n</code></pre>"},{"location":"api/fsspeckit.storage_options.base/#from_yaml","title":"<code>from_yaml()</code>","text":"<p>Load storage options from YAML file.</p> Parameter Type Description <code>path</code> <code>str</code> Path to YAML configuration file <code>fs</code> <code>AbstractFileSystem</code> Filesystem to use for reading file Returns Type Description <code>BaseStorageOptions</code> <code>BaseStorageOptions</code> Loaded storage options instance <p>Example:</p> <pre><code># Load from local file\nfrom fsspeckit.storage_options.base import BaseStorageOptions\nfrom fsspec.implementations.local import LocalFileSystem\n\n# Assuming 'config.yml' exists and contains valid YAML for BaseStorageOptions\n# For example, a file named config.yml with content:\n# protocol: s3\n#\n# To make this example runnable, we'll create a dummy config.yml\nfs_local = LocalFileSystem()\nfs_local.write_text(\"config.yml\", \"protocol: s3\")\n\noptions = BaseStorageOptions.from_yaml(\"config.yml\")\nprint(options.protocol)\n# 's3'\n\n# Clean up the dummy file\nfs_local.rm(\"config.yml\")\n</code></pre>"},{"location":"api/fsspeckit.storage_options.base/#to_yaml","title":"<code>to_yaml()</code>","text":"<p>Save storage options to YAML file.</p> Parameter Type Description <code>path</code> <code>str</code> Path where to save configuration <code>fs</code> <code>AbstractFileSystem</code> Filesystem to use for writing <p>Example:</p> <pre><code>from fsspeckit.storage_options.base import BaseStorageOptions\nfrom fsspec.implementations.local import LocalFileSystem\n\noptions = BaseStorageOptions(protocol=\"s3\")\nfs_local = LocalFileSystem()\noptions.to_yaml(\"config.yml\", fs=fs_local) # Specify filesystem for writing\n</code></pre>"},{"location":"api/fsspeckit.storage_options.base/#to_filesystem","title":"<code>to_filesystem()</code>","text":"<p>Create fsspec filesystem instance from options.</p> Returns Type Description <code>AbstractFileSystem</code> <code>AbstractFileSystem</code> Configured filesystem instance <p>Example:</p> <pre><code>from fsspeckit.storage_options.base import BaseStorageOptions\n\noptions = BaseStorageOptions(protocol=\"file\")\nfs = options.to_filesystem()\n# Example usage: list files in a dummy directory\nimport os\nimport tempfile\n\nwith tempfile.TemporaryDirectory() as tmpdir:\n    dummy_file_path = os.path.join(tmpdir, \"test.txt\")\n    with open(dummy_file_path, \"w\") as f:\n        f.write(\"dummy content\")\n    fs_temp = options.to_filesystem()\n    files = fs_temp.ls(tmpdir)\n    print(files)\n</code></pre>"},{"location":"api/fsspeckit.storage_options.base/#update","title":"<code>update()</code>","text":"<p>Update storage options with new values.</p> Parameter Type Description <code>**kwargs</code> <code>Any</code> New option values to set Returns Type Description <code>BaseStorageOptions</code> <code>BaseStorageOptions</code> Updated instance <p>Example:</p> <pre><code>from fsspeckit.storage_options.base import BaseStorageOptions\n\noptions = BaseStorageOptions(protocol=\"s3\")\noptions = options.update(region=\"us-east-1\")\nprint(options.region)\n# 'us-east-1'\n</code></pre>"},{"location":"api/fsspeckit.storage_options.cloud/","title":"<code>fsspeckit.storage_options.cloud</code> API Documentation","text":"<p>This module defines storage option classes for various cloud providers, including Azure, Google Cloud Storage (GCS), and Amazon Web Services (AWS) S3. These classes provide structured ways to configure access to cloud storage, supporting different authentication methods and specific cloud service parameters.</p>"},{"location":"api/fsspeckit.storage_options.cloud/#azurestorageoptions","title":"<code>AzureStorageOptions</code>","text":"<p>Azure Storage configuration options.</p> <p>Provides configuration for Azure storage services:</p> <ul> <li>Azure Blob Storage (<code>az://</code>)</li> <li>Azure Data Lake Storage Gen2 (<code>abfs://</code>)</li> <li>Azure Data Lake Storage Gen1 (<code>adl://</code>)</li> </ul> <p>Supports multiple authentication methods:</p> <ul> <li>Connection string</li> <li>Account key</li> <li>Service principal</li> <li>Managed identity</li> <li>SAS token</li> </ul> <p>Attributes:</p> <ul> <li><code>protocol</code> (<code>str</code>): Storage protocol (\"az\", \"abfs\", or \"adl\")</li> <li><code>account_name</code> (<code>str</code>): Storage account name</li> <li><code>account_key</code> (<code>str</code>): Storage account access key</li> <li><code>connection_string</code> (<code>str</code>): Full connection string</li> <li><code>tenant_id</code> (<code>str</code>): Azure AD tenant ID</li> <li><code>client_id</code> (<code>str</code>): Service principal client ID</li> <li><code>client_secret</code> (<code>str</code>): Service principal client secret</li> <li><code>sas_token</code> (<code>str</code>): SAS token for limited access</li> </ul> <p>Example:</p> <pre><code>from fsspeckit.storage_options.cloud import AzureStorageOptions\n\n# Blob Storage with account key\noptions = AzureStorageOptions(\n    protocol=\"az\",\n    account_name=\"mystorageacct\",\n    account_key=\"key123...\"\n)\n\n# Data Lake with service principal\noptions = AzureStorageOptions(\n    protocol=\"abfs\",\n    account_name=\"mydatalake\",\n    tenant_id=\"tenant123\",\n    client_id=\"client123\",\n    client_secret=\"secret123\"\n)\n\n# Simple connection string auth\noptions = AzureStorageOptions(\n    protocol=\"az\",\n    connection_string=\"DefaultEndpoints...\"\n)\n</code></pre>"},{"location":"api/fsspeckit.storage_options.cloud/#from_env","title":"<code>from_env()</code>","text":"<p>Create storage options from environment variables.</p> <p>Reads standard Azure environment variables:</p> <ul> <li><code>AZURE_STORAGE_PROTOCOL</code></li> <li><code>AZURE_STORAGE_ACCOUNT_NAME</code></li> <li><code>AZURE_STORAGE_ACCOUNT_KEY</code></li> <li><code>AZURE_STORAGE_CONNECTION_STRING</code></li> <li><code>AZURE_TENANT_ID</code></li> <li><code>AZURE_CLIENT_ID</code></li> <li><code>AZURE_CLIENT_SECRET</code></li> <li><code>AZURE_STORAGE_SAS_TOKEN</code></li> </ul> Returns Type Description <code>AzureStorageOptions</code> <code>AzureStorageOptions</code> Configured storage options <p>Example:</p> <pre><code># With environment variables set:\nfrom fsspeckit.storage_options.cloud import AzureStorageOptions\nimport os\n\n# Set environment variables for testing (replace with actual values if needed)\nos.environ[\"AZURE_STORAGE_ACCOUNT_NAME\"] = \"mystorageacct\"\nos.environ[\"AZURE_STORAGE_ACCOUNT_KEY\"] = \"dummy_key\" # Dummy key for example\n\noptions = AzureStorageOptions.from_env()\nprint(options.account_name)  # From AZURE_STORAGE_ACCOUNT_NAME\n# 'mystorageacct'\n\n# Clean up environment variables\ndel os.environ[\"AZURE_STORAGE_ACCOUNT_NAME\"]\ndel os.environ[\"AZURE_STORAGE_ACCOUNT_KEY\"]\n</code></pre>"},{"location":"api/fsspeckit.storage_options.cloud/#to_env","title":"<code>to_env()</code>","text":"<p>Export options to environment variables.</p> <p>Sets standard Azure environment variables.</p> <p>Example:</p> <pre><code>from fsspeckit.storage_options.cloud import AzureStorageOptions\nimport os\n\noptions = AzureStorageOptions(\n    protocol=\"az\",\n    account_name=\"mystorageacct\",\n    account_key=\"key123\"\n)\noptions.to_env()\nprint(os.getenv(\"AZURE_STORAGE_ACCOUNT_NAME\"))\n# 'mystorageacct'\n\n# Clean up environment variables\ndel os.environ[\"AZURE_STORAGE_ACCOUNT_NAME\"]\ndel os.environ[\"AZURE_STORAGE_ACCOUNT_KEY\"]\n</code></pre>"},{"location":"api/fsspeckit.storage_options.cloud/#gcsstorageoptions","title":"<code>GcsStorageOptions</code>","text":"<p>Google Cloud Storage configuration options.</p> <p>Provides configuration for GCS access with support for:</p> <ul> <li>Service account authentication</li> <li>Default application credentials</li> <li>Token-based authentication</li> <li>Project configuration</li> <li>Custom endpoints</li> </ul> <p>Attributes:</p> <ul> <li><code>protocol</code> (<code>str</code>): Storage protocol (\"gs\" or \"gcs\")</li> <li><code>token</code> (<code>str</code>): Path to service account JSON file</li> <li><code>project</code> (<code>str</code>): Google Cloud project ID</li> <li><code>access_token</code> (<code>str</code>): OAuth2 access token</li> <li><code>endpoint_url</code> (<code>str</code>): Custom storage endpoint</li> <li><code>timeout</code> (<code>int</code>): Request timeout in seconds</li> </ul> <p>Example:</p> <pre><code>from fsspeckit.storage_options.cloud import GcsStorageOptions\n\n# Service account auth\noptions = GcsStorageOptions(\n    protocol=\"gs\",\n    token=\"path/to/service-account.json\",\n    project=\"my-project-123\"\n)\n\n# Application default credentials\noptions = GcsStorageOptions(\n    protocol=\"gcs\",\n    project=\"my-project-123\"\n)\n\n# Custom endpoint (e.g., test server)\noptions = GcsStorageOptions(\n    protocol=\"gs\",\n    endpoint_url=\"http://localhost:4443\",\n    token=\"test-token.json\"\n)\n</code></pre>"},{"location":"api/fsspeckit.storage_options.cloud/#from_env_1","title":"<code>from_env()</code>","text":"<p>Create storage options from environment variables.</p> <p>Reads standard GCP environment variables:</p> <ul> <li><code>GOOGLE_CLOUD_PROJECT</code>: Project</li> <li><code>GOOGLE_APPLICATION_CREDENTIALS</code>: Service account file path</li> <li><code>STORAGE_EMULATOR_HOST</code>: Custom endpoint (for testing)</li> <li><code>GCS_OAUTH_TOKEN</code>: OAuth2 access token</li> </ul> Returns Type Description <code>GcsStorageOptions</code> <code>GcsStorageOptions</code> Configured storage options <p>Example:</p> <pre><code># With environment variables set:\nfrom fsspeckit.storage_options.cloud import GcsStorageOptions\nimport os\n\n# Set environment variables for testing (replace with actual values if needed)\nos.environ[\"GOOGLE_CLOUD_PROJECT\"] = \"my-project-123\"\n\noptions = GcsStorageOptions.from_env()\nprint(options.project)  # From GOOGLE_CLOUD_PROJECT\n# 'my-project-123'\n\n# Clean up environment variables\ndel os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n</code></pre>"},{"location":"api/fsspeckit.storage_options.cloud/#to_env_1","title":"<code>to_env()</code>","text":"<p>Export options to environment variables.</p> <p>Sets standard GCP environment variables.</p> <p>Example:</p> <pre><code>from fsspeckit.storage_options.cloud import GcsStorageOptions\nimport os\n\noptions = GcsStorageOptions(\n    protocol=\"gs\",\n    project=\"my-project\",\n    token=\"service-account.json\"\n)\noptions.to_env()\nprint(os.getenv(\"GOOGLE_CLOUD_PROJECT\"))\n# 'my-project'\n\n# Clean up environment variables\ndel os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n</code></pre>"},{"location":"api/fsspeckit.storage_options.cloud/#to_fsspec_kwargs","title":"<code>to_fsspec_kwargs()</code>","text":"<p>Convert options to fsspec filesystem arguments.</p> Returns Type Description <code>dict</code> <code>dict</code> Arguments suitable for GCSFileSystem <p>Example:</p> <pre><code>from fsspeckit.storage_options.cloud import GcsStorageOptions\nfrom fsspeckit.core.base import filesystem\n\noptions = GcsStorageOptions(\n    protocol=\"gs\",\n    token=\"service-account.json\",\n    project=\"my-project\"\n)\nkwargs = options.to_fsspec_kwargs()\nfs = filesystem(\"gcs\", **kwargs)\n</code></pre>"},{"location":"api/fsspeckit.storage_options.cloud/#awsstorageoptions","title":"<code>AwsStorageOptions</code>","text":"<p>AWS S3 storage configuration options.</p> <p>Provides comprehensive configuration for S3 access with support for:</p> <ul> <li>Multiple authentication methods (keys, profiles, environment)</li> <li>Custom endpoints for S3-compatible services</li> <li>Region configuration</li> <li>SSL/TLS settings</li> <li>Anonymous access for public buckets</li> </ul> <p>Attributes:</p> <ul> <li><code>protocol</code> (<code>str</code>): Always \"s3\" for S3 storage</li> <li><code>access_key_id</code> (<code>str</code>): AWS access key ID</li> <li><code>secret_access_key</code> (<code>str</code>): AWS secret access key</li> <li><code>session_token</code> (<code>str</code>): AWS session token</li> <li><code>endpoint_url</code> (<code>str</code>): Custom S3 endpoint URL</li> <li><code>region</code> (<code>str</code>): AWS region name</li> <li><code>allow_invalid_certificates</code> (<code>bool</code>): Skip SSL certificate validation</li> <li><code>allow_http</code> (<code>bool</code>): Allow unencrypted HTTP connections</li> <li><code>anonymous</code> (<code>bool</code>): Use anonymous (unsigned) S3 access</li> </ul> <p>Example:</p> <pre><code># Basic credentials\noptions = AwsStorageOptions(\n    access_key_id=\"AKIAXXXXXXXX\",\n    secret_access_key=\"SECRETKEY\",\n    region=\"us-east-1\"\n)\n\n# Profile-based auth\noptions = AwsStorageOptions.create(profile=\"dev\")\n\n# S3-compatible service (MinIO)\noptions = AwsStorageOptions(\n    endpoint_url=\"http://localhost:9000\",\n    access_key_id=\"minioadmin\",\n    secret_access_key=\"minioadmin\",\n    allow_http=True\n)\n\n# Anonymous access for public buckets\noptions = AwsStorageOptions(anonymous=True)\n</code></pre>"},{"location":"api/fsspeckit.storage_options.cloud/#create","title":"<code>create()</code>","text":"<p>Creates an <code>AwsStorageOptions</code> instance, handling aliases and profile loading.</p> Parameter Type Description <code>protocol</code> <code>str</code> Storage protocol, defaults to \"s3\". <code>access_key_id</code> <code>str | None</code> AWS access key ID. <code>secret_access_key</code> <code>str | None</code> AWS secret access key. <code>session_token</code> <code>str | None</code> AWS session token. <code>endpoint_url</code> <code>str | None</code> Custom S3 endpoint URL. <code>region</code> <code>str | None</code> AWS region name. <code>allow_invalid_certificates</code> <code>bool | None</code> Skip SSL certificate validation. <code>allow_http</code> <code>bool | None</code> Allow unencrypted HTTP connections. <code>anonymous</code> <code>bool | None</code> Use anonymous (unsigned) S3 access. <code>key</code> <code>str | None</code> Alias for <code>access_key_id</code>. <code>secret</code> <code>str | None</code> Alias for <code>secret_access_key</code>. <code>token</code> <code>str | None</code> Alias for <code>session_token</code>. <code>profile</code> <code>str | None</code> AWS credentials profile name to load credentials from. Returns Type Description <code>AwsStorageOptions</code> <code>AwsStorageOptions</code> An initialized <code>AwsStorageOptions</code> instance."},{"location":"api/fsspeckit.storage_options.cloud/#from_aws_credentials","title":"<code>from_aws_credentials()</code>","text":"<p>Create storage options from AWS credentials file.</p> <p>Loads credentials from <code>~/.aws/credentials</code> and <code>~/.aws/config</code> files.</p> Parameter Type Description <code>profile</code> <code>str</code> AWS credentials profile name <code>allow_invalid_certificates</code> <code>bool</code> Skip SSL certificate validation <code>allow_http</code> <code>bool</code> Allow unencrypted HTTP connections <code>anonymous</code> <code>bool</code> Use anonymous (unsigned) S3 access Returns Type Description <code>AwsStorageOptions</code> <code>AwsStorageOptions</code> Configured storage options Raises Type Description <code>ValueError</code> <code>ValueError</code> If profile not found <code>FileNotFoundError</code> <code>FileNotFoundError</code> If credentials files missing <p>Example:</p> <pre><code># Load developer profile\noptions = AwsStorageOptions.from_aws_credentials(\n    profile=\"dev\",\n    allow_http=True  # For local testing\n)\n</code></pre>"},{"location":"api/fsspeckit.storage_options.cloud/#from_env_2","title":"<code>from_env()</code>","text":"<p>Create storage options from environment variables.</p> <p>Reads standard AWS environment variables:</p> <ul> <li><code>AWS_ACCESS_KEY_ID</code></li> <li><code>AWS_SECRET_ACCESS_KEY</code></li> <li><code>AWS_SESSION_TOKEN</code></li> <li><code>AWS_ENDPOINT_URL</code></li> <li><code>AWS_DEFAULT_REGION</code></li> <li><code>ALLOW_INVALID_CERTIFICATE</code></li> <li><code>AWS_ALLOW_HTTP</code></li> <li><code>AWS_S3_ANONYMOUS</code></li> </ul> Returns Type Description <code>AwsStorageOptions</code> <code>AwsStorageOptions</code> Configured storage options <p>Example:</p> <pre><code># Load from environment\nfrom fsspeckit.storage_options.cloud import AwsStorageOptions\nimport os\n\n# Set environment variables for testing (replace with actual values if needed)\nos.environ[\"AWS_DEFAULT_REGION\"] = \"us-east-1\"\n\noptions = AwsStorageOptions.from_env()\nprint(options.region)\n# 'us-east-1'  # From AWS_DEFAULT_REGION\n\n# Clean up environment variables\ndel os.environ[\"AWS_DEFAULT_REGION\"]\n</code></pre>"},{"location":"api/fsspeckit.storage_options.cloud/#to_fsspec_kwargs_1","title":"<code>to_fsspec_kwargs()</code>","text":"<p>Convert options to fsspec filesystem arguments.</p> Returns Type Description <code>dict</code> <code>dict</code> Arguments suitable for fsspec S3FileSystem <p>Example:</p> <pre><code>options = AwsStorageOptions(\n    access_key_id=\"KEY\",\n    secret_access_key=\"SECRET\",\n    region=\"us-west-2\"\n)\nkwargs = options.to_fsspec_kwargs()\nfs = filesystem(\"s3\", **kwargs)\n</code></pre>"},{"location":"api/fsspeckit.storage_options.cloud/#to_object_store_kwargs","title":"<code>to_object_store_kwargs()</code>","text":"<p>Convert options to object store arguments.</p> Parameter Type Description <code>with_conditional_put</code> <code>bool</code> Add etag-based conditional put support Returns Type Description <code>dict</code> <code>dict</code> Arguments suitable for object store clients <p>Example:</p> <pre><code>from fsspeckit.storage_options.cloud import AwsStorageOptions\n# Assuming ObjectStore is a hypothetical client for demonstration\n# from some_object_store_library import ObjectStore\n\noptions = AwsStorageOptions(\n    access_key_id=\"KEY\",\n    secret_access_key=\"SECRET\"\n)\nkwargs = options.to_object_store_kwargs()\n# client = ObjectStore(**kwargs)\n</code></pre>"},{"location":"api/fsspeckit.storage_options.cloud/#to_env_2","title":"<code>to_env()</code>","text":"<p>Export options to environment variables.</p> <p>Sets standard AWS environment variables.</p> <p>Example:</p> <pre><code>from fsspeckit.storage_options.cloud import AwsStorageOptions\nimport os\n\noptions = AwsStorageOptions(\n    access_key_id=\"KEY\",\n    secret_access_key=\"SECRET\",\n    region=\"us-east-1\"\n)\noptions.to_env()\nprint(os.getenv(\"AWS_ACCESS_KEY_ID\"))\n# 'KEY'\n\n# Clean up environment variables\ndel os.environ[\"AWS_ACCESS_KEY_ID\"]\ndel os.environ[\"AWS_SECRET_ACCESS_KEY\"]\nif \"AWS_DEFAULT_REGION\" in os.environ: # Only delete if it was set\n    del os.environ[\"AWS_DEFAULT_REGION\"]\n</code></pre>"},{"location":"api/fsspeckit.storage_options.cloud/#to_filesystem","title":"<code>to_filesystem()</code>","text":"<p>Create fsspec filesystem instance from options.</p> Returns Type Description <code>AbstractFileSystem</code> <code>AbstractFileSystem</code> Configured filesystem instance"},{"location":"api/fsspeckit.storage_options.core/","title":"<code>fsspeckit.storage_options.core</code> API Reference","text":""},{"location":"api/fsspeckit.storage_options.core/#localstorageoptions","title":"<code>LocalStorageOptions</code>","text":"<p>Local filesystem configuration options.</p> <p>Provides basic configuration for local file access. While this class is simple, it maintains consistency with other storage options and enables transparent switching between local and remote storage.</p> <p>Attributes:</p> <ul> <li><code>protocol</code> (<code>str</code>): Always \"file\" for local filesystem</li> <li><code>auto_mkdir</code> (<code>bool</code>): Create directories automatically</li> <li><code>mode</code> (<code>int</code>): Default file creation mode (unix-style)</li> </ul> <p>Example: <pre><code># Basic local access\noptions = LocalStorageOptions()\nfs = options.to_filesystem()\nfiles = fs.ls(\"/path/to/data\")\n\n# With auto directory creation\noptions = LocalStorageOptions(auto_mkdir=True)\nfs = options.to_filesystem()\nwith fs.open(\"/new/path/file.txt\", \"w\") as f:\n    f.write(\"test\")  # Creates /new/path/ automatically\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.core/#to_fsspec_kwargs","title":"<code>to_fsspec_kwargs()</code>","text":"<p>Convert options to fsspec filesystem arguments.</p> <p>Returns:</p> <ul> <li><code>dict</code>: Arguments suitable for LocalFileSystem</li> </ul> <p>Example: <pre><code>options = LocalStorageOptions(auto_mkdir=True)\nkwargs = options.to_fsspec_kwargs()\nfs = filesystem(\"file\", **kwargs)\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.core/#from_dict","title":"<code>from_dict()</code>","text":"<p>Create appropriate storage options instance from dictionary.</p> <p>Factory function that creates the correct storage options class based on protocol.</p> <p>Parameters:</p> Name Type Description <code>protocol</code> <code>str</code> Storage protocol identifier (e.g., \"s3\", \"gs\", \"file\") <code>storage_options</code> <code>dict</code> Dictionary of configuration options <p>Returns:</p> <ul> <li><code>BaseStorageOptions</code>: Appropriate storage options instance</li> </ul> <p>Raises:</p> <ul> <li><code>ValueError</code>: If protocol is not supported</li> </ul> <p>Example: <pre><code># Create S3 options\noptions = from_dict(\"s3\", {\n    \"access_key_id\": \"KEY\",\n    \"secret_access_key\": \"SECRET\"\n})\nprint(type(options).__name__)\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.core/#from_env","title":"<code>from_env()</code>","text":"<p>Create storage options from environment variables.</p> <p>Factory function that creates and configures storage options from protocol-specific environment variables.</p> <p>Parameters:</p> Name Type Description <code>protocol</code> <code>str</code> Storage protocol identifier (e.g., \"s3\", \"github\") <p>Returns:</p> <ul> <li><code>BaseStorageOptions</code>: Configured storage options instance</li> </ul> <p>Raises:</p> <ul> <li><code>ValueError</code>: If protocol is not supported</li> </ul> <p>Example: <pre><code># With AWS credentials in environment\noptions = from_env(\"s3\")\nprint(options.access_key_id)\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.core/#infer_protocol_from_uri","title":"<code>infer_protocol_from_uri()</code>","text":"<p>Infer the storage protocol from a URI string.</p> <p>Analyzes the URI to determine the appropriate storage protocol based on the scheme or path format.</p> <p>Parameters:</p> Name Type Description <code>uri</code> <code>str</code> URI or path string to analyze. Examples: - \"s3://bucket/path\" - \"gs://bucket/path\" - \"github://org/repo\" - \"/local/path\" <p>Returns:</p> <ul> <li><code>str</code>: Inferred protocol identifier</li> </ul> <p>Example: <pre><code># S3 protocol\ninfer_protocol_from_uri(\"s3://my-bucket/data\")\n\n# Local file\ninfer_protocol_from_uri(\"/home/user/data\")\n\n# GitHub repository\ninfer_protocol_from_uri(\"github://microsoft/vscode\")\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.core/#storage_options_from_uri","title":"<code>storage_options_from_uri()</code>","text":"<p>Create storage options instance from a URI string.</p> <p>Infers the protocol and extracts relevant configuration from the URI to create appropriate storage options.</p> <p>Parameters:</p> Name Type Description <code>uri</code> <code>str</code> URI string containing protocol and optional configuration. Examples: - \"s3://bucket/path\" - \"gs://project/bucket/path\" - \"github://org/repo\" <p>Returns:</p> <ul> <li><code>BaseStorageOptions</code>: Configured storage options instance</li> </ul> <p>Example: <pre><code># S3 options\nopts = storage_options_from_uri(\"s3://my-bucket/data\")\nprint(opts.protocol)\n\n# GitHub options\nopts = storage_options_from_uri(\"github://microsoft/vscode\")\nprint(opts.org)\nprint(opts.repo)\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.core/#merge_storage_options","title":"<code>merge_storage_options()</code>","text":"<p>Merge multiple storage options into a single configuration.</p> <p>Combines options from multiple sources with control over precedence.</p> <p>Parameters:</p> Name Type Description <code>*options</code> <code>BaseStorageOptions</code> or <code>dict</code> Storage options to merge. Can be: - BaseStorageOptions instances - Dictionaries of options - None values (ignored) <code>overwrite</code> <code>bool</code> Whether later options override earlier ones <p>Returns:</p> <ul> <li><code>BaseStorageOptions</code>: Combined storage options</li> </ul> <p>Example: <pre><code># Merge with overwrite\nbase = AwsStorageOptions(\n    region=\"us-east-1\",\n    access_key_id=\"OLD_KEY\"\n)\noverride = {\"access_key_id\": \"NEW_KEY\"}\nmerged = merge_storage_options(base, override)\nprint(merged.access_key_id)\n\n# Preserve existing values\nmerged = merge_storage_options(\n    base,\n    override,\n    overwrite=False\n)\nprint(merged.access_key_id)\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.core/#storageoptions","title":"<code>StorageOptions</code>","text":"<p>High-level storage options container and factory.</p> <p>Provides a unified interface for creating and managing storage options for different protocols.</p> <p>Attributes:</p> <ul> <li><code>storage_options</code> (<code>BaseStorageOptions</code>): Underlying storage options instance</li> </ul> <p>Example: <pre><code># Create from protocol\noptions = StorageOptions.create(\n    protocol=\"s3\",\n    access_key_id=\"KEY\",\n    secret_access_key=\"SECRET\"\n)\n\n# Create from existing options\ns3_opts = AwsStorageOptions(access_key_id=\"KEY\")\noptions = StorageOptions(storage_options=s3_opts)\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.core/#create","title":"<code>create()</code>","text":"<p>Create storage options from arguments.</p> <p>Parameters:</p> Name Type Description <code>**data</code> <code>dict</code> Either: - protocol and configuration options - storage_options=pre-configured instance <p>Returns:</p> <ul> <li><code>StorageOptions</code>: Configured storage options instance</li> </ul> <p>Raises:</p> <ul> <li><code>ValueError</code>: If protocol missing or invalid</li> </ul> <p>Example: <pre><code># Direct protocol config\noptions = StorageOptions.create(\n    protocol=\"s3\",\n    region=\"us-east-1\"\n)\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.core/#from_yaml","title":"<code>from_yaml()</code>","text":"<p>Create storage options from YAML configuration.</p> <p>Parameters:</p> Name Type Description <code>path</code> <code>str</code> Path to YAML configuration file <code>fs</code> <code>fsspec.AbstractFileSystem</code>, optional Filesystem for reading configuration <p>Returns:</p> <ul> <li><code>StorageOptions</code>: Configured storage options</li> </ul> <p>Example: <pre><code># Load from config file\noptions = StorageOptions.from_yaml(\"storage.yml\")\nprint(options.storage_options.protocol)\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.core/#from_env_1","title":"<code>from_env()</code>","text":"<p>Create storage options from environment variables.</p> <p>Parameters:</p> Name Type Description <code>protocol</code> <code>str</code> Storage protocol to configure <p>Returns:</p> <ul> <li><code>StorageOptions</code>: Environment-configured options</li> </ul> <p>Example: <pre><code># Load AWS config from environment\noptions = StorageOptions.from_env(\"s3\")\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.core/#to_filesystem","title":"<code>to_filesystem()</code>","text":"<p>Create fsspec filesystem instance.</p> <p>Returns:</p> <ul> <li><code>AbstractFileSystem</code>: Configured filesystem instance</li> </ul> <p>Example: <pre><code>options = StorageOptions.create(protocol=\"file\")\nfs = options.to_filesystem()\nfiles = fs.ls(\"/data\")\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.core/#to_dict","title":"<code>to_dict()</code>","text":"<p>Convert storage options to dictionary.</p> <p>Parameters:</p> Name Type Description <code>with_protocol</code> <code>bool</code> Whether to include protocol in output <p>Returns:</p> <ul> <li><code>dict</code>: Storage options as dictionary</li> </ul> <p>Example: <pre><code>options = StorageOptions.create(\n    protocol=\"s3\",\n    region=\"us-east-1\"\n)\nprint(options.to_dict())\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.core/#to_object_store_kwargs","title":"<code>to_object_store_kwargs()</code>","text":"<p>Get options formatted for object store clients.</p> <p>Parameters:</p> Name Type Description <code>with_conditional_put</code> <code>bool</code> Add etag-based conditional put support <p>Returns:</p> <ul> <li><code>dict</code>: Object store configuration dictionary</li> </ul> <p>Example: <pre><code>options = StorageOptions.create(protocol=\"s3\")\nkwargs = options.to_object_store_kwargs()\n# store = ObjectStore(**kwargs)\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.core/#basestorageoptions","title":"<code>BaseStorageOptions</code>","text":"<p>Base class for filesystem storage configuration options.</p> <p>Provides common functionality for all storage option classes including: - YAML serialization/deserialization - Dictionary conversion - Filesystem instance creation - Configuration updates</p> <p>Attributes:</p> <ul> <li><code>protocol</code> (<code>str</code>): Storage protocol identifier (e.g., \"s3\", \"gs\", \"file\")</li> </ul> <p>Example: <pre><code># Create and save options\noptions = BaseStorageOptions(protocol=\"s3\")\noptions.to_yaml(\"config.yml\")\n\n# Load from YAML\nloaded = BaseStorageOptions.from_yaml(\"config.yml\")\nprint(loaded.protocol)\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.core/#to_dict_1","title":"<code>to_dict()</code>","text":"<p>Convert storage options to dictionary.</p> <p>Parameters:</p> Name Type Description <code>with_protocol</code> <code>bool</code> Whether to include protocol in output dictionary <p>Returns:</p> <ul> <li><code>dict</code>: Dictionary of storage options with non-None values</li> </ul> <p>Example: <pre><code>options = BaseStorageOptions(protocol=\"s3\")\nprint(options.to_dict())\nprint(options.to_dict(with_protocol=True))\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.core/#from_yaml_1","title":"<code>from_yaml()</code>","text":"<p>Load storage options from YAML file.</p> <p>Parameters:</p> Name Type Description <code>path</code> <code>str</code> Path to YAML configuration file <code>fs</code> <code>fsspec.AbstractFileSystem</code>, optional Filesystem to use for reading file <p>Returns:</p> <ul> <li><code>BaseStorageOptions</code>: Loaded storage options instance</li> </ul> <p>Example: <pre><code># Load from local file\noptions = BaseStorageOptions.from_yaml(\"config.yml\")\nprint(options.protocol)\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.core/#to_yaml","title":"<code>to_yaml()</code>","text":"<p>Save storage options to YAML file.</p> <p>Parameters:</p> Name Type Description <code>path</code> <code>str</code> Path where to save configuration <code>fs</code> <code>fsspec.AbstractFileSystem</code>, optional Filesystem to use for writing <p>Example: <pre><code>options = BaseStorageOptions(protocol=\"s3\")\noptions.to_yaml(\"config.yml\")\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.core/#to_filesystem_1","title":"<code>to_filesystem()</code>","text":"<p>Create fsspec filesystem instance from options.</p> <p>Returns:</p> <ul> <li><code>AbstractFileSystem</code>: Configured filesystem instance</li> </ul> <p>Example: <pre><code>options = BaseStorageOptions(protocol=\"file\")\nfs = options.to_filesystem()\nfiles = fs.ls(\"/path/to/data\")\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.core/#update","title":"<code>update()</code>","text":"<p>Update storage options with new values.</p> <p>Parameters:</p> Name Type Description <code>**kwargs</code> <code>dict</code> New option values to set <p>Returns:</p> <ul> <li><code>BaseStorageOptions</code>: Updated instance</li> </ul> <p>Example: <pre><code>options = BaseStorageOptions(protocol=\"s3\")\noptions = options.update(region=\"us-east-1\")\nprint(options.region)\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.git/","title":"<code>fsspeckit.storage_options.git</code> API Reference","text":""},{"location":"api/fsspeckit.storage_options.git/#githubstorageoptions","title":"<code>GitHubStorageOptions</code>","text":"<p>GitHub repository storage configuration options.</p> <p>Provides access to files in GitHub repositories with support for: - Public and private repositories - Branch/tag/commit selection - Token-based authentication - Custom GitHub Enterprise instances</p> <p>Attributes:</p> <ul> <li><code>protocol</code> (<code>str</code>): Always \"github\" for GitHub storage</li> <li><code>org</code> (<code>str</code>): Organization or user name</li> <li><code>repo</code> (<code>str</code>): Repository name</li> <li><code>ref</code> (<code>str</code>): Git reference (branch, tag, or commit SHA</li> <li><code>token</code> (<code>str</code>): GitHub personal access token</li> <li><code>api_url</code> (<code>str</code>): Custom GitHub API URL for enterprise instances</li> </ul> <p>Example: <pre><code># Public repository\noptions = GitHubStorageOptions(\n    org=\"microsoft\",\n    repo=\"vscode\",\n    ref=\"main\"\n)\n\n# Private repository\noptions = GitHubStorageOptions(\n    org=\"myorg\",\n    repo=\"private-repo\",\n    token=\"ghp_xxxx\",\n    ref=\"develop\"\n)\n\n# Enterprise instance\noptions = GitHubStorageOptions(\n    org=\"company\",\n    repo=\"internal\",\n    api_url=\"https://github.company.com/api/v3\",\n    token=\"ghp_xxxx\"\n)\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.git/#from_env","title":"<code>from_env()</code>","text":"<p>Create storage options from environment variables.</p> <p>Reads standard GitHub environment variables: - GITHUB_ORG: Organization or user name - GITHUB_REPO: Repository name - GITHUB_REF: Git reference - GITHUB_TOKEN: Personal access token - GITHUB_API_URL: Custom API URL</p> <p>Returns:</p> <ul> <li><code>GitHubStorageOptions</code>: Configured storage options</li> </ul> <p>Example: <pre><code># With environment variables set:\noptions = GitHubStorageOptions.from_env()\nprint(options.org)  # From GITHUB_ORG 'microsoft'\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.git/#to_env","title":"<code>to_env()</code>","text":"<p>Export options to environment variables.</p> <p>Sets standard GitHub environment variables.</p> <p>Example: <pre><code>options = GitHubStorageOptions(\n    org=\"microsoft\",\n    repo=\"vscode\",\n    token=\"ghp_xxxx\"\n)\noptions.to_env()\nprint(os.getenv(\"GITHUB_ORG\"))  # 'microsoft'\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.git/#to_fsspec_kwargs","title":"<code>to_fsspec_kwargs()</code>","text":"<p>Convert options to fsspec filesystem arguments.</p> <p>Returns:</p> <ul> <li><code>dict</code>: Arguments suitable for GitHubFileSystem</li> </ul> <p>Example: <pre><code>options = GitHubStorageOptions(\n    org=\"microsoft\",\n    repo=\"vscode\",\n    token=\"ghp_xxxx\"\n)\nkwargs = options.to_fsspec_kwargs()\nfs = filesystem(\"github\", **kwargs)\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.git/#gitlabstorageoptions","title":"<code>GitLabStorageOptions</code>","text":"<p>GitLab repository storage configuration options.</p> <p>Provides access to files in GitLab repositories with support for: - Public and private repositories - Self-hosted GitLab instances - Project ID or name-based access - Branch/tag/commit selection - Token-based authentication</p> <p>Attributes:</p> <ul> <li><code>protocol</code> (<code>str</code>): Always \"gitlab\" for GitLab storage</li> <li><code>base_url</code> (<code>str</code>): GitLab instance URL, defaults to gitlab.com</li> <li><code>project_id</code> (<code>str</code> | <code>int</code>): Project ID number</li> <li><code>project_name</code> (<code>str</code>): Project name/path</li> <li><code>ref</code> (<code>str</code>): Git reference (branch, tag, or commit SHA)</li> <li><code>token</code> (<code>str</code>): GitLab personal access token</li> <li><code>api_version</code> (<code>str</code>): API version to use</li> </ul> <p>Example: <pre><code># Public project on gitlab.com\noptions = GitLabStorageOptions(\n    project_name=\"group/project\",\n    ref=\"main\"\n)\n\n# Private project with token\noptions = GitLabStorageOptions(\n    project_id=12345,\n    token=\"glpat_xxxx\",\n    ref=\"develop\"\n)\n\n# Self-hosted instance\noptions = GitLabStorageOptions(\n    base_url=\"https://gitlab.company.com\",\n    project_name=\"internal/project\",\n    token=\"glpat_xxxx\"\n)\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.git/#from_env_1","title":"<code>from_env()</code>","text":"<p>Create storage options from environment variables.</p> <p>Reads standard GitLab environment variables: - GITLAB_URL: Instance URL - GITLAB_PROJECT_ID: Project ID - GITLAB_PROJECT_NAME: Project name/path - GITLAB_REF: Git reference - GITLAB_TOKEN: Personal access token - GITLAB_API_VERSION: API version</p> <p>Returns:</p> <ul> <li><code>GitLabStorageOptions</code>: Configured storage options</li> </ul> <p>Example: <pre><code># With environment variables set:\noptions = GitLabStorageOptions.from_env()\nprint(options.project_id)  # From GITLAB_PROJECT_ID '12345'\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.git/#to_env_1","title":"<code>to_env()</code>","text":"<p>Export options to environment variables.</p> <p>Sets standard GitLab environment variables.</p> <p>Example: <pre><code>options = GitLabStorageOptions(\n    project_id=12345,\n    token=\"glpat_xxxx\"\n)\noptions.to_env()\nprint(os.getenv(\"GITLAB_PROJECT_ID\"))  # '12345'\n</code></pre></p>"},{"location":"api/fsspeckit.storage_options.git/#to_fsspec_kwargs_1","title":"<code>to_fsspec_kwargs()</code>","text":"<p>Convert options to fsspec filesystem arguments.</p> <p>Returns:</p> <ul> <li><code>dict</code>: Arguments suitable for GitLabFileSystem</li> </ul> <p>Example: <pre><code>options = GitLabStorageOptions(\n    project_id=12345,\n    token=\"glpat_xxxx\"\n)\nkwargs = options.to_fsspec_kwargs()\nfs = filesystem(\"gitlab\", **kwargs)\n</code></pre></p>"},{"location":"api/fsspeckit.utils.datetime/","title":"<code>fsspeckit.utils.datetime</code> API Reference","text":""},{"location":"api/fsspeckit.utils.datetime/#get_timestamp_column","title":"<code>get_timestamp_column()</code>","text":"<p>Parameters:</p> Name Type Description <code>df</code> <code>polars.DataFrame</code> Input DataFrame. <p>Example:</p> <pre><code>import polars as pl\nfrom fsspeckit.utils.datetime import get_timestamp_column\n\ndf = pl.DataFrame({\n    \"timestamp_col\": [1678886400, 1678972800],\n    \"value\": [10, 20]\n})\ncol_name = get_timestamp_column(df)\nprint(col_name)\n# \"timestamp_col\"\n</code></pre> <p>Returns:</p> <ul> <li><code>None</code></li> </ul>"},{"location":"api/fsspeckit.utils.datetime/#get_timedelta_str","title":"<code>get_timedelta_str()</code>","text":"<p>Parameters:</p> Name Type Description <code>timedelta_string</code> <code>str</code> Timedelta string (e.g., \"1h\", \"2d\", \"3w\"). <p>Example:</p> <pre><code>from fsspeckit.utils.datetime import get_timedelta_str\n\n# Convert to Polars duration string\npolars_duration = get_timedelta_str(\"1h\")\nprint(polars_duration)\n# \"1h\"\n\n# Convert to Pandas timedelta string\npandas_timedelta = get_timedelta_str(\"2d\", to=\"pandas\")\nprint(pandas_timedelta)\n# \"2 days\"\n</code></pre> <p>| <code>to</code> | <code>str</code> | Defaults to 'polars' |</p> <p>Returns:</p> <ul> <li><code>None</code></li> </ul>"},{"location":"api/fsspeckit.utils.datetime/#timestamp_from_string","title":"<code>timestamp_from_string()</code>","text":"<p>Converts a timestamp string (ISO 8601 format) into a datetime, date, or time object</p> <p>using only standard Python libraries. Handles strings with or without timezone information (e.g., '2023-01-01T10:00:00+02:00', '2023-01-01', '10:00:00'). Supports timezone offsets like '+HH:MM' or '+HHMM'. For named timezones (e.g., 'Europe/Paris'), requires Python 3.9+ and the 'tzdata' package to be installed.</p> <p>Parameters:</p> Name Type Description <code>timestamp_str</code> <code>str</code> The string representation of the timestamp (ISO 8601 format). <code>tz</code> <code>str</code>, optional Target timezone identifier (e.g., 'UTC', '+02:00', 'Europe/Paris'). If provided, the output datetime/time will be localized or converted to this timezone. Defaults to None. <code>naive</code> <code>bool</code>, optional If True, return a naive datetime/time (no timezone info), even if the input string or <code>tz</code> parameter specifies one. Defaults to False. <p>Returns:</p> <ul> <li><code>Union[dt.datetime, dt.date, dt.time]</code>: The parsed datetime, date, or time object.</li> </ul> <p>Example:</p> <pre><code>from fsspeckit.utils.datetime import timestamp_from_string\n\n# Parse a timestamp string with timezone\ndt_obj = timestamp_from_string(\"2023-01-01T10:00:00+02:00\")\nprint(dt_obj)\n# 2023-01-01 10:00:00+02:00\n\n# Parse a date string\ndate_obj = timestamp_from_string(\"2023-01-01\")\nprint(date_obj)\n# 2023-01-01\n\n# Parse a time string and localize to UTC\ntime_obj = timestamp_from_string(\"15:30:00\", tz=\"UTC\")\nprint(time_obj)\n# 15:30:00+00:00\n\n# Parse a timestamp and return as naive datetime\nnaive_dt_obj = timestamp_from_string(\"2023-01-01T10:00:00+02:00\", naive=True)\nprint(naive_dt_obj)\n# 2023-01-01 10:00:00\n</code></pre> <p>Raises:</p> <ul> <li><code>ValueError</code>: If the timestamp string format is invalid or the timezone is invalid/unsupported.</li> </ul>"},{"location":"api/fsspeckit.utils.logging/","title":"<code>fsspeckit.utils.logging</code> API Reference","text":""},{"location":"api/fsspeckit.utils.logging/#setup_logging","title":"<code>setup_logging()</code>","text":"<p>Configure the Loguru logger for fsspec-utils.</p> <p>Removes the default handler and adds a new one targeting stderr with customizable level and format.</p> <p>Parameters:</p> Name Type Description <code>level</code> <code>str</code>, optional Log level (DEBUG, INFO, WARNING, ERROR, CRITICAL). If None, uses fsspeckit_LOG_LEVEL environment variable or defaults to \"INFO\". <code>disable</code> <code>bool</code> Whether to disable logging for fsspec-utils package. <code>format_string</code> <code>str</code>, optional Custom format string for log messages. If None, uses a default comprehensive format. <p>Returns:</p> <ul> <li><code>None</code></li> </ul> <p>Example: <pre><code># Basic setup\nsetup_logging()\n\n# Custom level and format\nsetup_logging(level=\"DEBUG\", format_string=\"{time} | {level} | {message}\")\n\n# Disable logging\nsetup_logging(disable=True)\n</code></pre></p>"},{"location":"api/fsspeckit.utils.logging/#get_logger","title":"<code>get_logger()</code>","text":"<p>Get a logger instance for the given name.</p> <p>Parameters:</p> Name Type Description <code>name</code> <code>str</code> Logger name, typically the module name. <p>Returns:</p> <ul> <li><code>Logger</code>: Configured logger instance.</li> </ul> <p>Example: <pre><code>logger = get_logger(__name__)\nlogger.info(\"This is a log message\")\n</code></pre></p>"},{"location":"api/fsspeckit.utils.misc/","title":"<code>fsspeckit.utils.misc</code> API Reference","text":""},{"location":"api/fsspeckit.utils.misc/#run_parallel","title":"<code>run_parallel()</code>","text":"<p>Run a function for a list of parameters in parallel.</p> <p>Provides parallel execution with progress tracking and flexible argument handling.</p> <p>Parameters:</p> Name Type Description <code>func</code> <code>Callable</code> The function to be executed in parallel. <code>*args</code> <code>Any</code> Positional arguments to pass to <code>func</code>. If an iterable, <code>func</code> will be called for each item. <code>n_jobs</code> <code>int</code> The number of CPU cores to use. -1 means all available cores. <code>backend</code> <code>str</code> The backend to use for parallel processing. Options include 'loky', 'threading', 'multiprocessing', and 'sequential'. <code>verbose</code> <code>bool</code> If True, a progress bar will be displayed during execution. <code>**kwargs</code> <code>Any</code> Keyword arguments to pass to <code>func</code>. If an iterable, <code>func</code> will be called for each item. <p>Returns:</p> <ul> <li><code>list</code>: List of function outputs in the same order as inputs.</li> </ul> <p>Raises:</p> <ul> <li><code>ValueError</code>: If no iterable arguments provided or length mismatch.</li> </ul> <p>Examples: <pre><code># Single iterable argument\nrun_parallel(str.upper, [\"hello\", \"world\"])\n\n# Multiple iterables in args and kwargs\ndef add(x, y, offset=0):\n    return x + y + offset\nrun_parallel(add, [1, 2, 3], y=[4, 5, 6], offset=10)\n\n# Fixed and iterable arguments\nrun_parallel(pow, [2, 3, 4], exp=2)\n</code></pre></p>"},{"location":"api/fsspeckit.utils.misc/#get_partitions_from_path","title":"<code>get_partitions_from_path()</code>","text":"<p>Extract dataset partitions from a file path.</p> <p>Parses file paths to extract partition information based on different partitioning schemes.</p> <p>Parameters:</p> Name Type Description <code>path</code> <code>str</code> The file path from which to extract partition information. <code>partitioning</code> <code>str</code> or <code>list[str]</code> or <code>None</code> The partitioning scheme to use. Can be \"hive\" for Hive-style, a string for a single partition column, a list of strings for multiple partition columns, or None for no specific partitioning. <p>Returns:</p> <ul> <li><code>list[tuple[str, str]]</code>: List of tuples containing (column, value) pairs.</li> </ul> <p>Examples: <pre><code># Hive-style partitioning\nget_partitions_from_path(\"data/year=2023/month=01/file.parquet\", \"hive\")\n\n# Single partition column\nget_partitions_from_path(\"data/2023/01/file.parquet\", \"year\")\n\n# Multiple partition columns\nget_partitions_from_path(\"data/2023/01/file.parquet\", [\"year\", \"month\"])\n</code></pre></p>"},{"location":"api/fsspeckit.utils.misc/#path_to_glob","title":"<code>path_to_glob()</code>","text":"<p>Convert a path to a glob pattern for file matching.</p> <p>Intelligently converts paths to glob patterns that match files of the specified format, handling various directory and wildcard patterns.</p> <p>Parameters:</p> Name Type Description <code>path</code> <code>str</code> The file or directory path to convert into a glob pattern. <code>format</code> <code>str</code> or <code>None</code> The desired file format or extension to match (e.g., \"parquet\", \"csv\", \"json\"). If None, the format is inferred from the path. <p>Returns:</p> <ul> <li><code>str</code>: Glob pattern for matching files</li> </ul> <p>Example: <pre><code># Directory to parquet files glob\npath_to_glob(\"data/\", \"parquet\")\n\n# Already a glob pattern\npath_to_glob(\"data/*.csv\")\n\n# Specific file\npath_to_glob(\"data/file.json\")\n</code></pre></p>"},{"location":"api/fsspeckit.utils.misc/#check_optional_dependency","title":"<code>check_optional_dependency()</code>","text":"<p>Check if an optional dependency is available.</p> <p>Parameters:</p> Name Type Description <code>package_name</code> <code>str</code> The name of the optional package to check for availability. <code>feature_name</code> <code>str</code> A descriptive name of the feature that requires this package. <p>Raises:</p> <ul> <li><code>ImportError</code>: If the package is not available</li> </ul>"},{"location":"api/fsspeckit.utils.polars/","title":"<code>fsspeckit.utils.polars</code> API Reference","text":""},{"location":"api/fsspeckit.utils.polars/#opt_dtype","title":"<code>opt_dtype()</code>","text":"<p>Optimize data types of a Polars DataFrame for performance and memory efficiency.</p> <p>This function analyzes each column and converts it to the most appropriate data type based on content, handling string-to-type conversions and numeric type downcasting.</p> <p>Parameters:</p> Name Type Description <code>df</code> <code>polars.DataFrame</code> The input Polars DataFrame to optimize. <code>include</code> <code>list[str]</code> or <code>None</code> Optional list of column names to include in the optimization process. If None, all columns are considered. <code>exclude</code> <code>list[str]</code> or <code>None</code> Optional list of column names to exclude from the optimization process. <code>time_zone</code> <code>str</code> or <code>None</code> Optional time zone string for datetime parsing. <code>shrink_numerics</code> <code>bool</code> If True, numeric columns will be downcasted to smaller data types if possible without losing precision. <code>allow_unsigned</code> <code>bool</code> If True, unsigned integer types will be considered for numeric column optimization. <code>allow_null</code> <code>bool</code> If True, columns containing only null values will be cast to the Null type. <code>sample_size</code> <code>int</code> or <code>None</code> Maximum number of cleaned values inspected during regex-based inference (<code>1024</code> by default). The inferred schema is based solely on this sample before casting the full column. <code>sample_method</code> <code>str</code> Which subset to inspect (<code>\"first\"</code> or <code>\"random\"</code>) when sampling values for inference. <code>strict</code> <code>bool</code> If True, an error will be raised if any column cannot be optimized (e.g., due to type inference issues). <p>Example:</p> <pre><code>import polars as pl\nfrom fsspeckit.utils.polars import opt_dtype\n\ndf = pl.DataFrame({\n    \"col_int\": [\"1\", \"2\", \"3\"],\n    \"col_float\": [\"1.1\", \"2.2\", \"3.3\"],\n    \"col_bool\": [\"True\", \"False\", \"True\"],\n    \"col_date\": [\"2023-01-01\", \"2023-01-02\", \"2023-01-03\"],\n    \"col_str\": [\"a\", \"b\", \"c\"],\n    \"col_null\": [None, None, None]\n})\noptimized_df = opt_dtype(df, shrink_numerics=True)\nprint(optimized_df.schema)\n# Expected output similar to:\n# Schema({\n#     'col_int': Int8,\n#     'col_float': Float32,\n#     'col_bool': Boolean,\n#     'col_date': Date,\n#     'col_str': Utf8,\n#     'col_null': Null\n# })\n</code></pre> <p>Returns:</p> <ul> <li><code>polars.DataFrame</code>: DataFrame with optimized data types</li> </ul>"},{"location":"api/fsspeckit.utils.polars/#unnest_all","title":"<code>unnest_all()</code>","text":"<p>Parameters:</p> Name Type Description <code>df</code> <code>polars.DataFrame</code> The input Polars DataFrame. <code>seperator</code> <code>str</code> The separator used to flatten nested column names. Defaults to '_'. <code>fields</code> <code>list[str]</code> or <code>None</code> Optional list of specific fields (structs) to unnest. If None, all struct columns will be unnested. <p>Example:</p> <pre><code>import polars as pl\nfrom fsspeckit.utils.polars import explode_all\n\ndf = pl.DataFrame({\n    \"id\": [1, 2],\n    \"values\": [[10, 20], [30]]\n})\nexploded_df = explode_all(df)\nprint(exploded_df)\n# shape: (3, 2)\n# \u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n# \u2502 id  \u2506 values \u2502\n# \u2502 --- \u2506 ---    \u2502\n# \u2502 i64 \u2506 i64    \u2502\n# \u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n# \u2502 1   \u2506 10     \u2502\n# \u2502 1   \u2506 20     \u2502\n# \u2502 2   \u2506 30     \u2502\n# \u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <pre><code>import polars as pl\nfrom fsspeckit.utils.polars import unnest_all\n\ndf = pl.DataFrame({\n    \"id\": [1, 2],\n    \"data\": [\n        {\"a\": 1, \"b\": {\"c\": 3}},\n        {\"a\": 4, \"b\": {\"c\": 6}}\n    ]\n})\nunnested_df = unnest_all(df, seperator='__')\nprint(unnested_df)\n# shape: (2, 3)\n# \u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n# \u2502 id  \u2506 data__a \u2506 data__b__c \u2502\n# \u2502 --- \u2506 ---  \u2506 ---     \u2502\n# \u2502 i64 \u2506 i64  \u2506 i64     \u2502\n# \u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n# \u2502 1   \u2506 1    \u2506 3       \u2502\n# \u2502 2   \u2506 4    \u2506 6       \u2502\n# \u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Returns:</p> <ul> <li><code>None</code></li> </ul>"},{"location":"api/fsspeckit.utils.polars/#explode_all","title":"<code>explode_all()</code>","text":"<p>Parameters:</p> Name Type Description <code>df</code> <code>polars.DataFrame</code> The input Polars DataFrame. <p>Example:</p> <pre><code>import polars as pl\nfrom fsspeckit.utils.polars import drop_null_columns\n\ndf = pl.DataFrame({\n    \"col1\": [1, 2, 3],\n    \"col2\": [None, None, None],\n    \"col3\": [\"a\", None, \"c\"]\n})\ndf_cleaned = drop_null_columns(df)\nprint(df_cleaned)\n# shape: (3, 2)\n# \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n# \u2502 col1 \u2506 col3  \u2502\n# \u2502 ---  \u2506 ---   \u2502\n# \u2502 i64  \u2506 str   \u2502\n# \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n# \u2502 1    \u2506 a     \u2502\n# \u2502 2    \u2506 null  \u2502\n# \u2502 3    \u2506 c     \u2502\n# \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Returns:</p> <ul> <li><code>None</code></li> </ul>"},{"location":"api/fsspeckit.utils.polars/#with_strftime_columns","title":"<code>with_strftime_columns()</code>","text":"<p>Parameters:</p> Name Type Description <code>df</code> <code>polars.DataFrame</code> The input Polars DataFrame. <code>strftime</code> <code>str</code> The <code>strftime</code> format string (e.g., \"%Y-%m-%d\" for date, \"%H\" for hour). <code>timestamp_column</code> <code>str</code> The name of the timestamp column to use. Defaults to 'auto' (attempts to infer). <code>column_names</code> <code>list[str]</code> or <code>None</code> Optional list of new column names to use for the generated columns. If None, names are derived from the <code>strftime</code> format. <p>Returns:</p> <ul> <li><code>None</code></li> </ul>"},{"location":"api/fsspeckit.utils.polars/#with_truncated_columns","title":"<code>with_truncated_columns()</code>","text":"<p>Parameters:</p> Name Type Description <code>df</code> <code>polars.DataFrame</code> The input Polars DataFrame. <code>truncate_by</code> <code>str</code> The duration string to truncate by (e.g., \"1h\", \"1d\", \"1mo\"). <code>timestamp_column</code> <code>str</code> The name of the timestamp column to truncate. Defaults to 'auto' (attempts to infer). <code>column_names</code> <code>list[str]</code> or <code>None</code> Optional list of new column names for the truncated columns. If None, names are derived automatically. <p>Returns:</p> <ul> <li><code>None</code></li> </ul>"},{"location":"api/fsspeckit.utils.polars/#with_datepart_columns","title":"<code>with_datepart_columns()</code>","text":"<p>Parameters:</p> Name Type Description <code>df</code> <code>polars.DataFrame</code> The input Polars DataFrame. <code>timestamp_column</code> <code>str</code> The name of the timestamp column to extract date parts from. Defaults to 'auto' (attempts to infer). <code>year</code> <code>bool</code> If True, extract the year as a new column. <code>month</code> <code>bool</code> If True, extract the month as a new column. <code>week</code> <code>bool</code> If True, extract the week of the year as a new column. <code>yearday</code> <code>bool</code> If True, extract the day of the year as a new column. <code>monthday</code> <code>bool</code> If True, extract the day of the month as a new column. <code>day</code> <code>bool</code> If True, extract the day of the week (1-7, Monday=1) as a new column. <code>weekday</code> <code>bool</code> If True, extract the weekday (0-6, Monday=0) as a new column. <code>hour</code> <code>bool</code> If True, extract the hour as a new column. <code>minute</code> <code>bool</code> If True, extract the minute as a new column. <code>strftime</code> <code>str</code> or <code>None</code> Optional <code>strftime</code> format string to apply to the timestamp column before extracting parts. <p>Returns:</p> <ul> <li><code>None</code></li> </ul>"},{"location":"api/fsspeckit.utils.polars/#with_row_count","title":"<code>with_row_count()</code>","text":"<p>Parameters:</p> Name Type Description <code>df</code> <code>polars.DataFrame</code> The input Polars DataFrame. <code>over</code> <code>list[str]</code> or <code>None</code> Optional list of column names to partition the data by before adding row counts. If None, a global row count is added. <p>Returns:</p> <ul> <li><code>None</code></li> </ul>"},{"location":"api/fsspeckit.utils.polars/#drop_null_columns","title":"<code>drop_null_columns()</code>","text":"<p>Remove columns with all null values from the DataFrame.</p> <p>Parameters:</p> Name Type Description <code>df</code> <code>polars.DataFrame</code> The input Polars DataFrame. <p>Returns:</p> <ul> <li><code>None</code></li> </ul>"},{"location":"api/fsspeckit.utils.polars/#unify_schemas","title":"<code>unify_schemas()</code>","text":"<p>Parameters:</p> Name Type Description <code>dfs</code> <code>list[polars.DataFrame]</code> A list of Polars DataFrames to unify their schemas. <p>Returns:</p> <ul> <li><code>None</code></li> </ul>"},{"location":"api/fsspeckit.utils.polars/#cast_relaxed","title":"<code>cast_relaxed()</code>","text":"<p>Parameters:</p> Name Type Description <code>df</code> <code>polars.DataFrame</code> The input Polars DataFrame to cast. <code>schema</code> <code>dict</code> or <code>polars.Schema</code> The target schema to cast the DataFrame to. Can be a dictionary mapping column names to data types or a Polars Schema object. <p>Returns:</p> <ul> <li><code>None</code></li> </ul>"},{"location":"api/fsspeckit.utils.polars/#delta","title":"<code>delta()</code>","text":"<p>Parameters:</p> Name Type Description <code>df1</code> <code>polars.DataFrame</code> The first Polars DataFrame. <code>df2</code> <code>polars.DataFrame</code> The second Polars DataFrame. <code>subset</code> <code>list[str]</code> or <code>None</code> Optional list of column names to consider when calculating the delta. If None, all columns are used. <code>eager</code> <code>bool</code> If True, the delta calculation is performed eagerly. Defaults to False (lazy). <p>Returns:</p> <ul> <li><code>None</code></li> </ul>"},{"location":"api/fsspeckit.utils.polars/#partition_by","title":"<code>partition_by()</code>","text":"<p>Parameters:</p> Name Type Description <code>df</code> <code>polars.DataFrame</code> The input Polars DataFrame to partition. <code>timestamp_column</code> <code>str</code> or <code>None</code> The name of the timestamp column to use for time-based partitioning. Defaults to None. <code>columns</code> <code>list[str]</code> or <code>None</code> Optional list of column names to partition by. Defaults to None. <code>strftime</code> <code>str</code> or <code>None</code> Optional <code>strftime</code> format string for time-based partitioning. Defaults to None. <code>timedelta</code> <code>str</code> or <code>None</code> Optional timedelta string (e.g., \"1h\", \"1d\") for time-based partitioning. Defaults to None. <code>num_rows</code> <code>int</code> or <code>None</code> Optional number of rows per partition for row-based partitioning. Defaults to None. <p>Returns:</p> <ul> <li><code>None</code></li> </ul>"},{"location":"api/fsspeckit.utils.pyarrow/","title":"<code>fsspeckit.utils.pyarrow</code> API Reference","text":""},{"location":"api/fsspeckit.utils.pyarrow/#dominant_timezone_per_column","title":"<code>dominant_timezone_per_column()</code>","text":"<p>For each timestamp column (by name) across all schemas, detect the most frequent timezone (including None).</p> <p>If None and a timezone are tied, prefer the timezone. Returns a dict: {column_name: dominant_timezone}</p> <p>Parameters:</p> Name Type Description <code>schemas</code> <code>list[pyarrow.Schema]</code> A list of PyArrow schemas to analyze. <p>Example:</p> <pre><code>import pyarrow as pa\nfrom fsspeckit.utils.pyarrow import dominant_timezone_per_column\n\nschema1 = pa.schema([(\"ts\", pa.timestamp(\"ns\", tz=\"UTC\"))])\nschema2 = pa.schema([(\"ts\", pa.timestamp(\"ns\", tz=\"Europe/Berlin\"))])\nschema3 = pa.schema([(\"ts\", pa.timestamp(\"ns\"))]) # naive\nschemas = [schema1, schema2, schema3]\n\ndominant_tz = dominant_timezone_per_column(schemas)\nprint(dominant_tz)\n# Expected: {'ts': 'UTC'} (or 'Europe/Berlin' depending on logic)\n</code></pre> <p>Returns:</p> <ul> <li><code>dict</code>: {column_name: dominant_timezone}</li> </ul>"},{"location":"api/fsspeckit.utils.pyarrow/#standardize_schema_timezones_by_majority","title":"<code>standardize_schema_timezones_by_majority()</code>","text":"<p>For each timestamp column (by name) across all schemas, set the timezone to the most frequent (with tie-breaking).</p> <p>Parameters:</p> Name Type Description <code>schemas</code> <code>list[pyarrow.Schema]</code> A list of PyArrow schemas to standardize. <p>Example:</p> <pre><code>import pyarrow as pa\nfrom fsspeckit.utils.pyarrow import standardize_schema_timezones_by_majority\n\nschema1 = pa.schema([(\"ts\", pa.timestamp(\"ns\", tz=\"UTC\"))])\nschema2 = pa.schema([(\"ts\", pa.timestamp(\"ns\", tz=\"Europe/Berlin\"))])\nschemas = [schema1, schema2]\n\nstandardized_schemas = standardize_schema_timezones_by_majority(schemas)\nprint(standardized_schemas[0].field(\"ts\").type)\nprint(standardized_schemas[1].field(\"ts\").type)\n# Expected: timestamp[ns, tz=Europe/Berlin] (or UTC, depending on tie-breaking)\n</code></pre> <p>Returns:</p> <ul> <li><code>list[pyarrow.Schema]</code>: A new list of schemas with updated timestamp timezones.</li> </ul>"},{"location":"api/fsspeckit.utils.pyarrow/#standardize_schema_timezones","title":"<code>standardize_schema_timezones()</code>","text":"<p>Standardize timezone info for all timestamp columns in a list of PyArrow schemas.</p> <p>Parameters:</p> Name Type Description <code>schemas</code> <code>list[pyarrow.Schema]</code> The list of PyArrow schemas to process. <code>timezone</code> <code>str</code> or <code>None</code> The target timezone to apply to timestamp columns. If None, timezones are removed. If \"auto\", the most frequent timezone across schemas is used. <p>Example:</p> <pre><code>import pyarrow as pa\nfrom fsspeckit.utils.pyarrow import standardize_schema_timezones\n\nschema1 = pa.schema([(\"ts\", pa.timestamp(\"ns\", tz=\"UTC\"))])\nschema2 = pa.schema([(\"ts\", pa.timestamp(\"ns\"))]) # naive\nschemas = [schema1, schema2]\n\n# Remove timezones\nnew_schemas_naive = standardize_schema_timezones(schemas, timezone=None)\nprint(new_schemas_naive[0].field(\"ts\").type)\n# Expected: timestamp[ns]\n\n# Set a specific timezone\nnew_schemas_berlin = standardize_schema_timezones(schemas, timezone=\"Europe/Berlin\")\nprint(new_schemas_berlin[0].field(\"ts\").type)\n# Expected: timestamp[ns, tz=Europe/Berlin]\n</code></pre> <p>Returns:</p> <ul> <li><code>list[pyarrow.Schema]</code>: New schemas with standardized timezone info.</li> </ul>"},{"location":"api/fsspeckit.utils.pyarrow/#unify_schemas","title":"<code>unify_schemas()</code>","text":"<p>Unify a list of PyArrow schemas into a single schema.</p> <p>Parameters:</p> Name Type Description <code>schemas</code> <code>list[pyarrow.Schema]</code> List of PyArrow schemas to unify. <code>use_large_dtypes</code> <code>bool</code> If True, keep large types like large_string. <code>timezone</code> <code>str</code> or <code>None</code> If specified, standardize all timestamp columns to this timezone. If \"auto\", use the most frequent timezone across schemas. If None, remove timezone from all timestamp columns. <code>standardize_timezones</code> <code>bool</code> If True, standardize all timestamp columns to the most frequent timezone. <p>Returns:</p> <ul> <li><code>pyarrow.Schema</code>: A unified PyArrow schema.</li> </ul>"},{"location":"api/fsspeckit.utils.pyarrow/#cast_schema","title":"<code>cast_schema()</code>","text":"<p>Cast a PyArrow table to a given schema, updating the schema to match the table's columns.</p> <p>Parameters:</p> Name Type Description <code>table</code> <code>pyarrow.Table</code> The PyArrow table to cast. <code>schema</code> <code>pyarrow.Schema</code> The target schema to cast the table to. <p>Returns:</p> <ul> <li><code>pyarrow.Table</code>: A new PyArrow table with the specified schema.</li> </ul>"},{"location":"api/fsspeckit.utils.pyarrow/#convert_large_types_to_normal","title":"<code>convert_large_types_to_normal()</code>","text":"<p>Convert large types in a PyArrow schema to their standard types.</p> <p>Parameters:</p> Name Type Description <code>schema</code> <code>pyarrow.Schema</code> The PyArrow schema to convert. <p>Returns:</p> <ul> <li><code>pyarrow.Schema</code>: A new PyArrow schema with large types converted to standard types.</li> </ul>"},{"location":"api/fsspeckit.utils.pyarrow/#merge_parquet_dataset_pyarrow","title":"<code>merge_parquet_dataset_pyarrow()</code>","text":"<p>Merge a source PyArrow table or parquet dataset into a target parquet dataset directory using PyArrow-only primitives.</p> <p>Parameters:</p> Name Type Description <code>source</code> <code>pyarrow.Table</code> or <code>str</code> In-memory table or path to a parquet dataset containing new data. <code>target_path</code> <code>str</code> Directory containing the parquet dataset to update. <code>key_columns</code> <code>list[str]</code> or <code>str</code> Column(s) that uniquely identify rows. <code>strategy</code> <code>Literal[\"upsert\", \"insert\", \"update\", \"full_merge\", \"deduplicate\"]</code> Merge strategy mirroring the DuckDB helper semantics. <code>dedup_order_by</code> <code>list[str]</code>, optional Columns to sort by (descending) before deduplicating the source when <code>strategy=\"deduplicate\"</code>. <code>compression</code> <code>str</code>, optional Compression codec for rewritten parquet files (<code>\"snappy\"</code> by default). <code>filesystem</code> <code>fsspec.AbstractFileSystem</code>, optional Filesystem implementation for both source (if path) and target datasets. <code>batch_rows</code> <code>int</code>, optional Number of source rows per batch when building filtered target scanners (default: <code>10_000</code>). <p>Example:</p> <pre><code>from fsspeckit.utils.pyarrow import merge_parquet_dataset_pyarrow\nimport pyarrow as pa\n\nsource = pa.table({\n    \"user_id\": [101, 102],\n    \"value\": [\"gold\", \"silver\"],\n})\n\nstats = merge_parquet_dataset_pyarrow(\n    source,\n    target_path=\"/data/customers/\",\n    key_columns=\"user_id\",\n    strategy=\"upsert\",\n)\n\nprint(stats)\n# {'inserted': 1, 'updated': 1, 'deleted': 0, 'total': 42}\n</code></pre> <p>Returns:</p> <ul> <li><code>dict[str, int]</code>: Merge statistics with keys <code>inserted</code>, <code>updated</code>, <code>deleted</code>, and final <code>total</code> rows.</li> </ul>"},{"location":"api/fsspeckit.utils.pyarrow/#opt_dtype","title":"<code>opt_dtype()</code>","text":"<p>Optimize data types of a PyArrow Table for performance and memory efficiency.</p> <p>Parameters:</p> Name Type Description <code>table</code> <code>pyarrow.Table</code> <code>include</code> <code>list[str]</code>, optional <code>exclude</code> <code>list[str]</code>, optional <code>time_zone</code> <code>str</code>, optional <code>shrink_numerics</code> <code>bool</code> <code>allow_unsigned</code> <code>bool</code> <code>use_large_dtypes</code> <code>bool</code> <code>strict</code> <code>bool</code> <code>allow_null</code> <code>bool</code> If False, columns that only hold null-like values will not be converted to pyarrow.null(). <code>sample_size</code> <code>int</code> or <code>None</code> Maximum number of cleaned values inspected during regex-based inference (<code>1024</code> by default). The inferred schema is derived solely from the samples before casting the complete column. <code>sample_method</code> <code>str</code> Sampling strategy (<code>\"first\"</code> or <code>\"random\"</code>) for the inference subset. <p>Returns:</p> <ul> <li><code>pyarrow.Table</code>: A new table casted to the optimal schema.</li> </ul>"},{"location":"api/fsspeckit.utils.sql/","title":"<code>fsspeckit.utils.sql</code> API Reference","text":""},{"location":"api/fsspeckit.utils.sql/#sql2pyarrow_filter","title":"<code>sql2pyarrow_filter()</code>","text":"<p>Generates a filter expression for PyArrow based on a given string and schema.</p> <p>Parameters:</p> Name Type Description <code>string</code> <code>str</code> The string containing the filter expression. <code>schema</code> <code>pyarrow.Schema</code> The PyArrow schema used to validate the filter expression. <p>Returns:</p> <ul> <li><code>pyarrow.compute.Expression</code>: The generated filter expression.</li> </ul> <p>Raises:</p> <ul> <li><code>ValueError</code>: If the input string is invalid or contains unsupported operations.</li> </ul>"},{"location":"api/fsspeckit.utils.sql/#sql2polars_filter","title":"<code>sql2polars_filter()</code>","text":"<p>Generates a filter expression for Polars based on a given string and schema.</p> <p>Parameters:</p> Name Type Description <p>| <code>string</code> | <code>str</code> | The string containing the filter expression. | | <code>schema</code> | <code>polars.Schema</code> | The Polars schema used to validate the filter expression. |</p> <p>Returns:</p> <ul> <li><code>polars.Expr</code>: The generated filter expression.</li> </ul> <p>Raises:</p> <ul> <li><code>ValueError</code>: If the input string is invalid or contains unsupported operations.</li> </ul>"},{"location":"api/fsspeckit.utils.sql/#get_table_names","title":"<code>get_table_names()</code>","text":"<p>Parameters:</p> Name Type Description <code>sql_query</code> <code>str</code> The SQL query string to parse. <p>Example:</p> <pre><code>from fsspeckit.utils.sql import get_table_names\n\nquery = \"SELECT a FROM my_table WHERE b &gt; 10\"\ntables = get_table_names(query)\nprint(tables)\n# Expected: ['my_table']\n\nquery_join = \"SELECT t1.a, t2.b FROM table1 AS t1 JOIN table2 AS t2 ON t1.id = t2.id\"\ntables_join = get_table_names(query_join)\nprint(tables_join)\n# Expected: ['table1', 'table2']\n</code></pre> <p>Returns:</p> <ul> <li><code>None</code></li> </ul>"},{"location":"api/fsspeckit.utils.types/","title":"<code>fsspeckit.utils.types</code> API Reference","text":""},{"location":"api/fsspeckit.utils.types/#dict_to_dataframe","title":"<code>dict_to_dataframe()</code>","text":"<p>Convert a dictionary or list of dictionaries to a Polars DataFrame.</p> <p>Handles various input formats: - Single dict with list values -&gt; DataFrame rows - Single dict with scalar values -&gt; Single row DataFrame - List of dicts with scalar values -&gt; Multi-row DataFrame - List of dicts with list values -&gt; DataFrame with list columns</p> <p>Parameters:</p> Name Type Description <code>data</code> <code>dict</code> or <code>list[dict]</code> The input data, either a dictionary or a list of dictionaries. <code>unique</code> <code>bool</code> If True, duplicate rows will be removed from the resulting DataFrame. <p>Returns:</p> <ul> <li><code>polars.DataFrame</code>: Polars DataFrame containing the converted data.</li> </ul> <p>Examples: <pre><code># Single dict with list values\ndata = {'a': [1, 2, 3], 'b': [4, 5, 6]}\ndict_to_dataframe(data)\n\n# Single dict with scalar values\ndata = {'a': 1, 'b': 2}\ndict_to_dataframe(data)\n\n# List of dicts with scalar values\ndata = [{'a': 1, 'b': 2}, {'a': 3, 'b': 4}]\ndict_to_dataframe(data)\n</code></pre></p>"},{"location":"api/fsspeckit.utils.types/#to_pyarrow_table","title":"<code>to_pyarrow_table()</code>","text":"<p>Convert various data formats to PyArrow Table.</p> <p>Handles conversion from Polars DataFrames, Pandas DataFrames, dictionaries, and lists of these types to PyArrow Tables.</p> <p>Parameters:</p> Name Type Description <code>data</code> <code>Any</code> Input data to convert. <code>concat</code> <code>bool</code> Whether to concatenate multiple inputs into single table. <code>unique</code> <code>bool</code> If True, duplicate rows will be removed from the resulting Table. <p>Example:</p> <pre><code>import polars as pl\nimport pyarrow as pa\nfrom fsspeckit.utils.types import to_pyarrow_table\n\n# Convert Polars DataFrame to PyArrow Table\ndf = pl.DataFrame({\"a\": [1, 2], \"b\": [3, 4]})\ntable = to_pyarrow_table(df)\nprint(table.schema)\n\n# Convert list of dicts to PyArrow Table\ndata = [{\"a\": 1, \"b\": 10}, {\"a\": 2, \"b\": 20}]\ntable_from_dict = to_pyarrow_table(data)\nprint(table_from_dict.to_pydf())\n</code></pre> <p>Returns:</p> <ul> <li><code>pyarrow.Table</code>: PyArrow Table containing the converted data.</li> </ul> <p>Example: <pre><code>df = pl.DataFrame({\"a\": [1, 2], \"b\": [3, 4]})\ntable = to_pyarrow_table(df)\nprint(table.schema)\n</code></pre></p>"}]}